# DP: Changes for the Linaro 5-2015.09 release.

LANG=C git diff 2006973fa839ccbe189a1e7408400dc96ed880b4..ac19ac6481a3f326d9f41403f5dadab548b2c8a6 \
 | egrep -v '^(diff|index) ' \
 | filterdiff --strip=1 --addoldprefix=a/src/  --addnewprefix=b/src/

--- a/src/fixincludes/mkfixinc.sh
+++ b/src/fixincludes/mkfixinc.sh
@@ -19,7 +19,8 @@ case $machine in
     powerpc-*-eabi*    | \
     powerpc-*-rtems*   | \
     powerpcle-*-eabisim* | \
-    powerpcle-*-eabi* )
+    powerpcle-*-eabi* | \
+    *-musl* )
 	#  IF there is no include fixing,
 	#  THEN create a no-op fixer and exit
 	(echo "#! /bin/sh" ; echo "exit 0" ) > ${target}
--- a/src//dev/null
+++ b/src/gcc/LINARO-VERSION
@@ -0,0 +1 @@
+5.1-2015.07~dev
--- a/src/gcc/Makefile.in
+++ b/src/gcc/Makefile.in
@@ -527,10 +527,6 @@ xm_include_list=@xm_include_list@
 xm_defines=@xm_defines@
 lang_checks=
 lang_checks_parallelized=
-# Upper limit to which it is useful to parallelize this lang target.
-# It doesn't make sense to try e.g. 128 goals for small testsuites
-# like objc or go.
-check_gcc_parallelize=10000
 lang_opt_files=@lang_opt_files@ $(srcdir)/c-family/c.opt $(srcdir)/common.opt
 lang_specs_files=@lang_specs_files@
 lang_tree_files=@lang_tree_files@
@@ -814,10 +810,12 @@ BASEVER     := $(srcdir)/BASE-VER  # 4.x.y
 DEVPHASE    := $(srcdir)/DEV-PHASE # experimental, prerelease, ""
 DATESTAMP   := $(srcdir)/DATESTAMP # YYYYMMDD or empty
 REVISION    := $(srcdir)/REVISION  # [BRANCH revision XXXXXX]
+LINAROVER   := $(srcdir)/LINARO-VERSION # M.x-YYYY.MM[-S][~dev]
 
 BASEVER_c   := $(shell cat $(BASEVER))
 DEVPHASE_c  := $(shell cat $(DEVPHASE))
 DATESTAMP_c := $(shell cat $(DATESTAMP))
+LINAROVER_c := $(shell cat $(LINAROVER))
 
 ifeq (,$(wildcard $(REVISION)))
 REVISION_c  :=
@@ -844,6 +842,7 @@ DATESTAMP_s := \
   "\"$(if $(DEVPHASE_c)$(filter-out 0,$(PATCHLEVEL_c)), $(DATESTAMP_c))\""
 PKGVERSION_s:= "\"@PKGVERSION@\""
 BUGURL_s    := "\"@REPORT_BUGS_TO@\""
+LINAROVER_s := "\"$(LINAROVER_c)\""
 
 PKGVERSION  := @PKGVERSION@
 BUGURL_TEXI := @REPORT_BUGS_TEXI@
@@ -2623,8 +2622,9 @@ PREPROCESSOR_DEFINES = \
   -DSTANDARD_EXEC_PREFIX=\"$(libdir)/gcc/\" \
   @TARGET_SYSTEM_ROOT_DEFINE@
 
-CFLAGS-cppbuiltin.o += $(PREPROCESSOR_DEFINES) -DBASEVER=$(BASEVER_s)
-cppbuiltin.o: $(BASEVER)
+CFLAGS-cppbuiltin.o += $(PREPROCESSOR_DEFINES) -DBASEVER=$(BASEVER_s) \
+	-DLINAROVER=$(LINAROVER_s)
+cppbuiltin.o: $(BASEVER) $(LINAROVER)
 
 CFLAGS-cppdefault.o += $(PREPROCESSOR_DEFINES)
 
@@ -3736,7 +3736,9 @@ check_p_subdirs=$(wordlist 1,$(check_p_count),$(wordlist 1, \
 #
 # To parallelize some language check, add the corresponding check-$lang
 # to lang_checks_parallelized variable and define check_$lang_parallelize
-# variable (see above check_gcc_parallelize description).
+# variable.  This is the upper limit to which it is useful to parallelize the
+# check-$lang target.  It doesn't make sense to try e.g. 128 goals for small
+# testsuites like objc or go.
 $(lang_checks_parallelized): check-% : site.exp
 	-rm -rf $(TESTSUITEDIR)/$*-parallel
 	@if [ "$(filter -j, $(MFLAGS))" = "-j" ]; then \
--- a/src/gcc/ada/gcc-interface/Make-lang.in
+++ b/src/gcc/ada/gcc-interface/Make-lang.in
@@ -811,6 +811,7 @@ ada.mostlyclean:
 	-$(RM) ada/*$(coverageexts)
 	-$(RM) ada/sdefault.adb ada/stamp-sdefault ada/stamp-snames
 	-$(RMDIR) ada/tools
+	-$(RM) gnatbind$(exeext) gnat1$(exeext)
 ada.clean:
 ada.distclean:
 	-$(RM) ada/Makefile
--- a/src/gcc/c/Make-lang.in
+++ b/src/gcc/c/Make-lang.in
@@ -95,6 +95,8 @@ c.srcman:
 # List of targets that can use the generic check- rule and its // variant.
 lang_checks += check-gcc
 lang_checks_parallelized += check-gcc
+# For description see the check_$lang_parallelize comment in gcc/Makefile.in.
+check_gcc_parallelize=10000
 
 # 'make check' in gcc/ looks for check-c.  Redirect it to check-gcc.
 check-c : check-gcc
--- a/src/gcc/combine.c
+++ b/src/gcc/combine.c
@@ -1650,6 +1650,73 @@ setup_incoming_promotions (rtx_insn *first)
     }
 }
 
+#ifdef SHORT_IMMEDIATES_SIGN_EXTEND
+/* If MODE has a precision lower than PREC and SRC is a non-negative constant
+   that would appear negative in MODE, sign-extend SRC for use in nonzero_bits
+   because some machines (maybe most) will actually do the sign-extension and
+   this is the conservative approach.
+
+   ??? For 2.5, try to tighten up the MD files in this regard instead of this
+   kludge.  */
+
+static rtx
+sign_extend_short_imm (rtx src, machine_mode mode, unsigned int prec)
+{
+  if (GET_MODE_PRECISION (mode) < prec
+      && CONST_INT_P (src)
+      && INTVAL (src) > 0
+      && val_signbit_known_set_p (mode, INTVAL (src)))
+    src = GEN_INT (INTVAL (src) | ~GET_MODE_MASK (mode));
+
+  return src;
+}
+#endif
+
+/* Update RSP for pseudo-register X from INSN's REG_EQUAL note (if one exists)
+   and SET.  */
+
+static void
+update_rsp_from_reg_equal (reg_stat_type *rsp, rtx_insn *insn, const_rtx set,
+			   rtx x)
+{
+  rtx reg_equal_note = insn ? find_reg_equal_equiv_note (insn) : NULL_RTX;
+  unsigned HOST_WIDE_INT bits = 0;
+  rtx reg_equal = NULL, src = SET_SRC (set);
+  unsigned int num = 0;
+
+  if (reg_equal_note)
+    reg_equal = XEXP (reg_equal_note, 0);
+
+#ifdef SHORT_IMMEDIATES_SIGN_EXTEND
+  src = sign_extend_short_imm (src, GET_MODE (x), BITS_PER_WORD);
+  if (reg_equal)
+    reg_equal = sign_extend_short_imm (reg_equal, GET_MODE (x), BITS_PER_WORD);
+#endif
+
+  /* Don't call nonzero_bits if it cannot change anything.  */
+  if (rsp->nonzero_bits != ~(unsigned HOST_WIDE_INT) 0)
+    {
+      bits = nonzero_bits (src, nonzero_bits_mode);
+      if (reg_equal && bits)
+	bits &= nonzero_bits (reg_equal, nonzero_bits_mode);
+      rsp->nonzero_bits |= bits;
+    }
+
+  /* Don't call num_sign_bit_copies if it cannot change anything.  */
+  if (rsp->sign_bit_copies != 1)
+    {
+      num = num_sign_bit_copies (SET_SRC (set), GET_MODE (x));
+      if (reg_equal && num != GET_MODE_PRECISION (GET_MODE (x)))
+	{
+	  unsigned int numeq = num_sign_bit_copies (reg_equal, GET_MODE (x));
+	  if (num == 0 || numeq > num)
+	    num = numeq;
+	}
+      if (rsp->sign_bit_copies == 0 || num < rsp->sign_bit_copies)
+	rsp->sign_bit_copies = num;
+    }
+}
+
 /* Called via note_stores.  If X is a pseudo that is narrower than
    HOST_BITS_PER_WIDE_INT and is being set, record what bits are known zero.
 
@@ -1665,7 +1732,6 @@ static void
 set_nonzero_bits_and_sign_copies (rtx x, const_rtx set, void *data)
 {
   rtx_insn *insn = (rtx_insn *) data;
-  unsigned int num;
 
   if (REG_P (x)
       && REGNO (x) >= FIRST_PSEUDO_REGISTER
@@ -1725,34 +1791,7 @@ set_nonzero_bits_and_sign_copies (rtx x, const_rtx set, void *data)
       if (SET_DEST (set) == x
 	  || (paradoxical_subreg_p (SET_DEST (set))
 	      && SUBREG_REG (SET_DEST (set)) == x))
-	{
-	  rtx src = SET_SRC (set);
-
-#ifdef SHORT_IMMEDIATES_SIGN_EXTEND
-	  /* If X is narrower than a word and SRC is a non-negative
-	     constant that would appear negative in the mode of X,
-	     sign-extend it for use in reg_stat[].nonzero_bits because some
-	     machines (maybe most) will actually do the sign-extension
-	     and this is the conservative approach.
-
-	     ??? For 2.5, try to tighten up the MD files in this regard
-	     instead of this kludge.  */
-
-	  if (GET_MODE_PRECISION (GET_MODE (x)) < BITS_PER_WORD
-	      && CONST_INT_P (src)
-	      && INTVAL (src) > 0
-	      && val_signbit_known_set_p (GET_MODE (x), INTVAL (src)))
-	    src = GEN_INT (INTVAL (src) | ~GET_MODE_MASK (GET_MODE (x)));
-#endif
-
-	  /* Don't call nonzero_bits if it cannot change anything.  */
-	  if (rsp->nonzero_bits != ~(unsigned HOST_WIDE_INT) 0)
-	    rsp->nonzero_bits |= nonzero_bits (src, nonzero_bits_mode);
-	  num = num_sign_bit_copies (SET_SRC (set), GET_MODE (x));
-	  if (rsp->sign_bit_copies == 0
-	      || rsp->sign_bit_copies > num)
-	    rsp->sign_bit_copies = num;
-	}
+	update_rsp_from_reg_equal (rsp, insn, set, x);
       else
 	{
 	  rsp->nonzero_bits = GET_MODE_MASK (GET_MODE (x));
@@ -1914,6 +1953,15 @@ can_combine_p (rtx_insn *insn, rtx_insn *i3, rtx_insn *pred ATTRIBUTE_UNUSED,
   set = expand_field_assignment (set);
   src = SET_SRC (set), dest = SET_DEST (set);
 
+  /* Do not eliminate user-specified register if it is in an
+     asm input because we may break the register asm usage defined
+     in GCC manual if allow to do so.
+     Be aware that this may cover more cases than we expect but this
+     should be harmless.  */
+  if (REG_P (dest) && REG_USERVAR_P (dest) && HARD_REGISTER_P (dest)
+      && extract_asm_operands (PATTERN (i3)))
+    return 0;
+
   /* Don't eliminate a store in the stack pointer.  */
   if (dest == stack_pointer_rtx
       /* Don't combine with an insn that sets a register to itself if it has
@@ -7723,9 +7771,8 @@ extract_left_shift (rtx x, int count)
    We try, as much as possible, to re-use rtl expressions to save memory.
 
    IN_CODE says what kind of expression we are processing.  Normally, it is
-   SET.  In a memory address (inside a MEM, PLUS or minus, the latter two
-   being kludges), it is MEM.  When processing the arguments of a comparison
-   or a COMPARE against zero, it is COMPARE.  */
+   SET.  In a memory address it is MEM.  When processing the arguments of
+   a comparison or a COMPARE against zero, it is COMPARE.  */
 
 rtx
 make_compound_operation (rtx x, enum rtx_code in_code)
@@ -7745,8 +7792,6 @@ make_compound_operation (rtx x, enum rtx_code in_code)
      but once inside, go back to our default of SET.  */
 
   next_code = (code == MEM ? MEM
-	       : ((code == PLUS || code == MINUS)
-		  && SCALAR_INT_MODE_P (mode)) ? MEM
 	       : ((code == COMPARE || COMPARISON_P (x))
 		  && XEXP (x, 1) == const0_rtx) ? COMPARE
 	       : in_code == COMPARE ? SET : in_code);
@@ -9797,20 +9842,8 @@ reg_nonzero_bits_for_combine (const_rtx x, machine_mode mode,
   if (tem)
     {
 #ifdef SHORT_IMMEDIATES_SIGN_EXTEND
-      /* If X is narrower than MODE and TEM is a non-negative
-	 constant that would appear negative in the mode of X,
-	 sign-extend it for use in reg_nonzero_bits because some
-	 machines (maybe most) will actually do the sign-extension
-	 and this is the conservative approach.
-
-	 ??? For 2.5, try to tighten up the MD files in this regard
-	 instead of this kludge.  */
-
-      if (GET_MODE_PRECISION (GET_MODE (x)) < GET_MODE_PRECISION (mode)
-	  && CONST_INT_P (tem)
-	  && INTVAL (tem) > 0
-	  && val_signbit_known_set_p (GET_MODE (x), INTVAL (tem)))
-	tem = GEN_INT (INTVAL (tem) | ~GET_MODE_MASK (GET_MODE (x)));
+      tem = sign_extend_short_imm (tem, GET_MODE (x),
+				   GET_MODE_PRECISION (mode));
 #endif
       return tem;
     }
--- a/src/gcc/config.gcc
+++ b/src/gcc/config.gcc
@@ -575,7 +575,7 @@ case ${target} in
 esac
 
 # Common C libraries.
-tm_defines="$tm_defines LIBC_GLIBC=1 LIBC_UCLIBC=2 LIBC_BIONIC=3"
+tm_defines="$tm_defines LIBC_GLIBC=1 LIBC_UCLIBC=2 LIBC_BIONIC=3 LIBC_MUSL=4"
 
 # 32-bit x86 processors supported by --with-arch=.  Each processor
 # MUST be separated by exactly one space.
@@ -720,6 +720,9 @@ case ${target} in
     *-*-*uclibc*)
       tm_defines="$tm_defines DEFAULT_LIBC=LIBC_UCLIBC"
       ;;
+    *-*-*musl*)
+      tm_defines="$tm_defines DEFAULT_LIBC=LIBC_MUSL"
+      ;;
     *)
       tm_defines="$tm_defines DEFAULT_LIBC=LIBC_GLIBC"
       ;;
--- a/src/gcc/config.host
+++ b/src/gcc/config.host
@@ -99,6 +99,14 @@ case ${host} in
 esac
 
 case ${host} in
+  aarch64*-*-linux*)
+    case ${target} in
+      aarch64*-*-*)
+	host_extra_gcc_objs="driver-aarch64.o"
+	host_xmake_file="${host_xmake_file} aarch64/x-aarch64"
+	;;
+    esac
+    ;;
   arm*-*-freebsd* | arm*-*-linux*)
     case ${target} in
       arm*-*-*)
--- a/src/gcc/config/aarch64/aarch64-cores.def
+++ b/src/gcc/config/aarch64/aarch64-cores.def
@@ -21,7 +21,7 @@
 
    Before using #include to read this file, define a macro:
 
-      AARCH64_CORE(CORE_NAME, CORE_IDENT, SCHEDULER_IDENT, ARCH, FLAGS, COSTS)
+      AARCH64_CORE(CORE_NAME, CORE_IDENT, SCHEDULER_IDENT, ARCH, FLAGS, COSTS, IMP, PART)
 
    The CORE_NAME is the name of the core, represented as a string constant.
    The CORE_IDENT is the name of the core, represented as an identifier.
@@ -30,18 +30,23 @@
    ARCH is the architecture revision implemented by the chip.
    FLAGS are the bitwise-or of the traits that apply to that core.
    This need not include flags implied by the architecture.
-   COSTS is the name of the rtx_costs routine to use.  */
+   COSTS is the name of the rtx_costs routine to use.
+   IMP is the implementer ID of the CPU vendor.  On a GNU/Linux system it can
+   be found in /proc/cpuinfo.
+   PART is the part number of the CPU.  On a GNU/Linux system it can be found
+   in /proc/cpuinfo.  For big.LITTLE systems this should have the form at of
+   "<big core part number>.<LITTLE core part number>".  */
 
 /* V8 Architecture Processors.  */
 
-AARCH64_CORE("cortex-a53",  cortexa53, cortexa53, 8,  AARCH64_FL_FOR_ARCH8 | AARCH64_FL_CRC, cortexa53)
-AARCH64_CORE("cortex-a57",  cortexa57, cortexa57, 8,  AARCH64_FL_FOR_ARCH8 | AARCH64_FL_CRC, cortexa57)
-AARCH64_CORE("cortex-a72",  cortexa72, cortexa57, 8,  AARCH64_FL_FOR_ARCH8 | AARCH64_FL_CRC, cortexa57)
-AARCH64_CORE("exynos-m1",   exynosm1,  cortexa57, 8,  AARCH64_FL_FOR_ARCH8 | AARCH64_FL_CRC | AARCH64_FL_CRYPTO, cortexa57)
-AARCH64_CORE("thunderx",    thunderx,  thunderx, 8,  AARCH64_FL_FOR_ARCH8 | AARCH64_FL_CRC | AARCH64_FL_CRYPTO, thunderx)
-AARCH64_CORE("xgene1",      xgene1,    xgene1,    8,  AARCH64_FL_FOR_ARCH8, xgene1)
+AARCH64_CORE("cortex-a53",  cortexa53, cortexa53, 8,  AARCH64_FL_FOR_ARCH8 | AARCH64_FL_CRC, cortexa53, "0x41", "0xd03")
+AARCH64_CORE("cortex-a57",  cortexa57, cortexa57, 8,  AARCH64_FL_FOR_ARCH8 | AARCH64_FL_CRC, cortexa57, "0x41", "0xd07")
+AARCH64_CORE("cortex-a72",  cortexa72, cortexa57, 8,  AARCH64_FL_FOR_ARCH8 | AARCH64_FL_CRC, cortexa57, "0x41", "0xd08")
+AARCH64_CORE("exynos-m1",   exynosm1,  cortexa57, 8,  AARCH64_FL_FOR_ARCH8 | AARCH64_FL_CRC | AARCH64_FL_CRYPTO, cortexa57, "0x53", "0x001")
+AARCH64_CORE("thunderx",    thunderx,  thunderx,  8,  AARCH64_FL_FOR_ARCH8 | AARCH64_FL_CRC | AARCH64_FL_CRYPTO, thunderx,  "0x43", "0x0a1")
+AARCH64_CORE("xgene1",      xgene1,    xgene1,    8,  AARCH64_FL_FOR_ARCH8, xgene1, "0x50", "0x000")
 
 /* V8 big.LITTLE implementations.  */
 
-AARCH64_CORE("cortex-a57.cortex-a53",  cortexa57cortexa53, cortexa53, 8,  AARCH64_FL_FOR_ARCH8 | AARCH64_FL_CRC, cortexa57)
-AARCH64_CORE("cortex-a72.cortex-a53",  cortexa72cortexa53, cortexa53, 8,  AARCH64_FL_FOR_ARCH8 | AARCH64_FL_CRC, cortexa57)
+AARCH64_CORE("cortex-a57.cortex-a53",  cortexa57cortexa53, cortexa53, 8,  AARCH64_FL_FOR_ARCH8 | AARCH64_FL_CRC, cortexa57, "0x41", "0xd07.0xd03")
+AARCH64_CORE("cortex-a72.cortex-a53",  cortexa72cortexa53, cortexa53, 8,  AARCH64_FL_FOR_ARCH8 | AARCH64_FL_CRC, cortexa57, "0x41", "0xd08.0xd03")
--- a/src/gcc/config/aarch64/aarch64-cost-tables.h
+++ b/src/gcc/config/aarch64/aarch64-cost-tables.h
@@ -83,7 +83,9 @@ const struct cpu_cost_table thunderx_extra_costs =
     0,			/* N/A: Stm_regs_per_insn_subsequent.  */
     0,			/* Storef.  */
     0,			/* Stored.  */
-    COSTS_N_INSNS (1)  /* Store_unaligned.  */
+    COSTS_N_INSNS (1),	/* Store_unaligned.  */
+    COSTS_N_INSNS (1),	/* Loadv.  */
+    COSTS_N_INSNS (1)	/* Storev.  */
   },
   {
     /* FP SFmode */
--- a/src/gcc/config/aarch64/aarch64-elf.h
+++ b/src/gcc/config/aarch64/aarch64-elf.h
@@ -132,7 +132,8 @@
 #undef DRIVER_SELF_SPECS
 #define DRIVER_SELF_SPECS \
   " %{!mbig-endian:%{!mlittle-endian:" ENDIAN_SPEC "}}" \
-  " %{!mabi=*:" ABI_SPEC "}"
+  " %{!mabi=*:" ABI_SPEC "}" \
+  MCPU_MTUNE_NATIVE_SPECS
 
 #ifdef HAVE_AS_MABI_OPTION
 #define ASM_MABI_SPEC	"%{mabi=*:-mabi=%*}"
--- a/src/gcc/config/aarch64/aarch64-linux.h
+++ b/src/gcc/config/aarch64/aarch64-linux.h
@@ -23,6 +23,9 @@
 
 #define GLIBC_DYNAMIC_LINKER "/lib/ld-linux-aarch64%{mbig-endian:_be}%{mabi=ilp32:_ilp32}.so.1"
 
+#undef MUSL_DYNAMIC_LINKER
+#define MUSL_DYNAMIC_LINKER "/lib/ld-musl-aarch64%{mbig-endian:_be}%{mabi=ilp32:_ilp32}.so.1"
+
 #undef  ASAN_CC1_SPEC
 #define ASAN_CC1_SPEC "%{%:sanitize(address):-funwind-tables}"
 
--- a/src/gcc/config/aarch64/aarch64-option-extensions.def
+++ b/src/gcc/config/aarch64/aarch64-option-extensions.def
@@ -21,18 +21,25 @@
 
    Before using #include to read this file, define a macro:
 
-      AARCH64_OPT_EXTENSION(EXT_NAME, FLAGS_ON, FLAGS_OFF)
+      AARCH64_OPT_EXTENSION(EXT_NAME, FLAGS_ON, FLAGS_OFF, FEATURE_STRING)
 
    EXT_NAME is the name of the extension, represented as a string constant.
    FLAGS_ON are the bitwise-or of the features that the extension adds.
-   FLAGS_OFF are the bitwise-or of the features that the extension removes.  */
+   FLAGS_OFF are the bitwise-or of the features that the extension removes.
+   FEAT_STRING is a string containing the entries in the 'Features' field of
+   /proc/cpuinfo on a GNU/Linux system that correspond to this architecture
+   extension being available.  Sometimes multiple entries are needed to enable
+   the extension (for example, the 'crypto' extension depends on four
+   entries: aes, pmull, sha1, sha2 being present).  In that case this field
+   should contain a whitespace-separated list of the strings in 'Features'
+   that are required.  Their order is not important.  */
 
 /* V8 Architecture Extensions.
    This list currently contains example extensions for CPUs that implement
    AArch64, and therefore serves as a template for adding more CPUs in the
    future.  */
 
-AARCH64_OPT_EXTENSION("fp",	AARCH64_FL_FP,	AARCH64_FL_FPSIMD | AARCH64_FL_CRYPTO)
-AARCH64_OPT_EXTENSION("simd",	AARCH64_FL_FPSIMD,	AARCH64_FL_SIMD | AARCH64_FL_CRYPTO)
-AARCH64_OPT_EXTENSION("crypto",	AARCH64_FL_CRYPTO | AARCH64_FL_FPSIMD,	AARCH64_FL_CRYPTO)
-AARCH64_OPT_EXTENSION("crc",	AARCH64_FL_CRC,	AARCH64_FL_CRC)
+AARCH64_OPT_EXTENSION("fp",	AARCH64_FL_FP,                          AARCH64_FL_FPSIMD | AARCH64_FL_CRYPTO, "fp")
+AARCH64_OPT_EXTENSION("simd",	AARCH64_FL_FPSIMD,                      AARCH64_FL_SIMD | AARCH64_FL_CRYPTO,   "asimd")
+AARCH64_OPT_EXTENSION("crypto",	AARCH64_FL_CRYPTO | AARCH64_FL_FPSIMD,  AARCH64_FL_CRYPTO,                     "aes pmull sha1 sha2")
+AARCH64_OPT_EXTENSION("crc",	AARCH64_FL_CRC,                         AARCH64_FL_CRC,                        "crc32")
--- a/src/gcc/config/aarch64/aarch64-opts.h
+++ b/src/gcc/config/aarch64/aarch64-opts.h
@@ -25,7 +25,7 @@
 /* The various cores that implement AArch64.  */
 enum aarch64_processor
 {
-#define AARCH64_CORE(NAME, INTERNAL_IDENT, SCHED, ARCH, FLAGS, COSTS) \
+#define AARCH64_CORE(NAME, INTERNAL_IDENT, SCHED, ARCH, FLAGS, COSTS, IMP, PART) \
   INTERNAL_IDENT,
 #include "aarch64-cores.def"
 #undef AARCH64_CORE
--- a/src/gcc/config/aarch64/aarch64-protos.h
+++ b/src/gcc/config/aarch64/aarch64-protos.h
@@ -162,12 +162,20 @@ struct cpu_vector_cost
   const int cond_not_taken_branch_cost;  /* Cost of not taken branch.  */
 };
 
+/* Branch costs.  */
+struct cpu_branch_cost
+{
+  const int predictable;    /* Predictable branch or optimizing for size.  */
+  const int unpredictable;  /* Unpredictable branch or optimizing for speed.  */
+};
+
 struct tune_params
 {
   const struct cpu_cost_table *const insn_extra_cost;
   const struct cpu_addrcost_table *const addr_cost;
   const struct cpu_regmove_cost *const regmove_cost;
   const struct cpu_vector_cost *const vec_costs;
+  const struct cpu_branch_cost *const branch_costs;
   const int memmov_cost;
   const int issue_rate;
   const unsigned int fuseable_ops;
@@ -177,11 +185,14 @@ struct tune_params
   const int int_reassoc_width;
   const int fp_reassoc_width;
   const int vec_reassoc_width;
+  const int min_div_recip_mul_sf;
+  const int min_div_recip_mul_df;
 };
 
 HOST_WIDE_INT aarch64_initial_elimination_offset (unsigned, unsigned);
 int aarch64_get_condition_code (rtx);
 bool aarch64_bitmask_imm (HOST_WIDE_INT val, machine_mode);
+int aarch64_branch_cost (bool, bool);
 enum aarch64_symbol_type
 aarch64_classify_symbolic_expression (rtx, enum aarch64_symbol_context);
 bool aarch64_const_vec_all_same_int_p (rtx, HOST_WIDE_INT);
@@ -264,12 +275,6 @@ void init_aarch64_simd_builtins (void);
 
 void aarch64_simd_emit_reg_reg_move (rtx *, enum machine_mode, unsigned int);
 
-/* Emit code to place a AdvSIMD pair result in memory locations (with equal
-   registers).  */
-void aarch64_simd_emit_pair_result_insn (machine_mode,
-					 rtx (*intfn) (rtx, rtx, rtx), rtx,
-					 rtx);
-
 /* Expand builtins for SIMD intrinsics.  */
 rtx aarch64_simd_expand_builtin (int, tree, rtx);
 
--- a/src/gcc/config/aarch64/aarch64-simd.md
+++ b/src/gcc/config/aarch64/aarch64-simd.md
@@ -2057,13 +2057,13 @@
 })
 
 (define_expand "aarch64_vcond_internal<mode><mode>"
-  [(set (match_operand:VDQ_I 0 "register_operand")
-	(if_then_else:VDQ_I
+  [(set (match_operand:VSDQ_I_DI 0 "register_operand")
+	(if_then_else:VSDQ_I_DI
 	  (match_operator 3 "comparison_operator"
-	    [(match_operand:VDQ_I 4 "register_operand")
-	     (match_operand:VDQ_I 5 "nonmemory_operand")])
-	  (match_operand:VDQ_I 1 "nonmemory_operand")
-	  (match_operand:VDQ_I 2 "nonmemory_operand")))]
+	    [(match_operand:VSDQ_I_DI 4 "register_operand")
+	     (match_operand:VSDQ_I_DI 5 "nonmemory_operand")])
+	  (match_operand:VSDQ_I_DI 1 "nonmemory_operand")
+	  (match_operand:VSDQ_I_DI 2 "nonmemory_operand")))]
   "TARGET_SIMD"
 {
   rtx op1 = operands[1];
@@ -2365,13 +2365,13 @@
 })
 
 (define_expand "vcond<mode><mode>"
-  [(set (match_operand:VALL 0 "register_operand")
-	(if_then_else:VALL
+  [(set (match_operand:VALLDI 0 "register_operand")
+	(if_then_else:VALLDI
 	  (match_operator 3 "comparison_operator"
-	    [(match_operand:VALL 4 "register_operand")
-	     (match_operand:VALL 5 "nonmemory_operand")])
-	  (match_operand:VALL 1 "nonmemory_operand")
-	  (match_operand:VALL 2 "nonmemory_operand")))]
+	    [(match_operand:VALLDI 4 "register_operand")
+	     (match_operand:VALLDI 5 "nonmemory_operand")])
+	  (match_operand:VALLDI 1 "nonmemory_operand")
+	  (match_operand:VALLDI 2 "nonmemory_operand")))]
   "TARGET_SIMD"
 {
   emit_insn (gen_aarch64_vcond_internal<mode><mode> (operands[0], operands[1],
@@ -2398,13 +2398,13 @@
 })
 
 (define_expand "vcondu<mode><mode>"
-  [(set (match_operand:VDQ_I 0 "register_operand")
-	(if_then_else:VDQ_I
+  [(set (match_operand:VSDQ_I_DI 0 "register_operand")
+	(if_then_else:VSDQ_I_DI
 	  (match_operator 3 "comparison_operator"
-	    [(match_operand:VDQ_I 4 "register_operand")
-	     (match_operand:VDQ_I 5 "nonmemory_operand")])
-	  (match_operand:VDQ_I 1 "nonmemory_operand")
-	  (match_operand:VDQ_I 2 "nonmemory_operand")))]
+	    [(match_operand:VSDQ_I_DI 4 "register_operand")
+	     (match_operand:VSDQ_I_DI 5 "nonmemory_operand")])
+	  (match_operand:VSDQ_I_DI 1 "nonmemory_operand")
+	  (match_operand:VSDQ_I_DI 2 "nonmemory_operand")))]
   "TARGET_SIMD"
 {
   emit_insn (gen_aarch64_vcond_internal<mode><mode> (operands[0], operands[1],
@@ -3955,6 +3955,7 @@
   [(set_attr "type" "neon_store2_2reg<q>")]
 )
 
+;; RTL uses GCC vector extension indices, so flip only for assembly.
 (define_insn "vec_store_lanesoi_lane<mode>"
   [(set (match_operand:<V_TWO_ELEM> 0 "aarch64_simd_struct_operand" "=Utv")
 	(unspec:<V_TWO_ELEM> [(match_operand:OI 1 "register_operand" "w")
@@ -3962,7 +3963,10 @@
 		    (match_operand:SI 2 "immediate_operand" "i")]
                    UNSPEC_ST2_LANE))]
   "TARGET_SIMD"
-  "st2\\t{%S1.<Vetype> - %T1.<Vetype>}[%2], %0"
+  {
+    operands[2] = GEN_INT (ENDIAN_LANE_N (<MODE>mode, INTVAL (operands[2])));
+    return "st2\\t{%S1.<Vetype> - %T1.<Vetype>}[%2], %0";
+  }
   [(set_attr "type" "neon_store3_one_lane<q>")]
 )
 
@@ -4046,6 +4050,7 @@
   [(set_attr "type" "neon_store3_3reg<q>")]
 )
 
+;; RTL uses GCC vector extension indices, so flip only for assembly.
 (define_insn "vec_store_lanesci_lane<mode>"
   [(set (match_operand:<V_THREE_ELEM> 0 "aarch64_simd_struct_operand" "=Utv")
 	(unspec:<V_THREE_ELEM> [(match_operand:CI 1 "register_operand" "w")
@@ -4053,7 +4058,10 @@
 		    (match_operand:SI 2 "immediate_operand" "i")]
                    UNSPEC_ST3_LANE))]
   "TARGET_SIMD"
-  "st3\\t{%S1.<Vetype> - %U1.<Vetype>}[%2], %0"
+  {
+    operands[2] = GEN_INT (ENDIAN_LANE_N (<MODE>mode, INTVAL (operands[2])));
+    return "st3\\t{%S1.<Vetype> - %U1.<Vetype>}[%2], %0";
+  }
   [(set_attr "type" "neon_store3_one_lane<q>")]
 )
 
@@ -4137,6 +4145,7 @@
   [(set_attr "type" "neon_store4_4reg<q>")]
 )
 
+;; RTL uses GCC vector extension indices, so flip only for assembly.
 (define_insn "vec_store_lanesxi_lane<mode>"
   [(set (match_operand:<V_FOUR_ELEM> 0 "aarch64_simd_struct_operand" "=Utv")
 	(unspec:<V_FOUR_ELEM> [(match_operand:XI 1 "register_operand" "w")
@@ -4144,7 +4153,10 @@
 		    (match_operand:SI 2 "immediate_operand" "i")]
                    UNSPEC_ST4_LANE))]
   "TARGET_SIMD"
-  "st4\\t{%S1.<Vetype> - %V1.<Vetype>}[%2], %0"
+  {
+    operands[2] = GEN_INT (ENDIAN_LANE_N (<MODE>mode, INTVAL (operands[2])));
+    return "st4\\t{%S1.<Vetype> - %V1.<Vetype>}[%2], %0";
+  }
   [(set_attr "type" "neon_store4_one_lane<q>")]
 )
 
--- a/src/gcc/config/aarch64/aarch64.c
+++ b/src/gcc/config/aarch64/aarch64.c
@@ -339,12 +339,20 @@ static const struct cpu_vector_cost xgene1_vector_cost =
 #define AARCH64_FUSE_ADRP_LDR	(1 << 3)
 #define AARCH64_FUSE_CMP_BRANCH	(1 << 4)
 
+/* Generic costs for branch instructions.  */
+static const struct cpu_branch_cost generic_branch_cost =
+{
+  2,  /* Predictable.  */
+  2   /* Unpredictable.  */
+};
+
 static const struct tune_params generic_tunings =
 {
   &cortexa57_extra_costs,
   &generic_addrcost_table,
   &generic_regmove_cost,
   &generic_vector_cost,
+  &generic_branch_cost,
   4, /* memmov_cost  */
   2, /* issue_rate  */
   AARCH64_FUSE_NOTHING, /* fuseable_ops  */
@@ -353,7 +361,9 @@ static const struct tune_params generic_tunings =
   4,	/* loop_align.  */
   2,	/* int_reassoc_width.  */
   4,	/* fp_reassoc_width.  */
-  1	/* vec_reassoc_width.  */
+  1,	/* vec_reassoc_width.  */
+  2,	/* min_div_recip_mul_sf.  */
+  2	/* min_div_recip_mul_df.  */
 };
 
 static const struct tune_params cortexa53_tunings =
@@ -362,6 +372,7 @@ static const struct tune_params cortexa53_tunings =
   &generic_addrcost_table,
   &cortexa53_regmove_cost,
   &generic_vector_cost,
+  &generic_branch_cost,
   4, /* memmov_cost  */
   2, /* issue_rate  */
   (AARCH64_FUSE_MOV_MOVK | AARCH64_FUSE_ADRP_ADD
@@ -371,7 +382,9 @@ static const struct tune_params cortexa53_tunings =
   4,	/* loop_align.  */
   2,	/* int_reassoc_width.  */
   4,	/* fp_reassoc_width.  */
-  1	/* vec_reassoc_width.  */
+  1,	/* vec_reassoc_width.  */
+  2,	/* min_div_recip_mul_sf.  */
+  2	/* min_div_recip_mul_df.  */
 };
 
 static const struct tune_params cortexa57_tunings =
@@ -380,6 +393,7 @@ static const struct tune_params cortexa57_tunings =
   &cortexa57_addrcost_table,
   &cortexa57_regmove_cost,
   &cortexa57_vector_cost,
+  &generic_branch_cost,
   4, /* memmov_cost  */
   3, /* issue_rate  */
   (AARCH64_FUSE_MOV_MOVK | AARCH64_FUSE_ADRP_ADD
@@ -389,7 +403,9 @@ static const struct tune_params cortexa57_tunings =
   4,	/* loop_align.  */
   2,	/* int_reassoc_width.  */
   4,	/* fp_reassoc_width.  */
-  1	/* vec_reassoc_width.  */
+  1,	/* vec_reassoc_width.  */
+  2,	/* min_div_recip_mul_sf.  */
+  2	/* min_div_recip_mul_df.  */
 };
 
 static const struct tune_params thunderx_tunings =
@@ -398,6 +414,7 @@ static const struct tune_params thunderx_tunings =
   &generic_addrcost_table,
   &thunderx_regmove_cost,
   &generic_vector_cost,
+  &generic_branch_cost,
   6, /* memmov_cost  */
   2, /* issue_rate  */
   AARCH64_FUSE_CMP_BRANCH, /* fuseable_ops  */
@@ -406,7 +423,9 @@ static const struct tune_params thunderx_tunings =
   8,	/* loop_align.  */
   2,	/* int_reassoc_width.  */
   4,	/* fp_reassoc_width.  */
-  1	/* vec_reassoc_width.  */
+  1,	/* vec_reassoc_width.  */
+  2,	/* min_div_recip_mul_sf.  */
+  2	/* min_div_recip_mul_df.  */
 };
 
 static const struct tune_params xgene1_tunings =
@@ -415,6 +434,7 @@ static const struct tune_params xgene1_tunings =
   &xgene1_addrcost_table,
   &xgene1_regmove_cost,
   &xgene1_vector_cost,
+  &generic_branch_cost,
   6, /* memmov_cost  */
   4, /* issue_rate  */
   AARCH64_FUSE_NOTHING, /* fuseable_ops  */
@@ -423,7 +443,9 @@ static const struct tune_params xgene1_tunings =
   16,	/* loop_align.  */
   2,	/* int_reassoc_width.  */
   4,	/* fp_reassoc_width.  */
-  1	/* vec_reassoc_width.  */
+  1,	/* vec_reassoc_width.  */
+  2,	/* min_div_recip_mul_sf.  */
+  2	/* min_div_recip_mul_df.  */
 };
 
 /* A processor implementing AArch64.  */
@@ -440,7 +462,7 @@ struct processor
 /* Processor cores implementing AArch64.  */
 static const struct processor all_cores[] =
 {
-#define AARCH64_CORE(NAME, IDENT, SCHED, ARCH, FLAGS, COSTS) \
+#define AARCH64_CORE(NAME, IDENT, SCHED, ARCH, FLAGS, COSTS, IMP, PART) \
   {NAME, SCHED, #ARCH, ARCH, FLAGS, &COSTS##_tunings},
 #include "aarch64-cores.def"
 #undef AARCH64_CORE
@@ -477,7 +499,7 @@ struct aarch64_option_extension
 /* ISA extensions in AArch64.  */
 static const struct aarch64_option_extension all_extensions[] =
 {
-#define AARCH64_OPT_EXTENSION(NAME, FLAGS_ON, FLAGS_OFF) \
+#define AARCH64_OPT_EXTENSION(NAME, FLAGS_ON, FLAGS_OFF, FEATURE_STRING) \
   {NAME, FLAGS_ON, FLAGS_OFF},
 #include "aarch64-option-extensions.def"
 #undef AARCH64_OPT_EXTENSION
@@ -512,9 +534,11 @@ static const char * const aarch64_condition_codes[] =
 };
 
 static unsigned int
-aarch64_min_divisions_for_recip_mul (enum machine_mode mode ATTRIBUTE_UNUSED)
+aarch64_min_divisions_for_recip_mul (enum machine_mode mode)
 {
-  return 2;
+  if (GET_MODE_UNIT_SIZE (mode) == 4)
+    return aarch64_tune_params->min_div_recip_mul_sf;
+  return aarch64_tune_params->min_div_recip_mul_df;
 }
 
 static int
@@ -4901,8 +4925,9 @@ aarch64_class_max_nregs (reg_class_t regclass, machine_mode mode)
     case FP_REGS:
     case FP_LO_REGS:
       return
-	aarch64_vector_mode_p (mode) ? (GET_MODE_SIZE (mode) + 15) / 16 :
-				       (GET_MODE_SIZE (mode) + 7) / 8;
+	aarch64_vector_mode_p (mode)
+	  ? (GET_MODE_SIZE (mode) + UNITS_PER_VREG - 1) / UNITS_PER_VREG
+	  : (GET_MODE_SIZE (mode) + UNITS_PER_WORD - 1) / UNITS_PER_WORD;
     case STACK_REG:
       return 1;
 
@@ -5157,9 +5182,18 @@ aarch64_strip_extend (rtx x)
   return x;
 }
 
+/* Return true iff CODE is a shift supported in combination
+   with arithmetic instructions.  */
+
+static bool
+aarch64_shift_p (enum rtx_code code)
+{
+  return code == ASHIFT || code == ASHIFTRT || code == LSHIFTRT;
+}
+
 /* Helper function for rtx cost calculation.  Calculate the cost of
-   a MULT, which may be part of a multiply-accumulate rtx.  Return
-   the calculated cost of the expression, recursing manually in to
+   a MULT or ASHIFT, which may be part of a compound PLUS/MINUS rtx.
+   Return the calculated cost of the expression, recursing manually in to
    operands where needed.  */
 
 static int
@@ -5169,7 +5203,7 @@ aarch64_rtx_mult_cost (rtx x, int code, int outer, bool speed)
   const struct cpu_cost_table *extra_cost
     = aarch64_tune_params->insn_extra_cost;
   int cost = 0;
-  bool maybe_fma = (outer == PLUS || outer == MINUS);
+  bool compound_p = (outer == PLUS || outer == MINUS);
   machine_mode mode = GET_MODE (x);
 
   gcc_checking_assert (code == MULT);
@@ -5184,24 +5218,50 @@ aarch64_rtx_mult_cost (rtx x, int code, int outer, bool speed)
   if (GET_MODE_CLASS (mode) == MODE_INT)
     {
       /* The multiply will be canonicalized as a shift, cost it as such.  */
-      if (CONST_INT_P (op1)
-	  && exact_log2 (INTVAL (op1)) > 0)
+      if (aarch64_shift_p (GET_CODE (x))
+	  || (CONST_INT_P (op1)
+	      && exact_log2 (INTVAL (op1)) > 0))
 	{
+	  bool is_extend = GET_CODE (op0) == ZERO_EXTEND
+	                   || GET_CODE (op0) == SIGN_EXTEND;
 	  if (speed)
 	    {
-	      if (maybe_fma)
-		/* ADD (shifted register).  */
-		cost += extra_cost->alu.arith_shift;
+	      if (compound_p)
+	        {
+	          if (REG_P (op1))
+		    /* ARITH + shift-by-register.  */
+		    cost += extra_cost->alu.arith_shift_reg;
+		  else if (is_extend)
+		    /* ARITH + extended register.  We don't have a cost field
+		       for ARITH+EXTEND+SHIFT, so use extend_arith here.  */
+		    cost += extra_cost->alu.extend_arith;
+		  else
+		    /* ARITH + shift-by-immediate.  */
+		    cost += extra_cost->alu.arith_shift;
+		}
 	      else
 		/* LSL (immediate).  */
-		cost += extra_cost->alu.shift;
+	        cost += extra_cost->alu.shift;
+
 	    }
+	  /* Strip extends as we will have costed them in the case above.  */
+	  if (is_extend)
+	    op0 = aarch64_strip_extend (op0);
 
 	  cost += rtx_cost (op0, GET_CODE (op0), 0, speed);
 
 	  return cost;
 	}
 
+      /* MNEG or [US]MNEGL.  Extract the NEG operand and indicate that it's a
+	 compound and let the below cases handle it.  After all, MNEG is a
+	 special-case alias of MSUB.  */
+      if (GET_CODE (op0) == NEG)
+	{
+	  op0 = XEXP (op0, 0);
+	  compound_p = true;
+	}
+
       /* Integer multiplies or FMAs have zero/sign extending variants.  */
       if ((GET_CODE (op0) == ZERO_EXTEND
 	   && GET_CODE (op1) == ZERO_EXTEND)
@@ -5213,8 +5273,8 @@ aarch64_rtx_mult_cost (rtx x, int code, int outer, bool speed)
 
 	  if (speed)
 	    {
-	      if (maybe_fma)
-		/* MADD/SMADDL/UMADDL.  */
+	      if (compound_p)
+		/* SMADDL/UMADDL/UMSUBL/SMSUBL.  */
 		cost += extra_cost->mult[0].extend_add;
 	      else
 		/* MUL/SMULL/UMULL.  */
@@ -5224,15 +5284,15 @@ aarch64_rtx_mult_cost (rtx x, int code, int outer, bool speed)
 	  return cost;
 	}
 
-      /* This is either an integer multiply or an FMA.  In both cases
+      /* This is either an integer multiply or a MADD.  In both cases
 	 we want to recurse and cost the operands.  */
       cost += rtx_cost (op0, MULT, 0, speed)
 	      + rtx_cost (op1, MULT, 1, speed);
 
       if (speed)
 	{
-	  if (maybe_fma)
-	    /* MADD.  */
+	  if (compound_p)
+	    /* MADD/MSUB.  */
 	    cost += extra_cost->mult[mode == DImode].add;
 	  else
 	    /* MUL.  */
@@ -5250,7 +5310,7 @@ aarch64_rtx_mult_cost (rtx x, int code, int outer, bool speed)
 	     which case FNMUL is different than FMUL with operand negation.  */
 	  bool neg0 = GET_CODE (op0) == NEG;
 	  bool neg1 = GET_CODE (op1) == NEG;
-	  if (maybe_fma || !flag_rounding_math || (neg0 && neg1))
+	  if (compound_p || !flag_rounding_math || (neg0 && neg1))
 	    {
 	      if (neg0)
 		op0 = XEXP (op0, 0);
@@ -5258,7 +5318,7 @@ aarch64_rtx_mult_cost (rtx x, int code, int outer, bool speed)
 		op1 = XEXP (op1, 0);
 	    }
 
-	  if (maybe_fma)
+	  if (compound_p)
 	    /* FMADD/FNMADD/FNMSUB/FMSUB.  */
 	    cost += extra_cost->fp[mode == DFmode].fma;
 	  else
@@ -5367,6 +5427,23 @@ aarch64_address_cost (rtx x,
   return cost;
 }
 
+/* Return the cost of a branch.  If SPEED_P is true then the compiler is
+   optimizing for speed.  If PREDICTABLE_P is true then the branch is predicted
+   to be taken.  */
+
+int
+aarch64_branch_cost (bool speed_p, bool predictable_p)
+{
+  /* When optimizing for speed, use the cost of unpredictable branches.  */
+  const struct cpu_branch_cost *branch_costs =
+    aarch64_tune_params->branch_costs;
+
+  if (!speed_p || predictable_p)
+    return branch_costs->predictable;
+  else
+    return branch_costs->unpredictable;
+}
+
 /* Return true if the RTX X in mode MODE is a zero or sign extract
    usable in an ADD or SUB (extended register) instruction.  */
 static bool
@@ -5415,6 +5492,51 @@ aarch64_frint_unspec_p (unsigned int u)
     }
 }
 
+/* Return true iff X is an rtx that will match an extr instruction
+   i.e. as described in the *extr<mode>5_insn family of patterns.
+   OP0 and OP1 will be set to the operands of the shifts involved
+   on success and will be NULL_RTX otherwise.  */
+
+static bool
+aarch64_extr_rtx_p (rtx x, rtx *res_op0, rtx *res_op1)
+{
+  rtx op0, op1;
+  machine_mode mode = GET_MODE (x);
+
+  *res_op0 = NULL_RTX;
+  *res_op1 = NULL_RTX;
+
+  if (GET_CODE (x) != IOR)
+    return false;
+
+  op0 = XEXP (x, 0);
+  op1 = XEXP (x, 1);
+
+  if ((GET_CODE (op0) == ASHIFT && GET_CODE (op1) == LSHIFTRT)
+      || (GET_CODE (op1) == ASHIFT && GET_CODE (op0) == LSHIFTRT))
+    {
+     /* Canonicalise locally to ashift in op0, lshiftrt in op1.  */
+      if (GET_CODE (op1) == ASHIFT)
+        std::swap (op0, op1);
+
+      if (!CONST_INT_P (XEXP (op0, 1)) || !CONST_INT_P (XEXP (op1, 1)))
+        return false;
+
+      unsigned HOST_WIDE_INT shft_amnt_0 = UINTVAL (XEXP (op0, 1));
+      unsigned HOST_WIDE_INT shft_amnt_1 = UINTVAL (XEXP (op1, 1));
+
+      if (shft_amnt_0 < GET_MODE_BITSIZE (mode)
+          && shft_amnt_0 + shft_amnt_1 == GET_MODE_BITSIZE (mode))
+        {
+          *res_op0 = XEXP (op0, 0);
+          *res_op1 = XEXP (op1, 0);
+          return true;
+        }
+    }
+
+  return false;
+}
+
 /* Calculate the cost of calculating (if_then_else (OP0) (OP1) (OP2)),
    storing it in *COST.  Result is true if the total cost of the operation
    has now been calculated.  */
@@ -5505,16 +5627,6 @@ aarch64_rtx_costs (rtx x, int code, int outer ATTRIBUTE_UNUSED,
      above this default.  */
   *cost = COSTS_N_INSNS (1);
 
-  /* TODO: The cost infrastructure currently does not handle
-     vector operations.  Assume that all vector operations
-     are equally expensive.  */
-  if (VECTOR_MODE_P (mode))
-    {
-      if (speed)
-	*cost += extra_cost->vect.alu;
-      return true;
-    }
-
   switch (code)
     {
     case SET:
@@ -5529,7 +5641,9 @@ aarch64_rtx_costs (rtx x, int code, int outer ATTRIBUTE_UNUSED,
 	  if (speed)
 	    {
 	      rtx address = XEXP (op0, 0);
-	      if (GET_MODE_CLASS (mode) == MODE_INT)
+	      if (VECTOR_MODE_P (mode))
+		*cost += extra_cost->ldst.storev;
+	      else if (GET_MODE_CLASS (mode) == MODE_INT)
 		*cost += extra_cost->ldst.store;
 	      else if (mode == SFmode)
 		*cost += extra_cost->ldst.storef;
@@ -5550,15 +5664,22 @@ aarch64_rtx_costs (rtx x, int code, int outer ATTRIBUTE_UNUSED,
 
 	  /* Fall through.  */
 	case REG:
+	  /* The cost is one per vector-register copied.  */
+	  if (VECTOR_MODE_P (GET_MODE (op0)) && REG_P (op1))
+	    {
+	      int n_minus_1 = (GET_MODE_SIZE (GET_MODE (op0)) - 1)
+			      / GET_MODE_SIZE (V4SImode);
+	      *cost = COSTS_N_INSNS (n_minus_1 + 1);
+	    }
 	  /* const0_rtx is in general free, but we will use an
 	     instruction to set a register to 0.  */
-          if (REG_P (op1) || op1 == const0_rtx)
-            {
-              /* The cost is 1 per register copied.  */
-              int n_minus_1 = (GET_MODE_SIZE (GET_MODE (op0)) - 1)
+	  else if (REG_P (op1) || op1 == const0_rtx)
+	    {
+	      /* The cost is 1 per register copied.  */
+	      int n_minus_1 = (GET_MODE_SIZE (GET_MODE (op0)) - 1)
 			      / UNITS_PER_WORD;
-              *cost = COSTS_N_INSNS (n_minus_1 + 1);
-            }
+	      *cost = COSTS_N_INSNS (n_minus_1 + 1);
+	    }
           else
 	    /* Cost is just the cost of the RHS of the set.  */
 	    *cost += rtx_cost (op1, SET, 1, speed);
@@ -5656,7 +5777,9 @@ aarch64_rtx_costs (rtx x, int code, int outer ATTRIBUTE_UNUSED,
 	     approximation for the additional cost of the addressing
 	     mode.  */
 	  rtx address = XEXP (x, 0);
-	  if (GET_MODE_CLASS (mode) == MODE_INT)
+	  if (VECTOR_MODE_P (mode))
+	    *cost += extra_cost->ldst.loadv;
+	  else if (GET_MODE_CLASS (mode) == MODE_INT)
 	    *cost += extra_cost->ldst.load;
 	  else if (mode == SFmode)
 	    *cost += extra_cost->ldst.loadf;
@@ -5673,6 +5796,16 @@ aarch64_rtx_costs (rtx x, int code, int outer ATTRIBUTE_UNUSED,
     case NEG:
       op0 = XEXP (x, 0);
 
+      if (VECTOR_MODE_P (mode))
+	{
+	  if (speed)
+	    {
+	      /* FNEG.  */
+	      *cost += extra_cost->vect.alu;
+	    }
+	  return false;
+	}
+
       if (GET_MODE_CLASS (GET_MODE (x)) == MODE_INT)
        {
           if (GET_RTX_CLASS (GET_CODE (op0)) == RTX_COMPARE
@@ -5717,7 +5850,12 @@ aarch64_rtx_costs (rtx x, int code, int outer ATTRIBUTE_UNUSED,
     case CLRSB:
     case CLZ:
       if (speed)
-        *cost += extra_cost->alu.clz;
+	{
+	  if (VECTOR_MODE_P (mode))
+	    *cost += extra_cost->vect.alu;
+	  else
+	    *cost += extra_cost->alu.clz;
+	}
 
       return false;
 
@@ -5796,12 +5934,27 @@ aarch64_rtx_costs (rtx x, int code, int outer ATTRIBUTE_UNUSED,
 
           if (CONST_DOUBLE_P (op1) && aarch64_float_const_zero_rtx_p (op1))
             {
+              *cost += rtx_cost (op0, COMPARE, 0, speed);
               /* FCMP supports constant 0.0 for no extra cost. */
               return true;
             }
           return false;
         }
 
+      if (VECTOR_MODE_P (mode))
+	{
+	  /* Vector compare.  */
+	  if (speed)
+	    *cost += extra_cost->vect.alu;
+
+	  if (aarch64_float_const_zero_rtx_p (op1))
+	    {
+	      /* Vector cm (eq|ge|gt|lt|le) supports constant 0.0 for no extra
+		 cost.  */
+	      return true;
+	    }
+	  return false;
+	}
       return false;
 
     case MINUS:
@@ -5810,6 +5963,8 @@ aarch64_rtx_costs (rtx x, int code, int outer ATTRIBUTE_UNUSED,
 	op1 = XEXP (x, 1);
 
 cost_minus:
+	*cost += rtx_cost (op0, MINUS, 0, speed);
+
 	/* Detect valid immediates.  */
 	if ((GET_MODE_CLASS (mode) == MODE_INT
 	     || (GET_MODE_CLASS (mode) == MODE_CC
@@ -5817,20 +5972,17 @@ cost_minus:
 	    && CONST_INT_P (op1)
 	    && aarch64_uimm12_shift (INTVAL (op1)))
 	  {
-	    *cost += rtx_cost (op0, MINUS, 0, speed);
-
 	    if (speed)
 	      /* SUB(S) (immediate).  */
 	      *cost += extra_cost->alu.arith;
 	    return true;
-
 	  }
 
 	/* Look for SUB (extended register).  */
         if (aarch64_rtx_arith_op_extract_p (op1, mode))
 	  {
 	    if (speed)
-	      *cost += extra_cost->alu.arith_shift;
+	      *cost += extra_cost->alu.extend_arith;
 
 	    *cost += rtx_cost (XEXP (XEXP (op1, 0), 0),
 			       (enum rtx_code) GET_CODE (op1),
@@ -5842,13 +5994,12 @@ cost_minus:
 
 	/* Cost this as an FMA-alike operation.  */
 	if ((GET_CODE (new_op1) == MULT
-	     || GET_CODE (new_op1) == ASHIFT)
+	     || aarch64_shift_p (GET_CODE (new_op1)))
 	    && code != COMPARE)
 	  {
 	    *cost += aarch64_rtx_mult_cost (new_op1, MULT,
 					    (enum rtx_code) code,
 					    speed);
-	    *cost += rtx_cost (op0, MINUS, 0, speed);
 	    return true;
 	  }
 
@@ -5856,12 +6007,21 @@ cost_minus:
 
 	if (speed)
 	  {
-	    if (GET_MODE_CLASS (mode) == MODE_INT)
-	      /* SUB(S).  */
-	      *cost += extra_cost->alu.arith;
+	    if (VECTOR_MODE_P (mode))
+	      {
+		/* Vector SUB.  */
+		*cost += extra_cost->vect.alu;
+	      }
+	    else if (GET_MODE_CLASS (mode) == MODE_INT)
+	      {
+		/* SUB(S).  */
+		*cost += extra_cost->alu.arith;
+	      }
 	    else if (GET_MODE_CLASS (mode) == MODE_FLOAT)
-	      /* FSUB.  */
-	      *cost += extra_cost->fp[mode == DFmode].addsub;
+	      {
+		/* FSUB.  */
+		*cost += extra_cost->fp[mode == DFmode].addsub;
+	      }
 	  }
 	return true;
       }
@@ -5895,11 +6055,13 @@ cost_plus:
 	    return true;
 	  }
 
+	*cost += rtx_cost (op1, PLUS, 1, speed);
+
 	/* Look for ADD (extended register).  */
         if (aarch64_rtx_arith_op_extract_p (op0, mode))
 	  {
 	    if (speed)
-	      *cost += extra_cost->alu.arith_shift;
+	      *cost += extra_cost->alu.extend_arith;
 
 	    *cost += rtx_cost (XEXP (XEXP (op0, 0), 0),
 			       (enum rtx_code) GET_CODE (op0),
@@ -5912,25 +6074,32 @@ cost_plus:
 	new_op0 = aarch64_strip_extend (op0);
 
 	if (GET_CODE (new_op0) == MULT
-	    || GET_CODE (new_op0) == ASHIFT)
+	    || aarch64_shift_p (GET_CODE (new_op0)))
 	  {
 	    *cost += aarch64_rtx_mult_cost (new_op0, MULT, PLUS,
 					    speed);
-	    *cost += rtx_cost (op1, PLUS, 1, speed);
 	    return true;
 	  }
 
-	*cost += (rtx_cost (new_op0, PLUS, 0, speed)
-		  + rtx_cost (op1, PLUS, 1, speed));
+	*cost += rtx_cost (new_op0, PLUS, 0, speed);
 
 	if (speed)
 	  {
-	    if (GET_MODE_CLASS (mode) == MODE_INT)
-	      /* ADD.  */
-	      *cost += extra_cost->alu.arith;
+	    if (VECTOR_MODE_P (mode))
+	      {
+		/* Vector ADD.  */
+		*cost += extra_cost->vect.alu;
+	      }
+	    else if (GET_MODE_CLASS (mode) == MODE_INT)
+	      {
+		/* ADD.  */
+		*cost += extra_cost->alu.arith;
+	      }
 	    else if (GET_MODE_CLASS (mode) == MODE_FLOAT)
-	      /* FADD.  */
-	      *cost += extra_cost->fp[mode == DFmode].addsub;
+	      {
+		/* FADD.  */
+		*cost += extra_cost->fp[mode == DFmode].addsub;
+	      }
 	  }
 	return true;
       }
@@ -5939,8 +6108,12 @@ cost_plus:
       *cost = COSTS_N_INSNS (1);
 
       if (speed)
-        *cost += extra_cost->alu.rev;
-
+	{
+	  if (VECTOR_MODE_P (mode))
+	    *cost += extra_cost->vect.alu;
+	  else
+	    *cost += extra_cost->alu.rev;
+	}
       return false;
 
     case IOR:
@@ -5948,8 +6121,22 @@ cost_plus:
         {
           *cost = COSTS_N_INSNS (1);
 
+	  if (speed)
+	    {
+	      if (VECTOR_MODE_P (mode))
+		*cost += extra_cost->vect.alu;
+	      else
+		*cost += extra_cost->alu.rev;
+	    }
+	  return true;
+        }
+
+      if (aarch64_extr_rtx_p (x, &op0, &op1))
+        {
+          *cost += rtx_cost (op0, IOR, 0, speed)
+                   + rtx_cost (op1, IOR, 1, speed);
           if (speed)
-            *cost += extra_cost->alu.rev;
+            *cost += extra_cost->alu.shift;
 
           return true;
         }
@@ -5960,6 +6147,13 @@ cost_plus:
       op0 = XEXP (x, 0);
       op1 = XEXP (x, 1);
 
+      if (VECTOR_MODE_P (mode))
+	{
+	  if (speed)
+	    *cost += extra_cost->vect.alu;
+	  return true;
+	}
+
       if (code == AND
           && GET_CODE (op0) == MULT
           && CONST_INT_P (XEXP (op0, 1))
@@ -6025,13 +6219,52 @@ cost_plus:
       return false;
 
     case NOT:
+      x = XEXP (x, 0);
+      op0 = aarch64_strip_shift (x);
+
+      if (VECTOR_MODE_P (mode))
+	{
+	  /* Vector NOT.  */
+	  *cost += extra_cost->vect.alu;
+	  return false;
+	}
+
+      /* MVN-shifted-reg.  */
+      if (op0 != x)
+        {
+          *cost += rtx_cost (op0, (enum rtx_code) code, 0, speed);
+
+          if (speed)
+            *cost += extra_cost->alu.log_shift;
+
+          return true;
+        }
+      /* EON can have two forms: (xor (not a) b) but also (not (xor a b)).
+         Handle the second form here taking care that 'a' in the above can
+         be a shift.  */
+      else if (GET_CODE (op0) == XOR)
+        {
+          rtx newop0 = XEXP (op0, 0);
+          rtx newop1 = XEXP (op0, 1);
+          rtx op0_stripped = aarch64_strip_shift (newop0);
+
+          *cost += rtx_cost (newop1, (enum rtx_code) code, 1, speed)
+                   + rtx_cost (op0_stripped, XOR, 0, speed);
+
+          if (speed)
+            {
+              if (op0_stripped != newop0)
+                *cost += extra_cost->alu.log_shift;
+              else
+                *cost += extra_cost->alu.logical;
+            }
+
+          return true;
+        }
       /* MVN.  */
       if (speed)
 	*cost += extra_cost->alu.logical;
 
-      /* The logical instruction could have the shifted register form,
-         but the cost is the same if the shift is processed as a separate
-         instruction, so we don't bother with it here.  */
       return false;
 
     case ZERO_EXTEND:
@@ -6067,10 +6300,19 @@ cost_plus:
 	  return true;
 	}
 
-      /* UXTB/UXTH.  */
       if (speed)
-	*cost += extra_cost->alu.extend;
-
+	{
+	  if (VECTOR_MODE_P (mode))
+	    {
+	      /* UMOV.  */
+	      *cost += extra_cost->vect.alu;
+	    }
+	  else
+	    {
+	      /* UXTB/UXTH.  */
+	      *cost += extra_cost->alu.extend;
+	    }
+	}
       return false;
 
     case SIGN_EXTEND:
@@ -6090,7 +6332,12 @@ cost_plus:
 	}
 
       if (speed)
-	*cost += extra_cost->alu.extend;
+	{
+	  if (VECTOR_MODE_P (mode))
+	    *cost += extra_cost->vect.alu;
+	  else
+	    *cost += extra_cost->alu.extend;
+	}
       return false;
 
     case ASHIFT:
@@ -6099,10 +6346,20 @@ cost_plus:
 
       if (CONST_INT_P (op1))
         {
-	  /* LSL (immediate), UBMF, UBFIZ and friends.  These are all
-	     aliases.  */
 	  if (speed)
-	    *cost += extra_cost->alu.shift;
+	    {
+	      if (VECTOR_MODE_P (mode))
+		{
+		  /* Vector shift (immediate).  */
+		  *cost += extra_cost->vect.alu;
+		}
+	      else
+		{
+		  /* LSL (immediate), UBMF, UBFIZ and friends.  These are all
+		     aliases.  */
+		  *cost += extra_cost->alu.shift;
+		}
+	    }
 
           /* We can incorporate zero/sign extend for free.  */
           if (GET_CODE (op0) == ZERO_EXTEND
@@ -6114,10 +6371,19 @@ cost_plus:
         }
       else
         {
-	  /* LSLV.  */
 	  if (speed)
-	    *cost += extra_cost->alu.shift_reg;
-
+	    {
+	      if (VECTOR_MODE_P (mode))
+		{
+		  /* Vector shift (register).  */
+		  *cost += extra_cost->vect.alu;
+		}
+	      else
+		{
+		  /* LSLV.  */
+		  *cost += extra_cost->alu.shift_reg;
+		}
+	    }
 	  return false;  /* All arguments need to be in registers.  */
         }
 
@@ -6132,7 +6398,12 @@ cost_plus:
 	{
 	  /* ASR (immediate) and friends.  */
 	  if (speed)
-	    *cost += extra_cost->alu.shift;
+	    {
+	      if (VECTOR_MODE_P (mode))
+		*cost += extra_cost->vect.alu;
+	      else
+		*cost += extra_cost->alu.shift;
+	    }
 
 	  *cost += rtx_cost (op0, (enum rtx_code) code, 0, speed);
 	  return true;
@@ -6142,8 +6413,12 @@ cost_plus:
 
 	  /* ASR (register) and friends.  */
 	  if (speed)
-	    *cost += extra_cost->alu.shift_reg;
-
+	    {
+	      if (VECTOR_MODE_P (mode))
+		*cost += extra_cost->vect.alu;
+	      else
+		*cost += extra_cost->alu.shift_reg;
+	    }
 	  return false;  /* All arguments need to be in registers.  */
 	}
 
@@ -6191,7 +6466,12 @@ cost_plus:
     case SIGN_EXTRACT:
       /* UBFX/SBFX.  */
       if (speed)
-	*cost += extra_cost->alu.bfx;
+	{
+	  if (VECTOR_MODE_P (mode))
+	    *cost += extra_cost->vect.alu;
+	  else
+	    *cost += extra_cost->alu.bfx;
+	}
 
       /* We can trust that the immediates used will be correct (there
 	 are no by-register forms), so we need only cost op0.  */
@@ -6208,7 +6488,9 @@ cost_plus:
     case UMOD:
       if (speed)
 	{
-	  if (GET_MODE_CLASS (GET_MODE (x)) == MODE_INT)
+	  if (VECTOR_MODE_P (mode))
+	    *cost += extra_cost->vect.alu;
+	  else if (GET_MODE_CLASS (GET_MODE (x)) == MODE_INT)
 	    *cost += (extra_cost->mult[GET_MODE (x) == DImode].add
 		      + extra_cost->mult[GET_MODE (x) == DImode].idiv);
 	  else if (GET_MODE (x) == DFmode)
@@ -6225,7 +6507,9 @@ cost_plus:
     case SQRT:
       if (speed)
 	{
-	  if (GET_MODE_CLASS (mode) == MODE_INT)
+	  if (VECTOR_MODE_P (mode))
+	    *cost += extra_cost->vect.alu;
+	  else if (GET_MODE_CLASS (mode) == MODE_INT)
 	    /* There is no integer SQRT, so only DIV and UDIV can get
 	       here.  */
 	    *cost += extra_cost->mult[mode == DImode].idiv;
@@ -6257,7 +6541,12 @@ cost_plus:
       op2 = XEXP (x, 2);
 
       if (speed)
-	*cost += extra_cost->fp[mode == DFmode].fma;
+	{
+	  if (VECTOR_MODE_P (mode))
+	    *cost += extra_cost->vect.alu;
+	  else
+	    *cost += extra_cost->fp[mode == DFmode].fma;
+	}
 
       /* FMSUB, FNMADD, and FNMSUB are free.  */
       if (GET_CODE (op0) == NEG)
@@ -6295,14 +6584,36 @@ cost_plus:
       *cost += rtx_cost (op2, FMA, 2, speed);
       return true;
 
+    case FLOAT:
+    case UNSIGNED_FLOAT:
+      if (speed)
+	*cost += extra_cost->fp[mode == DFmode].fromint;
+      return false;
+
     case FLOAT_EXTEND:
       if (speed)
-	*cost += extra_cost->fp[mode == DFmode].widen;
+	{
+	  if (VECTOR_MODE_P (mode))
+	    {
+	      /*Vector truncate.  */
+	      *cost += extra_cost->vect.alu;
+	    }
+	  else
+	    *cost += extra_cost->fp[mode == DFmode].widen;
+	}
       return false;
 
     case FLOAT_TRUNCATE:
       if (speed)
-	*cost += extra_cost->fp[mode == DFmode].narrow;
+	{
+	  if (VECTOR_MODE_P (mode))
+	    {
+	      /*Vector conversion.  */
+	      *cost += extra_cost->vect.alu;
+	    }
+	  else
+	    *cost += extra_cost->fp[mode == DFmode].narrow;
+	}
       return false;
 
     case FIX:
@@ -6323,15 +6634,37 @@ cost_plus:
         }
 
       if (speed)
-        *cost += extra_cost->fp[GET_MODE (x) == DFmode].toint;
-
+	{
+	  if (VECTOR_MODE_P (mode))
+	    *cost += extra_cost->vect.alu;
+	  else
+	    *cost += extra_cost->fp[GET_MODE (x) == DFmode].toint;
+	}
       *cost += rtx_cost (x, (enum rtx_code) code, 0, speed);
       return true;
 
     case ABS:
-      if (GET_MODE_CLASS (mode) == MODE_FLOAT)
+      if (VECTOR_MODE_P (mode))
 	{
-	  /* FABS and FNEG are analogous.  */
+	  /* ABS (vector).  */
+	  if (speed)
+	    *cost += extra_cost->vect.alu;
+	}
+      else if (GET_MODE_CLASS (mode) == MODE_FLOAT)
+	{
+	  op0 = XEXP (x, 0);
+
+	  /* FABD, which is analogous to FADD.  */
+	  if (GET_CODE (op0) == MINUS)
+	    {
+	      *cost += rtx_cost (XEXP (op0, 0), MINUS, 0, speed);
+			+ rtx_cost (XEXP (op0, 1), MINUS, 1, speed);
+	      if (speed)
+		*cost += extra_cost->fp[mode == DFmode].addsub;
+
+	      return true;
+	    }
+	  /* Simple FABS is analogous to FNEG.  */
 	  if (speed)
 	    *cost += extra_cost->fp[mode == DFmode].neg;
 	}
@@ -6350,10 +6683,15 @@ cost_plus:
     case SMIN:
       if (speed)
 	{
-	  /* FMAXNM/FMINNM/FMAX/FMIN.
-	     TODO: This may not be accurate for all implementations, but
-	     we do not model this in the cost tables.  */
-	  *cost += extra_cost->fp[mode == DFmode].addsub;
+	  if (VECTOR_MODE_P (mode))
+	    *cost += extra_cost->vect.alu;
+	  else
+	    {
+	      /* FMAXNM/FMINNM/FMAX/FMIN.
+	         TODO: This may not be accurate for all implementations, but
+	         we do not model this in the cost tables.  */
+	      *cost += extra_cost->fp[mode == DFmode].addsub;
+	    }
 	}
       return false;
 
@@ -7830,6 +8168,26 @@ aapcs_vfp_sub_candidate (const_tree type, machine_mode *modep)
   return -1;
 }
 
+/* Return TRUE if the type, as described by TYPE and MODE, is a short vector
+   type as described in AAPCS64 \S 4.1.2.
+
+   See the comment above aarch64_composite_type_p for the notes on MODE.  */
+
+static bool
+aarch64_short_vector_p (const_tree type,
+			machine_mode mode)
+{
+  HOST_WIDE_INT size = -1;
+
+  if (type && TREE_CODE (type) == VECTOR_TYPE)
+    size = int_size_in_bytes (type);
+  else if (GET_MODE_CLASS (mode) == MODE_VECTOR_INT
+	    || GET_MODE_CLASS (mode) == MODE_VECTOR_FLOAT)
+    size = GET_MODE_SIZE (mode);
+
+  return (size == 8 || size == 16);
+}
+
 /* Return TRUE if the type, as described by TYPE and MODE, is a composite
    type as described in AAPCS64 \S 4.3.  This includes aggregate, union and
    array types.  The C99 floating-point complex types are also considered
@@ -7851,6 +8209,9 @@ static bool
 aarch64_composite_type_p (const_tree type,
 			  machine_mode mode)
 {
+  if (aarch64_short_vector_p (type, mode))
+    return false;
+
   if (type && (AGGREGATE_TYPE_P (type) || TREE_CODE (type) == COMPLEX_TYPE))
     return true;
 
@@ -7862,27 +8223,6 @@ aarch64_composite_type_p (const_tree type,
   return false;
 }
 
-/* Return TRUE if the type, as described by TYPE and MODE, is a short vector
-   type as described in AAPCS64 \S 4.1.2.
-
-   See the comment above aarch64_composite_type_p for the notes on MODE.  */
-
-static bool
-aarch64_short_vector_p (const_tree type,
-			machine_mode mode)
-{
-  HOST_WIDE_INT size = -1;
-
-  if (type && TREE_CODE (type) == VECTOR_TYPE)
-    size = int_size_in_bytes (type);
-  else if (!aarch64_composite_type_p (type, mode)
-	   && (GET_MODE_CLASS (mode) == MODE_VECTOR_INT
-	       || GET_MODE_CLASS (mode) == MODE_VECTOR_FLOAT))
-    size = GET_MODE_SIZE (mode);
-
-  return (size == 8 || size == 16) ? true : false;
-}
-
 /* Return TRUE if an argument, whose type is described by TYPE and MODE,
    shall be passed or returned in simd/fp register(s) (providing these
    parameter passing registers are available).
@@ -8581,24 +8921,6 @@ aarch64_simd_lane_bounds (rtx operand, HOST_WIDE_INT low, HOST_WIDE_INT high,
   }
 }
 
-/* Emit code to place a AdvSIMD pair result in memory locations (with equal
-   registers).  */
-void
-aarch64_simd_emit_pair_result_insn (machine_mode mode,
-			    rtx (*intfn) (rtx, rtx, rtx), rtx destaddr,
-                            rtx op1)
-{
-  rtx mem = gen_rtx_MEM (mode, destaddr);
-  rtx tmp1 = gen_reg_rtx (mode);
-  rtx tmp2 = gen_reg_rtx (mode);
-
-  emit_insn (intfn (tmp1, op1, tmp2));
-
-  emit_move_insn (mem, tmp1);
-  mem = adjust_address (mem, mode, GET_MODE_SIZE (mode));
-  emit_move_insn (mem, tmp2);
-}
-
 /* Return TRUE if OP is a valid vector addressing mode.  */
 bool
 aarch64_simd_mem_operand_p (rtx op)
@@ -8781,22 +9103,19 @@ aarch64_expand_vector_init (rtx target, rtx vals)
   machine_mode mode = GET_MODE (target);
   machine_mode inner_mode = GET_MODE_INNER (mode);
   int n_elts = GET_MODE_NUNITS (mode);
-  int n_var = 0, one_var = -1;
+  int n_var = 0;
+  rtx any_const = NULL_RTX;
   bool all_same = true;
-  rtx x, mem;
-  int i;
 
-  x = XVECEXP (vals, 0, 0);
-  if (!CONST_INT_P (x) && !CONST_DOUBLE_P (x))
-    n_var = 1, one_var = 0;
-  
-  for (i = 1; i < n_elts; ++i)
+  for (int i = 0; i < n_elts; ++i)
     {
-      x = XVECEXP (vals, 0, i);
+      rtx x = XVECEXP (vals, 0, i);
       if (!CONST_INT_P (x) && !CONST_DOUBLE_P (x))
-	++n_var, one_var = i;
+	++n_var;
+      else
+	any_const = x;
 
-      if (!rtx_equal_p (x, XVECEXP (vals, 0, 0)))
+      if (i > 0 && !rtx_equal_p (x, XVECEXP (vals, 0, 0)))
 	all_same = false;
     }
 
@@ -8813,36 +9132,60 @@ aarch64_expand_vector_init (rtx target, rtx vals)
   /* Splat a single non-constant element if we can.  */
   if (all_same)
     {
-      x = copy_to_mode_reg (inner_mode, XVECEXP (vals, 0, 0));
+      rtx x = copy_to_mode_reg (inner_mode, XVECEXP (vals, 0, 0));
       aarch64_emit_move (target, gen_rtx_VEC_DUPLICATE (mode, x));
       return;
     }
 
-  /* One field is non-constant.  Load constant then overwrite varying
-     field.  This is more efficient than using the stack.  */
-  if (n_var == 1)
+  /* Half the fields (or less) are non-constant.  Load constant then overwrite
+     varying fields.  Hope that this is more efficient than using the stack.  */
+  if (n_var <= n_elts/2)
     {
       rtx copy = copy_rtx (vals);
-      rtx index = GEN_INT (one_var);
-      enum insn_code icode;
 
-      /* Load constant part of vector, substitute neighboring value for
-	 varying element.  */
-      XVECEXP (copy, 0, one_var) = XVECEXP (vals, 0, one_var ^ 1);
+      /* Load constant part of vector.  We really don't care what goes into the
+	 parts we will overwrite, but we're more likely to be able to load the
+	 constant efficiently if it has fewer, larger, repeating parts
+	 (see aarch64_simd_valid_immediate).  */
+      for (int i = 0; i < n_elts; i++)
+	{
+	  rtx x = XVECEXP (vals, 0, i);
+	  if (CONST_INT_P (x) || CONST_DOUBLE_P (x))
+	    continue;
+	  rtx subst = any_const;
+	  for (int bit = n_elts / 2; bit > 0; bit /= 2)
+	    {
+	      /* Look in the copied vector, as more elements are const.  */
+	      rtx test = XVECEXP (copy, 0, i ^ bit);
+	      if (CONST_INT_P (test) || CONST_DOUBLE_P (test))
+		{
+		  subst = test;
+		  break;
+		}
+	    }
+	  XVECEXP (copy, 0, i) = subst;
+	}
       aarch64_expand_vector_init (target, copy);
 
-      /* Insert variable.  */
-      x = copy_to_mode_reg (inner_mode, XVECEXP (vals, 0, one_var));
-      icode = optab_handler (vec_set_optab, mode);
+      /* Insert variables.  */
+      enum insn_code icode = optab_handler (vec_set_optab, mode);
       gcc_assert (icode != CODE_FOR_nothing);
-      emit_insn (GEN_FCN (icode) (target, x, index));
+
+      for (int i = 0; i < n_elts; i++)
+	{
+	  rtx x = XVECEXP (vals, 0, i);
+	  if (CONST_INT_P (x) || CONST_DOUBLE_P (x))
+	    continue;
+	  x = copy_to_mode_reg (inner_mode, x);
+	  emit_insn (GEN_FCN (icode) (target, x, GEN_INT (i)));
+	}
       return;
     }
 
   /* Construct the vector in memory one field at a time
      and load the whole vector.  */
-  mem = assign_stack_temp (mode, GET_MODE_SIZE (mode));
-  for (i = 0; i < n_elts; i++)
+  rtx mem = assign_stack_temp (mode, GET_MODE_SIZE (mode));
+  for (int i = 0; i < n_elts; i++)
     emit_move_insn (adjust_address_nv (mem, inner_mode,
 				    i * GET_MODE_SIZE (inner_mode)),
 		    XVECEXP (vals, 0, i));
--- a/src/gcc/config/aarch64/aarch64.h
+++ b/src/gcc/config/aarch64/aarch64.h
@@ -506,7 +506,7 @@ enum reg_class
 
 enum target_cpus
 {
-#define AARCH64_CORE(NAME, INTERNAL_IDENT, SCHED, ARCH, FLAGS, COSTS) \
+#define AARCH64_CORE(NAME, INTERNAL_IDENT, SCHED, ARCH, FLAGS, COSTS, IMP, PART) \
   TARGET_CPU_##INTERNAL_IDENT,
 #include "aarch64-cores.def"
 #undef AARCH64_CORE
@@ -823,7 +823,8 @@ do {									     \
 #define TRAMPOLINE_SECTION text_section
 
 /* To start with.  */
-#define BRANCH_COST(SPEED_P, PREDICTABLE_P) 2
+#define BRANCH_COST(SPEED_P, PREDICTABLE_P) \
+  (aarch64_branch_cost (SPEED_P, PREDICTABLE_P))
 
 
 /* Assembly output.  */
@@ -929,11 +930,24 @@ extern const char *aarch64_rewrite_mcpu (int argc, const char **argv);
 #define BIG_LITTLE_CPU_SPEC_FUNCTIONS \
   { "rewrite_mcpu", aarch64_rewrite_mcpu },
 
+#if defined(__aarch64__)
+extern const char *host_detect_local_cpu (int argc, const char **argv);
+# define EXTRA_SPEC_FUNCTIONS						\
+  { "local_cpu_detect", host_detect_local_cpu },			\
+  BIG_LITTLE_CPU_SPEC_FUNCTIONS
+
+# define MCPU_MTUNE_NATIVE_SPECS					\
+   " %{march=native:%<march=native %:local_cpu_detect(arch)}"		\
+   " %{mcpu=native:%<mcpu=native %:local_cpu_detect(cpu)}"		\
+   " %{mtune=native:%<mtune=native %:local_cpu_detect(tune)}"
+#else
+# define MCPU_MTUNE_NATIVE_SPECS ""
+# define EXTRA_SPEC_FUNCTIONS BIG_LITTLE_CPU_SPEC_FUNCTIONS
+#endif
+
 #define ASM_CPU_SPEC \
    BIG_LITTLE_SPEC
 
-#define EXTRA_SPEC_FUNCTIONS BIG_LITTLE_CPU_SPEC_FUNCTIONS
-
 #define EXTRA_SPECS						\
   { "asm_cpu_spec",		ASM_CPU_SPEC }
 
--- a/src/gcc/config/aarch64/aarch64.md
+++ b/src/gcc/config/aarch64/aarch64.md
@@ -1414,18 +1414,28 @@
   "
   if (! aarch64_plus_operand (operands[2], VOIDmode))
     {
-      rtx subtarget = ((optimize && can_create_pseudo_p ())
-		       ? gen_reg_rtx (<MODE>mode) : operands[0]);
       HOST_WIDE_INT imm = INTVAL (operands[2]);
 
-      if (imm < 0)
-	imm = -(-imm & ~0xfff);
+      if (aarch64_move_imm (imm, <MODE>mode) && can_create_pseudo_p ())
+        {
+	  rtx tmp = gen_reg_rtx (<MODE>mode);
+	  emit_move_insn (tmp, operands[2]);
+	  operands[2] = tmp;
+        }
       else
-        imm &= ~0xfff;
-
-      emit_insn (gen_add<mode>3 (subtarget, operands[1], GEN_INT (imm)));
-      operands[1] = subtarget;
-      operands[2] = GEN_INT (INTVAL (operands[2]) - imm);
+        {
+	  rtx subtarget = ((optimize && can_create_pseudo_p ())
+			   ? gen_reg_rtx (<MODE>mode) : operands[0]);
+
+	  if (imm < 0)
+	    imm = -(-imm & ~0xfff);
+	  else
+	    imm &= ~0xfff;
+
+	  emit_insn (gen_add<mode>3 (subtarget, operands[1], GEN_INT (imm)));
+	  operands[1] = subtarget;
+	  operands[2] = GEN_INT (INTVAL (operands[2]) - imm);
+        }
     }
   "
 )
@@ -1529,6 +1539,38 @@
   [(set_attr "type" "alus_sreg,alus_imm,alus_imm")]
 )
 
+(define_insn "*adds_shift_imm_<mode>"
+  [(set (reg:CC_NZ CC_REGNUM)
+	(compare:CC_NZ
+	 (plus:GPI (ASHIFT:GPI 
+		    (match_operand:GPI 1 "register_operand" "r")
+		    (match_operand:QI 2 "aarch64_shift_imm_<mode>" "n"))
+		   (match_operand:GPI 3 "register_operand" "r"))
+	 (const_int 0)))
+   (set (match_operand:GPI 0 "register_operand" "=r")
+	(plus:GPI (ASHIFT:GPI (match_dup 1) (match_dup 2))
+		  (match_dup 3)))]
+  ""
+  "adds\\t%<w>0, %<w>3, %<w>1, <shift> %2"
+  [(set_attr "type" "alus_shift_imm")]
+)
+
+(define_insn "*subs_shift_imm_<mode>"
+  [(set (reg:CC_NZ CC_REGNUM)
+	(compare:CC_NZ
+	 (minus:GPI (match_operand:GPI 1 "register_operand" "r")
+		    (ASHIFT:GPI
+		     (match_operand:GPI 2 "register_operand" "r")
+		     (match_operand:QI 3 "aarch64_shift_imm_<mode>" "n")))
+	 (const_int 0)))
+   (set (match_operand:GPI 0 "register_operand" "=r")
+	(minus:GPI (match_dup 1)
+		   (ASHIFT:GPI (match_dup 2) (match_dup 3))))]
+  ""
+  "subs\\t%<w>0, %<w>1, %<w>2, <shift> %3"
+  [(set_attr "type" "alus_shift_imm")]
+)
+
 (define_insn "*adds_mul_imm_<mode>"
   [(set (reg:CC_NZ CC_REGNUM)
 	(compare:CC_NZ
@@ -1589,6 +1631,42 @@
   [(set_attr "type" "alus_ext")]
 )
 
+(define_insn "*adds_<optab><ALLX:mode>_shift_<GPI:mode>"
+  [(set (reg:CC_NZ CC_REGNUM)
+	(compare:CC_NZ
+	 (plus:GPI (ashift:GPI 
+		    (ANY_EXTEND:GPI 
+		     (match_operand:ALLX 1 "register_operand" "r"))
+		    (match_operand 2 "aarch64_imm3" "Ui3"))
+		   (match_operand:GPI 3 "register_operand" "r"))
+	 (const_int 0)))
+   (set (match_operand:GPI 0 "register_operand" "=rk")
+	(plus:GPI (ashift:GPI (ANY_EXTEND:GPI (match_dup 1))
+			      (match_dup 2))
+		  (match_dup 3)))]
+  ""
+  "adds\\t%<GPI:w>0, %<GPI:w>3, %<GPI:w>1, <su>xt<ALLX:size> %2"
+  [(set_attr "type" "alus_ext")]
+)
+
+(define_insn "*subs_<optab><ALLX:mode>_shift_<GPI:mode>"
+  [(set (reg:CC_NZ CC_REGNUM)
+	(compare:CC_NZ
+	 (minus:GPI (match_operand:GPI 1 "register_operand" "r")
+		    (ashift:GPI 
+		     (ANY_EXTEND:GPI
+		      (match_operand:ALLX 2 "register_operand" "r"))
+		     (match_operand 3 "aarch64_imm3" "Ui3")))
+	 (const_int 0)))
+   (set (match_operand:GPI 0 "register_operand" "=rk")
+	(minus:GPI (match_dup 1)
+		   (ashift:GPI (ANY_EXTEND:GPI (match_dup 2))
+			       (match_dup 3))))]
+  ""
+  "subs\\t%<GPI:w>0, %<GPI:w>1, %<GPI:w>2, <su>xt<ALLX:size> %3"
+  [(set_attr "type" "alus_ext")]
+)
+
 (define_insn "*adds_<optab><mode>_multp2"
   [(set (reg:CC_NZ CC_REGNUM)
 	(compare:CC_NZ
@@ -1884,6 +1962,38 @@
   [(set_attr "type" "adc_reg")]
 )
 
+(define_insn "*add_uxt<mode>_shift2"
+  [(set (match_operand:GPI 0 "register_operand" "=rk")
+	(plus:GPI (and:GPI
+		   (ashift:GPI (match_operand:GPI 1 "register_operand" "r")
+			       (match_operand 2 "aarch64_imm3" "Ui3"))
+		   (match_operand 3 "const_int_operand" "n"))
+		  (match_operand:GPI 4 "register_operand" "r")))]
+  "aarch64_uxt_size (INTVAL (operands[2]), INTVAL (operands[3])) != 0"
+  "*
+  operands[3] = GEN_INT (aarch64_uxt_size (INTVAL(operands[2]),
+					   INTVAL (operands[3])));
+  return \"add\t%<w>0, %<w>4, %<w>1, uxt%e3 %2\";"
+  [(set_attr "type" "alu_ext")]
+)
+
+;; zero_extend version of above
+(define_insn "*add_uxtsi_shift2_uxtw"
+  [(set (match_operand:DI 0 "register_operand" "=rk")
+	(zero_extend:DI
+	 (plus:SI (and:SI
+		   (ashift:SI (match_operand:SI 1 "register_operand" "r")
+			      (match_operand 2 "aarch64_imm3" "Ui3"))
+		   (match_operand 3 "const_int_operand" "n"))
+		  (match_operand:SI 4 "register_operand" "r"))))]
+  "aarch64_uxt_size (INTVAL (operands[2]), INTVAL (operands[3])) != 0"
+  "*
+  operands[3] = GEN_INT (aarch64_uxt_size (INTVAL (operands[2]),
+					   INTVAL (operands[3])));
+  return \"add\t%w0, %w4, %w1, uxt%e3 %2\";"
+  [(set_attr "type" "alu_ext")]
+)
+
 (define_insn "*add_uxt<mode>_multp2"
   [(set (match_operand:GPI 0 "register_operand" "=rk")
 	(plus:GPI (and:GPI
@@ -2140,6 +2250,38 @@
   [(set_attr "type" "adc_reg")]
 )
 
+(define_insn "*sub_uxt<mode>_shift2"
+  [(set (match_operand:GPI 0 "register_operand" "=rk")
+	(minus:GPI (match_operand:GPI 4 "register_operand" "rk")
+		   (and:GPI
+		    (ashift:GPI (match_operand:GPI 1 "register_operand" "r")
+				(match_operand 2 "aarch64_imm3" "Ui3"))
+		    (match_operand 3 "const_int_operand" "n"))))]
+  "aarch64_uxt_size (INTVAL (operands[2]),INTVAL (operands[3])) != 0"
+  "*
+  operands[3] = GEN_INT (aarch64_uxt_size (INTVAL (operands[2]),
+					   INTVAL (operands[3])));
+  return \"sub\t%<w>0, %<w>4, %<w>1, uxt%e3 %2\";"
+  [(set_attr "type" "alu_ext")]
+)
+
+;; zero_extend version of above
+(define_insn "*sub_uxtsi_shift2_uxtw"
+  [(set (match_operand:DI 0 "register_operand" "=rk")
+	(zero_extend:DI
+	 (minus:SI (match_operand:SI 4 "register_operand" "rk")
+		   (and:SI
+		    (ashift:SI (match_operand:SI 1 "register_operand" "r")
+			       (match_operand 2 "aarch64_imm3" "Ui3"))
+		    (match_operand 3 "const_int_operand" "n")))))]
+  "aarch64_uxt_size (INTVAL (operands[2]),INTVAL (operands[3])) != 0"
+  "*
+  operands[3] = GEN_INT (aarch64_uxt_size (INTVAL (operands[2]),
+					   INTVAL (operands[3])));
+  return \"sub\t%w0, %w4, %w1, uxt%e3 %2\";"
+  [(set_attr "type" "alu_ext")]
+)
+
 (define_insn "*sub_uxt<mode>_multp2"
   [(set (match_operand:GPI 0 "register_operand" "=rk")
 	(minus:GPI (match_operand:GPI 4 "register_operand" "rk")
@@ -3058,6 +3200,26 @@
    (set_attr "simd" "*,yes")]
 )
 
+(define_insn "*<NLOGICAL:optab>_one_cmplsidi3_ze"
+  [(set (match_operand:DI 0 "register_operand" "=r")
+	(zero_extend:DI
+	  (NLOGICAL:SI (not:SI (match_operand:SI 1 "register_operand" "r"))
+	               (match_operand:SI 2 "register_operand" "r"))))]
+  ""
+  "<NLOGICAL:nlogical>\\t%w0, %w2, %w1"
+  [(set_attr "type" "logic_reg")]
+)
+
+(define_insn "*xor_one_cmplsidi3_ze"
+  [(set (match_operand:DI 0 "register_operand" "=r")
+        (zero_extend:DI
+          (not:SI (xor:SI (match_operand:SI 1 "register_operand" "r")
+                          (match_operand:SI 2 "register_operand" "r")))))]
+  ""
+  "eon\\t%w0, %w1, %w2"
+  [(set_attr "type" "logic_reg")]
+)
+
 ;; (xor (not a) b) is simplify_rtx-ed down to (not (xor a b)).
 ;; eon does not operate on SIMD registers so the vector variant must be split.
 (define_insn_and_split "*xor_one_cmpl<mode>3"
@@ -3131,6 +3293,32 @@
   [(set_attr "type" "logics_shift_imm")]
 )
 
+(define_insn "*eor_one_cmpl_<SHIFT:optab><mode>3_alt"
+  [(set (match_operand:GPI 0 "register_operand" "=r")
+	(not:GPI (xor:GPI
+		      (SHIFT:GPI
+		       (match_operand:GPI 1 "register_operand" "r")
+		       (match_operand:QI 2 "aarch64_shift_imm_<mode>" "n"))
+		     (match_operand:GPI 3 "register_operand" "r"))))]
+  ""
+  "eon\\t%<w>0, %<w>3, %<w>1, <SHIFT:shift> %2"
+  [(set_attr "type" "logic_shift_imm")]
+)
+
+;; Zero-extend version of the above.
+(define_insn "*eor_one_cmpl_<SHIFT:optab>sidi3_alt_ze"
+  [(set (match_operand:DI 0 "register_operand" "=r")
+	(zero_extend:DI
+	  (not:SI (xor:SI
+		    (SHIFT:SI
+		      (match_operand:SI 1 "register_operand" "r")
+		      (match_operand:QI 2 "aarch64_shift_imm_si" "n"))
+		    (match_operand:SI 3 "register_operand" "r")))))]
+  ""
+  "eon\\t%w0, %w3, %w1, <SHIFT:shift> %2"
+  [(set_attr "type" "logic_shift_imm")]
+)
+
 (define_insn "*and_one_cmpl_<SHIFT:optab><mode>3_compare0"
   [(set (reg:CC_NZ CC_REGNUM)
 	(compare:CC_NZ
@@ -3551,6 +3739,21 @@
   [(set_attr "type" "shift_imm")]
 )
 
+;; There are no canonicalisation rules for ashift and lshiftrt inside an ior
+;; so we have to match both orderings.
+(define_insn "*extr<mode>5_insn_alt"
+  [(set (match_operand:GPI 0 "register_operand" "=r")
+	(ior:GPI  (lshiftrt:GPI (match_operand:GPI 2 "register_operand" "r")
+			        (match_operand 4 "const_int_operand" "n"))
+		  (ashift:GPI (match_operand:GPI 1 "register_operand" "r")
+			      (match_operand 3 "const_int_operand" "n"))))]
+  "UINTVAL (operands[3]) < GET_MODE_BITSIZE (<MODE>mode)
+   && (UINTVAL (operands[3]) + UINTVAL (operands[4])
+       == GET_MODE_BITSIZE (<MODE>mode))"
+  "extr\\t%<w>0, %<w>1, %<w>2, %4"
+  [(set_attr "type" "shift_imm")]
+)
+
 ;; zero_extend version of the above
 (define_insn "*extrsi5_insn_uxtw"
   [(set (match_operand:DI 0 "register_operand" "=r")
@@ -3565,6 +3768,19 @@
   [(set_attr "type" "shift_imm")]
 )
 
+(define_insn "*extrsi5_insn_uxtw_alt"
+  [(set (match_operand:DI 0 "register_operand" "=r")
+	(zero_extend:DI
+	 (ior:SI (lshiftrt:SI (match_operand:SI 2 "register_operand" "r")
+			       (match_operand 4 "const_int_operand" "n"))
+		 (ashift:SI (match_operand:SI 1 "register_operand" "r")
+			    (match_operand 3 "const_int_operand" "n")))))]
+  "UINTVAL (operands[3]) < 32 &&
+   (UINTVAL (operands[3]) + UINTVAL (operands[4]) == 32)"
+  "extr\\t%w0, %w1, %w2, %4"
+  [(set_attr "type" "shift_imm")]
+)
+
 (define_insn "*ror<mode>3_insn"
   [(set (match_operand:GPI 0 "register_operand" "=r")
 	(rotate:GPI (match_operand:GPI 1 "register_operand" "r")
--- a/src/gcc/config/aarch64/arm_neon.h
+++ b/src/gcc/config/aarch64/arm_neon.h
@@ -5665,8 +5665,6 @@ vaddlvq_u32 (uint32x4_t a)
 
 /* vcvt_high_f32_f16 not supported */
 
-static float32x2_t vdup_n_f32 (float32_t);
-
 #define vcvt_n_f32_s32(a, b)                                            \
   __extension__                                                         \
     ({                                                                  \
@@ -9824,272 +9822,6 @@ vrsqrtss_f32 (float32_t a, float32_t b)
        result;                                                          \
      })
 
-#define vst1_lane_f32(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       float32x2_t b_ = (b);                                            \
-       float32_t * a_ = (a);                                            \
-       __asm__ ("st1 {%1.s}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
-
-#define vst1_lane_f64(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       float64x1_t b_ = (b);                                            \
-       float64_t * a_ = (a);                                            \
-       __asm__ ("st1 {%1.d}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
-
-#define vst1_lane_p8(a, b, c)                                           \
-  __extension__                                                         \
-    ({                                                                  \
-       poly8x8_t b_ = (b);                                              \
-       poly8_t * a_ = (a);                                              \
-       __asm__ ("st1 {%1.b}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
-
-#define vst1_lane_p16(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       poly16x4_t b_ = (b);                                             \
-       poly16_t * a_ = (a);                                             \
-       __asm__ ("st1 {%1.h}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
-
-#define vst1_lane_s8(a, b, c)                                           \
-  __extension__                                                         \
-    ({                                                                  \
-       int8x8_t b_ = (b);                                               \
-       int8_t * a_ = (a);                                               \
-       __asm__ ("st1 {%1.b}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
-
-#define vst1_lane_s16(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       int16x4_t b_ = (b);                                              \
-       int16_t * a_ = (a);                                              \
-       __asm__ ("st1 {%1.h}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
-
-#define vst1_lane_s32(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       int32x2_t b_ = (b);                                              \
-       int32_t * a_ = (a);                                              \
-       __asm__ ("st1 {%1.s}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
-
-#define vst1_lane_s64(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       int64x1_t b_ = (b);                                              \
-       int64_t * a_ = (a);                                              \
-       __asm__ ("st1 {%1.d}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
-
-#define vst1_lane_u8(a, b, c)                                           \
-  __extension__                                                         \
-    ({                                                                  \
-       uint8x8_t b_ = (b);                                              \
-       uint8_t * a_ = (a);                                              \
-       __asm__ ("st1 {%1.b}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
-
-#define vst1_lane_u16(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       uint16x4_t b_ = (b);                                             \
-       uint16_t * a_ = (a);                                             \
-       __asm__ ("st1 {%1.h}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
-
-#define vst1_lane_u32(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       uint32x2_t b_ = (b);                                             \
-       uint32_t * a_ = (a);                                             \
-       __asm__ ("st1 {%1.s}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
-
-#define vst1_lane_u64(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       uint64x1_t b_ = (b);                                             \
-       uint64_t * a_ = (a);                                             \
-       __asm__ ("st1 {%1.d}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
-
-
-#define vst1q_lane_f32(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       float32x4_t b_ = (b);                                            \
-       float32_t * a_ = (a);                                            \
-       __asm__ ("st1 {%1.s}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
-
-#define vst1q_lane_f64(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       float64x2_t b_ = (b);                                            \
-       float64_t * a_ = (a);                                            \
-       __asm__ ("st1 {%1.d}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
-
-#define vst1q_lane_p8(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       poly8x16_t b_ = (b);                                             \
-       poly8_t * a_ = (a);                                              \
-       __asm__ ("st1 {%1.b}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
-
-#define vst1q_lane_p16(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       poly16x8_t b_ = (b);                                             \
-       poly16_t * a_ = (a);                                             \
-       __asm__ ("st1 {%1.h}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
-
-#define vst1q_lane_s8(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       int8x16_t b_ = (b);                                              \
-       int8_t * a_ = (a);                                               \
-       __asm__ ("st1 {%1.b}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
-
-#define vst1q_lane_s16(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       int16x8_t b_ = (b);                                              \
-       int16_t * a_ = (a);                                              \
-       __asm__ ("st1 {%1.h}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
-
-#define vst1q_lane_s32(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       int32x4_t b_ = (b);                                              \
-       int32_t * a_ = (a);                                              \
-       __asm__ ("st1 {%1.s}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
-
-#define vst1q_lane_s64(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       int64x2_t b_ = (b);                                              \
-       int64_t * a_ = (a);                                              \
-       __asm__ ("st1 {%1.d}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
-
-#define vst1q_lane_u8(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       uint8x16_t b_ = (b);                                             \
-       uint8_t * a_ = (a);                                              \
-       __asm__ ("st1 {%1.b}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
-
-#define vst1q_lane_u16(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       uint16x8_t b_ = (b);                                             \
-       uint16_t * a_ = (a);                                             \
-       __asm__ ("st1 {%1.h}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
-
-#define vst1q_lane_u32(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       uint32x4_t b_ = (b);                                             \
-       uint32_t * a_ = (a);                                             \
-       __asm__ ("st1 {%1.s}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
-
-#define vst1q_lane_u64(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       uint64x2_t b_ = (b);                                             \
-       uint64_t * a_ = (a);                                             \
-       __asm__ ("st1 {%1.d}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
-
-
 __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
 vtst_p8 (poly8x8_t a, poly8x8_t b)
 {
@@ -11668,25 +11400,25 @@ vbslq_u64 (uint64x2_t __a, uint64x2_t __b, uint64x2_t __c)
 
 /* vaes  */
 
-static __inline uint8x16_t
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
 vaeseq_u8 (uint8x16_t data, uint8x16_t key)
 {
   return __builtin_aarch64_crypto_aesev16qi_uuu (data, key);
 }
 
-static __inline uint8x16_t
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
 vaesdq_u8 (uint8x16_t data, uint8x16_t key)
 {
   return __builtin_aarch64_crypto_aesdv16qi_uuu (data, key);
 }
 
-static __inline uint8x16_t
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
 vaesmcq_u8 (uint8x16_t data)
 {
   return __builtin_aarch64_crypto_aesmcv16qi_uu (data);
 }
 
-static __inline uint8x16_t
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
 vaesimcq_u8 (uint8x16_t data)
 {
   return __builtin_aarch64_crypto_aesimcv16qi_uu (data);
@@ -11887,7 +11619,7 @@ vceq_s32 (int32x2_t __a, int32x2_t __b)
 __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
 vceq_s64 (int64x1_t __a, int64x1_t __b)
 {
-  return (uint64x1_t) {__a[0] == __b[0] ? -1ll : 0ll};
+  return (uint64x1_t) (__a == __b);
 }
 
 __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
@@ -11911,7 +11643,7 @@ vceq_u32 (uint32x2_t __a, uint32x2_t __b)
 __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
 vceq_u64 (uint64x1_t __a, uint64x1_t __b)
 {
-  return (uint64x1_t) {__a[0] == __b[0] ? -1ll : 0ll};
+  return (__a == __b);
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
@@ -12047,7 +11779,7 @@ vceqz_s32 (int32x2_t __a)
 __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
 vceqz_s64 (int64x1_t __a)
 {
-  return (uint64x1_t) {__a[0] == 0ll ? -1ll : 0ll};
+  return (uint64x1_t) (__a == __AARCH64_INT64_C (0));
 }
 
 __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
@@ -12071,7 +11803,7 @@ vceqz_u32 (uint32x2_t __a)
 __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
 vceqz_u64 (uint64x1_t __a)
 {
-  return (uint64x1_t) {__a[0] == 0ll ? -1ll : 0ll};
+  return (__a == __AARCH64_UINT64_C (0));
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
@@ -12201,7 +11933,7 @@ vcge_s32 (int32x2_t __a, int32x2_t __b)
 __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
 vcge_s64 (int64x1_t __a, int64x1_t __b)
 {
-  return (uint64x1_t) {__a[0] >= __b[0] ? -1ll : 0ll};
+  return (uint64x1_t) (__a >= __b);
 }
 
 __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
@@ -12225,7 +11957,7 @@ vcge_u32 (uint32x2_t __a, uint32x2_t __b)
 __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
 vcge_u64 (uint64x1_t __a, uint64x1_t __b)
 {
-  return (uint64x1_t) {__a[0] >= __b[0] ? -1ll : 0ll};
+  return (__a >= __b);
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
@@ -12349,7 +12081,7 @@ vcgez_s32 (int32x2_t __a)
 __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
 vcgez_s64 (int64x1_t __a)
 {
-  return (uint64x1_t) {__a[0] >= 0ll ? -1ll : 0ll};
+  return (uint64x1_t) (__a >= __AARCH64_INT64_C (0));
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
@@ -12443,7 +12175,7 @@ vcgt_s32 (int32x2_t __a, int32x2_t __b)
 __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
 vcgt_s64 (int64x1_t __a, int64x1_t __b)
 {
-  return (uint64x1_t) (__a[0] > __b[0] ? -1ll : 0ll);
+  return (uint64x1_t) (__a > __b);
 }
 
 __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
@@ -12467,7 +12199,7 @@ vcgt_u32 (uint32x2_t __a, uint32x2_t __b)
 __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
 vcgt_u64 (uint64x1_t __a, uint64x1_t __b)
 {
-  return (uint64x1_t) (__a[0] > __b[0] ? -1ll : 0ll);
+  return (__a > __b);
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
@@ -12591,7 +12323,7 @@ vcgtz_s32 (int32x2_t __a)
 __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
 vcgtz_s64 (int64x1_t __a)
 {
-  return (uint64x1_t) {__a[0] > 0ll ? -1ll : 0ll};
+  return (uint64x1_t) (__a > __AARCH64_INT64_C (0));
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
@@ -12685,7 +12417,7 @@ vcle_s32 (int32x2_t __a, int32x2_t __b)
 __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
 vcle_s64 (int64x1_t __a, int64x1_t __b)
 {
-  return (uint64x1_t) {__a[0] <= __b[0] ? -1ll : 0ll};
+  return (uint64x1_t) (__a <= __b);
 }
 
 __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
@@ -12709,7 +12441,7 @@ vcle_u32 (uint32x2_t __a, uint32x2_t __b)
 __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
 vcle_u64 (uint64x1_t __a, uint64x1_t __b)
 {
-  return (uint64x1_t) {__a[0] <= __b[0] ? -1ll : 0ll};
+  return (__a <= __b);
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
@@ -12833,7 +12565,7 @@ vclez_s32 (int32x2_t __a)
 __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
 vclez_s64 (int64x1_t __a)
 {
-  return (uint64x1_t) {__a[0] <= 0ll ? -1ll : 0ll};
+  return (uint64x1_t) (__a <= __AARCH64_INT64_C (0));
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
@@ -12927,7 +12659,7 @@ vclt_s32 (int32x2_t __a, int32x2_t __b)
 __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
 vclt_s64 (int64x1_t __a, int64x1_t __b)
 {
-  return (uint64x1_t) {__a[0] < __b[0] ? -1ll : 0ll};
+  return (uint64x1_t) (__a < __b);
 }
 
 __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
@@ -12951,7 +12683,7 @@ vclt_u32 (uint32x2_t __a, uint32x2_t __b)
 __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
 vclt_u64 (uint64x1_t __a, uint64x1_t __b)
 {
-  return (uint64x1_t) {__a[0] < __b[0] ? -1ll : 0ll};
+  return (__a < __b);
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
@@ -13075,7 +12807,7 @@ vcltz_s32 (int32x2_t __a)
 __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
 vcltz_s64 (int64x1_t __a)
 {
-  return (uint64x1_t) {__a[0] < 0ll ? -1ll : 0ll};
+  return (uint64x1_t) (__a < __AARCH64_INT64_C (0));
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
@@ -21321,72 +21053,74 @@ vrsrad_n_u64 (uint64_t __a, uint64_t __b, const int __c)
 
 /* vsha1  */
 
-static __inline uint32x4_t
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
 vsha1cq_u32 (uint32x4_t hash_abcd, uint32_t hash_e, uint32x4_t wk)
 {
   return __builtin_aarch64_crypto_sha1cv4si_uuuu (hash_abcd, hash_e, wk);
 }
-static __inline uint32x4_t
+
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
 vsha1mq_u32 (uint32x4_t hash_abcd, uint32_t hash_e, uint32x4_t wk)
 {
   return __builtin_aarch64_crypto_sha1mv4si_uuuu (hash_abcd, hash_e, wk);
 }
-static __inline uint32x4_t
+
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
 vsha1pq_u32 (uint32x4_t hash_abcd, uint32_t hash_e, uint32x4_t wk)
 {
   return __builtin_aarch64_crypto_sha1pv4si_uuuu (hash_abcd, hash_e, wk);
 }
 
-static __inline uint32_t
+__extension__ static __inline uint32_t __attribute__ ((__always_inline__))
 vsha1h_u32 (uint32_t hash_e)
 {
   return __builtin_aarch64_crypto_sha1hsi_uu (hash_e);
 }
 
-static __inline uint32x4_t
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
 vsha1su0q_u32 (uint32x4_t w0_3, uint32x4_t w4_7, uint32x4_t w8_11)
 {
   return __builtin_aarch64_crypto_sha1su0v4si_uuuu (w0_3, w4_7, w8_11);
 }
 
-static __inline uint32x4_t
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
 vsha1su1q_u32 (uint32x4_t tw0_3, uint32x4_t w12_15)
 {
   return __builtin_aarch64_crypto_sha1su1v4si_uuu (tw0_3, w12_15);
 }
 
-static __inline uint32x4_t
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
 vsha256hq_u32 (uint32x4_t hash_abcd, uint32x4_t hash_efgh, uint32x4_t wk)
 {
   return __builtin_aarch64_crypto_sha256hv4si_uuuu (hash_abcd, hash_efgh, wk);
 }
 
-static __inline uint32x4_t
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
 vsha256h2q_u32 (uint32x4_t hash_efgh, uint32x4_t hash_abcd, uint32x4_t wk)
 {
   return __builtin_aarch64_crypto_sha256h2v4si_uuuu (hash_efgh, hash_abcd, wk);
 }
 
-static __inline uint32x4_t
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
 vsha256su0q_u32 (uint32x4_t w0_3, uint32x4_t w4_7)
 {
   return __builtin_aarch64_crypto_sha256su0v4si_uuu (w0_3, w4_7);
 }
 
-static __inline uint32x4_t
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
 vsha256su1q_u32 (uint32x4_t tw0_3, uint32x4_t w8_11, uint32x4_t w12_15)
 {
   return __builtin_aarch64_crypto_sha256su1v4si_uuuu (tw0_3, w8_11, w12_15);
 }
 
-static __inline poly128_t
+__extension__ static __inline poly128_t __attribute__ ((__always_inline__))
 vmull_p64 (poly64_t a, poly64_t b)
 {
   return
     __builtin_aarch64_crypto_pmulldi_ppp (a, b);
 }
 
-static __inline poly128_t
+__extension__ static __inline poly128_t __attribute__ ((__always_inline__))
 vmull_high_p64 (poly64x2_t a, poly64x2_t b)
 {
   return __builtin_aarch64_crypto_pmullv2di_ppp (a, b);
@@ -22302,6 +22036,8 @@ vst1_u64 (uint64_t *a, uint64x1_t b)
   *a = b[0];
 }
 
+/* vst1q */
+
 __extension__ static __inline void __attribute__ ((__always_inline__))
 vst1q_f32 (float32_t *a, float32x4_t b)
 {
@@ -22314,8 +22050,6 @@ vst1q_f64 (float64_t *a, float64x2_t b)
   __builtin_aarch64_st1v2df ((__builtin_aarch64_simd_df *) a, b);
 }
 
-/* vst1q */
-
 __extension__ static __inline void __attribute__ ((__always_inline__))
 vst1q_p8 (poly8_t *a, poly8x16_t b)
 {
@@ -22382,6 +22116,154 @@ vst1q_u64 (uint64_t *a, uint64x2_t b)
 			     (int64x2_t) b);
 }
 
+/* vst1_lane */
+
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst1_lane_f32 (float32_t *__a, float32x2_t __b, const int __lane)
+{
+  *__a = __aarch64_vget_lane_any (__b, __lane);
+}
+
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst1_lane_f64 (float64_t *__a, float64x1_t __b, const int __lane)
+{
+  *__a = __aarch64_vget_lane_any (__b, __lane);
+}
+
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst1_lane_p8 (poly8_t *__a, poly8x8_t __b, const int __lane)
+{
+  *__a = __aarch64_vget_lane_any (__b, __lane);
+}
+
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst1_lane_p16 (poly16_t *__a, poly16x4_t __b, const int __lane)
+{
+  *__a = __aarch64_vget_lane_any (__b, __lane);
+}
+
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst1_lane_s8 (int8_t *__a, int8x8_t __b, const int __lane)
+{
+  *__a = __aarch64_vget_lane_any (__b, __lane);
+}
+
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst1_lane_s16 (int16_t *__a, int16x4_t __b, const int __lane)
+{
+  *__a = __aarch64_vget_lane_any (__b, __lane);
+}
+
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst1_lane_s32 (int32_t *__a, int32x2_t __b, const int __lane)
+{
+  *__a = __aarch64_vget_lane_any (__b, __lane);
+}
+
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst1_lane_s64 (int64_t *__a, int64x1_t __b, const int __lane)
+{
+  *__a = __aarch64_vget_lane_any (__b, __lane);
+}
+
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst1_lane_u8 (uint8_t *__a, uint8x8_t __b, const int __lane)
+{
+  *__a = __aarch64_vget_lane_any (__b, __lane);
+}
+
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst1_lane_u16 (uint16_t *__a, uint16x4_t __b, const int __lane)
+{
+  *__a = __aarch64_vget_lane_any (__b, __lane);
+}
+
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst1_lane_u32 (uint32_t *__a, uint32x2_t __b, const int __lane)
+{
+  *__a = __aarch64_vget_lane_any (__b, __lane);
+}
+
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst1_lane_u64 (uint64_t *__a, uint64x1_t __b, const int __lane)
+{
+  *__a = __aarch64_vget_lane_any (__b, __lane);
+}
+
+/* vst1q_lane */
+
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst1q_lane_f32 (float32_t *__a, float32x4_t __b, const int __lane)
+{
+  *__a = __aarch64_vget_lane_any (__b, __lane);
+}
+
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst1q_lane_f64 (float64_t *__a, float64x2_t __b, const int __lane)
+{
+  *__a = __aarch64_vget_lane_any (__b, __lane);
+}
+
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst1q_lane_p8 (poly8_t *__a, poly8x16_t __b, const int __lane)
+{
+  *__a = __aarch64_vget_lane_any (__b, __lane);
+}
+
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst1q_lane_p16 (poly16_t *__a, poly16x8_t __b, const int __lane)
+{
+  *__a = __aarch64_vget_lane_any (__b, __lane);
+}
+
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst1q_lane_s8 (int8_t *__a, int8x16_t __b, const int __lane)
+{
+  *__a = __aarch64_vget_lane_any (__b, __lane);
+}
+
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst1q_lane_s16 (int16_t *__a, int16x8_t __b, const int __lane)
+{
+  *__a = __aarch64_vget_lane_any (__b, __lane);
+}
+
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst1q_lane_s32 (int32_t *__a, int32x4_t __b, const int __lane)
+{
+  *__a = __aarch64_vget_lane_any (__b, __lane);
+}
+
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst1q_lane_s64 (int64_t *__a, int64x2_t __b, const int __lane)
+{
+  *__a = __aarch64_vget_lane_any (__b, __lane);
+}
+
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst1q_lane_u8 (uint8_t *__a, uint8x16_t __b, const int __lane)
+{
+  *__a = __aarch64_vget_lane_any (__b, __lane);
+}
+
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst1q_lane_u16 (uint16_t *__a, uint16x8_t __b, const int __lane)
+{
+  *__a = __aarch64_vget_lane_any (__b, __lane);
+}
+
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst1q_lane_u32 (uint32_t *__a, uint32x4_t __b, const int __lane)
+{
+  *__a = __aarch64_vget_lane_any (__b, __lane);
+}
+
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst1q_lane_u64 (uint64_t *__a, uint64x2_t __b, const int __lane)
+{
+  *__a = __aarch64_vget_lane_any (__b, __lane);
+}
+
 /* vstn */
 
 __extension__ static __inline void
@@ -23887,7 +23769,7 @@ vtst_s32 (int32x2_t __a, int32x2_t __b)
 __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
 vtst_s64 (int64x1_t __a, int64x1_t __b)
 {
-  return (uint64x1_t) {(__a[0] & __b[0]) ? -1ll : 0ll};
+  return (uint64x1_t) ((__a & __b) != __AARCH64_INT64_C (0));
 }
 
 __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
@@ -23911,7 +23793,7 @@ vtst_u32 (uint32x2_t __a, uint32x2_t __b)
 __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
 vtst_u64 (uint64x1_t __a, uint64x1_t __b)
 {
-  return (uint64x1_t) {(__a[0] & __b[0]) ? -1ll : 0ll};
+  return ((__a & __b) != __AARCH64_UINT64_C (0));
 }
 
 __extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
--- a/src//dev/null
+++ b/src/gcc/config/aarch64/driver-aarch64.c
@@ -0,0 +1,307 @@
+/* Native CPU detection for aarch64.
+   Copyright (C) 2015 Free Software Foundation, Inc.
+
+   This file is part of GCC.
+
+   GCC is free software; you can redistribute it and/or modify
+   it under the terms of the GNU General Public License as published by
+   the Free Software Foundation; either version 3, or (at your option)
+   any later version.
+
+   GCC is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+   GNU General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with GCC; see the file COPYING3.  If not see
+   <http://www.gnu.org/licenses/>.  */
+
+#include "config.h"
+#include "system.h"
+
+struct arch_extension
+{
+  const char *ext;
+  const char *feat_string;
+};
+
+#define AARCH64_OPT_EXTENSION(EXT_NAME, FLAGS_ON, FLAGS_OFF, FEATURE_STRING) \
+  { EXT_NAME, FEATURE_STRING },
+static struct arch_extension ext_to_feat_string[] =
+{
+#include "aarch64-option-extensions.def"
+};
+#undef AARCH64_OPT_EXTENSION
+
+
+struct aarch64_core_data
+{
+  const char* name;
+  const char* arch;
+  const char* implementer_id;
+  const char* part_no;
+};
+
+#define AARCH64_CORE(CORE_NAME, CORE_IDENT, SCHED, ARCH, FLAGS, COSTS, IMP, PART) \
+  { CORE_NAME, #ARCH, IMP, PART },
+
+static struct aarch64_core_data cpu_data [] =
+{
+#include "aarch64-cores.def"
+  { NULL, NULL, NULL, NULL }
+};
+
+#undef AARCH64_CORE
+
+struct aarch64_arch
+{
+  const char* id;
+  const char* name;
+};
+
+#define AARCH64_ARCH(NAME, CORE, ARCH, FLAGS) \
+  { #ARCH, NAME  },
+
+static struct aarch64_arch aarch64_arches [] =
+{
+#include "aarch64-arches.def"
+  {NULL, NULL}
+};
+
+#undef AARCH64_ARCH
+
+/* Return the full architecture name string corresponding to the
+   identifier ID.  */
+
+static const char*
+get_arch_name_from_id (const char* id)
+{
+  unsigned int i = 0;
+
+  for (i = 0; aarch64_arches[i].id != NULL; i++)
+    {
+      if (strcmp (id, aarch64_arches[i].id) == 0)
+        return aarch64_arches[i].name;
+    }
+
+  return NULL;
+}
+
+
+/* Check wether the string CORE contains the same CPU part numbers
+   as BL_STRING.  For example CORE="{0xd03, 0xd07}" and BL_STRING="0xd07.0xd03"
+   should return true.  */
+
+static bool
+valid_bL_string_p (const char** core, const char* bL_string)
+{
+  return strstr (bL_string, core[0]) != NULL
+         && strstr (bL_string, core[1]) != NULL;
+}
+
+/*  Return true iff ARR contains STR in one of its two elements.  */
+
+static bool
+contains_string_p (const char** arr, const char* str)
+{
+  bool res = false;
+
+  if (arr[0] != NULL)
+    {
+      res = strstr (arr[0], str) != NULL;
+      if (res)
+        return res;
+
+      if (arr[1] != NULL)
+        return strstr (arr[1], str) != NULL;
+    }
+
+  return false;
+}
+
+/* This will be called by the spec parser in gcc.c when it sees
+   a %:local_cpu_detect(args) construct.  Currently it will be called
+   with either "arch", "cpu" or "tune" as argument depending on if
+   -march=native, -mcpu=native or -mtune=native is to be substituted.
+
+   It returns a string containing new command line parameters to be
+   put at the place of the above two options, depending on what CPU
+   this is executed.  E.g. "-march=armv8-a" on a Cortex-A57 for
+   -march=native.  If the routine can't detect a known processor,
+   the -march or -mtune option is discarded.
+
+   For -mtune and -mcpu arguments it attempts to detect the CPU or
+   a big.LITTLE system.
+   ARGC and ARGV are set depending on the actual arguments given
+   in the spec.  */
+
+const char *
+host_detect_local_cpu (int argc, const char **argv)
+{
+  const char *arch_id = NULL;
+  const char *res = NULL;
+  static const int num_exts = ARRAY_SIZE (ext_to_feat_string);
+  char buf[128];
+  FILE *f = NULL;
+  bool arch = false;
+  bool tune = false;
+  bool cpu = false;
+  unsigned int i = 0;
+  unsigned int core_idx = 0;
+  const char* imps[2] = { NULL, NULL };
+  const char* cores[2] = { NULL, NULL };
+  unsigned int n_cores = 0;
+  unsigned int n_imps = 0;
+  bool processed_exts = false;
+  const char *ext_string = "";
+
+  gcc_assert (argc);
+
+  if (!argv[0])
+    goto not_found;
+
+  /* Are we processing -march, mtune or mcpu?  */
+  arch = strcmp (argv[0], "arch") == 0;
+  if (!arch)
+    tune = strcmp (argv[0], "tune") == 0;
+
+  if (!arch && !tune)
+    cpu = strcmp (argv[0], "cpu") == 0;
+
+  if (!arch && !tune && !cpu)
+    goto not_found;
+
+  f = fopen ("/proc/cpuinfo", "r");
+
+  if (f == NULL)
+    goto not_found;
+
+  /* Look through /proc/cpuinfo to determine the implementer
+     and then the part number that identifies a particular core.  */
+  while (fgets (buf, sizeof (buf), f) != NULL)
+    {
+      if (strstr (buf, "implementer") != NULL)
+	{
+	  for (i = 0; cpu_data[i].name != NULL; i++)
+	    if (strstr (buf, cpu_data[i].implementer_id) != NULL
+                && !contains_string_p (imps, cpu_data[i].implementer_id))
+	      {
+                if (n_imps == 2)
+                  goto not_found;
+
+                imps[n_imps++] = cpu_data[i].implementer_id;
+
+                break;
+	      }
+          continue;
+	}
+
+      if (strstr (buf, "part") != NULL)
+	{
+	  for (i = 0; cpu_data[i].name != NULL; i++)
+	    if (strstr (buf, cpu_data[i].part_no) != NULL
+                && !contains_string_p (cores, cpu_data[i].part_no))
+	      {
+                if (n_cores == 2)
+                  goto not_found;
+
+                cores[n_cores++] = cpu_data[i].part_no;
+	        core_idx = i;
+	        arch_id = cpu_data[i].arch;
+	        break;
+	      }
+          continue;
+        }
+      if (!tune && !processed_exts && strstr (buf, "Features") != NULL)
+        {
+          for (i = 0; i < num_exts; i++)
+            {
+              bool enabled = true;
+              char *p = NULL;
+              char *feat_string = concat (ext_to_feat_string[i].feat_string, NULL);
+
+              p = strtok (feat_string, " ");
+
+              while (p != NULL)
+                {
+                  if (strstr (buf, p) == NULL)
+                    {
+                      enabled = false;
+                      break;
+                    }
+                  p = strtok (NULL, " ");
+                }
+              ext_string = concat (ext_string, "+", enabled ? "" : "no",
+                                   ext_to_feat_string[i].ext, NULL);
+            }
+          processed_exts = true;
+        }
+    }
+
+  fclose (f);
+  f = NULL;
+
+  /* Weird cpuinfo format that we don't know how to handle.  */
+  if (n_cores == 0 || n_cores > 2 || n_imps != 1)
+    goto not_found;
+
+  if (arch && !arch_id)
+    goto not_found;
+
+  if (arch)
+    {
+      const char* arch_name = get_arch_name_from_id (arch_id);
+
+      /* We got some arch indentifier that's not in aarch64-arches.def?  */
+      if (!arch_name)
+        goto not_found;
+
+      res = concat ("-march=", arch_name, NULL);
+    }
+  /* We have big.LITTLE.  */
+  else if (n_cores == 2)
+    {
+      for (i = 0; cpu_data[i].name != NULL; i++)
+        {
+          if (strchr (cpu_data[i].part_no, '.') != NULL
+              && strncmp (cpu_data[i].implementer_id, imps[0], strlen (imps[0]) - 1) == 0
+              && valid_bL_string_p (cores, cpu_data[i].part_no))
+            {
+              res = concat ("-m", cpu ? "cpu" : "tune", "=", cpu_data[i].name, NULL);
+              break;
+            }
+        }
+      if (!res)
+        goto not_found;
+    }
+  /* The simple, non-big.LITTLE case.  */
+  else
+    {
+      if (strncmp (cpu_data[core_idx].implementer_id, imps[0],
+                   strlen (imps[0]) - 1) != 0)
+        goto not_found;
+
+      res = concat ("-m", cpu ? "cpu" : "tune", "=",
+                      cpu_data[core_idx].name, NULL);
+    }
+
+  if (tune)
+    return res;
+
+  res = concat (res, ext_string, NULL);
+
+  return res;
+
+not_found:
+  {
+   /* If detection fails we ignore the option.
+      Clean up and return empty string.  */
+
+    if (f)
+      fclose (f);
+
+    return "";
+  }
+}
+
--- a/src//dev/null
+++ b/src/gcc/config/aarch64/x-aarch64
@@ -0,0 +1,3 @@
+driver-aarch64.o: $(srcdir)/config/aarch64/driver-aarch64.c \
+  $(CONFIG_H) $(SYSTEM_H)
+	$(COMPILER) -c $(ALL_COMPILERFLAGS) $(ALL_CPPFLAGS) $(INCLUDES) $<
--- a/src/gcc/config/alpha/linux.h
+++ b/src/gcc/config/alpha/linux.h
@@ -61,10 +61,14 @@ along with GCC; see the file COPYING3.  If not see
 #define OPTION_GLIBC  (DEFAULT_LIBC == LIBC_GLIBC)
 #define OPTION_UCLIBC (DEFAULT_LIBC == LIBC_UCLIBC)
 #define OPTION_BIONIC (DEFAULT_LIBC == LIBC_BIONIC)
+#undef OPTION_MUSL
+#define OPTION_MUSL   (DEFAULT_LIBC == LIBC_MUSL)
 #else
 #define OPTION_GLIBC  (linux_libc == LIBC_GLIBC)
 #define OPTION_UCLIBC (linux_libc == LIBC_UCLIBC)
 #define OPTION_BIONIC (linux_libc == LIBC_BIONIC)
+#undef OPTION_MUSL
+#define OPTION_MUSL   (linux_libc == LIBC_MUSL)
 #endif
 
 /* Determine what functions are present at the runtime;
--- a/src/gcc/config/arm/aarch-common-protos.h
+++ b/src/gcc/config/arm/aarch-common-protos.h
@@ -102,6 +102,8 @@ struct mem_cost_table
   const int storef;		/* SFmode.  */
   const int stored;		/* DFmode.  */
   const int store_unaligned;	/* Extra for unaligned stores.  */
+  const int loadv;		/* Vector load.  */
+  const int storev;		/* Vector store.  */
 };
 
 struct fp_cost_table
--- a/src/gcc/config/arm/aarch-cost-tables.h
+++ b/src/gcc/config/arm/aarch-cost-tables.h
@@ -81,7 +81,9 @@ const struct cpu_cost_table generic_extra_costs =
     1,			/* stm_regs_per_insn_subsequent.  */
     COSTS_N_INSNS (2),	/* storef.  */
     COSTS_N_INSNS (3),	/* stored.  */
-    COSTS_N_INSNS (1)  /* store_unaligned.  */
+    COSTS_N_INSNS (1),	/* store_unaligned.  */
+    COSTS_N_INSNS (1),	/* loadv.  */
+    COSTS_N_INSNS (1)	/* storev.  */
   },
   {
     /* FP SFmode */
@@ -130,12 +132,12 @@ const struct cpu_cost_table cortexa53_extra_costs =
     0,			/* arith.  */
     0,			/* logical.  */
     COSTS_N_INSNS (1),	/* shift.  */
-    COSTS_N_INSNS (2),	/* shift_reg.  */
+    0,			/* shift_reg.  */
     COSTS_N_INSNS (1),	/* arith_shift.  */
-    COSTS_N_INSNS (2),	/* arith_shift_reg.  */
+    COSTS_N_INSNS (1),	/* arith_shift_reg.  */
     COSTS_N_INSNS (1),	/* log_shift.  */
-    COSTS_N_INSNS (2),	/* log_shift_reg.  */
-    0,			/* extend.  */
+    COSTS_N_INSNS (1),	/* log_shift_reg.  */
+    COSTS_N_INSNS (1),	/* extend.  */
     COSTS_N_INSNS (1),	/* extend_arith.  */
     COSTS_N_INSNS (1),	/* bfi.  */
     COSTS_N_INSNS (1),	/* bfx.  */
@@ -182,7 +184,9 @@ const struct cpu_cost_table cortexa53_extra_costs =
     2,				/* stm_regs_per_insn_subsequent.  */
     0,				/* storef.  */
     0,				/* stored.  */
-    COSTS_N_INSNS (1)		/* store_unaligned.  */
+    COSTS_N_INSNS (1),		/* store_unaligned.  */
+    COSTS_N_INSNS (1),		/* loadv.  */
+    COSTS_N_INSNS (1)		/* storev.  */
   },
   {
     /* FP SFmode */
@@ -283,7 +287,9 @@ const struct cpu_cost_table cortexa57_extra_costs =
     2,                         /* stm_regs_per_insn_subsequent.  */
     0,                         /* storef.  */
     0,                         /* stored.  */
-    COSTS_N_INSNS (1)          /* store_unaligned.  */
+    COSTS_N_INSNS (1),         /* store_unaligned.  */
+    COSTS_N_INSNS (1),         /* loadv.  */
+    COSTS_N_INSNS (1)          /* storev.  */
   },
   {
     /* FP SFmode */
@@ -385,6 +391,8 @@ const struct cpu_cost_table xgene1_extra_costs =
     0,                         /* storef.  */
     0,                         /* stored.  */
     0,                         /* store_unaligned.  */
+    COSTS_N_INSNS (1),         /* loadv.  */
+    COSTS_N_INSNS (1)          /* storev.  */
   },
   {
     /* FP SFmode */
--- a/src/gcc/config/arm/arm-cores.def
+++ b/src/gcc/config/arm/arm-cores.def
@@ -158,7 +158,7 @@ ARM_CORE("cortex-r7",		cortexr7, cortexr7,		7R,  FL_LDSCHED | FL_ARM_DIV, cortex
 ARM_CORE("cortex-m7",		cortexm7, cortexm7,		7EM, FL_LDSCHED | FL_NO_VOLATILE_CE, cortex_m7)
 ARM_CORE("cortex-m4",		cortexm4, cortexm4,		7EM, FL_LDSCHED, v7m)
 ARM_CORE("cortex-m3",		cortexm3, cortexm3,		7M,  FL_LDSCHED, v7m)
-ARM_CORE("marvell-pj4",		marvell_pj4, marvell_pj4,	7A,  FL_LDSCHED, 9e)
+ARM_CORE("marvell-pj4",		marvell_pj4, marvell_pj4,	7A,  FL_LDSCHED, marvell_pj4)
 
 /* V7 big.LITTLE implementations */
 ARM_CORE("cortex-a15.cortex-a7", cortexa15cortexa7, cortexa7,	7A,  FL_LDSCHED | FL_THUMB_DIV | FL_ARM_DIV, cortex_a15)
--- a/src/gcc/config/arm/arm-protos.h
+++ b/src/gcc/config/arm/arm-protos.h
@@ -66,10 +66,6 @@ extern rtx legitimize_tls_address (rtx, rtx);
 extern bool arm_legitimate_address_p (machine_mode, rtx, bool);
 extern int arm_legitimate_address_outer_p (machine_mode, rtx, RTX_CODE, int);
 extern int thumb_legitimate_offset_p (machine_mode, HOST_WIDE_INT);
-extern bool arm_legitimize_reload_address (rtx *, machine_mode, int, int,
-					   int);
-extern rtx thumb_legitimize_reload_address (rtx *, machine_mode, int, int,
-					    int);
 extern int thumb1_legitimate_address_p (machine_mode, rtx, int);
 extern bool ldm_stm_operation_p (rtx, bool, machine_mode mode,
                                  bool, bool);
@@ -257,13 +253,6 @@ struct cpu_vec_costs {
 
 struct cpu_cost_table;
 
-enum arm_sched_autopref
-  {
-    ARM_SCHED_AUTOPREF_OFF,
-    ARM_SCHED_AUTOPREF_RANK,
-    ARM_SCHED_AUTOPREF_FULL
-  };
-
 /* Dump function ARM_PRINT_TUNE_INFO should be updated whenever this
    structure is modified.  */
 
@@ -272,39 +261,57 @@ struct tune_params
   bool (*rtx_costs) (rtx, RTX_CODE, RTX_CODE, int *, bool);
   const struct cpu_cost_table *insn_extra_cost;
   bool (*sched_adjust_cost) (rtx_insn *, rtx, rtx_insn *, int *);
+  int (*branch_cost) (bool, bool);
+  /* Vectorizer costs.  */
+  const struct cpu_vec_costs* vec_costs;
   int constant_limit;
   /* Maximum number of instructions to conditionalise.  */
   int max_insns_skipped;
-  int num_prefetch_slots;
-  int l1_cache_size;
-  int l1_cache_line_size;
-  bool prefer_constant_pool;
-  int (*branch_cost) (bool, bool);
+  /* Maximum number of instructions to inline calls to memset.  */
+  int max_insns_inline_memset;
+  /* Issue rate of the processor.  */
+  unsigned int issue_rate;
+  /* Explicit prefetch data.  */
+  struct
+    {
+      int num_slots;
+      int l1_cache_size;
+      int l1_cache_line_size;
+    } prefetch;
+  enum {PREF_CONST_POOL_FALSE, PREF_CONST_POOL_TRUE}
+    prefer_constant_pool: 1;
   /* Prefer STRD/LDRD instructions over PUSH/POP/LDM/STM.  */
-  bool prefer_ldrd_strd;
+  enum {PREF_LDRD_FALSE, PREF_LDRD_TRUE} prefer_ldrd_strd: 1;
   /* The preference for non short cirtcuit operation when optimizing for
      performance. The first element covers Thumb state and the second one
      is for ARM state.  */
-  bool logical_op_non_short_circuit[2];
-  /* Vectorizer costs.  */
-  const struct cpu_vec_costs* vec_costs;
-  /* Prefer Neon for 64-bit bitops.  */
-  bool prefer_neon_for_64bits;
+  enum log_op_non_sc {LOG_OP_NON_SC_FALSE, LOG_OP_NON_SC_TRUE};
+  log_op_non_sc logical_op_non_short_circuit_thumb: 1;
+  log_op_non_sc logical_op_non_short_circuit_arm: 1;
   /* Prefer 32-bit encoding instead of flag-setting 16-bit encoding.  */
-  bool disparage_flag_setting_t16_encodings;
-  /* Prefer 32-bit encoding instead of 16-bit encoding where subset of flags
-     would be set.  */
-  bool disparage_partial_flag_setting_t16_encodings;
+  enum {DISPARAGE_FLAGS_NEITHER, DISPARAGE_FLAGS_PARTIAL, DISPARAGE_FLAGS_ALL}
+    disparage_flag_setting_t16_encodings: 2;
+  enum {PREF_NEON_64_FALSE, PREF_NEON_64_TRUE} prefer_neon_for_64bits: 1;
   /* Prefer to inline string operations like memset by using Neon.  */
-  bool string_ops_prefer_neon;
-  /* Maximum number of instructions to inline calls to memset.  */
-  int max_insns_inline_memset;
-  /* Bitfield encoding the fuseable pairs of instructions.  */
-  unsigned int fuseable_ops;
+  enum {PREF_NEON_STRINGOPS_FALSE, PREF_NEON_STRINGOPS_TRUE}
+    string_ops_prefer_neon: 1;
+  /* Bitfield encoding the fuseable pairs of instructions.  Use FUSE_OPS
+     in an initializer if multiple fusion operations are supported on a
+     target.  */
+  enum fuse_ops
+  {
+    FUSE_NOTHING   = 0,
+    FUSE_MOVW_MOVT = 1 << 0
+  } fuseable_ops: 1;
   /* Depth of scheduling queue to check for L2 autoprefetcher.  */
-  enum arm_sched_autopref sched_autopref;
+  enum {SCHED_AUTOPREF_OFF, SCHED_AUTOPREF_RANK, SCHED_AUTOPREF_FULL}
+    sched_autopref: 2;
 };
 
+/* Smash multiple fusion operations into a type that can be used for an
+   initializer.  */
+#define FUSE_OPS(x) ((tune_params::fuse_ops) (x))
+
 extern const struct tune_params *current_tune;
 extern int vfp3_const_double_for_fract_bits (rtx);
 /* return power of two from operand, otherwise 0.  */
--- a/src/gcc/config/arm/arm.c
+++ b/src/gcc/config/arm/arm.c
@@ -940,11 +940,13 @@ struct processors
 };
 
 
-#define ARM_PREFETCH_NOT_BENEFICIAL 0, -1, -1
-#define ARM_PREFETCH_BENEFICIAL(prefetch_slots,l1_size,l1_line_size) \
-  prefetch_slots, \
-  l1_size, \
-  l1_line_size
+#define ARM_PREFETCH_NOT_BENEFICIAL { 0, -1, -1 }
+#define ARM_PREFETCH_BENEFICIAL(num_slots,l1_size,l1_line_size) \
+  {								\
+    num_slots,							\
+    l1_size,							\
+    l1_line_size						\
+  }
 
 /* arm generic vectorizer costs.  */
 static const
@@ -1027,7 +1029,9 @@ const struct cpu_cost_table cortexa9_extra_costs =
     2,			/* stm_regs_per_insn_subsequent.  */
     COSTS_N_INSNS (1),	/* storef.  */
     COSTS_N_INSNS (1),	/* stored.  */
-    COSTS_N_INSNS (1)	/* store_unaligned.  */
+    COSTS_N_INSNS (1),	/* store_unaligned.  */
+    COSTS_N_INSNS (1),	/* loadv.  */
+    COSTS_N_INSNS (1)	/* storev.  */
   },
   {
     /* FP SFmode */
@@ -1128,7 +1132,9 @@ const struct cpu_cost_table cortexa8_extra_costs =
     2,			/* stm_regs_per_insn_subsequent.  */
     COSTS_N_INSNS (1),	/* storef.  */
     COSTS_N_INSNS (1),	/* stored.  */
-    COSTS_N_INSNS (1)	/* store_unaligned.  */
+    COSTS_N_INSNS (1),	/* store_unaligned.  */
+    COSTS_N_INSNS (1),	/* loadv.  */
+    COSTS_N_INSNS (1)	/* storev.  */
   },
   {
     /* FP SFmode */
@@ -1230,7 +1236,9 @@ const struct cpu_cost_table cortexa5_extra_costs =
     2,			/* stm_regs_per_insn_subsequent.  */
     COSTS_N_INSNS (2),	/* storef.  */
     COSTS_N_INSNS (2),	/* stored.  */
-    COSTS_N_INSNS (1)	/* store_unaligned.  */
+    COSTS_N_INSNS (1),	/* store_unaligned.  */
+    COSTS_N_INSNS (1),	/* loadv.  */
+    COSTS_N_INSNS (1)	/* storev.  */
   },
   {
     /* FP SFmode */
@@ -1333,7 +1341,9 @@ const struct cpu_cost_table cortexa7_extra_costs =
     2,			/* stm_regs_per_insn_subsequent.  */
     COSTS_N_INSNS (2),	/* storef.  */
     COSTS_N_INSNS (2),	/* stored.  */
-    COSTS_N_INSNS (1)	/* store_unaligned.  */
+    COSTS_N_INSNS (1),	/* store_unaligned.  */
+    COSTS_N_INSNS (1),	/* loadv.  */
+    COSTS_N_INSNS (1)	/* storev.  */
   },
   {
     /* FP SFmode */
@@ -1434,7 +1444,9 @@ const struct cpu_cost_table cortexa12_extra_costs =
     2,			/* stm_regs_per_insn_subsequent.  */
     COSTS_N_INSNS (2),	/* storef.  */
     COSTS_N_INSNS (2),	/* stored.  */
-    0			/* store_unaligned.  */
+    0,			/* store_unaligned.  */
+    COSTS_N_INSNS (1),	/* loadv.  */
+    COSTS_N_INSNS (1)	/* storev.  */
   },
   {
     /* FP SFmode */
@@ -1535,7 +1547,9 @@ const struct cpu_cost_table cortexa15_extra_costs =
     2,			/* stm_regs_per_insn_subsequent.  */
     0,			/* storef.  */
     0,			/* stored.  */
-    0			/* store_unaligned.  */
+    0,			/* store_unaligned.  */
+    COSTS_N_INSNS (1),	/* loadv.  */
+    COSTS_N_INSNS (1)	/* storev.  */
   },
   {
     /* FP SFmode */
@@ -1636,7 +1650,9 @@ const struct cpu_cost_table v7m_extra_costs =
     1,			/* stm_regs_per_insn_subsequent.  */
     COSTS_N_INSNS (2),	/* storef.  */
     COSTS_N_INSNS (3),	/* stored.  */
-    COSTS_N_INSNS (1)  /* store_unaligned.  */
+    COSTS_N_INSNS (1),	/* store_unaligned.  */
+    COSTS_N_INSNS (1),	/* loadv.  */
+    COSTS_N_INSNS (1)	/* storev.  */
   },
   {
     /* FP SFmode */
@@ -1678,49 +1694,50 @@ const struct cpu_cost_table v7m_extra_costs =
   }
 };
 
-#define ARM_FUSE_NOTHING	(0)
-#define ARM_FUSE_MOVW_MOVT	(1 << 0)
-
 const struct tune_params arm_slowmul_tune =
 {
   arm_slowmul_rtx_costs,
-  NULL,
-  NULL,						/* Sched adj cost.  */
+  NULL,					/* Insn extra costs.  */
+  NULL,					/* Sched adj cost.  */
+  arm_default_branch_cost,
+  &arm_default_vec_cost,
   3,						/* Constant limit.  */
   5,						/* Max cond insns.  */
+  8,						/* Memset max inline.  */
+  1,						/* Issue rate.  */
   ARM_PREFETCH_NOT_BENEFICIAL,
-  true,						/* Prefer constant pool.  */
-  arm_default_branch_cost,
-  false,					/* Prefer LDRD/STRD.  */
-  {true, true},					/* Prefer non short circuit.  */
-  &arm_default_vec_cost,                        /* Vectorizer costs.  */
-  false,                                        /* Prefer Neon for 64-bits bitops.  */
-  false, false,                                 /* Prefer 32-bit encodings.  */
-  false,					/* Prefer Neon for stringops.  */
-  8,						/* Maximum insns to inline memset.  */
-  ARM_FUSE_NOTHING,				/* Fuseable pairs of instructions.  */
-  ARM_SCHED_AUTOPREF_OFF			/* Sched L2 autopref.  */
+  tune_params::PREF_CONST_POOL_TRUE,
+  tune_params::PREF_LDRD_FALSE,
+  tune_params::LOG_OP_NON_SC_TRUE,		/* Thumb.  */
+  tune_params::LOG_OP_NON_SC_TRUE,		/* ARM.  */
+  tune_params::DISPARAGE_FLAGS_NEITHER,
+  tune_params::PREF_NEON_64_FALSE,
+  tune_params::PREF_NEON_STRINGOPS_FALSE,
+  tune_params::FUSE_NOTHING,
+  tune_params::SCHED_AUTOPREF_OFF
 };
 
 const struct tune_params arm_fastmul_tune =
 {
   arm_fastmul_rtx_costs,
-  NULL,
-  NULL,						/* Sched adj cost.  */
+  NULL,					/* Insn extra costs.  */
+  NULL,					/* Sched adj cost.  */
+  arm_default_branch_cost,
+  &arm_default_vec_cost,
   1,						/* Constant limit.  */
   5,						/* Max cond insns.  */
+  8,						/* Memset max inline.  */
+  1,						/* Issue rate.  */
   ARM_PREFETCH_NOT_BENEFICIAL,
-  true,						/* Prefer constant pool.  */
-  arm_default_branch_cost,
-  false,					/* Prefer LDRD/STRD.  */
-  {true, true},					/* Prefer non short circuit.  */
-  &arm_default_vec_cost,                        /* Vectorizer costs.  */
-  false,                                        /* Prefer Neon for 64-bits bitops.  */
-  false, false,                                 /* Prefer 32-bit encodings.  */
-  false,					/* Prefer Neon for stringops.  */
-  8,						/* Maximum insns to inline memset.  */
-  ARM_FUSE_NOTHING,				/* Fuseable pairs of instructions.  */
-  ARM_SCHED_AUTOPREF_OFF			/* Sched L2 autopref.  */
+  tune_params::PREF_CONST_POOL_TRUE,
+  tune_params::PREF_LDRD_FALSE,
+  tune_params::LOG_OP_NON_SC_TRUE,		/* Thumb.  */
+  tune_params::LOG_OP_NON_SC_TRUE,		/* ARM.  */
+  tune_params::DISPARAGE_FLAGS_NEITHER,
+  tune_params::PREF_NEON_64_FALSE,
+  tune_params::PREF_NEON_STRINGOPS_FALSE,
+  tune_params::FUSE_NOTHING,
+  tune_params::SCHED_AUTOPREF_OFF
 };
 
 /* StrongARM has early execution of branches, so a sequence that is worth
@@ -1729,233 +1746,279 @@ const struct tune_params arm_fastmul_tune =
 const struct tune_params arm_strongarm_tune =
 {
   arm_fastmul_rtx_costs,
-  NULL,
-  NULL,						/* Sched adj cost.  */
+  NULL,					/* Insn extra costs.  */
+  NULL,					/* Sched adj cost.  */
+  arm_default_branch_cost,
+  &arm_default_vec_cost,
   1,						/* Constant limit.  */
   3,						/* Max cond insns.  */
+  8,						/* Memset max inline.  */
+  1,						/* Issue rate.  */
   ARM_PREFETCH_NOT_BENEFICIAL,
-  true,						/* Prefer constant pool.  */
-  arm_default_branch_cost,
-  false,					/* Prefer LDRD/STRD.  */
-  {true, true},					/* Prefer non short circuit.  */
-  &arm_default_vec_cost,                        /* Vectorizer costs.  */
-  false,                                        /* Prefer Neon for 64-bits bitops.  */
-  false, false,                                 /* Prefer 32-bit encodings.  */
-  false,					/* Prefer Neon for stringops.  */
-  8,						/* Maximum insns to inline memset.  */
-  ARM_FUSE_NOTHING,				/* Fuseable pairs of instructions.  */
-  ARM_SCHED_AUTOPREF_OFF			/* Sched L2 autopref.  */
+  tune_params::PREF_CONST_POOL_TRUE,
+  tune_params::PREF_LDRD_FALSE,
+  tune_params::LOG_OP_NON_SC_TRUE,		/* Thumb.  */
+  tune_params::LOG_OP_NON_SC_TRUE,		/* ARM.  */
+  tune_params::DISPARAGE_FLAGS_NEITHER,
+  tune_params::PREF_NEON_64_FALSE,
+  tune_params::PREF_NEON_STRINGOPS_FALSE,
+  tune_params::FUSE_NOTHING,
+  tune_params::SCHED_AUTOPREF_OFF
 };
 
 const struct tune_params arm_xscale_tune =
 {
   arm_xscale_rtx_costs,
-  NULL,
+  NULL,					/* Insn extra costs.  */
   xscale_sched_adjust_cost,
+  arm_default_branch_cost,
+  &arm_default_vec_cost,
   2,						/* Constant limit.  */
   3,						/* Max cond insns.  */
+  8,						/* Memset max inline.  */
+  1,						/* Issue rate.  */
   ARM_PREFETCH_NOT_BENEFICIAL,
-  true,						/* Prefer constant pool.  */
-  arm_default_branch_cost,
-  false,					/* Prefer LDRD/STRD.  */
-  {true, true},					/* Prefer non short circuit.  */
-  &arm_default_vec_cost,                        /* Vectorizer costs.  */
-  false,                                        /* Prefer Neon for 64-bits bitops.  */
-  false, false,                                 /* Prefer 32-bit encodings.  */
-  false,					/* Prefer Neon for stringops.  */
-  8,						/* Maximum insns to inline memset.  */
-  ARM_FUSE_NOTHING,				/* Fuseable pairs of instructions.  */
-  ARM_SCHED_AUTOPREF_OFF			/* Sched L2 autopref.  */
+  tune_params::PREF_CONST_POOL_TRUE,
+  tune_params::PREF_LDRD_FALSE,
+  tune_params::LOG_OP_NON_SC_TRUE,		/* Thumb.  */
+  tune_params::LOG_OP_NON_SC_TRUE,		/* ARM.  */
+  tune_params::DISPARAGE_FLAGS_NEITHER,
+  tune_params::PREF_NEON_64_FALSE,
+  tune_params::PREF_NEON_STRINGOPS_FALSE,
+  tune_params::FUSE_NOTHING,
+  tune_params::SCHED_AUTOPREF_OFF
 };
 
 const struct tune_params arm_9e_tune =
 {
   arm_9e_rtx_costs,
-  NULL,
-  NULL,						/* Sched adj cost.  */
+  NULL,					/* Insn extra costs.  */
+  NULL,					/* Sched adj cost.  */
+  arm_default_branch_cost,
+  &arm_default_vec_cost,
   1,						/* Constant limit.  */
   5,						/* Max cond insns.  */
+  8,						/* Memset max inline.  */
+  1,						/* Issue rate.  */
   ARM_PREFETCH_NOT_BENEFICIAL,
-  true,						/* Prefer constant pool.  */
+  tune_params::PREF_CONST_POOL_TRUE,
+  tune_params::PREF_LDRD_FALSE,
+  tune_params::LOG_OP_NON_SC_TRUE,		/* Thumb.  */
+  tune_params::LOG_OP_NON_SC_TRUE,		/* ARM.  */
+  tune_params::DISPARAGE_FLAGS_NEITHER,
+  tune_params::PREF_NEON_64_FALSE,
+  tune_params::PREF_NEON_STRINGOPS_FALSE,
+  tune_params::FUSE_NOTHING,
+  tune_params::SCHED_AUTOPREF_OFF
+};
+
+const struct tune_params arm_marvell_pj4_tune =
+{
+  arm_9e_rtx_costs,
+  NULL,					/* Insn extra costs.  */
+  NULL,					/* Sched adj cost.  */
   arm_default_branch_cost,
-  false,					/* Prefer LDRD/STRD.  */
-  {true, true},					/* Prefer non short circuit.  */
-  &arm_default_vec_cost,                        /* Vectorizer costs.  */
-  false,                                        /* Prefer Neon for 64-bits bitops.  */
-  false, false,                                 /* Prefer 32-bit encodings.  */
-  false,					/* Prefer Neon for stringops.  */
-  8,						/* Maximum insns to inline memset.  */
-  ARM_FUSE_NOTHING,				/* Fuseable pairs of instructions.  */
-  ARM_SCHED_AUTOPREF_OFF			/* Sched L2 autopref.  */
+  &arm_default_vec_cost,
+  1,						/* Constant limit.  */
+  5,						/* Max cond insns.  */
+  8,						/* Memset max inline.  */
+  2,						/* Issue rate.  */
+  ARM_PREFETCH_NOT_BENEFICIAL,
+  tune_params::PREF_CONST_POOL_TRUE,
+  tune_params::PREF_LDRD_FALSE,
+  tune_params::LOG_OP_NON_SC_TRUE,		/* Thumb.  */
+  tune_params::LOG_OP_NON_SC_TRUE,		/* ARM.  */
+  tune_params::DISPARAGE_FLAGS_NEITHER,
+  tune_params::PREF_NEON_64_FALSE,
+  tune_params::PREF_NEON_STRINGOPS_FALSE,
+  tune_params::FUSE_NOTHING,
+  tune_params::SCHED_AUTOPREF_OFF
 };
 
 const struct tune_params arm_v6t2_tune =
 {
   arm_9e_rtx_costs,
-  NULL,
-  NULL,						/* Sched adj cost.  */
+  NULL,					/* Insn extra costs.  */
+  NULL,					/* Sched adj cost.  */
+  arm_default_branch_cost,
+  &arm_default_vec_cost,
   1,						/* Constant limit.  */
   5,						/* Max cond insns.  */
+  8,						/* Memset max inline.  */
+  1,						/* Issue rate.  */
   ARM_PREFETCH_NOT_BENEFICIAL,
-  false,					/* Prefer constant pool.  */
-  arm_default_branch_cost,
-  false,					/* Prefer LDRD/STRD.  */
-  {true, true},					/* Prefer non short circuit.  */
-  &arm_default_vec_cost,                        /* Vectorizer costs.  */
-  false,                                        /* Prefer Neon for 64-bits bitops.  */
-  false, false,                                 /* Prefer 32-bit encodings.  */
-  false,					/* Prefer Neon for stringops.  */
-  8,						/* Maximum insns to inline memset.  */
-  ARM_FUSE_NOTHING,				/* Fuseable pairs of instructions.  */
-  ARM_SCHED_AUTOPREF_OFF			/* Sched L2 autopref.  */
+  tune_params::PREF_CONST_POOL_FALSE,
+  tune_params::PREF_LDRD_FALSE,
+  tune_params::LOG_OP_NON_SC_TRUE,		/* Thumb.  */
+  tune_params::LOG_OP_NON_SC_TRUE,		/* ARM.  */
+  tune_params::DISPARAGE_FLAGS_NEITHER,
+  tune_params::PREF_NEON_64_FALSE,
+  tune_params::PREF_NEON_STRINGOPS_FALSE,
+  tune_params::FUSE_NOTHING,
+  tune_params::SCHED_AUTOPREF_OFF
 };
 
+
 /* Generic Cortex tuning.  Use more specific tunings if appropriate.  */
 const struct tune_params arm_cortex_tune =
 {
   arm_9e_rtx_costs,
   &generic_extra_costs,
-  NULL,						/* Sched adj cost.  */
+  NULL,					/* Sched adj cost.  */
+  arm_default_branch_cost,
+  &arm_default_vec_cost,
   1,						/* Constant limit.  */
   5,						/* Max cond insns.  */
+  8,						/* Memset max inline.  */
+  2,						/* Issue rate.  */
   ARM_PREFETCH_NOT_BENEFICIAL,
-  false,					/* Prefer constant pool.  */
-  arm_default_branch_cost,
-  false,					/* Prefer LDRD/STRD.  */
-  {true, true},					/* Prefer non short circuit.  */
-  &arm_default_vec_cost,                        /* Vectorizer costs.  */
-  false,                                        /* Prefer Neon for 64-bits bitops.  */
-  false, false,                                 /* Prefer 32-bit encodings.  */
-  false,					/* Prefer Neon for stringops.  */
-  8,						/* Maximum insns to inline memset.  */
-  ARM_FUSE_NOTHING,				/* Fuseable pairs of instructions.  */
-  ARM_SCHED_AUTOPREF_OFF			/* Sched L2 autopref.  */
+  tune_params::PREF_CONST_POOL_FALSE,
+  tune_params::PREF_LDRD_FALSE,
+  tune_params::LOG_OP_NON_SC_TRUE,		/* Thumb.  */
+  tune_params::LOG_OP_NON_SC_TRUE,		/* ARM.  */
+  tune_params::DISPARAGE_FLAGS_NEITHER,
+  tune_params::PREF_NEON_64_FALSE,
+  tune_params::PREF_NEON_STRINGOPS_FALSE,
+  tune_params::FUSE_NOTHING,
+  tune_params::SCHED_AUTOPREF_OFF
 };
 
 const struct tune_params arm_cortex_a8_tune =
 {
   arm_9e_rtx_costs,
   &cortexa8_extra_costs,
-  NULL,						/* Sched adj cost.  */
+  NULL,					/* Sched adj cost.  */
+  arm_default_branch_cost,
+  &arm_default_vec_cost,
   1,						/* Constant limit.  */
   5,						/* Max cond insns.  */
+  8,						/* Memset max inline.  */
+  2,						/* Issue rate.  */
   ARM_PREFETCH_NOT_BENEFICIAL,
-  false,					/* Prefer constant pool.  */
-  arm_default_branch_cost,
-  false,					/* Prefer LDRD/STRD.  */
-  {true, true},					/* Prefer non short circuit.  */
-  &arm_default_vec_cost,                        /* Vectorizer costs.  */
-  false,                                        /* Prefer Neon for 64-bits bitops.  */
-  false, false,                                 /* Prefer 32-bit encodings.  */
-  true,						/* Prefer Neon for stringops.  */
-  8,						/* Maximum insns to inline memset.  */
-  ARM_FUSE_NOTHING,				/* Fuseable pairs of instructions.  */
-  ARM_SCHED_AUTOPREF_OFF			/* Sched L2 autopref.  */
+  tune_params::PREF_CONST_POOL_FALSE,
+  tune_params::PREF_LDRD_FALSE,
+  tune_params::LOG_OP_NON_SC_TRUE,		/* Thumb.  */
+  tune_params::LOG_OP_NON_SC_TRUE,		/* ARM.  */
+  tune_params::DISPARAGE_FLAGS_NEITHER,
+  tune_params::PREF_NEON_64_FALSE,
+  tune_params::PREF_NEON_STRINGOPS_TRUE,
+  tune_params::FUSE_NOTHING,
+  tune_params::SCHED_AUTOPREF_OFF
 };
 
 const struct tune_params arm_cortex_a7_tune =
 {
   arm_9e_rtx_costs,
   &cortexa7_extra_costs,
-  NULL,
+  NULL,					/* Sched adj cost.  */
+  arm_default_branch_cost,
+  &arm_default_vec_cost,
   1,						/* Constant limit.  */
   5,						/* Max cond insns.  */
+  8,						/* Memset max inline.  */
+  2,						/* Issue rate.  */
   ARM_PREFETCH_NOT_BENEFICIAL,
-  false,					/* Prefer constant pool.  */
-  arm_default_branch_cost,
-  false,					/* Prefer LDRD/STRD.  */
-  {true, true},					/* Prefer non short circuit.  */
-  &arm_default_vec_cost,			/* Vectorizer costs.  */
-  false,					/* Prefer Neon for 64-bits bitops.  */
-  false, false,                                 /* Prefer 32-bit encodings.  */
-  true,						/* Prefer Neon for stringops.  */
-  8,						/* Maximum insns to inline memset.  */
-  ARM_FUSE_NOTHING,				/* Fuseable pairs of instructions.  */
-  ARM_SCHED_AUTOPREF_OFF			/* Sched L2 autopref.  */
+  tune_params::PREF_CONST_POOL_FALSE,
+  tune_params::PREF_LDRD_FALSE,
+  tune_params::LOG_OP_NON_SC_TRUE,		/* Thumb.  */
+  tune_params::LOG_OP_NON_SC_TRUE,		/* ARM.  */
+  tune_params::DISPARAGE_FLAGS_NEITHER,
+  tune_params::PREF_NEON_64_FALSE,
+  tune_params::PREF_NEON_STRINGOPS_TRUE,
+  tune_params::FUSE_NOTHING,
+  tune_params::SCHED_AUTOPREF_OFF
 };
 
 const struct tune_params arm_cortex_a15_tune =
 {
   arm_9e_rtx_costs,
   &cortexa15_extra_costs,
-  NULL,						/* Sched adj cost.  */
+  NULL,					/* Sched adj cost.  */
+  arm_default_branch_cost,
+  &arm_default_vec_cost,
   1,						/* Constant limit.  */
   2,						/* Max cond insns.  */
+  8,						/* Memset max inline.  */
+  3,						/* Issue rate.  */
   ARM_PREFETCH_NOT_BENEFICIAL,
-  false,					/* Prefer constant pool.  */
-  arm_default_branch_cost,
-  true,						/* Prefer LDRD/STRD.  */
-  {true, true},					/* Prefer non short circuit.  */
-  &arm_default_vec_cost,                        /* Vectorizer costs.  */
-  false,                                        /* Prefer Neon for 64-bits bitops.  */
-  true, true,                                   /* Prefer 32-bit encodings.  */
-  true,						/* Prefer Neon for stringops.  */
-  8,						/* Maximum insns to inline memset.  */
-  ARM_FUSE_NOTHING,				/* Fuseable pairs of instructions.  */
-  ARM_SCHED_AUTOPREF_FULL			/* Sched L2 autopref.  */
+  tune_params::PREF_CONST_POOL_FALSE,
+  tune_params::PREF_LDRD_TRUE,
+  tune_params::LOG_OP_NON_SC_TRUE,		/* Thumb.  */
+  tune_params::LOG_OP_NON_SC_TRUE,		/* ARM.  */
+  tune_params::DISPARAGE_FLAGS_ALL,
+  tune_params::PREF_NEON_64_FALSE,
+  tune_params::PREF_NEON_STRINGOPS_TRUE,
+  tune_params::FUSE_NOTHING,
+  tune_params::SCHED_AUTOPREF_FULL
 };
 
 const struct tune_params arm_cortex_a53_tune =
 {
   arm_9e_rtx_costs,
   &cortexa53_extra_costs,
-  NULL,						/* Scheduler cost adjustment.  */
+  NULL,					/* Sched adj cost.  */
+  arm_default_branch_cost,
+  &arm_default_vec_cost,
   1,						/* Constant limit.  */
   5,						/* Max cond insns.  */
+  8,						/* Memset max inline.  */
+  2,						/* Issue rate.  */
   ARM_PREFETCH_NOT_BENEFICIAL,
-  false,					/* Prefer constant pool.  */
-  arm_default_branch_cost,
-  false,					/* Prefer LDRD/STRD.  */
-  {true, true},					/* Prefer non short circuit.  */
-  &arm_default_vec_cost,			/* Vectorizer costs.  */
-  false,					/* Prefer Neon for 64-bits bitops.  */
-  false, false,                                 /* Prefer 32-bit encodings.  */
-  true,						/* Prefer Neon for stringops.  */
-  8,						/* Maximum insns to inline memset.  */
-  ARM_FUSE_MOVW_MOVT,				/* Fuseable pairs of instructions.  */
-  ARM_SCHED_AUTOPREF_OFF			/* Sched L2 autopref.  */
+  tune_params::PREF_CONST_POOL_FALSE,
+  tune_params::PREF_LDRD_FALSE,
+  tune_params::LOG_OP_NON_SC_TRUE,		/* Thumb.  */
+  tune_params::LOG_OP_NON_SC_TRUE,		/* ARM.  */
+  tune_params::DISPARAGE_FLAGS_NEITHER,
+  tune_params::PREF_NEON_64_FALSE,
+  tune_params::PREF_NEON_STRINGOPS_TRUE,
+  FUSE_OPS (tune_params::FUSE_MOVW_MOVT),
+  tune_params::SCHED_AUTOPREF_OFF
 };
 
 const struct tune_params arm_cortex_a57_tune =
 {
   arm_9e_rtx_costs,
   &cortexa57_extra_costs,
-  NULL,                                         /* Scheduler cost adjustment.  */
-  1,                                           /* Constant limit.  */
-  2,                                           /* Max cond insns.  */
-  ARM_PREFETCH_NOT_BENEFICIAL,
-  false,                                       /* Prefer constant pool.  */
+  NULL,					/* Sched adj cost.  */
   arm_default_branch_cost,
-  true,                                       /* Prefer LDRD/STRD.  */
-  {true, true},                                /* Prefer non short circuit.  */
-  &arm_default_vec_cost,                       /* Vectorizer costs.  */
-  false,                                       /* Prefer Neon for 64-bits bitops.  */
-  true, true,                                  /* Prefer 32-bit encodings.  */
-  true,						/* Prefer Neon for stringops.  */
-  8,						/* Maximum insns to inline memset.  */
-  ARM_FUSE_MOVW_MOVT,				/* Fuseable pairs of instructions.  */
-  ARM_SCHED_AUTOPREF_FULL			/* Sched L2 autopref.  */
+  &arm_default_vec_cost,
+  1,						/* Constant limit.  */
+  2,						/* Max cond insns.  */
+  8,						/* Memset max inline.  */
+  3,						/* Issue rate.  */
+  ARM_PREFETCH_NOT_BENEFICIAL,
+  tune_params::PREF_CONST_POOL_FALSE,
+  tune_params::PREF_LDRD_TRUE,
+  tune_params::LOG_OP_NON_SC_TRUE,		/* Thumb.  */
+  tune_params::LOG_OP_NON_SC_TRUE,		/* ARM.  */
+  tune_params::DISPARAGE_FLAGS_ALL,
+  tune_params::PREF_NEON_64_FALSE,
+  tune_params::PREF_NEON_STRINGOPS_TRUE,
+  FUSE_OPS (tune_params::FUSE_MOVW_MOVT),
+  tune_params::SCHED_AUTOPREF_FULL
 };
 
 const struct tune_params arm_xgene1_tune =
 {
   arm_9e_rtx_costs,
   &xgene1_extra_costs,
-  NULL,                                        /* Scheduler cost adjustment.  */
-  1,                                           /* Constant limit.  */
-  2,                                           /* Max cond insns.  */
-  ARM_PREFETCH_NOT_BENEFICIAL,
-  false,                                       /* Prefer constant pool.  */
+  NULL,					/* Sched adj cost.  */
   arm_default_branch_cost,
-  true,                                        /* Prefer LDRD/STRD.  */
-  {true, true},                                /* Prefer non short circuit.  */
-  &arm_default_vec_cost,                       /* Vectorizer costs.  */
-  false,                                       /* Prefer Neon for 64-bits bitops.  */
-  true, true,                                  /* Prefer 32-bit encodings.  */
-  false,				       /* Prefer Neon for stringops.  */
-  32,					       /* Maximum insns to inline memset.  */
-  ARM_FUSE_NOTHING,				/* Fuseable pairs of instructions.  */
-  ARM_SCHED_AUTOPREF_OFF			/* Sched L2 autopref.  */
+  &arm_default_vec_cost,
+  1,						/* Constant limit.  */
+  2,						/* Max cond insns.  */
+  32,						/* Memset max inline.  */
+  4,						/* Issue rate.  */
+  ARM_PREFETCH_NOT_BENEFICIAL,
+  tune_params::PREF_CONST_POOL_FALSE,
+  tune_params::PREF_LDRD_TRUE,
+  tune_params::LOG_OP_NON_SC_TRUE,		/* Thumb.  */
+  tune_params::LOG_OP_NON_SC_TRUE,		/* ARM.  */
+  tune_params::DISPARAGE_FLAGS_ALL,
+  tune_params::PREF_NEON_64_FALSE,
+  tune_params::PREF_NEON_STRINGOPS_FALSE,
+  tune_params::FUSE_NOTHING,
+  tune_params::SCHED_AUTOPREF_OFF
 };
 
 /* Branches can be dual-issued on Cortex-A5, so conditional execution is
@@ -1965,21 +2028,23 @@ const struct tune_params arm_cortex_a5_tune =
 {
   arm_9e_rtx_costs,
   &cortexa5_extra_costs,
-  NULL,						/* Sched adj cost.  */
+  NULL,					/* Sched adj cost.  */
+  arm_cortex_a5_branch_cost,
+  &arm_default_vec_cost,
   1,						/* Constant limit.  */
   1,						/* Max cond insns.  */
+  8,						/* Memset max inline.  */
+  2,						/* Issue rate.  */
   ARM_PREFETCH_NOT_BENEFICIAL,
-  false,					/* Prefer constant pool.  */
-  arm_cortex_a5_branch_cost,
-  false,					/* Prefer LDRD/STRD.  */
-  {false, false},				/* Prefer non short circuit.  */
-  &arm_default_vec_cost,                        /* Vectorizer costs.  */
-  false,                                        /* Prefer Neon for 64-bits bitops.  */
-  false, false,                                 /* Prefer 32-bit encodings.  */
-  true,						/* Prefer Neon for stringops.  */
-  8,						/* Maximum insns to inline memset.  */
-  ARM_FUSE_NOTHING,				/* Fuseable pairs of instructions.  */
-  ARM_SCHED_AUTOPREF_OFF			/* Sched L2 autopref.  */
+  tune_params::PREF_CONST_POOL_FALSE,
+  tune_params::PREF_LDRD_FALSE,
+  tune_params::LOG_OP_NON_SC_FALSE,		/* Thumb.  */
+  tune_params::LOG_OP_NON_SC_FALSE,		/* ARM.  */
+  tune_params::DISPARAGE_FLAGS_NEITHER,
+  tune_params::PREF_NEON_64_FALSE,
+  tune_params::PREF_NEON_STRINGOPS_TRUE,
+  tune_params::FUSE_NOTHING,
+  tune_params::SCHED_AUTOPREF_OFF
 };
 
 const struct tune_params arm_cortex_a9_tune =
@@ -1987,41 +2052,45 @@ const struct tune_params arm_cortex_a9_tune =
   arm_9e_rtx_costs,
   &cortexa9_extra_costs,
   cortex_a9_sched_adjust_cost,
+  arm_default_branch_cost,
+  &arm_default_vec_cost,
   1,						/* Constant limit.  */
   5,						/* Max cond insns.  */
+  8,						/* Memset max inline.  */
+  2,						/* Issue rate.  */
   ARM_PREFETCH_BENEFICIAL(4,32,32),
-  false,					/* Prefer constant pool.  */
-  arm_default_branch_cost,
-  false,					/* Prefer LDRD/STRD.  */
-  {true, true},					/* Prefer non short circuit.  */
-  &arm_default_vec_cost,                        /* Vectorizer costs.  */
-  false,                                        /* Prefer Neon for 64-bits bitops.  */
-  false, false,                                 /* Prefer 32-bit encodings.  */
-  false,					/* Prefer Neon for stringops.  */
-  8,						/* Maximum insns to inline memset.  */
-  ARM_FUSE_NOTHING,				/* Fuseable pairs of instructions.  */
-  ARM_SCHED_AUTOPREF_OFF			/* Sched L2 autopref.  */
+  tune_params::PREF_CONST_POOL_FALSE,
+  tune_params::PREF_LDRD_FALSE,
+  tune_params::LOG_OP_NON_SC_TRUE,		/* Thumb.  */
+  tune_params::LOG_OP_NON_SC_TRUE,		/* ARM.  */
+  tune_params::DISPARAGE_FLAGS_NEITHER,
+  tune_params::PREF_NEON_64_FALSE,
+  tune_params::PREF_NEON_STRINGOPS_FALSE,
+  tune_params::FUSE_NOTHING,
+  tune_params::SCHED_AUTOPREF_OFF
 };
 
 const struct tune_params arm_cortex_a12_tune =
 {
   arm_9e_rtx_costs,
   &cortexa12_extra_costs,
-  NULL,						/* Sched adj cost.  */
+  NULL,					/* Sched adj cost.  */
+  arm_default_branch_cost,
+  &arm_default_vec_cost,                        /* Vectorizer costs.  */
   1,						/* Constant limit.  */
   2,						/* Max cond insns.  */
+  8,						/* Memset max inline.  */
+  2,						/* Issue rate.  */
   ARM_PREFETCH_NOT_BENEFICIAL,
-  false,					/* Prefer constant pool.  */
-  arm_default_branch_cost,
-  true,						/* Prefer LDRD/STRD.  */
-  {true, true},					/* Prefer non short circuit.  */
-  &arm_default_vec_cost,                        /* Vectorizer costs.  */
-  false,                                        /* Prefer Neon for 64-bits bitops.  */
-  true, true,                                   /* Prefer 32-bit encodings.  */
-  true,						/* Prefer Neon for stringops.  */
-  8,						/* Maximum insns to inline memset.  */
-  ARM_FUSE_MOVW_MOVT,				/* Fuseable pairs of instructions.  */
-  ARM_SCHED_AUTOPREF_OFF			/* Sched L2 autopref.  */
+  tune_params::PREF_CONST_POOL_FALSE,
+  tune_params::PREF_LDRD_TRUE,
+  tune_params::LOG_OP_NON_SC_TRUE,		/* Thumb.  */
+  tune_params::LOG_OP_NON_SC_TRUE,		/* ARM.  */
+  tune_params::DISPARAGE_FLAGS_ALL,
+  tune_params::PREF_NEON_64_FALSE,
+  tune_params::PREF_NEON_STRINGOPS_TRUE,
+  FUSE_OPS (tune_params::FUSE_MOVW_MOVT),
+  tune_params::SCHED_AUTOPREF_OFF
 };
 
 /* armv7m tuning.  On Cortex-M4 cores for example, MOVW/MOVT take a single
@@ -2035,21 +2104,23 @@ const struct tune_params arm_v7m_tune =
 {
   arm_9e_rtx_costs,
   &v7m_extra_costs,
-  NULL,						/* Sched adj cost.  */
+  NULL,					/* Sched adj cost.  */
+  arm_cortex_m_branch_cost,
+  &arm_default_vec_cost,
   1,						/* Constant limit.  */
   2,						/* Max cond insns.  */
+  8,						/* Memset max inline.  */
+  1,						/* Issue rate.  */
   ARM_PREFETCH_NOT_BENEFICIAL,
-  true,						/* Prefer constant pool.  */
-  arm_cortex_m_branch_cost,
-  false,					/* Prefer LDRD/STRD.  */
-  {false, false},				/* Prefer non short circuit.  */
-  &arm_default_vec_cost,                        /* Vectorizer costs.  */
-  false,                                        /* Prefer Neon for 64-bits bitops.  */
-  false, false,                                 /* Prefer 32-bit encodings.  */
-  false,					/* Prefer Neon for stringops.  */
-  8,						/* Maximum insns to inline memset.  */
-  ARM_FUSE_NOTHING,				/* Fuseable pairs of instructions.  */
-  ARM_SCHED_AUTOPREF_OFF			/* Sched L2 autopref.  */
+  tune_params::PREF_CONST_POOL_TRUE,
+  tune_params::PREF_LDRD_FALSE,
+  tune_params::LOG_OP_NON_SC_FALSE,		/* Thumb.  */
+  tune_params::LOG_OP_NON_SC_FALSE,		/* ARM.  */
+  tune_params::DISPARAGE_FLAGS_NEITHER,
+  tune_params::PREF_NEON_64_FALSE,
+  tune_params::PREF_NEON_STRINGOPS_FALSE,
+  tune_params::FUSE_NOTHING,
+  tune_params::SCHED_AUTOPREF_OFF
 };
 
 /* Cortex-M7 tuning.  */
@@ -2058,21 +2129,23 @@ const struct tune_params arm_cortex_m7_tune =
 {
   arm_9e_rtx_costs,
   &v7m_extra_costs,
-  NULL,						/* Sched adj cost.  */
+  NULL,					/* Sched adj cost.  */
+  arm_cortex_m7_branch_cost,
+  &arm_default_vec_cost,
   0,						/* Constant limit.  */
   1,						/* Max cond insns.  */
+  8,						/* Memset max inline.  */
+  2,						/* Issue rate.  */
   ARM_PREFETCH_NOT_BENEFICIAL,
-  true,						/* Prefer constant pool.  */
-  arm_cortex_m7_branch_cost,
-  false,					/* Prefer LDRD/STRD.  */
-  {true, true},					/* Prefer non short circuit.  */
-  &arm_default_vec_cost,                        /* Vectorizer costs.  */
-  false,                                        /* Prefer Neon for 64-bits bitops.  */
-  false, false,                                 /* Prefer 32-bit encodings.  */
-  false,					/* Prefer Neon for stringops.  */
-  8,						/* Maximum insns to inline memset.  */
-  ARM_FUSE_NOTHING,				/* Fuseable pairs of instructions.  */
-  ARM_SCHED_AUTOPREF_OFF			/* Sched L2 autopref.  */
+  tune_params::PREF_CONST_POOL_TRUE,
+  tune_params::PREF_LDRD_FALSE,
+  tune_params::LOG_OP_NON_SC_TRUE,		/* Thumb.  */
+  tune_params::LOG_OP_NON_SC_TRUE,		/* ARM.  */
+  tune_params::DISPARAGE_FLAGS_NEITHER,
+  tune_params::PREF_NEON_64_FALSE,
+  tune_params::PREF_NEON_STRINGOPS_FALSE,
+  tune_params::FUSE_NOTHING,
+  tune_params::SCHED_AUTOPREF_OFF
 };
 
 /* The arm_v6m_tune is duplicated from arm_cortex_tune, rather than
@@ -2080,43 +2153,47 @@ const struct tune_params arm_cortex_m7_tune =
 const struct tune_params arm_v6m_tune =
 {
   arm_9e_rtx_costs,
-  NULL,
-  NULL,						/* Sched adj cost.  */
+  NULL,					/* Insn extra costs.  */
+  NULL,					/* Sched adj cost.  */
+  arm_default_branch_cost,
+  &arm_default_vec_cost,                        /* Vectorizer costs.  */
   1,						/* Constant limit.  */
   5,						/* Max cond insns.  */
+  8,						/* Memset max inline.  */
+  1,						/* Issue rate.  */
   ARM_PREFETCH_NOT_BENEFICIAL,
-  false,					/* Prefer constant pool.  */
-  arm_default_branch_cost,
-  false,					/* Prefer LDRD/STRD.  */
-  {false, false},				/* Prefer non short circuit.  */
-  &arm_default_vec_cost,                        /* Vectorizer costs.  */
-  false,                                        /* Prefer Neon for 64-bits bitops.  */
-  false, false,                                 /* Prefer 32-bit encodings.  */
-  false,					/* Prefer Neon for stringops.  */
-  8,						/* Maximum insns to inline memset.  */
-  ARM_FUSE_NOTHING,				/* Fuseable pairs of instructions.  */
-  ARM_SCHED_AUTOPREF_OFF			/* Sched L2 autopref.  */
+  tune_params::PREF_CONST_POOL_FALSE,
+  tune_params::PREF_LDRD_FALSE,
+  tune_params::LOG_OP_NON_SC_FALSE,		/* Thumb.  */
+  tune_params::LOG_OP_NON_SC_FALSE,		/* ARM.  */
+  tune_params::DISPARAGE_FLAGS_NEITHER,
+  tune_params::PREF_NEON_64_FALSE,
+  tune_params::PREF_NEON_STRINGOPS_FALSE,
+  tune_params::FUSE_NOTHING,
+  tune_params::SCHED_AUTOPREF_OFF
 };
 
 const struct tune_params arm_fa726te_tune =
 {
   arm_9e_rtx_costs,
-  NULL,
+  NULL,					/* Insn extra costs.  */
   fa726te_sched_adjust_cost,
+  arm_default_branch_cost,
+  &arm_default_vec_cost,
   1,						/* Constant limit.  */
   5,						/* Max cond insns.  */
+  8,						/* Memset max inline.  */
+  2,						/* Issue rate.  */
   ARM_PREFETCH_NOT_BENEFICIAL,
-  true,						/* Prefer constant pool.  */
-  arm_default_branch_cost,
-  false,					/* Prefer LDRD/STRD.  */
-  {true, true},					/* Prefer non short circuit.  */
-  &arm_default_vec_cost,                        /* Vectorizer costs.  */
-  false,                                        /* Prefer Neon for 64-bits bitops.  */
-  false, false,                                 /* Prefer 32-bit encodings.  */
-  false,					/* Prefer Neon for stringops.  */
-  8,						/* Maximum insns to inline memset.  */
-  ARM_FUSE_NOTHING,				/* Fuseable pairs of instructions.  */
-  ARM_SCHED_AUTOPREF_OFF			/* Sched L2 autopref.  */
+  tune_params::PREF_CONST_POOL_TRUE,
+  tune_params::PREF_LDRD_FALSE,
+  tune_params::LOG_OP_NON_SC_TRUE,		/* Thumb.  */
+  tune_params::LOG_OP_NON_SC_TRUE,		/* ARM.  */
+  tune_params::DISPARAGE_FLAGS_NEITHER,
+  tune_params::PREF_NEON_64_FALSE,
+  tune_params::PREF_NEON_STRINGOPS_FALSE,
+  tune_params::FUSE_NOTHING,
+  tune_params::SCHED_AUTOPREF_OFF
 };
 
 
@@ -3140,31 +3217,33 @@ arm_option_override (void)
       && abi_version_at_least(2))
     flag_strict_volatile_bitfields = 1;
 
-  /* Enable sw prefetching at -O3 for CPUS that have prefetch, and we have deemed
-     it beneficial (signified by setting num_prefetch_slots to 1 or more.)  */
+  /* Enable sw prefetching at -O3 for CPUS that have prefetch, and we
+     have deemed it beneficial (signified by setting
+     prefetch.num_slots to 1 or more).  */
   if (flag_prefetch_loop_arrays < 0
       && HAVE_prefetch
       && optimize >= 3
-      && current_tune->num_prefetch_slots > 0)
+      && current_tune->prefetch.num_slots > 0)
     flag_prefetch_loop_arrays = 1;
 
-  /* Set up parameters to be used in prefetching algorithm.  Do not override the
-     defaults unless we are tuning for a core we have researched values for.  */
-  if (current_tune->num_prefetch_slots > 0)
+  /* Set up parameters to be used in prefetching algorithm.  Do not
+     override the defaults unless we are tuning for a core we have
+     researched values for.  */
+  if (current_tune->prefetch.num_slots > 0)
     maybe_set_param_value (PARAM_SIMULTANEOUS_PREFETCHES,
-                           current_tune->num_prefetch_slots,
-                           global_options.x_param_values,
-                           global_options_set.x_param_values);
-  if (current_tune->l1_cache_line_size >= 0)
+			   current_tune->prefetch.num_slots,
+			   global_options.x_param_values,
+			   global_options_set.x_param_values);
+  if (current_tune->prefetch.l1_cache_line_size >= 0)
     maybe_set_param_value (PARAM_L1_CACHE_LINE_SIZE,
-                           current_tune->l1_cache_line_size,
-                           global_options.x_param_values,
-                           global_options_set.x_param_values);
-  if (current_tune->l1_cache_size >= 0)
+			   current_tune->prefetch.l1_cache_line_size,
+			   global_options.x_param_values,
+			   global_options_set.x_param_values);
+  if (current_tune->prefetch.l1_cache_size >= 0)
     maybe_set_param_value (PARAM_L1_CACHE_SIZE,
-                           current_tune->l1_cache_size,
-                           global_options.x_param_values,
-                           global_options_set.x_param_values);
+			   current_tune->prefetch.l1_cache_size,
+			   global_options.x_param_values,
+			   global_options_set.x_param_values);
 
   /* Use Neon to perform 64-bits operations rather than core
      registers.  */
@@ -3174,24 +3253,35 @@ arm_option_override (void)
 
   /* Use the alternative scheduling-pressure algorithm by default.  */
   maybe_set_param_value (PARAM_SCHED_PRESSURE_ALGORITHM, SCHED_PRESSURE_MODEL,
-                         global_options.x_param_values,
-                         global_options_set.x_param_values);
+			 global_options.x_param_values,
+			 global_options_set.x_param_values);
 
   /* Look through ready list and all of queue for instructions
      relevant for L2 auto-prefetcher.  */
   int param_sched_autopref_queue_depth;
-  if (current_tune->sched_autopref == ARM_SCHED_AUTOPREF_OFF)
-    param_sched_autopref_queue_depth = -1;
-  else if (current_tune->sched_autopref == ARM_SCHED_AUTOPREF_RANK)
-    param_sched_autopref_queue_depth = 0;
-  else if (current_tune->sched_autopref == ARM_SCHED_AUTOPREF_FULL)
-    param_sched_autopref_queue_depth = max_insn_queue_index + 1;
-  else
-    gcc_unreachable ();
+
+  switch (current_tune->sched_autopref)
+    {
+    case tune_params::SCHED_AUTOPREF_OFF:
+      param_sched_autopref_queue_depth = -1;
+      break;
+
+    case tune_params::SCHED_AUTOPREF_RANK:
+      param_sched_autopref_queue_depth = 0;
+      break;
+
+    case tune_params::SCHED_AUTOPREF_FULL:
+      param_sched_autopref_queue_depth = max_insn_queue_index + 1;
+      break;
+
+    default:
+      gcc_unreachable ();
+    }
+
   maybe_set_param_value (PARAM_SCHED_AUTOPREF_QUEUE_DEPTH,
 			 param_sched_autopref_queue_depth,
-                         global_options.x_param_values,
-                         global_options_set.x_param_values);
+			 global_options.x_param_values,
+			 global_options_set.x_param_values);
 
   /* Disable shrink-wrap when optimizing function for size, since it tends to
      generate additional returns.  */
@@ -7946,236 +8036,6 @@ thumb_legitimize_address (rtx x, rtx orig_x, machine_mode mode)
   return x;
 }
 
-bool
-arm_legitimize_reload_address (rtx *p,
-			       machine_mode mode,
-			       int opnum, int type,
-			       int ind_levels ATTRIBUTE_UNUSED)
-{
-  /* We must recognize output that we have already generated ourselves.  */
-  if (GET_CODE (*p) == PLUS
-      && GET_CODE (XEXP (*p, 0)) == PLUS
-      && REG_P (XEXP (XEXP (*p, 0), 0))
-      && CONST_INT_P (XEXP (XEXP (*p, 0), 1))
-      && CONST_INT_P (XEXP (*p, 1)))
-    {
-      push_reload (XEXP (*p, 0), NULL_RTX, &XEXP (*p, 0), NULL,
-		   MODE_BASE_REG_CLASS (mode), GET_MODE (*p),
-		   VOIDmode, 0, 0, opnum, (enum reload_type) type);
-      return true;
-    }
-
-  if (GET_CODE (*p) == PLUS
-      && REG_P (XEXP (*p, 0))
-      && ARM_REGNO_OK_FOR_BASE_P (REGNO (XEXP (*p, 0)))
-      /* If the base register is equivalent to a constant, let the generic
-	 code handle it.  Otherwise we will run into problems if a future
-	 reload pass decides to rematerialize the constant.  */
-      && !reg_equiv_constant (ORIGINAL_REGNO (XEXP (*p, 0)))
-      && CONST_INT_P (XEXP (*p, 1)))
-    {
-      HOST_WIDE_INT val = INTVAL (XEXP (*p, 1));
-      HOST_WIDE_INT low, high;
-
-      /* Detect coprocessor load/stores.  */
-      bool coproc_p = ((TARGET_HARD_FLOAT
-			&& TARGET_VFP
-			&& (mode == SFmode || mode == DFmode))
-		       || (TARGET_REALLY_IWMMXT
-			   && VALID_IWMMXT_REG_MODE (mode))
-		       || (TARGET_NEON
-			   && (VALID_NEON_DREG_MODE (mode)
-			       || VALID_NEON_QREG_MODE (mode))));
-
-      /* For some conditions, bail out when lower two bits are unaligned.  */
-      if ((val & 0x3) != 0
-	  /* Coprocessor load/store indexes are 8-bits + '00' appended.  */
-	  && (coproc_p
-	      /* For DI, and DF under soft-float: */
-	      || ((mode == DImode || mode == DFmode)
-		  /* Without ldrd, we use stm/ldm, which does not
-		     fair well with unaligned bits.  */
-		  && (! TARGET_LDRD
-		      /* Thumb-2 ldrd/strd is [-1020,+1020] in steps of 4.  */
-		      || TARGET_THUMB2))))
-	return false;
-
-      /* When breaking down a [reg+index] reload address into [(reg+high)+low],
-	 of which the (reg+high) gets turned into a reload add insn,
-	 we try to decompose the index into high/low values that can often
-	 also lead to better reload CSE.
-	 For example:
-	         ldr r0, [r2, #4100]  // Offset too large
-		 ldr r1, [r2, #4104]  // Offset too large
-
-	 is best reloaded as:
-	         add t1, r2, #4096
-		 ldr r0, [t1, #4]
-		 add t2, r2, #4096
-		 ldr r1, [t2, #8]
-
-	 which post-reload CSE can simplify in most cases to eliminate the
-	 second add instruction:
-	         add t1, r2, #4096
-		 ldr r0, [t1, #4]
-		 ldr r1, [t1, #8]
-
-	 The idea here is that we want to split out the bits of the constant
-	 as a mask, rather than as subtracting the maximum offset that the
-	 respective type of load/store used can handle.
-
-	 When encountering negative offsets, we can still utilize it even if
-	 the overall offset is positive; sometimes this may lead to an immediate
-	 that can be constructed with fewer instructions.
-	 For example:
-	         ldr r0, [r2, #0x3FFFFC]
-
-	 This is best reloaded as:
-	         add t1, r2, #0x400000
-		 ldr r0, [t1, #-4]
-
-	 The trick for spotting this for a load insn with N bits of offset
-	 (i.e. bits N-1:0) is to look at bit N; if it is set, then chose a
-	 negative offset that is going to make bit N and all the bits below
-	 it become zero in the remainder part.
-
-	 The SIGN_MAG_LOW_ADDR_BITS macro below implements this, with respect
-	 to sign-magnitude addressing (i.e. separate +- bit, or 1's complement),
-	 used in most cases of ARM load/store instructions.  */
-
-#define SIGN_MAG_LOW_ADDR_BITS(VAL, N)					\
-      (((VAL) & ((1 << (N)) - 1))					\
-       ? (((VAL) & ((1 << ((N) + 1)) - 1)) ^ (1 << (N))) - (1 << (N))	\
-       : 0)
-
-      if (coproc_p)
-	{
-	  low = SIGN_MAG_LOW_ADDR_BITS (val, 10);
-
-	  /* NEON quad-word load/stores are made of two double-word accesses,
-	     so the valid index range is reduced by 8. Treat as 9-bit range if
-	     we go over it.  */
-	  if (TARGET_NEON && VALID_NEON_QREG_MODE (mode) && low >= 1016)
-	    low = SIGN_MAG_LOW_ADDR_BITS (val, 9);
-	}
-      else if (GET_MODE_SIZE (mode) == 8)
-	{
-	  if (TARGET_LDRD)
-	    low = (TARGET_THUMB2
-		   ? SIGN_MAG_LOW_ADDR_BITS (val, 10)
-		   : SIGN_MAG_LOW_ADDR_BITS (val, 8));
-	  else
-	    /* For pre-ARMv5TE (without ldrd), we use ldm/stm(db/da/ib)
-	       to access doublewords. The supported load/store offsets are
-	       -8, -4, and 4, which we try to produce here.  */
-	    low = ((val & 0xf) ^ 0x8) - 0x8;
-	}
-      else if (GET_MODE_SIZE (mode) < 8)
-	{
-	  /* NEON element load/stores do not have an offset.  */
-	  if (TARGET_NEON_FP16 && mode == HFmode)
-	    return false;
-
-	  if (TARGET_THUMB2)
-	    {
-	      /* Thumb-2 has an asymmetrical index range of (-256,4096).
-		 Try the wider 12-bit range first, and re-try if the result
-		 is out of range.  */
-	      low = SIGN_MAG_LOW_ADDR_BITS (val, 12);
-	      if (low < -255)
-		low = SIGN_MAG_LOW_ADDR_BITS (val, 8);
-	    }
-	  else
-	    {
-	      if (mode == HImode || mode == HFmode)
-		{
-		  if (arm_arch4)
-		    low = SIGN_MAG_LOW_ADDR_BITS (val, 8);
-		  else
-		    {
-		      /* The storehi/movhi_bytes fallbacks can use only
-			 [-4094,+4094] of the full ldrb/strb index range.  */
-		      low = SIGN_MAG_LOW_ADDR_BITS (val, 12);
-		      if (low == 4095 || low == -4095)
-			return false;
-		    }
-		}
-	      else
-		low = SIGN_MAG_LOW_ADDR_BITS (val, 12);
-	    }
-	}
-      else
-	return false;
-
-      high = ((((val - low) & (unsigned HOST_WIDE_INT) 0xffffffff)
-	       ^ (unsigned HOST_WIDE_INT) 0x80000000)
-	      - (unsigned HOST_WIDE_INT) 0x80000000);
-      /* Check for overflow or zero */
-      if (low == 0 || high == 0 || (high + low != val))
-	return false;
-
-      /* Reload the high part into a base reg; leave the low part
-	 in the mem.
-	 Note that replacing this gen_rtx_PLUS with plus_constant is
-	 wrong in this case because we rely on the
-	 (plus (plus reg c1) c2) structure being preserved so that
-	 XEXP (*p, 0) in push_reload below uses the correct term.  */
-      *p = gen_rtx_PLUS (GET_MODE (*p),
-			 gen_rtx_PLUS (GET_MODE (*p), XEXP (*p, 0),
-				       GEN_INT (high)),
-			 GEN_INT (low));
-      push_reload (XEXP (*p, 0), NULL_RTX, &XEXP (*p, 0), NULL,
-		   MODE_BASE_REG_CLASS (mode), GET_MODE (*p),
-		   VOIDmode, 0, 0, opnum, (enum reload_type) type);
-      return true;
-    }
-
-  return false;
-}
-
-rtx
-thumb_legitimize_reload_address (rtx *x_p,
-				 machine_mode mode,
-				 int opnum, int type,
-				 int ind_levels ATTRIBUTE_UNUSED)
-{
-  rtx x = *x_p;
-
-  if (GET_CODE (x) == PLUS
-      && GET_MODE_SIZE (mode) < 4
-      && REG_P (XEXP (x, 0))
-      && XEXP (x, 0) == stack_pointer_rtx
-      && CONST_INT_P (XEXP (x, 1))
-      && !thumb_legitimate_offset_p (mode, INTVAL (XEXP (x, 1))))
-    {
-      rtx orig_x = x;
-
-      x = copy_rtx (x);
-      push_reload (orig_x, NULL_RTX, x_p, NULL, MODE_BASE_REG_CLASS (mode),
-		   Pmode, VOIDmode, 0, 0, opnum, (enum reload_type) type);
-      return x;
-    }
-
-  /* If both registers are hi-regs, then it's better to reload the
-     entire expression rather than each register individually.  That
-     only requires one reload register rather than two.  */
-  if (GET_CODE (x) == PLUS
-      && REG_P (XEXP (x, 0))
-      && REG_P (XEXP (x, 1))
-      && !REG_MODE_OK_FOR_REG_BASE_P (XEXP (x, 0), mode)
-      && !REG_MODE_OK_FOR_REG_BASE_P (XEXP (x, 1), mode))
-    {
-      rtx orig_x = x;
-
-      x = copy_rtx (x);
-      push_reload (orig_x, NULL_RTX, x_p, NULL, MODE_BASE_REG_CLASS (mode),
-		   Pmode, VOIDmode, 0, 0, opnum, (enum reload_type) type);
-      return x;
-    }
-
-  return NULL;
-}
-
 /* Return TRUE if X contains any TLS symbol references.  */
 
 bool
@@ -9399,7 +9259,8 @@ static bool
 arm_unspec_cost (rtx x, enum rtx_code /* outer_code */, bool speed_p, int *cost)
 {
   const struct cpu_cost_table *extra_cost = current_tune->insn_extra_cost;
-  gcc_assert (GET_CODE (x) == UNSPEC);
+  rtx_code code = GET_CODE (x);
+  gcc_assert (code == UNSPEC || code == UNSPEC_VOLATILE);
 
   switch (XINT (x, 1))
     {
@@ -9445,7 +9306,7 @@ arm_unspec_cost (rtx x, enum rtx_code /* outer_code */, bool speed_p, int *cost)
       *cost = COSTS_N_INSNS (2);
       break;
     }
-  return false;
+  return true;
 }
 
 /* Cost of a libcall.  We assume one insn per argument, an amount for the
@@ -11008,6 +10869,7 @@ arm_new_rtx_costs (rtx x, enum rtx_code code, enum rtx_code outer_code,
       *cost = LIBCALL_COST (1);
       return false;
 
+    case UNSPEC_VOLATILE:
     case UNSPEC:
       return arm_unspec_cost (x, outer_code, speed_p, cost);
 
@@ -17287,14 +17149,16 @@ thumb2_reorg (void)
 
   FOR_EACH_BB_FN (bb, cfun)
     {
-      if (current_tune->disparage_flag_setting_t16_encodings
+      if ((current_tune->disparage_flag_setting_t16_encodings
+	   == tune_params::DISPARAGE_FLAGS_ALL)
 	  && optimize_bb_for_speed_p (bb))
 	continue;
 
       rtx_insn *insn;
       Convert_Action action = SKIP;
       Convert_Action action_for_partial_flag_setting
-	= (current_tune->disparage_partial_flag_setting_t16_encodings
+	= ((current_tune->disparage_flag_setting_t16_encodings
+	    != tune_params::DISPARAGE_FLAGS_NEITHER)
 	   && optimize_bb_for_speed_p (bb))
 	  ? SKIP : CONV;
 
@@ -25660,12 +25524,12 @@ arm_print_tune_info (void)
 	       current_tune->constant_limit);
   asm_fprintf (asm_out_file, "\t\t@max_insns_skipped:\t%d\n",
 	       current_tune->max_insns_skipped);
-  asm_fprintf (asm_out_file, "\t\t@num_prefetch_slots:\t%d\n",
-	       current_tune->num_prefetch_slots);
-  asm_fprintf (asm_out_file, "\t\t@l1_cache_size:\t%d\n",
-	       current_tune->l1_cache_size);
-  asm_fprintf (asm_out_file, "\t\t@l1_cache_line_size:\t%d\n",
-	       current_tune->l1_cache_line_size);
+  asm_fprintf (asm_out_file, "\t\t@prefetch.num_slots:\t%d\n",
+	       current_tune->prefetch.num_slots);
+  asm_fprintf (asm_out_file, "\t\t@prefetch.l1_cache_size:\t%d\n",
+	       current_tune->prefetch.l1_cache_size);
+  asm_fprintf (asm_out_file, "\t\t@prefetch.l1_cache_line_size:\t%d\n",
+	       current_tune->prefetch.l1_cache_line_size);
   asm_fprintf (asm_out_file, "\t\t@prefer_constant_pool:\t%d\n",
 	       (int) current_tune->prefer_constant_pool);
   asm_fprintf (asm_out_file, "\t\t@branch_cost:\t(s:speed, p:predictable)\n");
@@ -25681,17 +25545,13 @@ arm_print_tune_info (void)
   asm_fprintf (asm_out_file, "\t\t@prefer_ldrd_strd:\t%d\n",
 	       (int) current_tune->prefer_ldrd_strd);
   asm_fprintf (asm_out_file, "\t\t@logical_op_non_short_circuit:\t[%d,%d]\n",
-	       (int) current_tune->logical_op_non_short_circuit[0],
-	       (int) current_tune->logical_op_non_short_circuit[1]);
+	       (int) current_tune->logical_op_non_short_circuit_thumb,
+	       (int) current_tune->logical_op_non_short_circuit_arm);
   asm_fprintf (asm_out_file, "\t\t@prefer_neon_for_64bits:\t%d\n",
 	       (int) current_tune->prefer_neon_for_64bits);
   asm_fprintf (asm_out_file,
 	       "\t\t@disparage_flag_setting_t16_encodings:\t%d\n",
 	       (int) current_tune->disparage_flag_setting_t16_encodings);
-  asm_fprintf (asm_out_file,
-	       "\t\t@disparage_partial_flag_setting_t16_encodings:\t%d\n",
-	       (int) current_tune
-	               ->disparage_partial_flag_setting_t16_encodings);
   asm_fprintf (asm_out_file, "\t\t@string_ops_prefer_neon:\t%d\n",
 	       (int) current_tune->string_ops_prefer_neon);
   asm_fprintf (asm_out_file, "\t\t@max_insns_inline_memset:\t%d\n",
@@ -27213,40 +27073,12 @@ thumb2_output_casesi (rtx *operands)
     }
 }
 
-/* Most ARM cores are single issue, but some newer ones can dual issue.
-   The scheduler descriptions rely on this being correct.  */
+/* Implement TARGET_SCHED_ISSUE_RATE.  Lookup the issue rate in the
+   per-core tuning structs.  */
 static int
 arm_issue_rate (void)
 {
-  switch (arm_tune)
-    {
-    case xgene1:
-      return 4;
-
-    case cortexa15:
-    case cortexa57:
-    case exynosm1:
-      return 3;
-
-    case cortexm7:
-    case cortexr4:
-    case cortexr4f:
-    case cortexr5:
-    case genericv7a:
-    case cortexa5:
-    case cortexa7:
-    case cortexa8:
-    case cortexa9:
-    case cortexa12:
-    case cortexa17:
-    case cortexa53:
-    case fa726te:
-    case marvell_pj4:
-      return 2;
-
-    default:
-      return 1;
-    }
+  return current_tune->issue_rate;
 }
 
 /* Return how many instructions should scheduler lookahead to choose the
@@ -29411,7 +29243,7 @@ arm_gen_setmem (rtx *operands)
 static bool
 arm_macro_fusion_p (void)
 {
-  return current_tune->fuseable_ops != ARM_FUSE_NOTHING;
+  return current_tune->fuseable_ops != tune_params::FUSE_NOTHING;
 }
 
 
@@ -29432,44 +29264,44 @@ aarch_macro_fusion_pair_p (rtx_insn* prev, rtx_insn* curr)
   if (!arm_macro_fusion_p ())
     return false;
 
-  if (current_tune->fuseable_ops & ARM_FUSE_MOVW_MOVT)
+  if (current_tune->fuseable_ops & tune_params::FUSE_MOVW_MOVT)
     {
       /* We are trying to fuse
-         movw imm / movt imm
-         instructions as a group that gets scheduled together.  */
+	 movw imm / movt imm
+	 instructions as a group that gets scheduled together.  */
 
       set_dest = SET_DEST (curr_set);
 
       if (GET_MODE (set_dest) != SImode)
-        return false;
+	return false;
 
       /* We are trying to match:
-         prev (movw)  == (set (reg r0) (const_int imm16))
-         curr (movt) == (set (zero_extract (reg r0)
-                                           (const_int 16)
-                                           (const_int 16))
-                             (const_int imm16_1))
-         or
-         prev (movw) == (set (reg r1)
-                              (high (symbol_ref ("SYM"))))
-         curr (movt) == (set (reg r0)
-                             (lo_sum (reg r1)
-                                     (symbol_ref ("SYM"))))  */
+	 prev (movw)  == (set (reg r0) (const_int imm16))
+	 curr (movt) == (set (zero_extract (reg r0)
+					  (const_int 16)
+					   (const_int 16))
+			     (const_int imm16_1))
+	 or
+	 prev (movw) == (set (reg r1)
+			      (high (symbol_ref ("SYM"))))
+	 curr (movt) == (set (reg r0)
+			     (lo_sum (reg r1)
+				     (symbol_ref ("SYM"))))  */
       if (GET_CODE (set_dest) == ZERO_EXTRACT)
-        {
-          if (CONST_INT_P (SET_SRC (curr_set))
-              && CONST_INT_P (SET_SRC (prev_set))
-              && REG_P (XEXP (set_dest, 0))
-              && REG_P (SET_DEST (prev_set))
-              && REGNO (XEXP (set_dest, 0)) == REGNO (SET_DEST (prev_set)))
-            return true;
-        }
+	{
+	  if (CONST_INT_P (SET_SRC (curr_set))
+	      && CONST_INT_P (SET_SRC (prev_set))
+	      && REG_P (XEXP (set_dest, 0))
+	      && REG_P (SET_DEST (prev_set))
+	      && REGNO (XEXP (set_dest, 0)) == REGNO (SET_DEST (prev_set)))
+	    return true;
+	}
       else if (GET_CODE (SET_SRC (curr_set)) == LO_SUM
-               && REG_P (SET_DEST (curr_set))
-               && REG_P (SET_DEST (prev_set))
-               && GET_CODE (SET_SRC (prev_set)) == HIGH
-               && REGNO (SET_DEST (curr_set)) == REGNO (SET_DEST (prev_set)))
-             return true;
+	       && REG_P (SET_DEST (curr_set))
+	       && REG_P (SET_DEST (prev_set))
+	       && GET_CODE (SET_SRC (prev_set)) == HIGH
+	       && REGNO (SET_DEST (curr_set)) == REGNO (SET_DEST (prev_set)))
+	     return true;
     }
   return false;
 }
--- a/src/gcc/config/arm/arm.h
+++ b/src/gcc/config/arm/arm.h
@@ -1360,46 +1360,6 @@ enum reg_class
      ? GENERAL_REGS : NO_REGS)					\
     : THUMB_SECONDARY_INPUT_RELOAD_CLASS (CLASS, MODE, X)))
 
-/* Try a machine-dependent way of reloading an illegitimate address
-   operand.  If we find one, push the reload and jump to WIN.  This
-   macro is used in only one place: `find_reloads_address' in reload.c.
-
-   For the ARM, we wish to handle large displacements off a base
-   register by splitting the addend across a MOV and the mem insn.
-   This can cut the number of reloads needed.  */
-#define ARM_LEGITIMIZE_RELOAD_ADDRESS(X, MODE, OPNUM, TYPE, IND, WIN)	   \
-  do									   \
-    {									   \
-      if (arm_legitimize_reload_address (&X, MODE, OPNUM, TYPE, IND))	   \
-	goto WIN;							   \
-    }									   \
-  while (0)
-
-/* XXX If an HImode FP+large_offset address is converted to an HImode
-   SP+large_offset address, then reload won't know how to fix it.  It sees
-   only that SP isn't valid for HImode, and so reloads the SP into an index
-   register, but the resulting address is still invalid because the offset
-   is too big.  We fix it here instead by reloading the entire address.  */
-/* We could probably achieve better results by defining PROMOTE_MODE to help
-   cope with the variances between the Thumb's signed and unsigned byte and
-   halfword load instructions.  */
-/* ??? This should be safe for thumb2, but we may be able to do better.  */
-#define THUMB_LEGITIMIZE_RELOAD_ADDRESS(X, MODE, OPNUM, TYPE, IND_L, WIN)     \
-do {									      \
-  rtx new_x = thumb_legitimize_reload_address (&X, MODE, OPNUM, TYPE, IND_L); \
-  if (new_x)								      \
-    {									      \
-      X = new_x;							      \
-      goto WIN;								      \
-    }									      \
-} while (0)
-
-#define LEGITIMIZE_RELOAD_ADDRESS(X, MODE, OPNUM, TYPE, IND_LEVELS, WIN)   \
-  if (TARGET_ARM)							   \
-    ARM_LEGITIMIZE_RELOAD_ADDRESS (X, MODE, OPNUM, TYPE, IND_LEVELS, WIN); \
-  else									   \
-    THUMB_LEGITIMIZE_RELOAD_ADDRESS (X, MODE, OPNUM, TYPE, IND_LEVELS, WIN)
-
 /* Return the maximum number of consecutive registers
    needed to represent mode MODE in a register of class CLASS.
    ARM regs are UNITS_PER_WORD bits.  
@@ -2096,10 +2056,11 @@ enum arm_auto_incmodes
   (current_tune->branch_cost (speed_p, predictable_p))
 
 /* False if short circuit operation is preferred.  */
-#define LOGICAL_OP_NON_SHORT_CIRCUIT				\
-  ((optimize_size)						\
-   ? (TARGET_THUMB ? false : true)				\
-   : (current_tune->logical_op_non_short_circuit[TARGET_ARM]))
+#define LOGICAL_OP_NON_SHORT_CIRCUIT					\
+  ((optimize_size)							\
+   ? (TARGET_THUMB ? false : true)					\
+   : TARGET_THUMB ? static_cast<bool> (current_tune->logical_op_non_short_circuit_thumb) \
+   : static_cast<bool> (current_tune->logical_op_non_short_circuit_arm))
 
 
 /* Position Independent Code.  */
--- a/src/gcc/config/arm/arm.md
+++ b/src/gcc/config/arm/arm.md
@@ -1177,9 +1177,9 @@
 
 ; ??? Check Thumb-2 split length
 (define_insn_and_split "*arm_subsi3_insn"
-  [(set (match_operand:SI           0 "s_register_operand" "=l,l ,l ,l ,r ,r,r,rk,r")
-	(minus:SI (match_operand:SI 1 "reg_or_int_operand" "l ,0 ,l ,Pz,rI,r,r,k ,?n")
-		  (match_operand:SI 2 "reg_or_int_operand" "l ,Py,Pd,l ,r ,I,r,r ,r")))]
+  [(set (match_operand:SI           0 "s_register_operand" "=l,l ,l ,l ,r,r,r,rk,r")
+	(minus:SI (match_operand:SI 1 "reg_or_int_operand" "l ,0 ,l ,Pz,I,r,r,k ,?n")
+		  (match_operand:SI 2 "reg_or_int_operand" "l ,Py,Pd,l ,r,I,r,r ,r")))]
   "TARGET_32BIT"
   "@
    sub%?\\t%0, %1, %2
@@ -2768,6 +2768,55 @@
 		      (const_string "logic_shift_reg")))]
 )
 
+;; Shifted bics pattern used to set up CC status register and not reusing
+;; bics output.  Pattern restricts Thumb2 shift operand as bics for Thumb2
+;; does not support shift by register.
+(define_insn "andsi_not_shiftsi_si_scc_no_reuse"
+  [(set (reg:CC_NOOV CC_REGNUM)
+	(compare:CC_NOOV
+		(and:SI (not:SI (match_operator:SI 0 "shift_operator"
+			[(match_operand:SI 1 "s_register_operand" "r")
+			 (match_operand:SI 2 "arm_rhs_operand" "rM")]))
+			(match_operand:SI 3 "s_register_operand" "r"))
+		(const_int 0)))
+   (clobber (match_scratch:SI 4 "=r"))]
+  "TARGET_ARM || (TARGET_THUMB2 && CONST_INT_P (operands[2]))"
+  "bic%.%?\\t%4, %3, %1%S0"
+  [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
+   (set_attr "conds" "set")
+   (set_attr "shift" "1")
+   (set (attr "type") (if_then_else (match_operand 2 "const_int_operand" "")
+		      (const_string "logic_shift_imm")
+		      (const_string "logic_shift_reg")))]
+)
+
+;; Same as andsi_not_shiftsi_si_scc_no_reuse, but the bics result is also
+;; getting reused later.
+(define_insn "andsi_not_shiftsi_si_scc"
+  [(parallel [(set (reg:CC_NOOV CC_REGNUM)
+	(compare:CC_NOOV
+		(and:SI (not:SI (match_operator:SI 0 "shift_operator"
+			[(match_operand:SI 1 "s_register_operand" "r")
+			 (match_operand:SI 2 "arm_rhs_operand" "rM")]))
+			(match_operand:SI 3 "s_register_operand" "r"))
+		(const_int 0)))
+	(set (match_operand:SI 4 "s_register_operand" "=r")
+	     (and:SI (not:SI (match_op_dup 0
+		     [(match_dup 1)
+		      (match_dup 2)]))
+		     (match_dup 3)))])]
+  "TARGET_ARM || (TARGET_THUMB2 && CONST_INT_P (operands[2]))"
+  "bic%.%?\\t%4, %3, %1%S0"
+  [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
+   (set_attr "conds" "set")
+   (set_attr "shift" "1")
+   (set (attr "type") (if_then_else (match_operand 2 "const_int_operand" "")
+		      (const_string "logic_shift_imm")
+		      (const_string "logic_shift_reg")))]
+)
+
 (define_insn "*andsi_notsi_si_compare0"
   [(set (reg:CC_NOOV CC_REGNUM)
 	(compare:CC_NOOV
@@ -5076,7 +5125,7 @@
 
 (define_split
   [(set (match_operand:SI 0 "s_register_operand" "")
-	(ior_xor:SI (and:SI (ashift:SI
+	(IOR_XOR:SI (and:SI (ashift:SI
 			     (match_operand:SI 1 "s_register_operand" "")
 			     (match_operand:SI 2 "const_int_operand" ""))
 			    (match_operand:SI 3 "const_int_operand" ""))
@@ -5088,7 +5137,7 @@
        == (GET_MODE_MASK (GET_MODE (operands[5]))
            & (GET_MODE_MASK (GET_MODE (operands[5]))
 	      << (INTVAL (operands[2])))))"
-  [(set (match_dup 0) (ior_xor:SI (ashift:SI (match_dup 1) (match_dup 2))
+  [(set (match_dup 0) (IOR_XOR:SI (ashift:SI (match_dup 1) (match_dup 2))
 				  (match_dup 4)))
    (set (match_dup 0) (zero_extend:SI (match_dup 5)))]
   "operands[5] = gen_lowpart (GET_MODE (operands[5]), operands[0]);"
@@ -5667,7 +5716,7 @@
   [(set_attr "predicable" "yes")
    (set_attr "predicable_short_it" "no")
    (set_attr "length" "4")
-   (set_attr "type" "mov_imm")]
+   (set_attr "type" "alu_sreg")]
 )
 
 (define_insn "*arm_movsi_insn"
@@ -6712,7 +6761,7 @@
 
   /* Support only fixed point registers.  */
   if (!CONST_INT_P (operands[2])
-      || INTVAL (operands[2]) > 14
+      || INTVAL (operands[2]) > MAX_LDM_STM_OPS
       || INTVAL (operands[2]) < 2
       || !MEM_P (operands[1])
       || !REG_P (operands[0])
@@ -6737,7 +6786,7 @@
 
   /* Support only fixed point registers.  */
   if (!CONST_INT_P (operands[2])
-      || INTVAL (operands[2]) > 14
+      || INTVAL (operands[2]) > MAX_LDM_STM_OPS
       || INTVAL (operands[2]) < 2
       || !REG_P (operands[1])
       || !MEM_P (operands[0])
@@ -6922,7 +6971,7 @@
   [(set_attr "conds" "set")
    (set_attr "shift" "1")
    (set_attr "arch" "32,a,a")
-   (set_attr "type" "alus_shift_imm,alu_shift_reg,alus_shift_imm")])
+   (set_attr "type" "alus_shift_imm,alus_shift_reg,alus_shift_imm")])
 
 (define_insn "*cmpsi_shiftsi_swp"
   [(set (reg:CC_SWP CC_REGNUM)
@@ -6935,7 +6984,7 @@
   [(set_attr "conds" "set")
    (set_attr "shift" "1")
    (set_attr "arch" "32,a,a")
-   (set_attr "type" "alus_shift_imm,alu_shift_reg,alus_shift_imm")])
+   (set_attr "type" "alus_shift_imm,alus_shift_reg,alus_shift_imm")])
 
 (define_insn "*arm_cmpsi_negshiftsi_si"
   [(set (reg:CC_Z CC_REGNUM)
@@ -7528,10 +7577,10 @@
                                         (const_string "mov_imm")
                                         (const_string "mov_reg"))
                           (const_string "mvn_imm")
-                          (const_string "mov_reg")
-                          (const_string "mov_reg")
-                          (const_string "mov_reg")
-                          (const_string "mov_reg")])]
+                          (const_string "multiple")
+                          (const_string "multiple")
+                          (const_string "multiple")
+                          (const_string "multiple")])]
 )
 
 (define_insn "*movsfcc_soft_insn"
@@ -7884,7 +7933,7 @@
 )
 
 (define_expand "<return_str>return"
-  [(returns)]
+  [(RETURNS)]
   "(TARGET_ARM || (TARGET_THUMB2
                    && ARM_FUNC_TYPE (arm_current_func_type ()) == ARM_FT_NORMAL
                    && !IS_STACKALIGN (arm_current_func_type ())))
@@ -7922,7 +7971,7 @@
   [(set (pc)
         (if_then_else (match_operator 0 "arm_comparison_operator"
 		       [(match_operand 1 "cc_register" "") (const_int 0)])
-                      (returns)
+                      (RETURNS)
                       (pc)))]
   "TARGET_ARM  <return_cond_true>"
   "*
@@ -7945,7 +7994,7 @@
         (if_then_else (match_operator 0 "arm_comparison_operator"
 		       [(match_operand 1 "cc_register" "") (const_int 0)])
                       (pc)
-		      (returns)))]
+		      (RETURNS)))]
   "TARGET_ARM <return_cond_true>"
   "*
   {
@@ -8279,7 +8328,7 @@
 
 (define_insn "*<arith_shift_insn>_multsi"
   [(set (match_operand:SI 0 "s_register_operand" "=r,r")
-	(shiftable_ops:SI
+	(SHIFTABLE_OPS:SI
 	 (mult:SI (match_operand:SI 2 "s_register_operand" "r,r")
 		  (match_operand:SI 3 "power_of_two_operand" ""))
 	 (match_operand:SI 1 "s_register_operand" "rk,<t2_binop0>")))]
@@ -8293,7 +8342,7 @@
 
 (define_insn "*<arith_shift_insn>_shiftsi"
   [(set (match_operand:SI 0 "s_register_operand" "=r,r,r")
-	(shiftable_ops:SI
+	(SHIFTABLE_OPS:SI
 	 (match_operator:SI 2 "shift_nomul_operator"
 	  [(match_operand:SI 3 "s_register_operand" "r,r,r")
 	   (match_operand:SI 4 "shift_amount_operand" "M,M,r")])
@@ -8689,7 +8738,14 @@
     return \"\";
   "
   [(set_attr "conds" "use")
-   (set_attr "type" "mov_reg,mov_reg,multiple")
+   (set_attr_alternative "type"
+                         [(if_then_else (match_operand 2 "const_int_operand" "")
+                                        (const_string "mov_imm")
+                                        (const_string "mov_reg"))
+                          (if_then_else (match_operand 1 "const_int_operand" "")
+                                        (const_string "mov_imm")
+                                        (const_string "mov_reg"))
+                          (const_string "multiple")])
    (set_attr "length" "4,4,8")]
 )
 
@@ -9485,8 +9541,8 @@
                                         (const_string "alu_imm" )
                                         (const_string "alu_sreg"))
                           (const_string "alu_imm")
-                          (const_string "alu_sreg")
-                          (const_string "alu_sreg")])]
+                          (const_string "multiple")
+                          (const_string "multiple")])]
 )
 
 (define_insn "*ifcompare_move_plus"
@@ -9523,7 +9579,13 @@
    sub%D4\\t%0, %2, #%n3\;mov%d4\\t%0, %1"
   [(set_attr "conds" "use")
    (set_attr "length" "4,4,8,8")
-   (set_attr "type" "alu_sreg,alu_imm,multiple,multiple")]
+   (set_attr_alternative "type"
+                         [(if_then_else (match_operand 3 "const_int_operand" "")
+                                        (const_string "alu_imm" )
+                                        (const_string "alu_sreg"))
+                          (const_string "alu_imm")
+                          (const_string "multiple")
+                          (const_string "multiple")])]
 )
 
 (define_insn "*ifcompare_arith_arith"
@@ -9618,7 +9680,11 @@
    %I5%d4\\t%0, %2, %3\;mov%D4\\t%0, %1"
   [(set_attr "conds" "use")
    (set_attr "length" "4,8")
-   (set_attr "type" "alu_shift_reg,multiple")]
+   (set_attr_alternative "type"
+                         [(if_then_else (match_operand 3 "const_int_operand" "")
+                                        (const_string "alu_shift_imm" )
+                                        (const_string "alu_shift_reg"))
+                          (const_string "multiple")])]
 )
 
 (define_insn "*ifcompare_move_arith"
@@ -9679,7 +9745,11 @@
    %I5%D4\\t%0, %2, %3\;mov%d4\\t%0, %1"
   [(set_attr "conds" "use")
    (set_attr "length" "4,8")
-   (set_attr "type" "alu_shift_reg,multiple")]
+   (set_attr_alternative "type"
+                         [(if_then_else (match_operand 3 "const_int_operand" "")
+                                        (const_string "alu_shift_imm" )
+                                        (const_string "alu_shift_reg"))
+                          (const_string "multiple")])]
 )
 
 (define_insn "*ifcompare_move_not"
@@ -9786,7 +9856,12 @@
   [(set_attr "conds" "use")
    (set_attr "shift" "2")
    (set_attr "length" "4,8,8")
-   (set_attr "type" "mov_shift_reg,multiple,multiple")]
+   (set_attr_alternative "type"
+                         [(if_then_else (match_operand 3 "const_int_operand" "")
+                                        (const_string "mov_shift" )
+                                        (const_string "mov_shift_reg"))
+                          (const_string "multiple")
+                          (const_string "multiple")])]
 )
 
 (define_insn "*ifcompare_move_shift"
@@ -9824,7 +9899,12 @@
   [(set_attr "conds" "use")
    (set_attr "shift" "2")
    (set_attr "length" "4,8,8")
-   (set_attr "type" "mov_shift_reg,multiple,multiple")]
+   (set_attr_alternative "type"
+                         [(if_then_else (match_operand 3 "const_int_operand" "")
+                                        (const_string "mov_shift" )
+                                        (const_string "mov_shift_reg"))
+                          (const_string "multiple")
+                          (const_string "multiple")])]
 )
 
 (define_insn "*ifcompare_shift_shift"
@@ -10905,7 +10985,7 @@
  [(set_attr "predicable" "yes")
   (set_attr "predicable_short_it" "no")
   (set_attr "length" "4")
-  (set_attr "type" "mov_imm")]
+  (set_attr "type" "alu_sreg")]
 )
 
 (define_insn "*arm_rev"
--- a/src/gcc/config/arm/iterators.md
+++ b/src/gcc/config/arm/iterators.md
@@ -181,39 +181,53 @@
 ;; compare a second time.
 (define_code_iterator LTUGEU [ltu geu])
 
+;; The signed gt, ge comparisons
+(define_code_iterator GTGE [gt ge])
+
+;; The unsigned gt, ge comparisons
+(define_code_iterator GTUGEU [gtu geu])
+
+;; Comparisons for vc<cmp>
+(define_code_iterator COMPARISONS [eq gt ge le lt])
+
 ;; A list of ...
-(define_code_iterator ior_xor [ior xor])
+(define_code_iterator IOR_XOR [ior xor])
 
 ;; Operations on two halves of a quadword vector.
-(define_code_iterator vqh_ops [plus smin smax umin umax])
+(define_code_iterator VQH_OPS [plus smin smax umin umax])
 
 ;; Operations on two halves of a quadword vector,
 ;; without unsigned variants (for use with *SFmode pattern).
-(define_code_iterator vqhs_ops [plus smin smax])
+(define_code_iterator VQHS_OPS [plus smin smax])
 
 ;; A list of widening operators
 (define_code_iterator SE [sign_extend zero_extend])
 
 ;; Right shifts
-(define_code_iterator rshifts [ashiftrt lshiftrt])
+(define_code_iterator RSHIFTS [ashiftrt lshiftrt])
 
 ;; Iterator for integer conversions
 (define_code_iterator FIXUORS [fix unsigned_fix])
 
 ;; Binary operators whose second operand can be shifted.
-(define_code_iterator shiftable_ops [plus minus ior xor and])
+(define_code_iterator SHIFTABLE_OPS [plus minus ior xor and])
 
-;; plus and minus are the only shiftable_ops for which Thumb2 allows
+;; plus and minus are the only SHIFTABLE_OPS for which Thumb2 allows
 ;; a stack pointer opoerand.  The minus operation is a candidate for an rsub
 ;; and hence only plus is supported.
 (define_code_attr t2_binop0
   [(plus "rk") (minus "r") (ior "r") (xor "r") (and "r")])
 
-;; The instruction to use when a shiftable_ops has a shift operation as
+;; The instruction to use when a SHIFTABLE_OPS has a shift operation as
 ;; its first operand.
 (define_code_attr arith_shift_insn
   [(plus "add") (minus "rsb") (ior "orr") (xor "eor") (and "and")])
 
+(define_code_attr cmp_op [(eq "eq") (gt "gt") (ge "ge") (lt "lt") (le "le")
+                          (gtu "gt") (geu "ge")])
+
+(define_code_attr cmp_type [(eq "i") (gt "s") (ge "s") (lt "s") (le "s")])
+
 ;;----------------------------------------------------------------------------
 ;; Int iterators
 ;;----------------------------------------------------------------------------
@@ -221,6 +235,10 @@
 (define_int_iterator VRINT [UNSPEC_VRINTZ UNSPEC_VRINTP UNSPEC_VRINTM
                             UNSPEC_VRINTR UNSPEC_VRINTX UNSPEC_VRINTA])
 
+(define_int_iterator NEON_VCMP [UNSPEC_VCEQ UNSPEC_VCGT UNSPEC_VCGE UNSPEC_VCLT UNSPEC_VCLE])
+
+(define_int_iterator NEON_VACMP [UNSPEC_VCAGE UNSPEC_VCAGT])
+
 (define_int_iterator VCVT [UNSPEC_VRINTP UNSPEC_VRINTM UNSPEC_VRINTA])
 
 (define_int_iterator NEON_VRINT [UNSPEC_NVRINTP UNSPEC_NVRINTZ UNSPEC_NVRINTM
@@ -677,6 +695,11 @@
 
 ])
 
+(define_int_attr cmp_op_unsp [(UNSPEC_VCEQ "eq") (UNSPEC_VCGT "gt")
+                              (UNSPEC_VCGE "ge") (UNSPEC_VCLE "le")
+                              (UNSPEC_VCLT "lt") (UNSPEC_VCAGE "ge")
+                              (UNSPEC_VCAGT "gt")])
+
 (define_int_attr r [
   (UNSPEC_VRHADD_S "r") (UNSPEC_VRHADD_U "r")
   (UNSPEC_VHADD_S "") (UNSPEC_VHADD_U "")
@@ -774,7 +797,7 @@
                           (UNSPEC_SHA256H2 "V4SI") (UNSPEC_SHA256SU1 "V4SI")])
 
 ;; Both kinds of return insn.
-(define_code_iterator returns [return simple_return])
+(define_code_iterator RETURNS [return simple_return])
 (define_code_attr return_str [(return "") (simple_return "simple_")])
 (define_code_attr return_simple_p [(return "false") (simple_return "true")])
 (define_code_attr return_cond_false [(return " && USE_RETURN_INSN (FALSE)")
--- a/src/gcc/config/arm/iwmmxt.md
+++ b/src/gcc/config/arm/iwmmxt.md
@@ -107,8 +107,8 @@
 )
 
 (define_insn "*iwmmxt_arm_movdi"
-  [(set (match_operand:DI 0 "nonimmediate_di_operand" "=r, r, r, r, m,y,y,yr,y,yrUy,*w, r,*w,*w, *Uv")
-        (match_operand:DI 1 "di_operand"              "rDa,Db,Dc,mi,r,y,yr,y,yrUy,y, r,*w,*w,*Uvi,*w"))]
+  [(set (match_operand:DI 0 "nonimmediate_di_operand" "=r, r, r, r, m,y,y,r, y,Uy,*w, r,*w,*w, *Uv")
+        (match_operand:DI 1 "di_operand"              "rDa,Db,Dc,mi,r,y,r,y,Uy,y,  r,*w,*w,*Uvi,*w"))]
   "TARGET_REALLY_IWMMXT
    && (   register_operand (operands[0], DImode)
        || register_operand (operands[1], DImode))"
--- a/src/gcc/config/arm/linux-eabi.h
+++ b/src/gcc/config/arm/linux-eabi.h
@@ -77,6 +77,23 @@
     %{mfloat-abi=soft*:" GLIBC_DYNAMIC_LINKER_SOFT_FLOAT "} \
     %{!mfloat-abi=*:" GLIBC_DYNAMIC_LINKER_DEFAULT "}"
 
+/* For ARM musl currently supports four dynamic linkers:
+   - ld-musl-arm.so.1 - for the EABI-derived soft-float ABI
+   - ld-musl-armhf.so.1 - for the EABI-derived hard-float ABI
+   - ld-musl-armeb.so.1 - for the EABI-derived soft-float ABI, EB
+   - ld-musl-armebhf.so.1 - for the EABI-derived hard-float ABI, EB
+   musl does not support the legacy OABI mode.
+   All the dynamic linkers live in /lib.
+   We default to soft-float, EL. */
+#undef  MUSL_DYNAMIC_LINKER
+#if TARGET_BIG_ENDIAN_DEFAULT
+#define MUSL_DYNAMIC_LINKER_E "%{mlittle-endian:;:eb}"
+#else
+#define MUSL_DYNAMIC_LINKER_E "%{mbig-endian:eb}"
+#endif
+#define MUSL_DYNAMIC_LINKER \
+  "/lib/ld-musl-arm" MUSL_DYNAMIC_LINKER_E "%{mfloat-abi=hard:hf}.so.1"
+
 /* At this point, bpabi.h will have clobbered LINK_SPEC.  We want to
    use the GNU/Linux version, not the generic BPABI version.  */
 #undef  LINK_SPEC
@@ -107,6 +124,7 @@
 
 #undef	ENDFILE_SPEC
 #define ENDFILE_SPEC \
+  "%{Ofast|ffast-math|funsafe-math-optimizations:crtfastmath.o%s} "	\
   LINUX_OR_ANDROID_LD (GNU_USER_TARGET_ENDFILE_SPEC, ANDROID_ENDFILE_SPEC)
 
 /* Use the default LIBGCC_SPEC, not the version in linux-elf.h, as we
--- a/src/gcc/config/arm/neon.md
+++ b/src/gcc/config/arm/neon.md
@@ -1114,7 +1114,7 @@
 ;; lshrdi3_neon
 (define_insn_and_split "<shift>di3_neon"
   [(set (match_operand:DI 0 "s_register_operand"	     "= w, w,?&r,?r,?w,?w")
-	(rshifts:DI (match_operand:DI 1 "s_register_operand" " 0w, w, 0r, r,0w, w")
+	(RSHIFTS:DI (match_operand:DI 1 "s_register_operand" " 0w, w, 0r, r,0w, w")
 		    (match_operand:SI 2 "reg_or_int_operand" "  r, i,  r, i, r, i")))
    (clobber (match_scratch:SI 3				     "=2r, X, &r, X,2r, X"))
    (clobber (match_scratch:SI 4				     "= X, X, &r, X, X, X"))
@@ -1194,71 +1194,6 @@
   [(set_attr "type" "neon_add_widen")]
 )
 
-;; VEXT can be used to synthesize coarse whole-vector shifts with 8-bit
-;; shift-count granularity. That's good enough for the middle-end's current
-;; needs.
-
-;; Note that it's not safe to perform such an operation in big-endian mode,
-;; due to element-ordering issues.
-
-(define_expand "vec_shr_<mode>"
-  [(match_operand:VDQ 0 "s_register_operand" "")
-   (match_operand:VDQ 1 "s_register_operand" "")
-   (match_operand:SI 2 "const_multiple_of_8_operand" "")]
-  "TARGET_NEON && !BYTES_BIG_ENDIAN"
-{
-  rtx zero_reg;
-  HOST_WIDE_INT num_bits = INTVAL (operands[2]);
-  const int width = GET_MODE_BITSIZE (<MODE>mode);
-  const machine_mode bvecmode = (width == 128) ? V16QImode : V8QImode;
-  rtx (*gen_ext) (rtx, rtx, rtx, rtx) =
-    (width == 128) ? gen_neon_vextv16qi : gen_neon_vextv8qi;
-
-  if (num_bits == width)
-    {
-      emit_move_insn (operands[0], operands[1]);
-      DONE;
-    }
-
-  zero_reg = force_reg (bvecmode, CONST0_RTX (bvecmode));
-  operands[0] = gen_lowpart (bvecmode, operands[0]);
-  operands[1] = gen_lowpart (bvecmode, operands[1]);
-
-  emit_insn (gen_ext (operands[0], operands[1], zero_reg,
-		      GEN_INT (num_bits / BITS_PER_UNIT)));
-  DONE;
-})
-
-(define_expand "vec_shl_<mode>"
-  [(match_operand:VDQ 0 "s_register_operand" "")
-   (match_operand:VDQ 1 "s_register_operand" "")
-   (match_operand:SI 2 "const_multiple_of_8_operand" "")]
-  "TARGET_NEON && !BYTES_BIG_ENDIAN"
-{
-  rtx zero_reg;
-  HOST_WIDE_INT num_bits = INTVAL (operands[2]);
-  const int width = GET_MODE_BITSIZE (<MODE>mode);
-  const machine_mode bvecmode = (width == 128) ? V16QImode : V8QImode;
-  rtx (*gen_ext) (rtx, rtx, rtx, rtx) =
-    (width == 128) ? gen_neon_vextv16qi : gen_neon_vextv8qi;
-
-  if (num_bits == 0)
-    {
-      emit_move_insn (operands[0], CONST0_RTX (<MODE>mode));
-      DONE;
-    }
-
-  num_bits = width - num_bits;
-
-  zero_reg = force_reg (bvecmode, CONST0_RTX (bvecmode));
-  operands[0] = gen_lowpart (bvecmode, operands[0]);
-  operands[1] = gen_lowpart (bvecmode, operands[1]);
-
-  emit_insn (gen_ext (operands[0], zero_reg, operands[1],
-		      GEN_INT (num_bits / BITS_PER_UNIT)));
-  DONE;
-})
-
 ;; Helpers for quad-word reduction operations
 
 ; Add (or smin, smax...) the low N/2 elements of the N-element vector
@@ -1267,7 +1202,7 @@
 
 (define_insn "quad_halves_<code>v4si"
   [(set (match_operand:V2SI 0 "s_register_operand" "=w")
-        (vqh_ops:V2SI
+        (VQH_OPS:V2SI
           (vec_select:V2SI (match_operand:V4SI 1 "s_register_operand" "w")
                            (parallel [(const_int 0) (const_int 1)]))
           (vec_select:V2SI (match_dup 1)
@@ -1280,7 +1215,7 @@
 
 (define_insn "quad_halves_<code>v4sf"
   [(set (match_operand:V2SF 0 "s_register_operand" "=w")
-        (vqhs_ops:V2SF
+        (VQHS_OPS:V2SF
           (vec_select:V2SF (match_operand:V4SF 1 "s_register_operand" "w")
                            (parallel [(const_int 0) (const_int 1)]))
           (vec_select:V2SF (match_dup 1)
@@ -1293,7 +1228,7 @@
 
 (define_insn "quad_halves_<code>v8hi"
   [(set (match_operand:V4HI 0 "s_register_operand" "+w")
-        (vqh_ops:V4HI
+        (VQH_OPS:V4HI
           (vec_select:V4HI (match_operand:V8HI 1 "s_register_operand" "w")
                            (parallel [(const_int 0) (const_int 1)
 				      (const_int 2) (const_int 3)]))
@@ -1308,7 +1243,7 @@
 
 (define_insn "quad_halves_<code>v16qi"
   [(set (match_operand:V8QI 0 "s_register_operand" "+w")
-        (vqh_ops:V8QI
+        (VQH_OPS:V8QI
           (vec_select:V8QI (match_operand:V16QI 1 "s_register_operand" "w")
                            (parallel [(const_int 0) (const_int 1)
 				      (const_int 2) (const_int 3)
@@ -2200,134 +2135,140 @@
   [(set_attr "type" "neon_sub_halve_narrow_q")]
 )
 
-(define_insn "neon_vceq<mode>"
-  [(set (match_operand:<V_cmp_result> 0 "s_register_operand" "=w,w")
-        (unspec:<V_cmp_result>
-	  [(match_operand:VDQW 1 "s_register_operand" "w,w")
-	   (match_operand:VDQW 2 "reg_or_zero_operand" "w,Dz")]
-          UNSPEC_VCEQ))]
+;; These may expand to an UNSPEC pattern when a floating point mode is used
+;; without unsafe math optimizations.
+(define_expand "neon_vc<cmp_op><mode>"
+  [(match_operand:<V_cmp_result> 0 "s_register_operand" "=w,w")
+     (neg:<V_cmp_result>
+       (COMPARISONS:VDQW (match_operand:VDQW 1 "s_register_operand" "w,w")
+                         (match_operand:VDQW 2 "reg_or_zero_operand" "w,Dz")))]
   "TARGET_NEON"
-  "@
-  vceq.<V_if_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2
-  vceq.<V_if_elem>\t%<V_reg>0, %<V_reg>1, #0"
-  [(set (attr "type")
-      (if_then_else (match_test "<Is_float_mode>")
-                    (const_string "neon_fp_compare_s<q>")
-                    (if_then_else (match_operand 2 "zero_operand")
-                      (const_string "neon_compare_zero<q>")
-                      (const_string "neon_compare<q>"))))]
+  {
+    /* For FP comparisons use UNSPECS unless -funsafe-math-optimizations
+       are enabled.  */
+    if (GET_MODE_CLASS (<MODE>mode) == MODE_VECTOR_FLOAT
+        && !flag_unsafe_math_optimizations)
+      {
+        /* We don't just emit a gen_neon_vc<cmp_op><mode>_insn_unspec because
+           we define gen_neon_vceq<mode>_insn_unspec only for float modes
+           whereas this expander iterates over the integer modes as well,
+           but we will never expand to UNSPECs for the integer comparisons.  */
+        switch (<MODE>mode)
+          {
+            case V2SFmode:
+              emit_insn (gen_neon_vc<cmp_op>v2sf_insn_unspec (operands[0],
+                                                              operands[1],
+                                                              operands[2]));
+              break;
+            case V4SFmode:
+              emit_insn (gen_neon_vc<cmp_op>v4sf_insn_unspec (operands[0],
+                                                              operands[1],
+                                                              operands[2]));
+              break;
+            default:
+              gcc_unreachable ();
+          }
+      }
+    else
+      emit_insn (gen_neon_vc<cmp_op><mode>_insn (operands[0],
+                                                 operands[1],
+                                                 operands[2]));
+    DONE;
+  }
 )
 
-(define_insn "neon_vcge<mode>"
+(define_insn "neon_vc<cmp_op><mode>_insn"
   [(set (match_operand:<V_cmp_result> 0 "s_register_operand" "=w,w")
-        (unspec:<V_cmp_result>
-	  [(match_operand:VDQW 1 "s_register_operand" "w,w")
-	   (match_operand:VDQW 2 "reg_or_zero_operand" "w,Dz")]
-          UNSPEC_VCGE))]
-  "TARGET_NEON"
-  "@
-  vcge.<V_s_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2
-  vcge.<V_s_elem>\t%<V_reg>0, %<V_reg>1, #0"
+        (neg:<V_cmp_result>
+          (COMPARISONS:<V_cmp_result>
+            (match_operand:VDQW 1 "s_register_operand" "w,w")
+            (match_operand:VDQW 2 "reg_or_zero_operand" "w,Dz"))))]
+  "TARGET_NEON && !(GET_MODE_CLASS (<MODE>mode) == MODE_VECTOR_FLOAT
+                    && !flag_unsafe_math_optimizations)"
+  {
+    char pattern[100];
+    sprintf (pattern, "vc<cmp_op>.%s%%#<V_sz_elem>\t%%<V_reg>0,"
+                      " %%<V_reg>1, %s",
+                       GET_MODE_CLASS (<MODE>mode) == MODE_VECTOR_FLOAT
+                         ? "f" : "<cmp_type>",
+                       which_alternative == 0
+                         ? "%<V_reg>2" : "#0");
+    output_asm_insn (pattern, operands);
+    return "";
+  }
   [(set (attr "type")
-     (if_then_else (match_test "<Is_float_mode>")
-                   (const_string "neon_fp_compare_s<q>")
-                    (if_then_else (match_operand 2 "zero_operand")
+        (if_then_else (match_operand 2 "zero_operand")
                       (const_string "neon_compare_zero<q>")
-                      (const_string "neon_compare<q>"))))]
-)
-
-(define_insn "neon_vcgeu<mode>"
-  [(set (match_operand:<V_cmp_result> 0 "s_register_operand" "=w")
-        (unspec:<V_cmp_result>
-	  [(match_operand:VDQIW 1 "s_register_operand" "w")
-	   (match_operand:VDQIW 2 "s_register_operand" "w")]
-          UNSPEC_VCGEU))]
-  "TARGET_NEON"
-  "vcge.u%#<V_sz_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
-  [(set_attr "type" "neon_compare<q>")]
+                      (const_string "neon_compare<q>")))]
 )
 
-(define_insn "neon_vcgt<mode>"
+(define_insn "neon_vc<cmp_op_unsp><mode>_insn_unspec"
   [(set (match_operand:<V_cmp_result> 0 "s_register_operand" "=w,w")
         (unspec:<V_cmp_result>
-	  [(match_operand:VDQW 1 "s_register_operand" "w,w")
-	   (match_operand:VDQW 2 "reg_or_zero_operand" "w,Dz")]
-          UNSPEC_VCGT))]
+	  [(match_operand:VCVTF 1 "s_register_operand" "w,w")
+	   (match_operand:VCVTF 2 "reg_or_zero_operand" "w,Dz")]
+          NEON_VCMP))]
   "TARGET_NEON"
-  "@
-  vcgt.<V_s_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2
-  vcgt.<V_s_elem>\t%<V_reg>0, %<V_reg>1, #0"
-  [(set (attr "type")
-     (if_then_else (match_test "<Is_float_mode>")
-                   (const_string "neon_fp_compare_s<q>")
-                    (if_then_else (match_operand 2 "zero_operand")
-                      (const_string "neon_compare_zero<q>")
-                      (const_string "neon_compare<q>"))))]
+  {
+    char pattern[100];
+    sprintf (pattern, "vc<cmp_op_unsp>.f%%#<V_sz_elem>\t%%<V_reg>0,"
+                       " %%<V_reg>1, %s",
+                       which_alternative == 0
+                         ? "%<V_reg>2" : "#0");
+    output_asm_insn (pattern, operands);
+    return "";
+}
+  [(set_attr "type" "neon_fp_compare_s<q>")]
 )
 
-(define_insn "neon_vcgtu<mode>"
+(define_insn "neon_vc<cmp_op>u<mode>"
   [(set (match_operand:<V_cmp_result> 0 "s_register_operand" "=w")
-        (unspec:<V_cmp_result>
-	  [(match_operand:VDQIW 1 "s_register_operand" "w")
-	   (match_operand:VDQIW 2 "s_register_operand" "w")]
-          UNSPEC_VCGTU))]
+        (neg:<V_cmp_result>
+          (GTUGEU:<V_cmp_result>
+	    (match_operand:VDQIW 1 "s_register_operand" "w")
+	    (match_operand:VDQIW 2 "s_register_operand" "w"))))]
   "TARGET_NEON"
-  "vcgt.u%#<V_sz_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
+  "vc<cmp_op>.u%#<V_sz_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
   [(set_attr "type" "neon_compare<q>")]
 )
 
-;; VCLE and VCLT only support comparisons with immediate zero (register
-;; variants are VCGE and VCGT with operands reversed).
-
-(define_insn "neon_vcle<mode>"
-  [(set (match_operand:<V_cmp_result> 0 "s_register_operand" "=w")
-        (unspec:<V_cmp_result>
-	  [(match_operand:VDQW 1 "s_register_operand" "w")
-	   (match_operand:VDQW 2 "zero_operand" "Dz")]
-          UNSPEC_VCLE))]
-  "TARGET_NEON"
-  "vcle.<V_s_elem>\t%<V_reg>0, %<V_reg>1, #0"
-  [(set (attr "type")
-      (if_then_else (match_test "<Is_float_mode>")
-                    (const_string "neon_fp_compare_s<q>")
-                    (if_then_else (match_operand 2 "zero_operand")
-                      (const_string "neon_compare_zero<q>")
-                      (const_string "neon_compare<q>"))))]
-)
-
-(define_insn "neon_vclt<mode>"
-  [(set (match_operand:<V_cmp_result> 0 "s_register_operand" "=w")
-        (unspec:<V_cmp_result>
-	  [(match_operand:VDQW 1 "s_register_operand" "w")
-	   (match_operand:VDQW 2 "zero_operand" "Dz")]
-          UNSPEC_VCLT))]
+(define_expand "neon_vca<cmp_op><mode>"
+  [(set (match_operand:<V_cmp_result> 0 "s_register_operand")
+        (neg:<V_cmp_result>
+          (GTGE:<V_cmp_result>
+            (abs:VCVTF (match_operand:VCVTF 1 "s_register_operand"))
+            (abs:VCVTF (match_operand:VCVTF 2 "s_register_operand")))))]
   "TARGET_NEON"
-  "vclt.<V_s_elem>\t%<V_reg>0, %<V_reg>1, #0"
-  [(set (attr "type")
-      (if_then_else (match_test "<Is_float_mode>")
-                    (const_string "neon_fp_compare_s<q>")
-                    (if_then_else (match_operand 2 "zero_operand")
-                      (const_string "neon_compare_zero<q>")
-                      (const_string "neon_compare<q>"))))]
+  {
+    if (flag_unsafe_math_optimizations)
+      emit_insn (gen_neon_vca<cmp_op><mode>_insn (operands[0], operands[1],
+                                                  operands[2]));
+    else
+      emit_insn (gen_neon_vca<cmp_op><mode>_insn_unspec (operands[0],
+                                                         operands[1],
+                                                         operands[2]));
+    DONE;
+  }
 )
 
-(define_insn "neon_vcage<mode>"
+(define_insn "neon_vca<cmp_op><mode>_insn"
   [(set (match_operand:<V_cmp_result> 0 "s_register_operand" "=w")
-        (unspec:<V_cmp_result> [(match_operand:VCVTF 1 "s_register_operand" "w")
-		                (match_operand:VCVTF 2 "s_register_operand" "w")]
-                               UNSPEC_VCAGE))]
-  "TARGET_NEON"
-  "vacge.<V_if_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
+        (neg:<V_cmp_result>
+          (GTGE:<V_cmp_result>
+            (abs:VCVTF (match_operand:VCVTF 1 "s_register_operand" "w"))
+            (abs:VCVTF (match_operand:VCVTF 2 "s_register_operand" "w")))))]
+  "TARGET_NEON && flag_unsafe_math_optimizations"
+  "vac<cmp_op>.<V_if_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
   [(set_attr "type" "neon_fp_compare_s<q>")]
 )
 
-(define_insn "neon_vcagt<mode>"
+(define_insn "neon_vca<cmp_op_unsp><mode>_insn_unspec"
   [(set (match_operand:<V_cmp_result> 0 "s_register_operand" "=w")
         (unspec:<V_cmp_result> [(match_operand:VCVTF 1 "s_register_operand" "w")
 		                (match_operand:VCVTF 2 "s_register_operand" "w")]
-                               UNSPEC_VCAGT))]
+                               NEON_VACMP))]
   "TARGET_NEON"
-  "vacgt.<V_if_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
+  "vac<cmp_op_unsp>.<V_if_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
   [(set_attr "type" "neon_fp_compare_s<q>")]
 )
 
--- a/src/gcc/config/arm/thumb2.md
+++ b/src/gcc/config/arm/thumb2.md
@@ -300,7 +300,7 @@
    ldr%?\\t%0, %1
    str%?\\t%1, %0
    str%?\\t%1, %0"
-  [(set_attr "type" "mov_reg,alu_imm,alu_imm,alu_imm,mov_imm,load1,load1,store1,store1")
+  [(set_attr "type" "mov_reg,mov_imm,mov_imm,mvn_imm,mov_imm,load1,load1,store1,store1")
    (set_attr "length" "2,4,2,4,4,4,4,4,4")
    (set_attr "predicable" "yes")
    (set_attr "predicable_short_it" "yes,no,yes,no,no,no,no,no,no")
@@ -486,12 +486,12 @@
 )
 
 (define_insn_and_split "*thumb2_movsicc_insn"
-  [(set (match_operand:SI 0 "s_register_operand" "=l,l,r,r,r,r,r,r,r,r,r")
+  [(set (match_operand:SI 0 "s_register_operand" "=l,l,r,r,r,r,r,r,r,r,r,r")
 	(if_then_else:SI
 	 (match_operator 3 "arm_comparison_operator"
 	  [(match_operand 4 "cc_register" "") (const_int 0)])
-	 (match_operand:SI 1 "arm_not_operand" "0 ,lPy,0 ,0,rI,K,rI,rI,K ,K,r")
-	 (match_operand:SI 2 "arm_not_operand" "lPy,0 ,rI,K,0 ,0,rI,K ,rI,K,r")))]
+	 (match_operand:SI 1 "arm_not_operand" "0 ,lPy,0 ,0,rI,K,I ,r,rI,K ,K,r")
+	 (match_operand:SI 2 "arm_not_operand" "lPy,0 ,rI,K,0 ,0,rI,I,K ,rI,K,r")))]
   "TARGET_THUMB2"
   "@
    it\\t%D3\;mov%D3\\t%0, %2
@@ -504,12 +504,14 @@
    #
    #
    #
+   #
    #"
    ; alt 6: ite\\t%d3\;mov%d3\\t%0, %1\;mov%D3\\t%0, %2
-   ; alt 7: ite\\t%d3\;mov%d3\\t%0, %1\;mvn%D3\\t%0, #%B2
-   ; alt 8: ite\\t%d3\;mvn%d3\\t%0, #%B1\;mov%D3\\t%0, %2
-   ; alt 9: ite\\t%d3\;mvn%d3\\t%0, #%B1\;mvn%D3\\t%0, #%B2
-   ; alt 10: ite\\t%d3\;mov%d3\\t%0, %1\;mov%D3\\t%0, %2
+   ; alt 7: ite\\t%d3\;mov%d3\\t%0, %1\;mov%D3\\t%0, %2
+   ; alt 8: ite\\t%d3\;mov%d3\\t%0, %1\;mvn%D3\\t%0, #%B2
+   ; alt 9: ite\\t%d3\;mvn%d3\\t%0, #%B1\;mov%D3\\t%0, %2
+   ; alt 10: ite\\t%d3\;mvn%d3\\t%0, #%B1\;mvn%D3\\t%0, #%B2
+   ; alt 11: ite\\t%d3\;mov%d3\\t%0, %1\;mov%D3\\t%0, %2
   "&& reload_completed"
   [(const_int 0)]
   {
@@ -540,10 +542,30 @@
                                                operands[2])));
     DONE;
   }
-  [(set_attr "length" "4,4,6,6,6,6,10,10,10,10,6")
-   (set_attr "enabled_for_depr_it" "yes,yes,no,no,no,no,no,no,no,no,yes")
+  [(set_attr "length" "4,4,6,6,6,6,10,8,10,10,10,6")
+   (set_attr "enabled_for_depr_it" "yes,yes,no,no,no,no,no,no,no,no,no,yes")
    (set_attr "conds" "use")
-   (set_attr "type" "multiple")]
+   (set_attr_alternative "type"
+                         [(if_then_else (match_operand 2 "const_int_operand" "")
+                                        (const_string "mov_imm")
+                                        (const_string "mov_reg"))
+                          (if_then_else (match_operand 1 "const_int_operand" "")
+                                        (const_string "mov_imm")
+                                        (const_string "mov_reg"))
+                          (if_then_else (match_operand 2 "const_int_operand" "")
+                                        (const_string "mov_imm")
+                                        (const_string "mov_reg"))
+                          (const_string "mvn_imm")
+                          (if_then_else (match_operand 1 "const_int_operand" "")
+                                        (const_string "mov_imm")
+                                        (const_string "mov_reg"))
+                          (const_string "mvn_imm")
+                          (const_string "multiple")
+                          (const_string "multiple")
+                          (const_string "multiple")
+                          (const_string "multiple")
+                          (const_string "multiple")
+                          (const_string "multiple")])]
 )
 
 (define_insn "*thumb2_movsfcc_soft_insn"
@@ -1182,7 +1204,11 @@
   "
   [(set_attr "predicable" "yes")
    (set_attr "length" "2")
-   (set_attr "type" "alu_sreg")]
+   (set_attr_alternative "type"
+                         [(if_then_else (match_operand 2 "const_int_operand" "")
+                                        (const_string "alu_imm")
+                                        (const_string "alu_sreg"))
+                          (const_string "alu_imm")])]
 )
 
 (define_insn "*thumb2_subsi_short"
@@ -1247,14 +1273,21 @@
   "
   [(set_attr "conds" "set")
    (set_attr "length" "2,2,4")
-   (set_attr "type" "alu_sreg")]
+   (set_attr_alternative "type"
+                         [(if_then_else (match_operand 2 "const_int_operand" "")
+                                        (const_string "alus_imm")
+                                        (const_string "alus_sreg"))
+                          (const_string "alus_imm")
+                          (if_then_else (match_operand 2 "const_int_operand" "")
+                                        (const_string "alus_imm")
+                                        (const_string "alus_sreg"))])]
 )
 
 (define_insn "*thumb2_addsi3_compare0_scratch"
   [(set (reg:CC_NOOV CC_REGNUM)
 	(compare:CC_NOOV
-	  (plus:SI (match_operand:SI 0 "s_register_operand" "l,l,  r,r")
-		   (match_operand:SI 1 "arm_add_operand"    "Pv,l,IL,r"))
+	  (plus:SI (match_operand:SI 0 "s_register_operand" "l,  r")
+		   (match_operand:SI 1 "arm_add_operand"    "lPv,rIL"))
 	  (const_int 0)))]
   "TARGET_THUMB2"
   "*
@@ -1271,8 +1304,10 @@
       return \"cmn\\t%0, %1\";
   "
   [(set_attr "conds" "set")
-   (set_attr "length" "2,2,4,4")
-   (set_attr "type" "alus_imm,alus_sreg,alus_imm,alus_sreg")]
+   (set_attr "length" "2,4")
+   (set (attr "type") (if_then_else (match_operand 1 "const_int_operand" "")
+                                    (const_string "alus_imm")
+                                    (const_string "alus_sreg")))]
 )
 
 (define_insn "*thumb2_mulsi_short"
--- a/src/gcc/config/arm/unknown-elf.h
+++ b/src/gcc/config/arm/unknown-elf.h
@@ -32,7 +32,9 @@
 #define UNKNOWN_ELF_STARTFILE_SPEC	" crti%O%s crtbegin%O%s crt0%O%s"
 
 #undef  STARTFILE_SPEC
-#define STARTFILE_SPEC	UNKNOWN_ELF_STARTFILE_SPEC
+#define STARTFILE_SPEC	\
+  "%{Ofast|ffast-math|funsafe-math-optimizations:crtfastmath.o%s} "	\
+  UNKNOWN_ELF_STARTFILE_SPEC
 
 #define UNKNOWN_ELF_ENDFILE_SPEC	"crtend%O%s crtn%O%s"
 
@@ -80,7 +82,9 @@
 									\
       ASM_OUTPUT_ALIGN (FILE, floor_log2 (ALIGN / BITS_PER_UNIT));	\
       ASM_OUTPUT_LABEL (FILE, NAME);					\
-      fprintf (FILE, "\t.space\t%d\n", SIZE ? (int)(SIZE) : 1);		\
+      fprintf (FILE, "\t.space\t%d\n", SIZE ? (int) SIZE : 1);		\
+      fprintf (FILE, "\t.size\t%s, %d\n",				\
+	       NAME, SIZE ? (int) SIZE : 1);				\
     }									\
   while (0)
 
--- a/src/gcc/config/glibc-stdint.h
+++ b/src/gcc/config/glibc-stdint.h
@@ -22,6 +22,12 @@ a copy of the GCC Runtime Library Exception along with this program;
 see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
 <http://www.gnu.org/licenses/>.  */
 
+/* Systems using musl libc should use this header and make sure
+   OPTION_MUSL is defined correctly before using the TYPE macros. */
+#ifndef OPTION_MUSL
+#define OPTION_MUSL 0
+#endif
+
 #define SIG_ATOMIC_TYPE "int"
 
 #define INT8_TYPE "signed char"
@@ -43,12 +49,12 @@ see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
 #define UINT_LEAST64_TYPE (LONG_TYPE_SIZE == 64 ? "long unsigned int" : "long long unsigned int")
 
 #define INT_FAST8_TYPE "signed char"
-#define INT_FAST16_TYPE (LONG_TYPE_SIZE == 64 ? "long int" : "int")
-#define INT_FAST32_TYPE (LONG_TYPE_SIZE == 64 ? "long int" : "int")
+#define INT_FAST16_TYPE (LONG_TYPE_SIZE == 64 && !OPTION_MUSL ? "long int" : "int")
+#define INT_FAST32_TYPE (LONG_TYPE_SIZE == 64 && !OPTION_MUSL ? "long int" : "int")
 #define INT_FAST64_TYPE (LONG_TYPE_SIZE == 64 ? "long int" : "long long int")
 #define UINT_FAST8_TYPE "unsigned char"
-#define UINT_FAST16_TYPE (LONG_TYPE_SIZE == 64 ? "long unsigned int" : "unsigned int")
-#define UINT_FAST32_TYPE (LONG_TYPE_SIZE == 64 ? "long unsigned int" : "unsigned int")
+#define UINT_FAST16_TYPE (LONG_TYPE_SIZE == 64 && !OPTION_MUSL ? "long unsigned int" : "unsigned int")
+#define UINT_FAST32_TYPE (LONG_TYPE_SIZE == 64 && !OPTION_MUSL ? "long unsigned int" : "unsigned int")
 #define UINT_FAST64_TYPE (LONG_TYPE_SIZE == 64 ? "long unsigned int" : "long long unsigned int")
 
 #define INTPTR_TYPE (LONG_TYPE_SIZE == 64 ? "long int" : "int")
--- a/src/gcc/config/linux.h
+++ b/src/gcc/config/linux.h
@@ -32,10 +32,14 @@ see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
 #define OPTION_GLIBC  (DEFAULT_LIBC == LIBC_GLIBC)
 #define OPTION_UCLIBC (DEFAULT_LIBC == LIBC_UCLIBC)
 #define OPTION_BIONIC (DEFAULT_LIBC == LIBC_BIONIC)
+#undef OPTION_MUSL
+#define OPTION_MUSL   (DEFAULT_LIBC == LIBC_MUSL)
 #else
 #define OPTION_GLIBC  (linux_libc == LIBC_GLIBC)
 #define OPTION_UCLIBC (linux_libc == LIBC_UCLIBC)
 #define OPTION_BIONIC (linux_libc == LIBC_BIONIC)
+#undef OPTION_MUSL
+#define OPTION_MUSL   (linux_libc == LIBC_MUSL)
 #endif
 
 #define GNU_USER_TARGET_OS_CPP_BUILTINS()			\
@@ -50,21 +54,25 @@ see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
     } while (0)
 
 /* Determine which dynamic linker to use depending on whether GLIBC or
-   uClibc or Bionic is the default C library and whether
-   -muclibc or -mglibc or -mbionic has been passed to change the default.  */
+   uClibc or Bionic or musl is the default C library and whether
+   -muclibc or -mglibc or -mbionic or -mmusl has been passed to change
+   the default.  */
 
-#define CHOOSE_DYNAMIC_LINKER1(LIBC1, LIBC2, LIBC3, LD1, LD2, LD3)	\
-  "%{" LIBC2 ":" LD2 ";:%{" LIBC3 ":" LD3 ";:" LD1 "}}"
+#define CHOOSE_DYNAMIC_LINKER1(LIBC1, LIBC2, LIBC3, LIBC4, LD1, LD2, LD3, LD4)	\
+  "%{" LIBC2 ":" LD2 ";:%{" LIBC3 ":" LD3 ";:%{" LIBC4 ":" LD4 ";:" LD1 "}}}"
 
 #if DEFAULT_LIBC == LIBC_GLIBC
-#define CHOOSE_DYNAMIC_LINKER(G, U, B) \
-  CHOOSE_DYNAMIC_LINKER1 ("mglibc", "muclibc", "mbionic", G, U, B)
+#define CHOOSE_DYNAMIC_LINKER(G, U, B, M) \
+  CHOOSE_DYNAMIC_LINKER1 ("mglibc", "muclibc", "mbionic", "mmusl", G, U, B, M)
 #elif DEFAULT_LIBC == LIBC_UCLIBC
-#define CHOOSE_DYNAMIC_LINKER(G, U, B) \
-  CHOOSE_DYNAMIC_LINKER1 ("muclibc", "mglibc", "mbionic", U, G, B)
+#define CHOOSE_DYNAMIC_LINKER(G, U, B, M) \
+  CHOOSE_DYNAMIC_LINKER1 ("muclibc", "mglibc", "mbionic", "mmusl", U, G, B, M)
 #elif DEFAULT_LIBC == LIBC_BIONIC
-#define CHOOSE_DYNAMIC_LINKER(G, U, B) \
-  CHOOSE_DYNAMIC_LINKER1 ("mbionic", "mglibc", "muclibc", B, G, U)
+#define CHOOSE_DYNAMIC_LINKER(G, U, B, M) \
+  CHOOSE_DYNAMIC_LINKER1 ("mbionic", "mglibc", "muclibc", "mmusl", B, G, U, M)
+#elif DEFAULT_LIBC == LIBC_MUSL
+#define CHOOSE_DYNAMIC_LINKER(G, U, B, M) \
+  CHOOSE_DYNAMIC_LINKER1 ("mmusl", "mglibc", "muclibc", "mbionic", M, G, U, B)
 #else
 #error "Unsupported DEFAULT_LIBC"
 #endif /* DEFAULT_LIBC */
@@ -81,24 +89,100 @@ see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
 #define BIONIC_DYNAMIC_LINKER32 "/system/bin/linker"
 #define BIONIC_DYNAMIC_LINKER64 "/system/bin/linker64"
 #define BIONIC_DYNAMIC_LINKERX32 "/system/bin/linkerx32"
+/* Should be redefined for each target that supports musl.  */
+#define MUSL_DYNAMIC_LINKER "/dev/null"
+#define MUSL_DYNAMIC_LINKER32 "/dev/null"
+#define MUSL_DYNAMIC_LINKER64 "/dev/null"
+#define MUSL_DYNAMIC_LINKERX32 "/dev/null"
 
 #define GNU_USER_DYNAMIC_LINKER						\
   CHOOSE_DYNAMIC_LINKER (GLIBC_DYNAMIC_LINKER, UCLIBC_DYNAMIC_LINKER,	\
-			 BIONIC_DYNAMIC_LINKER)
+			 BIONIC_DYNAMIC_LINKER, MUSL_DYNAMIC_LINKER)
 #define GNU_USER_DYNAMIC_LINKER32					\
   CHOOSE_DYNAMIC_LINKER (GLIBC_DYNAMIC_LINKER32, UCLIBC_DYNAMIC_LINKER32, \
-			 BIONIC_DYNAMIC_LINKER32)
+			 BIONIC_DYNAMIC_LINKER32, MUSL_DYNAMIC_LINKER32)
 #define GNU_USER_DYNAMIC_LINKER64					\
   CHOOSE_DYNAMIC_LINKER (GLIBC_DYNAMIC_LINKER64, UCLIBC_DYNAMIC_LINKER64, \
-			 BIONIC_DYNAMIC_LINKER64)
+			 BIONIC_DYNAMIC_LINKER64, MUSL_DYNAMIC_LINKER64)
 #define GNU_USER_DYNAMIC_LINKERX32					\
   CHOOSE_DYNAMIC_LINKER (GLIBC_DYNAMIC_LINKERX32, UCLIBC_DYNAMIC_LINKERX32, \
-			 BIONIC_DYNAMIC_LINKERX32)
+			 BIONIC_DYNAMIC_LINKERX32, MUSL_DYNAMIC_LINKERX32)
 
 /* Whether we have Bionic libc runtime */
 #undef TARGET_HAS_BIONIC
 #define TARGET_HAS_BIONIC (OPTION_BIONIC)
 
+/* musl avoids problematic includes by rearranging the include directories.
+ * Unfortunately, this is mostly duplicated from cppdefault.c */
+#if DEFAULT_LIBC == LIBC_MUSL
+#define INCLUDE_DEFAULTS_MUSL_GPP			\
+    { GPLUSPLUS_INCLUDE_DIR, "G++", 1, 1,		\
+      GPLUSPLUS_INCLUDE_DIR_ADD_SYSROOT, 0 },		\
+    { GPLUSPLUS_TOOL_INCLUDE_DIR, "G++", 1, 1,		\
+      GPLUSPLUS_INCLUDE_DIR_ADD_SYSROOT, 1 },		\
+    { GPLUSPLUS_BACKWARD_INCLUDE_DIR, "G++", 1, 1,	\
+      GPLUSPLUS_INCLUDE_DIR_ADD_SYSROOT, 0 },
+
+#ifdef LOCAL_INCLUDE_DIR
+#define INCLUDE_DEFAULTS_MUSL_LOCAL			\
+    { LOCAL_INCLUDE_DIR, 0, 0, 1, 1, 2 },		\
+    { LOCAL_INCLUDE_DIR, 0, 0, 1, 1, 0 },
+#else
+#define INCLUDE_DEFAULTS_MUSL_LOCAL
+#endif
+
+#ifdef PREFIX_INCLUDE_DIR
+#define INCLUDE_DEFAULTS_MUSL_PREFIX			\
+    { PREFIX_INCLUDE_DIR, 0, 0, 1, 0, 0},
+#else
+#define INCLUDE_DEFAULTS_MUSL_PREFIX
+#endif
+
+#ifdef CROSS_INCLUDE_DIR
+#define INCLUDE_DEFAULTS_MUSL_CROSS			\
+    { CROSS_INCLUDE_DIR, "GCC", 0, 0, 0, 0},
+#else
+#define INCLUDE_DEFAULTS_MUSL_CROSS
+#endif
+
+#ifdef TOOL_INCLUDE_DIR
+#define INCLUDE_DEFAULTS_MUSL_TOOL			\
+    { TOOL_INCLUDE_DIR, "BINUTILS", 0, 1, 0, 0},
+#else
+#define INCLUDE_DEFAULTS_MUSL_TOOL
+#endif
+
+#ifdef NATIVE_SYSTEM_HEADER_DIR
+#define INCLUDE_DEFAULTS_MUSL_NATIVE			\
+    { NATIVE_SYSTEM_HEADER_DIR, 0, 0, 0, 1, 2 },	\
+    { NATIVE_SYSTEM_HEADER_DIR, 0, 0, 0, 1, 0 },
+#else
+#define INCLUDE_DEFAULTS_MUSL_NATIVE
+#endif
+
+#if defined (CROSS_DIRECTORY_STRUCTURE) && !defined (TARGET_SYSTEM_ROOT)
+# undef INCLUDE_DEFAULTS_MUSL_LOCAL
+# define INCLUDE_DEFAULTS_MUSL_LOCAL
+# undef INCLUDE_DEFAULTS_MUSL_NATIVE
+# define INCLUDE_DEFAULTS_MUSL_NATIVE
+#else
+# undef INCLUDE_DEFAULTS_MUSL_CROSS
+# define INCLUDE_DEFAULTS_MUSL_CROSS
+#endif
+
+#undef INCLUDE_DEFAULTS
+#define INCLUDE_DEFAULTS				\
+  {							\
+    INCLUDE_DEFAULTS_MUSL_GPP				\
+    INCLUDE_DEFAULTS_MUSL_PREFIX			\
+    INCLUDE_DEFAULTS_MUSL_CROSS				\
+    INCLUDE_DEFAULTS_MUSL_TOOL				\
+    INCLUDE_DEFAULTS_MUSL_NATIVE			\
+    { GCC_INCLUDE_DIR, "GCC", 0, 1, 0, 0 },		\
+    { 0, 0, 0, 0, 0, 0 }				\
+  }
+#endif
+
 #if (DEFAULT_LIBC == LIBC_UCLIBC) && defined (SINGLE_LIBC) /* uClinux */
 /* This is a *uclinux* target.  We don't define below macros to normal linux
    versions, because doing so would require *uclinux* targets to include
--- a/src/gcc/config/linux.opt
+++ b/src/gcc/config/linux.opt
@@ -28,5 +28,9 @@ Target Report RejectNegative Var(linux_libc,LIBC_GLIBC) Negative(muclibc)
 Use GNU C library
 
 muclibc
-Target Report RejectNegative Var(linux_libc,LIBC_UCLIBC) Negative(mbionic)
+Target Report RejectNegative Var(linux_libc,LIBC_UCLIBC) Negative(mmusl)
 Use uClibc C library
+
+mmusl
+Target Report RejectNegative Var(linux_libc,LIBC_MUSL) Negative(mbionic)
+Use musl C library
--- a/src/gcc/config/mips/linux.h
+++ b/src/gcc/config/mips/linux.h
@@ -37,7 +37,13 @@ along with GCC; see the file COPYING3.  If not see
 #define UCLIBC_DYNAMIC_LINKERN32 \
   "%{mnan=2008:/lib32/ld-uClibc-mipsn8.so.0;:/lib32/ld-uClibc.so.0}"
 
+#undef MUSL_DYNAMIC_LINKER32
+#define MUSL_DYNAMIC_LINKER32 "/lib/ld-musl-mips%{EL:el}%{msoft-float:-sf}.so.1"
+#undef MUSL_DYNAMIC_LINKER64
+#define MUSL_DYNAMIC_LINKER64 "/lib/ld-musl-mips64%{EL:el}%{msoft-float:-sf}.so.1"
+#define MUSL_DYNAMIC_LINKERN32 "/lib/ld-musl-mipsn32%{EL:el}%{msoft-float:-sf}.so.1"
+
 #define BIONIC_DYNAMIC_LINKERN32 "/system/bin/linker32"
 #define GNU_USER_DYNAMIC_LINKERN32 \
   CHOOSE_DYNAMIC_LINKER (GLIBC_DYNAMIC_LINKERN32, UCLIBC_DYNAMIC_LINKERN32, \
-                         BIONIC_DYNAMIC_LINKERN32)
+                         BIONIC_DYNAMIC_LINKERN32, MUSL_DYNAMIC_LINKERN32)
--- a/src/gcc/config/rs6000/linux.h
+++ b/src/gcc/config/rs6000/linux.h
@@ -30,10 +30,14 @@
 #define OPTION_GLIBC  (DEFAULT_LIBC == LIBC_GLIBC)
 #define OPTION_UCLIBC (DEFAULT_LIBC == LIBC_UCLIBC)
 #define OPTION_BIONIC (DEFAULT_LIBC == LIBC_BIONIC)
+#undef OPTION_MUSL
+#define OPTION_MUSL   (DEFAULT_LIBC == LIBC_MUSL)
 #else
 #define OPTION_GLIBC  (linux_libc == LIBC_GLIBC)
 #define OPTION_UCLIBC (linux_libc == LIBC_UCLIBC)
 #define OPTION_BIONIC (linux_libc == LIBC_BIONIC)
+#undef OPTION_MUSL
+#define OPTION_MUSL   (linux_libc == LIBC_MUSL)
 #endif
 
 /* Determine what functions are present at the runtime;
--- a/src/gcc/config/rs6000/linux64.h
+++ b/src/gcc/config/rs6000/linux64.h
@@ -299,10 +299,14 @@ extern int dot_symbols;
 #define OPTION_GLIBC  (DEFAULT_LIBC == LIBC_GLIBC)
 #define OPTION_UCLIBC (DEFAULT_LIBC == LIBC_UCLIBC)
 #define OPTION_BIONIC (DEFAULT_LIBC == LIBC_BIONIC)
+#undef OPTION_MUSL
+#define OPTION_MUSL   (DEFAULT_LIBC == LIBC_MUSL)
 #else
 #define OPTION_GLIBC  (linux_libc == LIBC_GLIBC)
 #define OPTION_UCLIBC (linux_libc == LIBC_UCLIBC)
 #define OPTION_BIONIC (linux_libc == LIBC_BIONIC)
+#undef OPTION_MUSL
+#define OPTION_MUSL   (linux_libc == LIBC_MUSL)
 #endif
 
 /* Determine what functions are present at the runtime;
--- a/src/gcc/configure
+++ b/src/gcc/configure
@@ -1699,7 +1699,8 @@ Optional Packages:
                           use sysroot as the system root during the build
   --with-sysroot[=DIR]    search for usr/lib, usr/include, et al, within DIR
   --with-specs=SPECS      add SPECS to driver command-line processing
-  --with-pkgversion=PKG   Use PKG in the version string in place of "GCC"
+  --with-pkgversion=PKG   Use PKG in the version string in place of "Linaro
+                          GCC `cat $srcdir/LINARO-VERSION`"
   --with-bugurl=URL       Direct users to URL to report a bug
   --with-multilib-list    select multilibs (AArch64, SH and x86-64 only)
   --with-gnu-ld           assume the C compiler uses GNU ld default=no
@@ -7362,7 +7363,7 @@ if test "${with_pkgversion+set}" = set; then :
       *)   PKGVERSION="($withval) " ;;
      esac
 else
-  PKGVERSION="(GCC) "
+  PKGVERSION="(Linaro GCC `cat $srcdir/LINARO-VERSION`) "
 
 fi
 
@@ -18162,7 +18163,7 @@ else
   lt_dlunknown=0; lt_dlno_uscore=1; lt_dlneed_uscore=2
   lt_status=$lt_dlunknown
   cat > conftest.$ac_ext <<_LT_EOF
-#line 18165 "configure"
+#line 18166 "configure"
 #include "confdefs.h"
 
 #if HAVE_DLFCN_H
@@ -18268,7 +18269,7 @@ else
   lt_dlunknown=0; lt_dlno_uscore=1; lt_dlneed_uscore=2
   lt_status=$lt_dlunknown
   cat > conftest.$ac_ext <<_LT_EOF
-#line 18271 "configure"
+#line 18272 "configure"
 #include "confdefs.h"
 
 #if HAVE_DLFCN_H
@@ -27742,6 +27743,9 @@ if test "${gcc_cv_libc_provides_ssp+set}" = set; then :
 else
   gcc_cv_libc_provides_ssp=no
     case "$target" in
+       *-*-musl*)
+	 # All versions of musl provide stack protector
+	 gcc_cv_libc_provides_ssp=yes;;
        *-*-linux* | *-*-kfreebsd*-gnu | *-*-knetbsd*-gnu)
       # glibc 2.4 and later provides __stack_chk_fail and
       # either __stack_chk_guard, or TLS access to stack guard canary.
@@ -27774,6 +27778,7 @@ fi
 	 # <http://gcc.gnu.org/ml/gcc/2008-10/msg00130.html>) and for now
 	 # simply assert that glibc does provide this, which is true for all
 	 # realistically usable GNU/Hurd configurations.
+	 # All supported versions of musl provide it as well
 	 gcc_cv_libc_provides_ssp=yes;;
        *-*-darwin* | *-*-freebsd*)
 	 ac_fn_c_check_func "$LINENO" "__stack_chk_fail" "ac_cv_func___stack_chk_fail"
@@ -27870,6 +27875,9 @@ case "$target" in
       gcc_cv_target_dl_iterate_phdr=no
     fi
     ;;
+  *-linux-musl*)
+    gcc_cv_target_dl_iterate_phdr=yes
+    ;;
 esac
 
 if test x$gcc_cv_target_dl_iterate_phdr = xyes; then
--- a/src/gcc/configure.ac
+++ b/src/gcc/configure.ac
@@ -862,7 +862,7 @@ AC_ARG_WITH(specs,
 )
 AC_SUBST(CONFIGURE_SPECS)
 
-ACX_PKGVERSION([GCC])
+ACX_PKGVERSION([Linaro GCC `cat $srcdir/LINARO-VERSION`])
 ACX_BUGURL([http://gcc.gnu.org/bugs.html])
 
 # Sanity check enable_languages in case someone does not run the toplevel
@@ -5229,6 +5229,9 @@ AC_CACHE_CHECK(__stack_chk_fail in target C library,
       gcc_cv_libc_provides_ssp,
       [gcc_cv_libc_provides_ssp=no
     case "$target" in
+       *-*-musl*)
+	 # All versions of musl provide stack protector
+	 gcc_cv_libc_provides_ssp=yes;;
        *-*-linux* | *-*-kfreebsd*-gnu | *-*-knetbsd*-gnu)
       # glibc 2.4 and later provides __stack_chk_fail and
       # either __stack_chk_guard, or TLS access to stack guard canary.
@@ -5255,6 +5258,7 @@ AC_CACHE_CHECK(__stack_chk_fail in target C library,
 	 # <http://gcc.gnu.org/ml/gcc/2008-10/msg00130.html>) and for now
 	 # simply assert that glibc does provide this, which is true for all
 	 # realistically usable GNU/Hurd configurations.
+	 # All supported versions of musl provide it as well
 	 gcc_cv_libc_provides_ssp=yes;;
        *-*-darwin* | *-*-freebsd*)
 	 AC_CHECK_FUNC(__stack_chk_fail,[gcc_cv_libc_provides_ssp=yes],
@@ -5328,6 +5332,9 @@ case "$target" in
       gcc_cv_target_dl_iterate_phdr=no
     fi
     ;;
+  *-linux-musl*)
+    gcc_cv_target_dl_iterate_phdr=yes
+    ;;
 esac
 GCC_TARGET_TEMPLATE([TARGET_DL_ITERATE_PHDR])
 if test x$gcc_cv_target_dl_iterate_phdr = xyes; then
--- a/src/gcc/cp/Make-lang.in
+++ b/src/gcc/cp/Make-lang.in
@@ -155,7 +155,7 @@ check-c++-subtargets : check-g++-subtargets
 # List of targets that can use the generic check- rule and its // variant.
 lang_checks += check-g++
 lang_checks_parallelized += check-g++
-# For description see comment above check_gcc_parallelize in gcc/Makefile.in.
+# For description see the check_$lang_parallelize comment in gcc/Makefile.in.
 check_g++_parallelize = 10000
 #
 # Install hooks:
@@ -221,6 +221,7 @@ c++.mostlyclean:
 	-rm -f doc/g++.1
 	-rm -f cp/*$(objext)
 	-rm -f cp/*$(coverageexts)
+	-rm -f xg++$(exeext) g++-cross$(exeext) cc1plus$(exeext)
 c++.clean:
 c++.distclean:
 	-rm -f cp/config.status cp/Makefile
--- a/src/gcc/cppbuiltin.c
+++ b/src/gcc/cppbuiltin.c
@@ -62,18 +62,41 @@ parse_basever (int *major, int *minor, int *patchlevel)
     *patchlevel = s_patchlevel;
 }
 
+/* Parse a LINAROVER version string of the format "M.m-year.month[-spin][~dev]"
+   to create Linaro release number YYYYMM and spin version.  */
+static void
+parse_linarover (int *release, int *spin)
+{
+  static int s_year = -1, s_month, s_spin;
+
+  if (s_year == -1)
+    if (sscanf (LINAROVER, "%*[^-]-%d.%d-%d", &s_year, &s_month, &s_spin) != 3)
+      {
+	sscanf (LINAROVER, "%*[^-]-%d.%d", &s_year, &s_month);
+	s_spin = 0;
+      }
+
+  if (release)
+    *release = s_year * 100 + s_month;
+
+  if (spin)
+    *spin = s_spin;
+}
 
 /* Define __GNUC__, __GNUC_MINOR__, __GNUC_PATCHLEVEL__ and __VERSION__.  */
 static void
 define__GNUC__ (cpp_reader *pfile)
 {
-  int major, minor, patchlevel;
+  int major, minor, patchlevel, linaro_release, linaro_spin;
 
   parse_basever (&major, &minor, &patchlevel);
+  parse_linarover (&linaro_release, &linaro_spin);
   cpp_define_formatted (pfile, "__GNUC__=%d", major);
   cpp_define_formatted (pfile, "__GNUC_MINOR__=%d", minor);
   cpp_define_formatted (pfile, "__GNUC_PATCHLEVEL__=%d", patchlevel);
   cpp_define_formatted (pfile, "__VERSION__=\"%s\"", version_string);
+  cpp_define_formatted (pfile, "__LINARO_RELEASE__=%d", linaro_release);
+  cpp_define_formatted (pfile, "__LINARO_SPIN__=%d", linaro_spin);
   cpp_define_formatted (pfile, "__ATOMIC_RELAXED=%d", MEMMODEL_RELAXED);
   cpp_define_formatted (pfile, "__ATOMIC_SEQ_CST=%d", MEMMODEL_SEQ_CST);
   cpp_define_formatted (pfile, "__ATOMIC_ACQUIRE=%d", MEMMODEL_ACQUIRE);
--- a/src/gcc/cprop.c
+++ b/src/gcc/cprop.c
@@ -285,6 +285,15 @@ cprop_constant_p (const_rtx x)
   return CONSTANT_P (x) && (GET_CODE (x) != CONST || shared_const_p (x));
 }
 
+/* Determine whether the rtx X should be treated as a register that can
+   be propagated.  Any pseudo-register is fine.  */
+
+static bool
+cprop_reg_p (const_rtx x)
+{
+  return REG_P (x) && !HARD_REGISTER_P (x);
+}
+
 /* Scan SET present in INSN and add an entry to the hash TABLE.
    IMPLICIT is true if it's an implicit set, false otherwise.  */
 
@@ -295,8 +304,7 @@ hash_scan_set (rtx set, rtx_insn *insn, struct hash_table_d *table,
   rtx src = SET_SRC (set);
   rtx dest = SET_DEST (set);
 
-  if (REG_P (dest)
-      && ! HARD_REGISTER_P (dest)
+  if (cprop_reg_p (dest)
       && reg_available_p (dest, insn)
       && can_copy_p (GET_MODE (dest)))
     {
@@ -321,9 +329,8 @@ hash_scan_set (rtx set, rtx_insn *insn, struct hash_table_d *table,
 	src = XEXP (note, 0), set = gen_rtx_SET (VOIDmode, dest, src);
 
       /* Record sets for constant/copy propagation.  */
-      if ((REG_P (src)
+      if ((cprop_reg_p (src)
 	   && src != dest
-	   && ! HARD_REGISTER_P (src)
 	   && reg_available_p (src, insn))
 	  || cprop_constant_p (src))
 	insert_set_in_table (dest, src, insn, table, implicit);
@@ -821,15 +828,15 @@ try_replace_reg (rtx from, rtx to, rtx_insn *insn)
   return success;
 }
 
-/* Find a set of REGNOs that are available on entry to INSN's block.  Return
-   NULL no such set is found.  */
+/* Find a set of REGNOs that are available on entry to INSN's block.  If found,
+   SET_RET[0] will be assigned a set with a register source and SET_RET[1] a
+   set with a constant source.  If not found the corresponding entry is set to
+   NULL.  */
 
-static struct cprop_expr *
-find_avail_set (int regno, rtx_insn *insn)
+static void
+find_avail_set (int regno, rtx_insn *insn, struct cprop_expr *set_ret[2])
 {
-  /* SET1 contains the last set found that can be returned to the caller for
-     use in a substitution.  */
-  struct cprop_expr *set1 = 0;
+  set_ret[0] = set_ret[1] = NULL;
 
   /* Loops are not possible here.  To get a loop we would need two sets
      available at the start of the block containing INSN.  i.e. we would
@@ -869,8 +876,10 @@ find_avail_set (int regno, rtx_insn *insn)
          If the source operand changed, we may still use it for the next
          iteration of this loop, but we may not use it for substitutions.  */
 
-      if (cprop_constant_p (src) || reg_not_set_p (src, insn))
-	set1 = set;
+      if (cprop_constant_p (src))
+	set_ret[1] = set;
+      else if (reg_not_set_p (src, insn))
+	set_ret[0] = set;
 
       /* If the source of the set is anything except a register, then
 	 we have reached the end of the copy chain.  */
@@ -881,10 +890,6 @@ find_avail_set (int regno, rtx_insn *insn)
 	 and see if we have an available copy into SRC.  */
       regno = REGNO (src);
     }
-
-  /* SET1 holds the last set that was available and anticipatable at
-     INSN.  */
-  return set1;
 }
 
 /* Subroutine of cprop_insn that tries to propagate constants into
@@ -1050,40 +1055,40 @@ cprop_insn (rtx_insn *insn)
   int changed = 0, changed_this_round;
   rtx note;
 
-retry:
-  changed_this_round = 0;
-  reg_use_count = 0;
-  note_uses (&PATTERN (insn), find_used_regs, NULL);
-
-  /* We may win even when propagating constants into notes.  */
-  note = find_reg_equal_equiv_note (insn);
-  if (note)
-    find_used_regs (&XEXP (note, 0), NULL);
-
-  for (i = 0; i < reg_use_count; i++)
+  do
     {
-      rtx reg_used = reg_use_table[i];
-      unsigned int regno = REGNO (reg_used);
-      rtx src;
-      struct cprop_expr *set;
+      changed_this_round = 0;
+      reg_use_count = 0;
+      note_uses (&PATTERN (insn), find_used_regs, NULL);
 
-      /* If the register has already been set in this block, there's
-	 nothing we can do.  */
-      if (! reg_not_set_p (reg_used, insn))
-	continue;
+      /* We may win even when propagating constants into notes.  */
+      note = find_reg_equal_equiv_note (insn);
+      if (note)
+	find_used_regs (&XEXP (note, 0), NULL);
 
-      /* Find an assignment that sets reg_used and is available
-	 at the start of the block.  */
-      set = find_avail_set (regno, insn);
-      if (! set)
-	continue;
+      for (i = 0; i < reg_use_count; i++)
+	{
+	  rtx reg_used = reg_use_table[i];
+	  unsigned int regno = REGNO (reg_used);
+	  rtx src_cst = NULL, src_reg = NULL;
+	  struct cprop_expr *set[2];
 
-      src = set->src;
+	  /* If the register has already been set in this block, there's
+	     nothing we can do.  */
+	  if (! reg_not_set_p (reg_used, insn))
+	    continue;
 
-      /* Constant propagation.  */
-      if (cprop_constant_p (src))
-	{
-          if (constprop_register (reg_used, src, insn))
+	  /* Find an assignment that sets reg_used and is available
+	     at the start of the block.  */
+	  find_avail_set (regno, insn, set);
+	  if (set[0])
+	    src_reg = set[0]->src;
+	  if (set[1])
+	    src_cst = set[1]->src;
+
+	  /* Constant propagation.  */
+	  if (src_cst && cprop_constant_p (src_cst)
+	      && constprop_register (reg_used, src_cst, insn))
 	    {
 	      changed_this_round = changed = 1;
 	      global_const_prop_count++;
@@ -1093,18 +1098,16 @@ retry:
 			   "GLOBAL CONST-PROP: Replacing reg %d in ", regno);
 		  fprintf (dump_file, "insn %d with constant ",
 			   INSN_UID (insn));
-		  print_rtl (dump_file, src);
+		  print_rtl (dump_file, src_cst);
 		  fprintf (dump_file, "\n");
 		}
 	      if (insn->deleted ())
 		return 1;
 	    }
-	}
-      else if (REG_P (src)
-	       && REGNO (src) >= FIRST_PSEUDO_REGISTER
-	       && REGNO (src) != regno)
-	{
-	  if (try_replace_reg (reg_used, src, insn))
+	  /* Copy propagation.  */
+	  else if (src_reg && cprop_reg_p (src_reg)
+		   && REGNO (src_reg) != regno
+		   && try_replace_reg (reg_used, src_reg, insn))
 	    {
 	      changed_this_round = changed = 1;
 	      global_copy_prop_count++;
@@ -1113,7 +1116,7 @@ retry:
 		  fprintf (dump_file,
 			   "GLOBAL COPY-PROP: Replacing reg %d in insn %d",
 			   regno, INSN_UID (insn));
-		  fprintf (dump_file, " with reg %d\n", REGNO (src));
+		  fprintf (dump_file, " with reg %d\n", REGNO (src_reg));
 		}
 
 	      /* The original insn setting reg_used may or may not now be
@@ -1123,12 +1126,10 @@ retry:
 		 and made things worse.  */
 	    }
 	}
-
-      /* If try_replace_reg simplified the insn, the regs found
-	 by find_used_regs may not be valid anymore.  Start over.  */
-      if (changed_this_round)
-	goto retry;
     }
+  /* If try_replace_reg simplified the insn, the regs found by find_used_regs
+     may not be valid anymore.  Start over.  */
+  while (changed_this_round);
 
   if (changed && DEBUG_INSN_P (insn))
     return 0;
@@ -1191,7 +1192,7 @@ do_local_cprop (rtx x, rtx_insn *insn)
   /* Rule out USE instructions and ASM statements as we don't want to
      change the hard registers mentioned.  */
   if (REG_P (x)
-      && (REGNO (x) >= FIRST_PSEUDO_REGISTER
+      && (cprop_reg_p (x)
           || (GET_CODE (PATTERN (insn)) != USE
 	      && asm_noperands (PATTERN (insn)) < 0)))
     {
@@ -1207,7 +1208,7 @@ do_local_cprop (rtx x, rtx_insn *insn)
 
 	  if (cprop_constant_p (this_rtx))
 	    newcnst = this_rtx;
-	  if (REG_P (this_rtx) && REGNO (this_rtx) >= FIRST_PSEUDO_REGISTER
+	  if (cprop_reg_p (this_rtx)
 	      /* Don't copy propagate if it has attached REG_EQUIV note.
 		 At this point this only function parameters should have
 		 REG_EQUIV notes and if the argument slot is used somewhere
@@ -1328,9 +1329,8 @@ implicit_set_cond_p (const_rtx cond)
   if (GET_CODE (cond) != EQ && GET_CODE (cond) != NE)
     return false;
 
-  /* The first operand of COND must be a pseudo-reg.  */
-  if (! REG_P (XEXP (cond, 0))
-      || HARD_REGISTER_P (XEXP (cond, 0)))
+  /* The first operand of COND must be a register we can propagate.  */
+  if (!cprop_reg_p (XEXP (cond, 0)))
     return false;
 
   /* The second operand of COND must be a suitable constant.  */
--- a/src/gcc/df-core.c
+++ b/src/gcc/df-core.c
@@ -642,7 +642,6 @@ void
 df_finish_pass (bool verify ATTRIBUTE_UNUSED)
 {
   int i;
-  int removed = 0;
 
 #ifdef ENABLE_DF_CHECKING
   int saved_flags;
@@ -658,21 +657,15 @@ df_finish_pass (bool verify ATTRIBUTE_UNUSED)
   saved_flags = df->changeable_flags;
 #endif
 
-  for (i = 0; i < df->num_problems_defined; i++)
+  /* We iterate over problems by index as each problem removed will
+     lead to problems_in_order to be reordered.  */
+  for (i = 0; i < DF_LAST_PROBLEM_PLUS1; i++)
     {
-      struct dataflow *dflow = df->problems_in_order[i];
-      struct df_problem *problem = dflow->problem;
+      struct dataflow *dflow = df->problems_by_index[i];
 
-      if (dflow->optional_p)
-	{
-	  gcc_assert (problem->remove_problem_fun);
-	  (problem->remove_problem_fun) ();
-	  df->problems_in_order[i] = NULL;
-	  df->problems_by_index[problem->id] = NULL;
-	  removed++;
-	}
+      if (dflow && dflow->optional_p)
+	df_remove_problem (dflow);
     }
-  df->num_problems_defined -= removed;
 
   /* Clear all of the flags.  */
   df->changeable_flags = 0;
--- a/src/gcc/fortran/Make-lang.in
+++ b/src/gcc/fortran/Make-lang.in
@@ -167,7 +167,7 @@ check-f95-subtargets : check-gfortran-subtargets
 check-fortran-subtargets : check-gfortran-subtargets
 lang_checks += check-gfortran
 lang_checks_parallelized += check-gfortran
-# For description see comment above check_gcc_parallelize in gcc/Makefile.in.
+# For description see the check_$lang_parallelize comment in gcc/Makefile.in.
 check_gfortran_parallelize = 10000
 
 # GFORTRAN documentation.
@@ -275,7 +275,7 @@ fortran.uninstall:
 # We just have to delete files specific to us.
 
 fortran.mostlyclean:
-	-rm -f f951$(exeext)
+	-rm -f gfortran$(exeext) gfortran-cross$(exeext) f951$(exeext)
 	-rm -f fortran/*.o
 
 fortran.clean:
--- a/src/gcc/genpreds.c
+++ b/src/gcc/genpreds.c
@@ -640,12 +640,14 @@ struct constraint_data
   const char *regclass;  /* for register constraints */
   rtx exp;               /* for other constraints */
   unsigned int lineno;   /* line of definition */
-  unsigned int is_register  : 1;
-  unsigned int is_const_int : 1;
-  unsigned int is_const_dbl : 1;
-  unsigned int is_extra     : 1;
-  unsigned int is_memory    : 1;
-  unsigned int is_address   : 1;
+  unsigned int is_register	: 1;
+  unsigned int is_const_int	: 1;
+  unsigned int is_const_dbl	: 1;
+  unsigned int is_extra		: 1;
+  unsigned int is_memory	: 1;
+  unsigned int is_address	: 1;
+  unsigned int maybe_allows_reg : 1;
+  unsigned int maybe_allows_mem : 1;
 };
 
 /* Overview of all constraints beginning with a given letter.  */
@@ -691,6 +693,9 @@ static unsigned int satisfied_start;
 static unsigned int const_int_start, const_int_end;
 static unsigned int memory_start, memory_end;
 static unsigned int address_start, address_end;
+static unsigned int maybe_allows_none_start, maybe_allows_none_end;
+static unsigned int maybe_allows_reg_start, maybe_allows_reg_end;
+static unsigned int maybe_allows_mem_start, maybe_allows_mem_end;
 
 /* Convert NAME, which contains angle brackets and/or underscores, to
    a string that can be used as part of a C identifier.  The string
@@ -711,6 +716,34 @@ mangle (const char *name)
   return XOBFINISH (rtl_obstack, const char *);
 }
 
+/* Return a bitmask, bit 1 if EXP maybe allows a REG/SUBREG, 2 if EXP
+   maybe allows a MEM.  Bits should be clear only when we are sure it
+   will not allow a REG/SUBREG or a MEM.  */
+static int
+compute_maybe_allows (rtx exp)
+{
+  switch (GET_CODE (exp))
+    {
+    case IF_THEN_ELSE:
+      /* Conservative answer is like IOR, of the THEN and ELSE branches.  */
+      return compute_maybe_allows (XEXP (exp, 1))
+	     | compute_maybe_allows (XEXP (exp, 2));
+    case AND:
+      return compute_maybe_allows (XEXP (exp, 0))
+	     & compute_maybe_allows (XEXP (exp, 1));
+    case IOR:
+      return compute_maybe_allows (XEXP (exp, 0))
+	     | compute_maybe_allows (XEXP (exp, 1));
+    case MATCH_CODE:
+      if (*XSTR (exp, 1) == '\0')
+	return (strstr (XSTR (exp, 0), "reg") != NULL ? 1 : 0)
+	       | (strstr (XSTR (exp, 0), "mem") != NULL ? 2 : 0);
+      /* FALLTHRU */
+    default:
+      return 3;
+    }
+}
+
 /* Add one constraint, of any sort, to the tables.  NAME is its name;
    REGCLASS is the register class, if any; EXP is the expression to
    test, if any;  IS_MEMORY and IS_ADDRESS indicate memory and address
@@ -866,6 +899,11 @@ add_constraint (const char *name, const char *regclass,
   c->is_extra = !(regclass || is_const_int || is_const_dbl);
   c->is_memory = is_memory;
   c->is_address = is_address;
+  int maybe_allows = 3;
+  if (exp)
+    maybe_allows = compute_maybe_allows (exp);
+  c->maybe_allows_reg = (maybe_allows & 1) != 0;
+  c->maybe_allows_mem = (maybe_allows & 2) != 0;
 
   c->next_this_letter = *slot;
   *slot = c;
@@ -940,8 +978,30 @@ choose_enum_order (void)
       enum_order[next++] = c;
   address_end = next;
 
+  maybe_allows_none_start = next;
+  FOR_ALL_CONSTRAINTS (c)
+    if (!c->is_register && !c->is_const_int && !c->is_memory && !c->is_address
+	&& !c->maybe_allows_reg && !c->maybe_allows_mem)
+      enum_order[next++] = c;
+  maybe_allows_none_end = next;
+
+  maybe_allows_reg_start = next;
+  FOR_ALL_CONSTRAINTS (c)
+    if (!c->is_register && !c->is_const_int && !c->is_memory && !c->is_address
+	&& c->maybe_allows_reg && !c->maybe_allows_mem)
+      enum_order[next++] = c;
+  maybe_allows_reg_end = next;
+
+  maybe_allows_mem_start = next;
+  FOR_ALL_CONSTRAINTS (c)
+    if (!c->is_register && !c->is_const_int && !c->is_memory && !c->is_address
+	&& !c->maybe_allows_reg && c->maybe_allows_mem)
+      enum_order[next++] = c;
+  maybe_allows_mem_end = next;
+
   FOR_ALL_CONSTRAINTS (c)
-    if (!c->is_register && !c->is_const_int && !c->is_memory && !c->is_address)
+    if (!c->is_register && !c->is_const_int && !c->is_memory && !c->is_address
+	&& c->maybe_allows_reg && c->maybe_allows_mem)
       enum_order[next++] = c;
   gcc_assert (next == num_constraints);
 }
@@ -1229,6 +1289,41 @@ write_range_function (const char *name, unsigned int start, unsigned int end)
 	    "}\n\n", name);
 }
 
+/* Write a definition for insn_extra_constraint_allows_reg_mem function.  */
+static void
+write_allows_reg_mem_function (void)
+{
+  printf ("static inline void\n"
+	  "insn_extra_constraint_allows_reg_mem (enum constraint_num c,\n"
+	  "\t\t\t\t      bool *allows_reg, bool *allows_mem)\n"
+	  "{\n");
+  if (maybe_allows_none_start != maybe_allows_none_end)
+    printf ("  if (c >= CONSTRAINT_%s && c <= CONSTRAINT_%s)\n"
+	    "    return;\n",
+	    enum_order[maybe_allows_none_start]->c_name,
+	    enum_order[maybe_allows_none_end - 1]->c_name);
+  if (maybe_allows_reg_start != maybe_allows_reg_end)
+    printf ("  if (c >= CONSTRAINT_%s && c <= CONSTRAINT_%s)\n"
+	    "    {\n"
+	    "      *allows_reg = true;\n"
+	    "      return;\n"
+	    "    }\n",
+	    enum_order[maybe_allows_reg_start]->c_name,
+	    enum_order[maybe_allows_reg_end - 1]->c_name);
+  if (maybe_allows_mem_start != maybe_allows_mem_end)
+    printf ("  if (c >= CONSTRAINT_%s && c <= CONSTRAINT_%s)\n"
+	    "    {\n"
+	    "      *allows_mem = true;\n"
+	    "      return;\n"
+	    "    }\n",
+	    enum_order[maybe_allows_mem_start]->c_name,
+	    enum_order[maybe_allows_mem_end - 1]->c_name);
+  printf ("  (void) c;\n"
+	  "  *allows_reg = true;\n"
+	  "  *allows_mem = true;\n"
+	  "}\n\n");
+}
+
 /* VEC is a list of key/value pairs, with the keys being lower bounds
    of a range.  Output a decision tree that handles the keys covered by
    [VEC[START], VEC[END]), returning FALLBACK for keys lower then VEC[START]'s.
@@ -1326,6 +1421,7 @@ write_tm_preds_h (void)
 			    memory_start, memory_end);
       write_range_function ("insn_extra_address_constraint",
 			    address_start, address_end);
+      write_allows_reg_mem_function ();
 
       if (constraint_max_namelen > 1)
         {
--- a/src/gcc/go/Make-lang.in
+++ b/src/gcc/go/Make-lang.in
@@ -197,6 +197,7 @@ go.uninstall:
 go.mostlyclean:
 	-rm -f go/*$(objext)
 	-rm -f go/*$(coverageexts)
+	-rm -f gccgo$(exeext) gccgo-cross$(exeext) go1$(exeext)
 go.clean:
 go.distclean:
 go.maintainer-clean:
--- a/src/gcc/ira-costs.c
+++ b/src/gcc/ira-costs.c
@@ -1380,8 +1380,6 @@ record_operand_costs (rtx_insn *insn, enum reg_class *pref)
       rtx dest = SET_DEST (set);
       rtx src = SET_SRC (set);
 
-      dest = SET_DEST (set);
-      src = SET_SRC (set);
       if (GET_CODE (dest) == SUBREG
 	  && (GET_MODE_SIZE (GET_MODE (dest))
 	      == GET_MODE_SIZE (GET_MODE (SUBREG_REG (dest)))))
--- a/src/gcc/jit/Make-lang.in
+++ b/src/gcc/jit/Make-lang.in
@@ -285,6 +285,10 @@ jit.uninstall:
 # We just have to delete files specific to us.
 
 jit.mostlyclean:
+	-rm -f $(LIBGCCJIT_FILENAME) $(LIBGCCJIT_SYMLINK)
+	-rm -f $(LIBGCCJIT_LINKER_NAME_SYMLINK) $(FULL_DRIVER_NAME)
+	-rm -f $(LIBGCCJIT_SONAME)
+	-rm -f $(jit_OBJS)
 
 jit.clean:
 
--- a/src/gcc/loop-invariant.c
+++ b/src/gcc/loop-invariant.c
@@ -740,8 +740,11 @@ create_new_invariant (struct def *def, rtx_insn *insn, bitmap depends_on,
 	 enough to not regress 410.bwaves either (by still moving reg+reg
 	 invariants).
 	 See http://gcc.gnu.org/ml/gcc-patches/2009-10/msg01210.html .  */
-      inv->cheap_address = address_cost (SET_SRC (set), word_mode,
-					 ADDR_SPACE_GENERIC, speed) < 3;
+      if (SCALAR_INT_MODE_P (GET_MODE (SET_DEST (set))))
+	inv->cheap_address = address_cost (SET_SRC (set), word_mode,
+					   ADDR_SPACE_GENERIC, speed) < 3;
+      else
+	inv->cheap_address = false;
     }
   else
     {
@@ -1174,6 +1177,7 @@ get_inv_cost (struct invariant *inv, int *comp_cost, unsigned *regs_needed,
     }
 
   if (!inv->cheap_address
+      || inv->def->n_uses == 0
       || inv->def->n_addr_uses < inv->def->n_uses)
     (*comp_cost) += inv->cost * inv->eqno;
 
@@ -1512,6 +1516,79 @@ replace_uses (struct invariant *inv, rtx reg, bool in_group)
   return 1;
 }
 
+/* Whether invariant INV setting REG can be moved out of LOOP, at the end of
+   the block preceding its header.  */
+
+static bool
+can_move_invariant_reg (struct loop *loop, struct invariant *inv, rtx reg)
+{
+  df_ref def, use;
+  unsigned int dest_regno, defs_in_loop_count = 0;
+  rtx_insn *insn = inv->insn;
+  basic_block bb = BLOCK_FOR_INSN (inv->insn);
+
+  /* We ignore hard register and memory access for cost and complexity reasons.
+     Hard register are few at this stage and expensive to consider as they
+     require building a separate data flow.  Memory access would require using
+     df_simulate_* and can_move_insns_across functions and is more complex.  */
+  if (!REG_P (reg) || HARD_REGISTER_P (reg))
+    return false;
+
+  /* Check whether the set is always executed.  We could omit this condition if
+     we know that the register is unused outside of the loop, but it does not
+     seem worth finding out.  */
+  if (!inv->always_executed)
+    return false;
+
+  /* Check that all uses that would be dominated by def are already dominated
+     by it.  */
+  dest_regno = REGNO (reg);
+  for (use = DF_REG_USE_CHAIN (dest_regno); use; use = DF_REF_NEXT_REG (use))
+    {
+      rtx_insn *use_insn;
+      basic_block use_bb;
+
+      use_insn = DF_REF_INSN (use);
+      use_bb = BLOCK_FOR_INSN (use_insn);
+
+      /* Ignore instruction considered for moving.  */
+      if (use_insn == insn)
+	continue;
+
+      /* Don't consider uses outside loop.  */
+      if (!flow_bb_inside_loop_p (loop, use_bb))
+	continue;
+
+      /* Don't move if a use is not dominated by def in insn.  */
+      if (use_bb == bb && DF_INSN_LUID (insn) >= DF_INSN_LUID (use_insn))
+	return false;
+      if (!dominated_by_p (CDI_DOMINATORS, use_bb, bb))
+	return false;
+    }
+
+  /* Check for other defs.  Any other def in the loop might reach a use
+     currently reached by the def in insn.  */
+  for (def = DF_REG_DEF_CHAIN (dest_regno); def; def = DF_REF_NEXT_REG (def))
+    {
+      basic_block def_bb = DF_REF_BB (def);
+
+      /* Defs in exit block cannot reach a use they weren't already.  */
+      if (single_succ_p (def_bb))
+	{
+	  basic_block def_bb_succ;
+
+	  def_bb_succ = single_succ (def_bb);
+	  if (!flow_bb_inside_loop_p (loop, def_bb_succ))
+	    continue;
+	}
+
+      if (++defs_in_loop_count > 1)
+	return false;
+    }
+
+  return true;
+}
+
 /* Move invariant INVNO out of the LOOP.  Returns true if this succeeds, false
    otherwise.  */
 
@@ -1545,11 +1622,8 @@ move_invariant_reg (struct loop *loop, unsigned invno)
 	    }
 	}
 
-      /* Move the set out of the loop.  If the set is always executed (we could
-	 omit this condition if we know that the register is unused outside of
-	 the loop, but it does not seem worth finding out) and it has no uses
-	 that would not be dominated by it, we may just move it (TODO).
-	 Otherwise we need to create a temporary register.  */
+      /* If possible, just move the set out of the loop.  Otherwise, we
+	 need to create a temporary register.  */
       set = single_set (inv->insn);
       reg = dest = SET_DEST (set);
       if (GET_CODE (reg) == SUBREG)
@@ -1557,19 +1631,25 @@ move_invariant_reg (struct loop *loop, unsigned invno)
       if (REG_P (reg))
 	regno = REGNO (reg);
 
-      reg = gen_reg_rtx_and_attrs (dest);
+      if (!can_move_invariant_reg (loop, inv, reg))
+	{
+	  reg = gen_reg_rtx_and_attrs (dest);
 
-      /* Try replacing the destination by a new pseudoregister.  */
-      validate_change (inv->insn, &SET_DEST (set), reg, true);
+	  /* Try replacing the destination by a new pseudoregister.  */
+	  validate_change (inv->insn, &SET_DEST (set), reg, true);
 
-      /* As well as all the dominated uses.  */
-      replace_uses (inv, reg, true);
+	  /* As well as all the dominated uses.  */
+	  replace_uses (inv, reg, true);
 
-      /* And validate all the changes.  */
-      if (!apply_change_group ())
-	goto fail;
+	  /* And validate all the changes.  */
+	  if (!apply_change_group ())
+	    goto fail;
 
-      emit_insn_after (gen_move_insn (dest, reg), inv->insn);
+	  emit_insn_after (gen_move_insn (dest, reg), inv->insn);
+	}
+      else if (dump_file)
+	fprintf (dump_file, "Invariant %d moved without introducing a new "
+			    "temporary register\n", invno);
       reorder_insns (inv->insn, inv->insn, BB_END (preheader));
 
       /* If there is a REG_EQUAL note on the insn we just moved, and the
--- a/src/gcc/lra-constraints.c
+++ b/src/gcc/lra-constraints.c
@@ -1656,8 +1656,7 @@ prohibited_class_reg_set_mode_p (enum reg_class rclass,
 {
   HARD_REG_SET temp;
   
-  // ??? Is this assert right
-  // lra_assert (hard_reg_set_subset_p (set, reg_class_contents[rclass]));
+  lra_assert (hard_reg_set_subset_p (reg_class_contents[rclass], set));
   COPY_HARD_REG_SET (temp, set);
   AND_COMPL_HARD_REG_SET (temp, lra_no_alloc_regs);
   return (hard_reg_set_subset_p
--- a/src/gcc/objc/Make-lang.in
+++ b/src/gcc/objc/Make-lang.in
@@ -114,6 +114,7 @@ objc.uninstall:
 objc.mostlyclean:
 	-rm -f objc/*$(objext) objc/xforward objc/fflags
 	-rm -f objc/*$(coverageexts)
+	-rm -f cc1obj$(exeext)
 objc.clean: objc.mostlyclean
 	-rm -rf objc-headers
 objc.distclean:
--- a/src/gcc/objcp/Make-lang.in
+++ b/src/gcc/objcp/Make-lang.in
@@ -142,6 +142,7 @@ obj-c++.uninstall:
 obj-c++.mostlyclean:
 	-rm -f objcp/*$(objext)
 	-rm -f objcp/*$(coverageexts)
+	-rm -f cc1objplus$(exeext)
 obj-c++.clean: obj-c++.mostlyclean
 obj-c++.distclean:
 	-rm -f objcp/config.status objcp/Makefile
--- a/src/gcc/optabs.c
+++ b/src/gcc/optabs.c
@@ -6544,18 +6544,28 @@ vector_compare_rtx (enum tree_code tcode, tree t_op0, tree t_op1,
 {
   struct expand_operand ops[2];
   rtx rtx_op0, rtx_op1;
+  machine_mode m0, m1;
   enum rtx_code rcode = get_rtx_code (tcode, unsignedp);
 
   gcc_assert (TREE_CODE_CLASS (tcode) == tcc_comparison);
 
-  /* Expand operands.  */
+  /* Expand operands.  For vector types with scalar modes, e.g. where int64x1_t
+     has mode DImode, this can produce a constant RTX of mode VOIDmode; in such
+     cases, use the original mode.  */
   rtx_op0 = expand_expr (t_op0, NULL_RTX, TYPE_MODE (TREE_TYPE (t_op0)),
 			 EXPAND_STACK_PARM);
+  m0 = GET_MODE (rtx_op0);
+  if (m0 == VOIDmode)
+    m0 = TYPE_MODE (TREE_TYPE (t_op0));
+
   rtx_op1 = expand_expr (t_op1, NULL_RTX, TYPE_MODE (TREE_TYPE (t_op1)),
 			 EXPAND_STACK_PARM);
+  m1 = GET_MODE (rtx_op1);
+  if (m1 == VOIDmode)
+    m1 = TYPE_MODE (TREE_TYPE (t_op1));
 
-  create_input_operand (&ops[0], rtx_op0, GET_MODE (rtx_op0));
-  create_input_operand (&ops[1], rtx_op1, GET_MODE (rtx_op1));
+  create_input_operand (&ops[0], rtx_op0, m0);
+  create_input_operand (&ops[1], rtx_op1, m1);
   if (!maybe_legitimize_operands (icode, 4, 2, ops))
     gcc_unreachable ();
   return gen_rtx_fmt_ee (rcode, VOIDmode, ops[0].value, ops[1].value);
--- a/src/gcc/params.def
+++ b/src/gcc/params.def
@@ -262,6 +262,14 @@ DEFPARAM(PARAM_MAX_HOIST_DEPTH,
 	 "Maximum depth of search in the dominator tree for expressions to hoist",
 	 30, 0, 0)
 
+
+/* When synthesizing expnonentiation by a real constant operations using square
+   roots, this controls how deep sqrt chains we are willing to generate.  */
+DEFPARAM(PARAM_MAX_POW_SQRT_DEPTH,
+	 "max-pow-sqrt-depth",
+	 "Maximum depth of sqrt chains to use when synthesizing exponentiation by a real constant",
+	 5, 1, 32)
+
 /* This parameter limits the number of insns in a loop that will be unrolled,
    and by how much the loop is unrolled.
 
--- a/src/gcc/rtlanal.c
+++ b/src/gcc/rtlanal.c
@@ -104,7 +104,10 @@ generic_subrtx_iterator <T>::add_single_to_queue (array_type &array,
 	  return base;
 	}
       gcc_checking_assert (i == LOCAL_ELEMS);
-      vec_safe_grow (array.heap, i + 1);
+      /* A previous iteration might also have moved from the stack to the
+	 heap, in which case the heap array will already be big enough.  */
+      if (vec_safe_length (array.heap) <= i)
+	vec_safe_grow (array.heap, i + 1);
       base = array.heap->address ();
       memcpy (base, array.stack, sizeof (array.stack));
       base[LOCAL_ELEMS] = x;
--- a/src/gcc/simplify-rtx.c
+++ b/src/gcc/simplify-rtx.c
@@ -1171,7 +1171,7 @@ simplify_unary_operation_1 (enum rtx_code code, machine_mode mode, rtx op)
          = (float_truncate:SF foo:DF).
 
          (float_truncate:DF (float_extend:XF foo:SF))
-         = (float_extend:SF foo:DF).  */
+         = (float_extend:DF foo:SF).  */
       if ((GET_CODE (op) == FLOAT_TRUNCATE
 	   && flag_unsafe_math_optimizations)
 	  || GET_CODE (op) == FLOAT_EXTEND)
@@ -1183,14 +1183,14 @@ simplify_unary_operation_1 (enum rtx_code code, machine_mode mode, rtx op)
 				   XEXP (op, 0), mode);
 
       /*  (float_truncate (float x)) is (float x)  */
-      if (GET_CODE (op) == FLOAT
+      if ((GET_CODE (op) == FLOAT || GET_CODE (op) == UNSIGNED_FLOAT)
 	  && (flag_unsafe_math_optimizations
 	      || (SCALAR_FLOAT_MODE_P (GET_MODE (op))
 		  && ((unsigned)significand_size (GET_MODE (op))
 		      >= (GET_MODE_PRECISION (GET_MODE (XEXP (op, 0)))
 			  - num_sign_bit_copies (XEXP (op, 0),
 						 GET_MODE (XEXP (op, 0))))))))
-	return simplify_gen_unary (FLOAT, mode,
+	return simplify_gen_unary (GET_CODE (op), mode,
 				   XEXP (op, 0),
 				   GET_MODE (XEXP (op, 0)));
 
@@ -1221,7 +1221,7 @@ simplify_unary_operation_1 (enum rtx_code code, machine_mode mode, rtx op)
 	  rounding can't happen.
           */
       if (GET_CODE (op) == FLOAT_EXTEND
-	  || (GET_CODE (op) == FLOAT
+	  || ((GET_CODE (op) == FLOAT || GET_CODE (op) == UNSIGNED_FLOAT)
 	      && SCALAR_FLOAT_MODE_P (GET_MODE (op))
 	      && ((unsigned)significand_size (GET_MODE (op))
 		  >= (GET_MODE_PRECISION (GET_MODE (XEXP (op, 0)))
--- a/src/gcc/stmt.c
+++ b/src/gcc/stmt.c
@@ -342,13 +342,7 @@ parse_output_constraint (const char **constraint_p, int operand_num,
 	else if (insn_extra_memory_constraint (cn))
 	  *allows_mem = true;
 	else
-	  {
-	    /* Otherwise we can't assume anything about the nature of
-	       the constraint except that it isn't purely registers.
-	       Treat it like "g" and hope for the best.  */
-	    *allows_reg = true;
-	    *allows_mem = true;
-	  }
+	  insn_extra_constraint_allows_reg_mem (cn, allows_reg, allows_mem);
 	break;
       }
 
@@ -465,13 +459,7 @@ parse_input_constraint (const char **constraint_p, int input_num,
 	else if (insn_extra_memory_constraint (cn))
 	  *allows_mem = true;
 	else
-	  {
-	    /* Otherwise we can't assume anything about the nature of
-	       the constraint except that it isn't purely registers.
-	       Treat it like "g" and hope for the best.  */
-	    *allows_reg = true;
-	    *allows_mem = true;
-	  }
+	  insn_extra_constraint_allows_reg_mem (cn, allows_reg, allows_mem);
 	break;
       }
 
--- a/src/gcc/target.def
+++ b/src/gcc/target.def
@@ -1975,7 +1975,7 @@ merging.",
 DEFHOOKPOD
 (attribute_table,
  "If defined, this target hook points to an array of @samp{struct\n\
-attribute_spec} (defined in @file{tree.h}) specifying the machine\n\
+attribute_spec} (defined in @file{tree-core.h}) specifying the machine\n\
 specific attributes for this target and some of the restrictions on the\n\
 entities to which these attributes are applied and the arguments they\n\
 take.",
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.c-torture/execute/pr65648.c
@@ -0,0 +1,34 @@
+/* PR target/65648 */
+
+int a = 0, *b = 0, c = 0;
+static int d = 0;
+short e = 1;
+static long long f = 0;
+long long *i = &f;
+unsigned char j = 0;
+
+__attribute__((noinline, noclone)) void
+foo (int x, int *y)
+{
+  asm volatile ("" : : "r" (x), "r" (y) : "memory");
+}
+
+__attribute__((noinline, noclone)) void
+bar (const char *x, long long y)
+{
+  asm volatile ("" : : "r" (x), "r" (&y) : "memory");
+  if (y != 0)
+    __builtin_abort ();
+}
+
+int
+main ()
+{
+  int k = 0;
+  b = &k;
+  j = (!a) - (c <= e);
+  *i = j;
+  foo (a, &k);
+  bar ("", f);
+  return 0;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.dg/loop-8.c
@@ -0,0 +1,24 @@
+/* { dg-do compile } */
+/* { dg-options "-O1 -fdump-rtl-loop2_invariant" } */
+
+void
+f (int *a, int *b)
+{
+  int i;
+
+  for (i = 0; i < 100; i++)
+    {
+      int d = 42;
+
+      a[i] = d;
+      if (i % 2)
+	d = i;
+      b[i] = d;
+    }
+}
+
+/* Load of 42 is moved out of the loop, introducing a new pseudo register.  */
+/* { dg-final { scan-rtl-dump-times "Decided" 1 "loop2_invariant" } } */
+/* { dg-final { scan-rtl-dump-not "without introducing a new temporary register" "loop2_invariant" } } */
+/* { dg-final { cleanup-rtl-dump "loop2_invariant" } } */
+
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.dg/loop-9.c
@@ -0,0 +1,16 @@
+/* { dg-do compile } */
+/* { dg-options "-O1 -fdump-rtl-loop2_invariant" } */
+
+void
+f (double *a)
+{
+  int i;
+  for (i = 0; i < 100; i++)
+    a[i] = 18.4242;
+}
+
+/* Load of x is moved out of the loop.  */
+/* { dg-final { scan-rtl-dump "Decided" "loop2_invariant" } } */
+/* { dg-final { scan-rtl-dump "without introducing a new temporary register" "loop2_invariant" } } */
+/* { dg-final { cleanup-rtl-dump "loop2_invariant" } } */
+
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.dg/loop-invariant.c
@@ -0,0 +1,43 @@
+/* { dg-do compile { target x86_64-*-* } } */
+/* { dg-options "-O2 -fdump-rtl-loop2_invariant" } */
+/* NOTE: The target list above could be extended to other targets that have
+         conditional moves, but don't have zero registers.  */
+
+enum test_type
+{
+  TYPE0,
+  TYPE1
+};
+
+struct type_node
+{
+  enum test_type type;
+};
+
+struct test_ref
+{
+  struct type_node *referring;
+};
+
+struct test_node
+{
+  struct test_node *next;
+};
+
+int iterate (struct test_node *, unsigned, struct test_ref **);
+
+int
+loop_invar (struct test_node *node)
+{
+  struct test_ref *ref;
+
+  for (unsigned i = 0; iterate (node, i, &ref); i++)
+    if (loop_invar ((ref->referring && ref->referring->type == TYPE0)
+                    ? ((struct test_node *) (ref->referring)) : 0))
+      return 1;
+
+  return 0;
+}
+
+/* { dg-final { scan-rtl-dump "Decided to move invariant" "loop2_invariant" } } */
+/* { dg-final { cleanup-rtl-dump "loop2_invariant" } } */
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.dg/pow-sqrt-1.c
@@ -0,0 +1,6 @@
+/* { dg-do run } */
+/* { dg-options "-O2 -ffast-math --param max-pow-sqrt-depth=5" } */
+
+#define EXPN (-6 * (0.5*0.5*0.5*0.5))
+
+#include "pow-sqrt.x"
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.dg/pow-sqrt-2.c
@@ -0,0 +1,5 @@
+/* { dg-do run } */
+/* { dg-options "-O2 -ffast-math --param max-pow-sqrt-depth=5" } */
+
+#define EXPN (-5.875)
+#include "pow-sqrt.x"
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.dg/pow-sqrt-3.c
@@ -0,0 +1,5 @@
+/* { dg-do run } */
+/* { dg-options "-O2 -ffast-math --param max-pow-sqrt-depth=3" } */
+
+#define EXPN (1.25)
+#include "pow-sqrt.x"
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.dg/pow-sqrt.x
@@ -0,0 +1,30 @@
+
+extern void abort (void);
+
+
+__attribute__((noinline)) double
+real_pow (double x, double pow_exp)
+{
+  return __builtin_pow (x, pow_exp);
+}
+
+#define EPS (0.000000000000000000001)
+
+#define SYNTH_POW(X, Y) __builtin_pow (X, Y)
+volatile double arg;
+
+int
+main (void)
+{
+  double i_arg = 0.1;
+
+  for (arg = i_arg; arg < 100.0; arg += 1.0)
+    {
+      double synth_res = SYNTH_POW (arg, EXPN);
+      double real_res = real_pow (arg, EXPN);
+
+      if (__builtin_abs (SYNTH_POW (arg, EXPN) - real_pow (arg, EXPN)) > EPS)
+	abort ();
+    }
+  return 0;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.dg/torture/pr66076.c
@@ -0,0 +1,11 @@
+/* { dg-do compile } */
+/* { dg-options "" } */
+/* { dg-options "-mno-prefer-avx128 -march=bdver4" { target i?86-*-* x86_64-*-* } } */
+
+void
+f0a (char *result, char *arg1, char *arg4, char temp_6)
+{
+  int idx = 0;
+  for (idx = 0; idx < 416; idx += 1)
+    result[idx] = (arg1[idx] + arg4[idx]) * temp_6;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.dg/tree-ssa/pr65447.c
@@ -0,0 +1,54 @@
+/* { dg-do compile } */
+/* { dg-options "-O2 -fdump-tree-ivopts-details" } */
+
+void foo (double *p)
+{
+  int i;
+  for (i = -20000; i < 200000; i+= 40)
+    {
+      p[i+0] = 1.0;
+      p[i+1] = 1.0;
+      p[i+2] = 1.0;
+      p[i+3] = 1.0;
+      p[i+4] = 1.0;
+      p[i+5] = 1.0;
+      p[i+6] = 1.0;
+      p[i+7] = 1.0;
+      p[i+8] = 1.0;
+      p[i+9] = 1.0;
+      p[i+10] = 1.0;
+      p[i+11] = 1.0;
+      p[i+12] = 1.0;
+      p[i+13] = 1.0;
+      p[i+14] = 1.0;
+      p[i+15] = 1.0;
+      p[i+16] = 1.0;
+      p[i+17] = 1.0;
+      p[i+18] = 1.0;
+      p[i+19] = 1.0;
+      p[i+20] = 1.0;
+      p[i+21] = 1.0;
+      p[i+22] = 1.0;
+      p[i+23] = 1.0;
+      p[i+24] = 1.0;
+      p[i+25] = 1.0;
+      p[i+26] = 1.0;
+      p[i+27] = 1.0;
+      p[i+28] = 1.0;
+      p[i+29] = 1.0;
+      p[i+30] = 1.0;
+      p[i+31] = 1.0;
+      p[i+32] = 1.0;
+      p[i+33] = 1.0;
+      p[i+34] = 1.0;
+      p[i+35] = 1.0;
+      p[i+36] = 1.0;
+      p[i+37] = 1.0;
+      p[i+38] = 1.0;
+      p[i+39] = 1.0;
+    }
+}
+
+/* We should groups address type IV uses.  */
+/* { dg-final { scan-tree-dump-not "\\nuse 2\\n" "ivopts" } }  */
+/* { dg-final { cleanup-tree-dump "ivopts" } }  */
--- a/src/gcc/testsuite/gcc.target/aarch64/aapcs64/func-ret-1.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/aapcs64/func-ret-1.c
@@ -12,6 +12,8 @@
 
 vf2_t vf2 = (vf2_t){ 17.f, 18.f };
 vi4_t vi4 = (vi4_t){ 0xdeadbabe, 0xbabecafe, 0xcafebeef, 0xbeefdead };
+vlf1_t vlf1 = (vlf1_t) { 17.0 };
+
 union int128_t qword;
 
 int *int_ptr = (int *)0xabcdef0123456789ULL;
@@ -41,4 +43,5 @@ FUNC_VAL_CHECK (11,   long double, 98765432123456789.987654321L, Q0, flat)
 FUNC_VAL_CHECK (12,         vf2_t,        vf2, D0, f32in64)
 FUNC_VAL_CHECK (13,         vi4_t,        vi4, Q0, i32in128)
 FUNC_VAL_CHECK (14,         int *,    int_ptr, X0, flat)
+FUNC_VAL_CHECK (15,         vlf1_t,    vlf1, Q0, flat)
 #endif
--- a/src/gcc/testsuite/gcc.target/aarch64/aapcs64/type-def.h
+++ b/src/gcc/testsuite/gcc.target/aarch64/aapcs64/type-def.h
@@ -10,6 +10,9 @@ typedef float vf4_t __attribute__((vector_size (16)));
 /* 128-bit vector of 4 ints.  */
 typedef int vi4_t __attribute__((vector_size (16)));
 
+/* 128-bit vector of 1 quad precision float.  */
+typedef long double vlf1_t __attribute__((vector_size (16)));
+
 /* signed quad-word (in an union for the convenience of initialization).  */
 union int128_t
 {
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/advsimd-intrinsics.exp
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/advsimd-intrinsics.exp
@@ -27,14 +27,26 @@ load_lib gcc-dg.exp
 
 # Initialize `dg'.
 load_lib c-torture.exp
-load_lib target-supports.exp
-load_lib torture-options.exp
 
 dg-init
 
-if {[istarget arm*-*-*]
-    && ![check_effective_target_arm_neon_ok]} then {
-  return
+# The default action for a test is 'run'.  Save current default.
+global dg-do-what-default
+set save-dg-do-what-default ${dg-do-what-default}
+
+# For ARM, make sure that we have a target compatible with NEON, and do
+# not attempt to run execution tests if the hardware doesn't support it.
+if {[istarget arm*-*-*]} then {
+    if {![check_effective_target_arm_neon_ok]} then {
+      return
+    }
+    if {![is-effective-target arm_neon_hw]} then {
+        set dg-do-what-default compile
+    } else {
+        set dg-do-what-default run
+    }
+} else {
+    set dg-do-what-default run
 }
 
 torture-init
@@ -44,22 +56,10 @@ set-torture-options $C_TORTURE_OPTIONS {{}} $LTO_TORTURE_OPTIONS
 set additional_flags [add_options_for_arm_neon ""]
 
 # Main loop.
-foreach src [lsort [glob -nocomplain $srcdir/$subdir/*.c]] {
-    # If we're only testing specific files and this isn't one of them, skip it.
-    if ![runtest_file_p $runtests $src] then {
-	continue
-    }
-
-    # runtest_file_p is already run above, and the code below can run
-    # runtest_file_p again, make sure everything for this test is
-    # performed if the above runtest_file_p decided this runtest
-    # instance should execute the test
-    gcc_parallel_test_enable 0
-    c-torture-execute $src $additional_flags
-    gcc-dg-runtest $src "" $additional_flags
-    gcc_parallel_test_enable 1
-}
+gcc-dg-runtest [lsort [glob -nocomplain $srcdir/$subdir/*.c]] \
+	       "" ${additional_flags}
 
 # All done.
+set dg-do-what-default ${save-dg-do-what-default}
 torture-finish
 dg-finish
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vqmovn.c
@@ -0,0 +1,134 @@
+#include <arm_neon.h>
+#include "arm-neon-ref.h"
+#include "compute-ref-data.h"
+
+/* Expected values of cumulative_saturation flag.  */
+int VECT_VAR(expected_cumulative_sat,int,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat,int,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat,int,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat,uint,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat,uint,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat,uint,32,2) = 0;
+
+/* Expected results.  */
+VECT_VAR_DECL(expected,int,8,8) [] = { 0x12, 0x12, 0x12, 0x12,
+				       0x12, 0x12, 0x12, 0x12 };
+VECT_VAR_DECL(expected,int,16,4) [] = { 0x1278, 0x1278, 0x1278, 0x1278 };
+VECT_VAR_DECL(expected,int,32,2) [] = { 0x12345678, 0x12345678 };
+VECT_VAR_DECL(expected,uint,8,8) [] = { 0x82, 0x82, 0x82, 0x82,
+					0x82, 0x82, 0x82, 0x82 };
+VECT_VAR_DECL(expected,uint,16,4) [] = { 0x8765, 0x8765, 0x8765, 0x8765 };
+VECT_VAR_DECL(expected,uint,32,2) [] = { 0x87654321, 0x87654321 };
+
+/* Expected values of cumulative_saturation flag when saturation occurs.  */
+int VECT_VAR(expected_cumulative_sat1,int,8,8) = 1;
+int VECT_VAR(expected_cumulative_sat1,int,16,4) = 1;
+int VECT_VAR(expected_cumulative_sat1,int,32,2) = 1;
+int VECT_VAR(expected_cumulative_sat1,uint,8,8) = 1;
+int VECT_VAR(expected_cumulative_sat1,uint,16,4) = 1;
+int VECT_VAR(expected_cumulative_sat1,uint,32,2) = 1;
+
+/* Expected results when saturation occurs.  */
+VECT_VAR_DECL(expected1,int,8,8) [] = { 0x7f, 0x7f, 0x7f, 0x7f,
+					0x7f, 0x7f, 0x7f, 0x7f };
+VECT_VAR_DECL(expected1,int,16,4) [] = { 0x7fff, 0x7fff, 0x7fff, 0x7fff };
+VECT_VAR_DECL(expected1,int,32,2) [] = { 0x7fffffff, 0x7fffffff };
+VECT_VAR_DECL(expected1,uint,8,8) [] = { 0xff, 0xff, 0xff, 0xff,
+					 0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected1,uint,16,4) [] = { 0xffff, 0xffff, 0xffff, 0xffff };
+VECT_VAR_DECL(expected1,uint,32,2) [] = { 0xffffffff, 0xffffffff };
+
+#define INSN_NAME vqmovn
+#define TEST_MSG "VQMOVN"
+
+#define FNNAME1(NAME) void exec_ ## NAME (void)
+#define FNNAME(NAME) FNNAME1(NAME)
+
+FNNAME (INSN_NAME)
+{
+  /* Basic test: y=OP(x), then store the result.  */
+#define TEST_UNARY_OP1(INSN, T1, T2, W, W2, N, EXPECTED_CUMULATIVE_SAT, CMT) \
+  Set_Neon_Cumulative_Sat(0, VECT_VAR(vector_res, T1, W, N));		\
+  VECT_VAR(vector_res, T1, W, N) =					\
+    INSN##_##T2##W2(VECT_VAR(vector, T1, W2, N));			\
+  vst1##_##T2##W(VECT_VAR(result, T1, W, N),				\
+		 VECT_VAR(vector_res, T1, W, N));			\
+  CHECK_CUMULATIVE_SAT(TEST_MSG, T1, W, N, EXPECTED_CUMULATIVE_SAT, CMT)
+
+#define TEST_UNARY_OP(INSN, T1, T2, W, W2, N, EXPECTED_CUMULATIVE_SAT, CMT) \
+  TEST_UNARY_OP1(INSN, T1, T2, W, W2, N, EXPECTED_CUMULATIVE_SAT, CMT)
+
+  /* No need for 64 bits variants.  */
+  DECL_VARIABLE(vector, int, 16, 8);
+  DECL_VARIABLE(vector, int, 32, 4);
+  DECL_VARIABLE(vector, int, 64, 2);
+  DECL_VARIABLE(vector, uint, 16, 8);
+  DECL_VARIABLE(vector, uint, 32, 4);
+  DECL_VARIABLE(vector, uint, 64, 2);
+
+  DECL_VARIABLE(vector_res, int, 8, 8);
+  DECL_VARIABLE(vector_res, int, 16, 4);
+  DECL_VARIABLE(vector_res, int, 32, 2);
+  DECL_VARIABLE(vector_res, uint, 8, 8);
+  DECL_VARIABLE(vector_res, uint, 16, 4);
+  DECL_VARIABLE(vector_res, uint, 32, 2);
+
+  clean_results ();
+
+  /* Fill input vector with arbitrary values.  */
+  VDUP(vector, q, int, s, 16, 8, 0x12);
+  VDUP(vector, q, int, s, 32, 4, 0x1278);
+  VDUP(vector, q, int, s, 64, 2, 0x12345678);
+  VDUP(vector, q, uint, u, 16, 8, 0x82);
+  VDUP(vector, q, uint, u, 32, 4, 0x8765);
+  VDUP(vector, q, uint, u, 64, 2, 0x87654321);
+
+  /* Apply a unary operator named INSN_NAME.  */
+#define CMT ""
+  TEST_UNARY_OP(INSN_NAME, int, s, 8, 16, 8, expected_cumulative_sat, CMT);
+  TEST_UNARY_OP(INSN_NAME, int, s, 16, 32, 4, expected_cumulative_sat, CMT);
+  TEST_UNARY_OP(INSN_NAME, int, s, 32, 64, 2, expected_cumulative_sat, CMT);
+  TEST_UNARY_OP(INSN_NAME, uint, u, 8, 16, 8, expected_cumulative_sat, CMT);
+  TEST_UNARY_OP(INSN_NAME, uint, u, 16, 32, 4, expected_cumulative_sat, CMT);
+  TEST_UNARY_OP(INSN_NAME, uint, u, 32, 64, 2, expected_cumulative_sat, CMT);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected, CMT);
+
+
+  /* Fill input vector with arbitrary values which cause cumulative
+     saturation.  */
+  VDUP(vector, q, int, s, 16, 8, 0x1234);
+  VDUP(vector, q, int, s, 32, 4, 0x12345678);
+  VDUP(vector, q, int, s, 64, 2, 0x1234567890ABLL);
+  VDUP(vector, q, uint, u, 16, 8, 0x8234);
+  VDUP(vector, q, uint, u, 32, 4, 0x87654321);
+  VDUP(vector, q, uint, u, 64, 2, 0x8765432187654321ULL);
+
+  /* Apply a unary operator named INSN_NAME.  */
+#undef CMT
+#define CMT " (with saturation)"
+  TEST_UNARY_OP(INSN_NAME, int, s, 8, 16, 8, expected_cumulative_sat1, CMT);
+  TEST_UNARY_OP(INSN_NAME, int, s, 16, 32, 4, expected_cumulative_sat1, CMT);
+  TEST_UNARY_OP(INSN_NAME, int, s, 32, 64, 2, expected_cumulative_sat1, CMT);
+  TEST_UNARY_OP(INSN_NAME, uint, u, 8, 16, 8, expected_cumulative_sat1, CMT);
+  TEST_UNARY_OP(INSN_NAME, uint, u, 16, 32, 4, expected_cumulative_sat1, CMT);
+  TEST_UNARY_OP(INSN_NAME, uint, u, 32, 64, 2, expected_cumulative_sat1, CMT);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected1, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected1, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected1, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected1, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected1, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected1, CMT);
+}
+
+int main (void)
+{
+  exec_vqmovn ();
+  return 0;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vqmovun.c
@@ -0,0 +1,93 @@
+#include <arm_neon.h>
+#include "arm-neon-ref.h"
+#include "compute-ref-data.h"
+
+/* Expected values of cumulative_saturation flag.  */
+int VECT_VAR(expected_cumulative_sat,uint,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat,uint,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat,uint,32,2) = 0;
+
+/* Expected results.  */
+VECT_VAR_DECL(expected,uint,8,8) [] = { 0x34, 0x34, 0x34, 0x34,
+					0x34, 0x34, 0x34, 0x34 };
+VECT_VAR_DECL(expected,uint,16,4) [] = { 0x5678, 0x5678, 0x5678, 0x5678 };
+VECT_VAR_DECL(expected,uint,32,2) [] = { 0x12345678, 0x12345678 };
+
+/* Expected values of cumulative_saturation flag with negative input.  */
+int VECT_VAR(expected_cumulative_sat_neg,uint,8,8) = 1;
+int VECT_VAR(expected_cumulative_sat_neg,uint,16,4) = 1;
+int VECT_VAR(expected_cumulative_sat_neg,uint,32,2) = 1;
+
+/* Expected results with negative input.  */
+VECT_VAR_DECL(expected_neg,uint,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					    0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_neg,uint,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_neg,uint,32,2) [] = { 0x0, 0x0 };
+
+#define INSN_NAME vqmovun
+#define TEST_MSG "VQMOVUN"
+
+#define FNNAME1(NAME) void exec_ ## NAME (void)
+#define FNNAME(NAME) FNNAME1(NAME)
+
+FNNAME (INSN_NAME)
+{
+  /* Basic test: y=OP(x), then store the result.  */
+#define TEST_UNARY_OP1(INSN, T1, T2, W, W2, N, EXPECTED_CUMULATIVE_SAT, CMT) \
+  Set_Neon_Cumulative_Sat(0, VECT_VAR(vector_res, T1, W, N));		\
+  VECT_VAR(vector_res, T1, W, N) =					\
+    INSN##_s##W2(VECT_VAR(vector, int, W2, N));				\
+  vst1##_##T2##W(VECT_VAR(result, T1, W, N),				\
+		 VECT_VAR(vector_res, T1, W, N));			\
+  CHECK_CUMULATIVE_SAT(TEST_MSG, T1, W, N, EXPECTED_CUMULATIVE_SAT, CMT)
+
+#define TEST_UNARY_OP(INSN, T1, T2, W, W2, N, EXPECTED_CUMULATIVE_SAT, CMT) \
+  TEST_UNARY_OP1(INSN, T1, T2, W, W2, N, EXPECTED_CUMULATIVE_SAT, CMT)
+
+  DECL_VARIABLE(vector, int, 16, 8);
+  DECL_VARIABLE(vector, int, 32, 4);
+  DECL_VARIABLE(vector, int, 64, 2);
+
+  DECL_VARIABLE(vector_res, uint, 8, 8);
+  DECL_VARIABLE(vector_res, uint, 16, 4);
+  DECL_VARIABLE(vector_res, uint, 32, 2);
+
+  clean_results ();
+
+  /* Fill input vector with arbitrary values.  */
+  VDUP(vector, q, int, s, 16, 8, 0x34);
+  VDUP(vector, q, int, s, 32, 4, 0x5678);
+  VDUP(vector, q, int, s, 64, 2, 0x12345678);
+
+  /* Apply a unary operator named INSN_NAME.  */
+#define CMT ""
+  TEST_UNARY_OP(INSN_NAME, uint, u, 8, 16, 8, expected_cumulative_sat, CMT);
+  TEST_UNARY_OP(INSN_NAME, uint, u, 16, 32, 4, expected_cumulative_sat, CMT);
+  TEST_UNARY_OP(INSN_NAME, uint, u, 32, 64, 2, expected_cumulative_sat, CMT);
+
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected, CMT);
+
+  /* Fill input vector with negative values.  */
+  VDUP(vector, q, int, s, 16, 8, 0x8234);
+  VDUP(vector, q, int, s, 32, 4, 0x87654321);
+  VDUP(vector, q, int, s, 64, 2, 0x8765432187654321LL);
+
+  /* Apply a unary operator named INSN_NAME.  */
+#undef CMT
+#define CMT " (negative input)"
+  TEST_UNARY_OP(INSN_NAME, uint, u, 8, 16, 8, expected_cumulative_sat_neg, CMT);
+  TEST_UNARY_OP(INSN_NAME, uint, u, 16, 32, 4, expected_cumulative_sat_neg, CMT);
+  TEST_UNARY_OP(INSN_NAME, uint, u, 32, 64, 2, expected_cumulative_sat_neg, CMT);
+
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_neg, CMT);
+}
+
+int main (void)
+{
+  exec_vqmovun ();
+  return 0;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vqrdmulh.c
@@ -0,0 +1,161 @@
+#include <arm_neon.h>
+#include "arm-neon-ref.h"
+#include "compute-ref-data.h"
+
+/* Expected values of cumulative_saturation flag.  */
+int VECT_VAR(expected_cumulative_sat,int,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat,int,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat,int,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat,int,32,4) = 0;
+
+/* Expected results.  */
+VECT_VAR_DECL(expected,int,16,4) [] = { 0xfff5, 0xfff6, 0xfff7, 0xfff7 };
+VECT_VAR_DECL(expected,int,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected,int,16,8) [] = { 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected,int,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+
+/* Expected values of cumulative_saturation flag when multiplication
+   saturates.  */
+int VECT_VAR(expected_cumulative_sat_mul,int,16,4) = 1;
+int VECT_VAR(expected_cumulative_sat_mul,int,32,2) = 1;
+int VECT_VAR(expected_cumulative_sat_mul,int,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat_mul,int,32,4) = 1;
+
+/* Expected results when multiplication saturates.  */
+VECT_VAR_DECL(expected_mul,int,16,4) [] = { 0x7fff, 0x7fff, 0x7fff, 0x7fff };
+VECT_VAR_DECL(expected_mul,int,32,2) [] = { 0x7fffffff, 0x7fffffff };
+VECT_VAR_DECL(expected_mul,int,16,8) [] = { 0x7fff, 0x7fff, 0x7fff, 0x7fff,
+					    0x7fff, 0x7fff, 0x7fff, 0x7fff };
+VECT_VAR_DECL(expected_mul,int,32,4) [] = { 0x7fffffff, 0x7fffffff,
+					    0x7fffffff, 0x7fffffff };
+
+/* Expected values of cumulative_saturation flag when rounding
+   should not cause saturation.  */
+int VECT_VAR(expected_cumulative_sat_round,int,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat_round,int,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat_round,int,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_round,int,32,4) = 0;
+
+/* Expected results when rounding should not cause saturation.  */
+VECT_VAR_DECL(expected_round,int,16,4) [] = { 0x7fff, 0x7fff, 0x7fff, 0x7fff };
+VECT_VAR_DECL(expected_round,int,32,2) [] = { 0x7fffffff, 0x7fffffff };
+VECT_VAR_DECL(expected_round,int,16,8) [] = { 0x7fff, 0x7fff, 0x7fff, 0x7fff,
+					      0x7fff, 0x7fff, 0x7fff, 0x7fff };
+VECT_VAR_DECL(expected_round,int,32,4) [] = { 0x7fffffff, 0x7fffffff,
+					      0x7fffffff, 0x7fffffff };
+
+#define INSN vqrdmulh
+#define TEST_MSG "VQRDMULH"
+
+#define FNNAME1(NAME) void exec_ ## NAME (void)
+#define FNNAME(NAME) FNNAME1(NAME)
+
+FNNAME (INSN)
+{
+  /* vector_res = vqrdmulh(vector,vector2), then store the result.  */
+#define TEST_VQRDMULH2(INSN, Q, T1, T2, W, N, EXPECTED_CUMULATIVE_SAT, CMT) \
+  Set_Neon_Cumulative_Sat(0, VECT_VAR(vector_res, T1, W, N));		\
+  VECT_VAR(vector_res, T1, W, N) =					\
+    INSN##Q##_##T2##W(VECT_VAR(vector, T1, W, N),			\
+		      VECT_VAR(vector2, T1, W, N));			\
+  vst1##Q##_##T2##W(VECT_VAR(result, T1, W, N),				\
+		    VECT_VAR(vector_res, T1, W, N));			\
+  CHECK_CUMULATIVE_SAT(TEST_MSG, T1, W, N, EXPECTED_CUMULATIVE_SAT, CMT)
+
+  /* Two auxliary macros are necessary to expand INSN */
+#define TEST_VQRDMULH1(INSN, Q, T1, T2, W, N, EXPECTED_CUMULATIVE_SAT, CMT) \
+  TEST_VQRDMULH2(INSN, Q, T1, T2, W, N, EXPECTED_CUMULATIVE_SAT, CMT)
+
+#define TEST_VQRDMULH(Q, T1, T2, W, N, EXPECTED_CUMULATIVE_SAT, CMT)	\
+  TEST_VQRDMULH1(INSN, Q, T1, T2, W, N, EXPECTED_CUMULATIVE_SAT, CMT)
+
+
+  DECL_VARIABLE(vector, int, 16, 4);
+  DECL_VARIABLE(vector, int, 32, 2);
+  DECL_VARIABLE(vector, int, 16, 8);
+  DECL_VARIABLE(vector, int, 32, 4);
+
+  DECL_VARIABLE(vector_res, int, 16, 4);
+  DECL_VARIABLE(vector_res, int, 32, 2);
+  DECL_VARIABLE(vector_res, int, 16, 8);
+  DECL_VARIABLE(vector_res, int, 32, 4);
+
+  DECL_VARIABLE(vector2, int, 16, 4);
+  DECL_VARIABLE(vector2, int, 32, 2);
+  DECL_VARIABLE(vector2, int, 16, 8);
+  DECL_VARIABLE(vector2, int, 32, 4);
+
+  clean_results ();
+
+  VLOAD(vector, buffer, , int, s, 16, 4);
+  VLOAD(vector, buffer, , int, s, 32, 2);
+  VLOAD(vector, buffer, q, int, s, 16, 8);
+  VLOAD(vector, buffer, q, int, s, 32, 4);
+
+  /* Initialize vector2.  */
+  VDUP(vector2, , int, s, 16, 4, 0x5555);
+  VDUP(vector2, , int, s, 32, 2, 0xBB);
+  VDUP(vector2, q, int, s, 16, 8, 0x33);
+  VDUP(vector2, q, int, s, 32, 4, 0x22);
+
+#define CMT ""
+  TEST_VQRDMULH(, int, s, 16, 4, expected_cumulative_sat, CMT);
+  TEST_VQRDMULH(, int, s, 32, 2, expected_cumulative_sat, CMT);
+  TEST_VQRDMULH(q, int, s, 16, 8, expected_cumulative_sat, CMT);
+  TEST_VQRDMULH(q, int, s, 32, 4, expected_cumulative_sat, CMT);
+
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected, CMT);
+
+  /* Now use input values such that the multiplication causes
+     saturation.  */
+#define TEST_MSG_MUL " (check mul cumulative saturation)"
+  VDUP(vector, , int, s, 16, 4, 0x8000);
+  VDUP(vector, , int, s, 32, 2, 0x80000000);
+  VDUP(vector, q, int, s, 16, 8, 0x8000);
+  VDUP(vector, q, int, s, 32, 4, 0x80000000);
+  VDUP(vector2, , int, s, 16, 4, 0x8000);
+  VDUP(vector2, , int, s, 32, 2, 0x80000000);
+  VDUP(vector2, q, int, s, 16, 8, 0x8000);
+  VDUP(vector2, q, int, s, 32, 4, 0x80000000);
+
+  TEST_VQRDMULH(, int, s, 16, 4, expected_cumulative_sat_mul, TEST_MSG_MUL);
+  TEST_VQRDMULH(, int, s, 32, 2, expected_cumulative_sat_mul, TEST_MSG_MUL);
+  TEST_VQRDMULH(q, int, s, 16, 8, expected_cumulative_sat_mul, TEST_MSG_MUL);
+  TEST_VQRDMULH(q, int, s, 32, 4, expected_cumulative_sat_mul, TEST_MSG_MUL);
+
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_mul, TEST_MSG_MUL);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_mul, TEST_MSG_MUL);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_mul, TEST_MSG_MUL);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_mul, TEST_MSG_MUL);
+
+  /* Use input values where rounding produces a result equal to the
+     saturation value, but does not set the saturation flag.  */
+#define TEST_MSG_ROUND " (check rounding)"
+  VDUP(vector, , int, s, 16, 4, 0x8000);
+  VDUP(vector, , int, s, 32, 2, 0x80000000);
+  VDUP(vector, q, int, s, 16, 8, 0x8000);
+  VDUP(vector, q, int, s, 32, 4, 0x80000000);
+  VDUP(vector2, , int, s, 16, 4, 0x8001);
+  VDUP(vector2, , int, s, 32, 2, 0x80000001);
+  VDUP(vector2, q, int, s, 16, 8, 0x8001);
+  VDUP(vector2, q, int, s, 32, 4, 0x80000001);
+
+  TEST_VQRDMULH(, int, s, 16, 4, expected_cumulative_sat_round, TEST_MSG_ROUND);
+  TEST_VQRDMULH(, int, s, 32, 2, expected_cumulative_sat_round, TEST_MSG_ROUND);
+  TEST_VQRDMULH(q, int, s, 16, 8, expected_cumulative_sat_round, TEST_MSG_ROUND);
+  TEST_VQRDMULH(q, int, s, 32, 4, expected_cumulative_sat_round, TEST_MSG_ROUND);
+
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_round, TEST_MSG_ROUND);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_round, TEST_MSG_ROUND);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_round, TEST_MSG_ROUND);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_round, TEST_MSG_ROUND);
+}
+
+int main (void)
+{
+  exec_vqrdmulh ();
+  return 0;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vqrdmulh_lane.c
@@ -0,0 +1,169 @@
+#include <arm_neon.h>
+#include "arm-neon-ref.h"
+#include "compute-ref-data.h"
+
+/* Expected values of cumulative_saturation flag.  */
+int VECT_VAR(expected_cumulative_sat,int,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat,int,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat,int,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat,int,32,4) = 0;
+
+/* Expected results.  */
+VECT_VAR_DECL(expected,int,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected,int,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected,int,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected,int,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+
+/* Expected values of cumulative_saturation flag when multiplication
+   saturates.  */
+int VECT_VAR(expected_cumulative_sat_mul,int,16,4) = 1;
+int VECT_VAR(expected_cumulative_sat_mul,int,32,2) = 1;
+int VECT_VAR(expected_cumulative_sat_mul,int,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat_mul,int,32,4) = 1;
+
+/* Expected results when multiplication saturates.  */
+VECT_VAR_DECL(expected_mul,int,16,4) [] = { 0x7fff, 0x7fff, 0x7fff, 0x7fff };
+VECT_VAR_DECL(expected_mul,int,32,2) [] = { 0x7fffffff, 0x7fffffff };
+VECT_VAR_DECL(expected_mul,int,16,8) [] = { 0x7fff, 0x7fff, 0x7fff, 0x7fff,
+					    0x7fff, 0x7fff, 0x7fff, 0x7fff };
+VECT_VAR_DECL(expected_mul,int,32,4) [] = { 0x7fffffff, 0x7fffffff,
+					    0x7fffffff, 0x7fffffff };
+
+/* Expected values of cumulative_saturation flag when rounding
+   should not cause saturation.  */
+int VECT_VAR(expected_cumulative_sat_round,int,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat_round,int,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat_round,int,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_round,int,32,4) = 0;
+
+/* Expected results when rounding should not cause saturation.  */
+VECT_VAR_DECL(expected_round,int,16,4) [] = { 0x7fff, 0x7fff, 0x7fff, 0x7fff };
+VECT_VAR_DECL(expected_round,int,32,2) [] = { 0x7fffffff, 0x7fffffff };
+VECT_VAR_DECL(expected_round,int,16,8) [] = { 0x7fff, 0x7fff, 0x7fff, 0x7fff,
+					      0x7fff, 0x7fff, 0x7fff, 0x7fff };
+VECT_VAR_DECL(expected_round,int,32,4) [] = { 0x7fffffff, 0x7fffffff,
+					      0x7fffffff, 0x7fffffff };
+
+#define INSN vqrdmulh
+#define TEST_MSG "VQRDMULH_LANE"
+
+#define FNNAME1(NAME) void exec_ ## NAME ## _lane (void)
+#define FNNAME(NAME) FNNAME1(NAME)
+
+FNNAME (INSN)
+{
+  /* vector_res = vqrdmulh_lane(vector,vector2,lane), then store the result.  */
+#define TEST_VQRDMULH_LANE2(INSN, Q, T1, T2, W, N, N2, L, EXPECTED_CUMULATIVE_SAT, CMT) \
+  Set_Neon_Cumulative_Sat(0, VECT_VAR(vector_res, T1, W, N));		\
+  VECT_VAR(vector_res, T1, W, N) =					\
+    INSN##Q##_lane_##T2##W(VECT_VAR(vector, T1, W, N),			\
+			   VECT_VAR(vector2, T1, W, N2),		\
+			   L);						\
+  vst1##Q##_##T2##W(VECT_VAR(result, T1, W, N),				\
+		    VECT_VAR(vector_res, T1, W, N));			\
+  CHECK_CUMULATIVE_SAT(TEST_MSG, T1, W, N, EXPECTED_CUMULATIVE_SAT, CMT)
+
+  /* Two auxliary macros are necessary to expand INSN */
+#define TEST_VQRDMULH_LANE1(INSN, Q, T1, T2, W, N, N2, L, EXPECTED_CUMULATIVE_SAT, CMT) \
+  TEST_VQRDMULH_LANE2(INSN, Q, T1, T2, W, N, N2, L, EXPECTED_CUMULATIVE_SAT, CMT)
+
+#define TEST_VQRDMULH_LANE(Q, T1, T2, W, N, N2, L, EXPECTED_CUMULATIVE_SAT, CMT) \
+  TEST_VQRDMULH_LANE1(INSN, Q, T1, T2, W, N, N2, L, EXPECTED_CUMULATIVE_SAT, CMT)
+
+
+  DECL_VARIABLE(vector, int, 16, 4);
+  DECL_VARIABLE(vector, int, 32, 2);
+  DECL_VARIABLE(vector, int, 16, 8);
+  DECL_VARIABLE(vector, int, 32, 4);
+
+  DECL_VARIABLE(vector_res, int, 16, 4);
+  DECL_VARIABLE(vector_res, int, 32, 2);
+  DECL_VARIABLE(vector_res, int, 16, 8);
+  DECL_VARIABLE(vector_res, int, 32, 4);
+
+  /* vector2: vqrdmulh_lane and vqrdmulhq_lane have a 2nd argument with
+     the same number of elements, so we need only one variable of each
+     type.  */
+  DECL_VARIABLE(vector2, int, 16, 4);
+  DECL_VARIABLE(vector2, int, 32, 2);
+
+  clean_results ();
+
+  VLOAD(vector, buffer, , int, s, 16, 4);
+  VLOAD(vector, buffer, , int, s, 32, 2);
+
+  VLOAD(vector, buffer, q, int, s, 16, 8);
+  VLOAD(vector, buffer, q, int, s, 32, 4);
+
+  /* Initialize vector2.  */
+  VDUP(vector2, , int, s, 16, 4, 0x55);
+  VDUP(vector2, , int, s, 32, 2, 0xBB);
+
+  /* Choose lane arbitrarily.  */
+#define CMT ""
+  TEST_VQRDMULH_LANE(, int, s, 16, 4, 4, 2, expected_cumulative_sat, CMT);
+  TEST_VQRDMULH_LANE(, int, s, 32, 2, 2, 1, expected_cumulative_sat, CMT);
+  TEST_VQRDMULH_LANE(q, int, s, 16, 8, 4, 3, expected_cumulative_sat, CMT);
+  TEST_VQRDMULH_LANE(q, int, s, 32, 4, 2, 0, expected_cumulative_sat, CMT);
+
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected, CMT);
+
+  /* Now use input values such that the multiplication causes
+     saturation.  */
+#define TEST_MSG_MUL " (check mul cumulative saturation)"
+  VDUP(vector, , int, s, 16, 4, 0x8000);
+  VDUP(vector, , int, s, 32, 2, 0x80000000);
+  VDUP(vector, q, int, s, 16, 8, 0x8000);
+  VDUP(vector, q, int, s, 32, 4, 0x80000000);
+  VDUP(vector2, , int, s, 16, 4, 0x8000);
+  VDUP(vector2, , int, s, 32, 2, 0x80000000);
+
+  TEST_VQRDMULH_LANE(, int, s, 16, 4, 4, 2, expected_cumulative_sat_mul,
+		     TEST_MSG_MUL);
+  TEST_VQRDMULH_LANE(, int, s, 32, 2, 2, 1, expected_cumulative_sat_mul,
+		     TEST_MSG_MUL);
+  TEST_VQRDMULH_LANE(q, int, s, 16, 8, 4, 3, expected_cumulative_sat_mul,
+		     TEST_MSG_MUL);
+  TEST_VQRDMULH_LANE(q, int, s, 32, 4, 2, 0, expected_cumulative_sat_mul,
+		     TEST_MSG_MUL);
+
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_mul, TEST_MSG_MUL);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_mul, TEST_MSG_MUL);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_mul, TEST_MSG_MUL);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_mul, TEST_MSG_MUL);
+
+  VDUP(vector, , int, s, 16, 4, 0x8000);
+  VDUP(vector, , int, s, 32, 2, 0x80000000);
+  VDUP(vector, q, int, s, 16, 8, 0x8000);
+  VDUP(vector, q, int, s, 32, 4, 0x80000000);
+  VDUP(vector2, , int, s, 16, 4, 0x8001);
+  VDUP(vector2, , int, s, 32, 2, 0x80000001);
+
+  /* Use input values where rounding produces a result equal to the
+     saturation value, but does not set the saturation flag.  */
+#define TEST_MSG_ROUND " (check rounding)"
+  TEST_VQRDMULH_LANE(, int, s, 16, 4, 4, 2, expected_cumulative_sat_round,
+		     TEST_MSG_ROUND);
+  TEST_VQRDMULH_LANE(, int, s, 32, 2, 2, 1, expected_cumulative_sat_round,
+		     TEST_MSG_ROUND);
+  TEST_VQRDMULH_LANE(q, int, s, 16, 8, 4, 3, expected_cumulative_sat_round,
+		     TEST_MSG_ROUND);
+  TEST_VQRDMULH_LANE(q, int, s, 32, 4, 2, 0, expected_cumulative_sat_round,
+		     TEST_MSG_ROUND);
+
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_round, TEST_MSG_ROUND);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_round, TEST_MSG_ROUND);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_round, TEST_MSG_ROUND);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_round, TEST_MSG_ROUND);
+}
+
+int main (void)
+{
+  exec_vqrdmulh_lane ();
+  return 0;
+}
+
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vqrdmulh_n.c
@@ -0,0 +1,155 @@
+#include <arm_neon.h>
+#include "arm-neon-ref.h"
+#include "compute-ref-data.h"
+
+/* Expected values of cumulative_saturation flag.  */
+int VECT_VAR(expected_cumulative_sat,int,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat,int,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat,int,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat,int,32,4) = 0;
+
+/* Expected results.  */
+VECT_VAR_DECL(expected,int,16,4) [] = { 0xfffc, 0xfffc, 0xfffc, 0xfffd };
+VECT_VAR_DECL(expected,int,32,2) [] = { 0xfffffffe, 0xfffffffe };
+VECT_VAR_DECL(expected,int,16,8) [] = { 0x6, 0x6, 0x6, 0x5,
+					0x5, 0x4, 0x4, 0x4 };
+VECT_VAR_DECL(expected,int,32,4) [] = { 0xfffffffe, 0xfffffffe,
+					0xfffffffe, 0xfffffffe };
+
+/* Expected values of cumulative_saturation flag when multiplication
+   saturates.  */
+int VECT_VAR(expected_cumulative_sat_mul,int,16,4) = 1;
+int VECT_VAR(expected_cumulative_sat_mul,int,32,2) = 1;
+int VECT_VAR(expected_cumulative_sat_mul,int,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat_mul,int,32,4) = 1;
+
+/* Expected results when multiplication saturates.  */
+VECT_VAR_DECL(expected_mul,int,16,4) [] = { 0x7fff, 0x7fff, 0x7fff, 0x7fff };
+VECT_VAR_DECL(expected_mul,int,32,2) [] = { 0x7fffffff, 0x7fffffff };
+VECT_VAR_DECL(expected_mul,int,16,8) [] = { 0x7fff, 0x7fff, 0x7fff, 0x7fff,
+					    0x7fff, 0x7fff, 0x7fff, 0x7fff };
+VECT_VAR_DECL(expected_mul,int,32,4) [] = { 0x7fffffff, 0x7fffffff,
+					    0x7fffffff, 0x7fffffff };
+
+/* Expected values of cumulative_saturation flag when rounding
+   should not cause saturation.  */
+int VECT_VAR(expected_cumulative_sat_round,int,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat_round,int,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat_round,int,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_round,int,32,4) = 0;
+
+/* Expected results when rounding should not cause saturation.  */
+VECT_VAR_DECL(expected_round,int,16,4) [] = { 0x7fff, 0x7fff, 0x7fff, 0x7fff };
+VECT_VAR_DECL(expected_round,int,32,2) [] = { 0x7fffffff, 0x7fffffff };
+VECT_VAR_DECL(expected_round,int,16,8) [] = { 0x7fff, 0x7fff, 0x7fff, 0x7fff,
+					      0x7fff, 0x7fff, 0x7fff, 0x7fff };
+VECT_VAR_DECL(expected_round,int,32,4) [] = { 0x7fffffff, 0x7fffffff,
+					      0x7fffffff, 0x7fffffff };
+
+#define INSN vqrdmulh
+#define TEST_MSG "VQRDMULH_N"
+
+#define FNNAME1(NAME) void exec_ ## NAME ## _n (void)
+#define FNNAME(NAME) FNNAME1(NAME)
+
+FNNAME (INSN)
+{
+  int i;
+
+  /* vector_res = vqrdmulh_n(vector,val), then store the result.  */
+#define TEST_VQRDMULH_N2(INSN, Q, T1, T2, W, N, L, EXPECTED_CUMULATIVE_SAT, CMT) \
+  Set_Neon_Cumulative_Sat(0, VECT_VAR(vector_res, T1, W, N));		\
+  VECT_VAR(vector_res, T1, W, N) =					\
+    INSN##Q##_n_##T2##W(VECT_VAR(vector, T1, W, N),			\
+			L);						\
+  vst1##Q##_##T2##W(VECT_VAR(result, T1, W, N),				\
+		    VECT_VAR(vector_res, T1, W, N));			\
+  CHECK_CUMULATIVE_SAT(TEST_MSG, T1, W, N, EXPECTED_CUMULATIVE_SAT, CMT)
+
+  /* Two auxliary macros are necessary to expand INSN */
+#define TEST_VQRDMULH_N1(INSN, Q, T1, T2, W, N, L, EXPECTED_CUMULATIVE_SAT, CMT) \
+  TEST_VQRDMULH_N2(INSN, Q, T1, T2, W, N, L, EXPECTED_CUMULATIVE_SAT, CMT)
+
+#define TEST_VQRDMULH_N(Q, T1, T2, W, N, L, EXPECTED_CUMULATIVE_SAT, CMT) \
+  TEST_VQRDMULH_N1(INSN, Q, T1, T2, W, N, L, EXPECTED_CUMULATIVE_SAT, CMT)
+
+
+  DECL_VARIABLE(vector, int, 16, 4);
+  DECL_VARIABLE(vector, int, 32, 2);
+  DECL_VARIABLE(vector, int, 16, 8);
+  DECL_VARIABLE(vector, int, 32, 4);
+
+  DECL_VARIABLE(vector_res, int, 16, 4);
+  DECL_VARIABLE(vector_res, int, 32, 2);
+  DECL_VARIABLE(vector_res, int, 16, 8);
+  DECL_VARIABLE(vector_res, int, 32, 4);
+
+  clean_results ();
+
+  VLOAD(vector, buffer, , int, s, 16, 4);
+  VLOAD(vector, buffer, , int, s, 32, 2);
+  VLOAD(vector, buffer, q, int, s, 16, 8);
+  VLOAD(vector, buffer, q, int, s, 32, 4);
+
+  /* Choose multiplier arbitrarily.  */
+#define CMT ""
+  TEST_VQRDMULH_N(, int, s, 16, 4, 0x2233, expected_cumulative_sat, CMT);
+  TEST_VQRDMULH_N(, int, s, 32, 2, 0x12345678, expected_cumulative_sat, CMT);
+  TEST_VQRDMULH_N(q, int, s, 16, 8, 0xCD12, expected_cumulative_sat, CMT);
+  TEST_VQRDMULH_N(q, int, s, 32, 4, 0xFA23456, expected_cumulative_sat, CMT);
+
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected, CMT);
+
+  /* Now use input values such that the multiplication causes
+     saturation.  */
+#define TEST_MSG_MUL " (check mul cumulative saturation)"
+  VDUP(vector, , int, s, 16, 4, 0x8000);
+  VDUP(vector, , int, s, 32, 2, 0x80000000);
+  VDUP(vector, q, int, s, 16, 8, 0x8000);
+  VDUP(vector, q, int, s, 32, 4, 0x80000000);
+
+  TEST_VQRDMULH_N(, int, s, 16, 4, 0x8000, expected_cumulative_sat_mul,
+		  TEST_MSG_MUL);
+  TEST_VQRDMULH_N(, int, s, 32, 2, 0x80000000, expected_cumulative_sat_mul,
+		  TEST_MSG_MUL);
+  TEST_VQRDMULH_N(q, int, s, 16, 8, 0x8000, expected_cumulative_sat_mul,
+		  TEST_MSG_MUL);
+  TEST_VQRDMULH_N(q, int, s, 32, 4, 0x80000000, expected_cumulative_sat_mul,
+		  TEST_MSG_MUL);
+
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_mul, TEST_MSG_MUL);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_mul, TEST_MSG_MUL);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_mul, TEST_MSG_MUL);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_mul, TEST_MSG_MUL);
+
+  /* Use input values where rounding produces a result equal to the
+     saturation value, but does not set the saturation flag.  */
+#define TEST_MSG_ROUND " (check rounding)"
+  VDUP(vector, , int, s, 16, 4, 0x8000);
+  VDUP(vector, , int, s, 32, 2, 0x80000000);
+  VDUP(vector, q, int, s, 16, 8, 0x8000);
+  VDUP(vector, q, int, s, 32, 4, 0x80000000);
+
+  TEST_VQRDMULH_N(, int, s, 16, 4, 0x8001, expected_cumulative_sat_round,
+		  TEST_MSG_ROUND);
+  TEST_VQRDMULH_N(, int, s, 32, 2, 0x80000001, expected_cumulative_sat_round,
+		  TEST_MSG_ROUND);
+  TEST_VQRDMULH_N(q, int, s, 16, 8, 0x8001, expected_cumulative_sat_round,
+		  TEST_MSG_ROUND);
+  TEST_VQRDMULH_N(q, int, s, 32, 4, 0x80000001, expected_cumulative_sat_round,
+		  TEST_MSG_ROUND);
+
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_round, TEST_MSG_ROUND);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_round, TEST_MSG_ROUND);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_round, TEST_MSG_ROUND);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_round, TEST_MSG_ROUND);
+}
+
+int main (void)
+{
+  exec_vqrdmulh_n ();
+  return 0;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vqrshl.c
@@ -0,0 +1,1090 @@
+#include <arm_neon.h>
+#include "arm-neon-ref.h"
+#include "compute-ref-data.h"
+
+/* Expected values of cumulative_saturation flag with input=0.  */
+int VECT_VAR(expected_cumulative_sat_0,int,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat_0,int,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat_0,int,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat_0,int,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat_0,uint,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat_0,uint,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat_0,uint,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat_0,uint,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat_0,int,8,16) = 0;
+int VECT_VAR(expected_cumulative_sat_0,int,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_0,int,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat_0,int,64,2) = 0;
+int VECT_VAR(expected_cumulative_sat_0,uint,8,16) = 0;
+int VECT_VAR(expected_cumulative_sat_0,uint,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_0,uint,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat_0,uint,64,2) = 0;
+
+/* Expected results with input=0.  */
+VECT_VAR_DECL(expected_0,int,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,int,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,int,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,int,64,1) [] = { 0x0 };
+VECT_VAR_DECL(expected_0,uint,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					  0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,uint,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,uint,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,uint,64,1) [] = { 0x0 };
+VECT_VAR_DECL(expected_0,int,8,16) [] = { 0x0, 0x0, 0x0, 0x0,
+					  0x0, 0x0, 0x0, 0x0,
+					  0x0, 0x0, 0x0, 0x0,
+					  0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,int,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					  0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,int,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,int,64,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,uint,8,16) [] = { 0x0, 0x0, 0x0, 0x0,
+					   0x0, 0x0, 0x0, 0x0,
+					   0x0, 0x0, 0x0, 0x0,
+					   0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,uint,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					   0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,uint,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,uint,64,2) [] = { 0x0, 0x0 };
+
+/* Expected values of cumulative_saturation flag with input=0 and
+   negative shift amount.  */
+int VECT_VAR(expected_cumulative_sat_0_neg,int,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,int,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,int,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,int,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,uint,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,uint,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,uint,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,uint,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,int,8,16) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,int,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,int,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,int,64,2) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,uint,8,16) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,uint,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,uint,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,uint,64,2) = 0;
+
+/* Expected results with input=0 and negative shift amount.  */
+VECT_VAR_DECL(expected_0_neg,int,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					     0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_neg,int,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_neg,int,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_neg,int,64,1) [] = { 0x0 };
+VECT_VAR_DECL(expected_0_neg,uint,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					      0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_neg,uint,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_neg,uint,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_neg,uint,64,1) [] = { 0x0 };
+VECT_VAR_DECL(expected_0_neg,int,8,16) [] = { 0x0, 0x0, 0x0, 0x0,
+					      0x0, 0x0, 0x0, 0x0,
+					      0x0, 0x0, 0x0, 0x0,
+					      0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_neg,int,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					      0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_neg,int,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_neg,int,64,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_neg,uint,8,16) [] = { 0x0, 0x0, 0x0, 0x0,
+					       0x0, 0x0, 0x0, 0x0,
+					       0x0, 0x0, 0x0, 0x0,
+					       0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_neg,uint,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					       0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_neg,uint,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_neg,uint,64,2) [] = { 0x0, 0x0 };
+
+/* Expected values of cumulative_saturation flag.  */
+int VECT_VAR(expected_cumulative_sat,int,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat,int,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat,int,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat,int,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat,uint,8,8) = 1;
+int VECT_VAR(expected_cumulative_sat,uint,16,4) = 1;
+int VECT_VAR(expected_cumulative_sat,uint,32,2) = 1;
+int VECT_VAR(expected_cumulative_sat,uint,64,1) = 1;
+int VECT_VAR(expected_cumulative_sat,int,8,16) = 1;
+int VECT_VAR(expected_cumulative_sat,int,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat,int,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat,int,64,2) = 1;
+int VECT_VAR(expected_cumulative_sat,uint,8,16) = 1;
+int VECT_VAR(expected_cumulative_sat,uint,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat,uint,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat,uint,64,2) = 1;
+
+/* Expected results.  */
+VECT_VAR_DECL(expected,int,8,8) [] = { 0xe0, 0xe2, 0xe4, 0xe6,
+				       0xe8, 0xea, 0xec, 0xee };
+VECT_VAR_DECL(expected,int,16,4) [] = { 0xff80, 0xff88, 0xff90, 0xff98 };
+VECT_VAR_DECL(expected,int,32,2) [] = { 0xfffff000, 0xfffff100 };
+VECT_VAR_DECL(expected,int,64,1) [] = { 0xffffffffffffff80 };
+VECT_VAR_DECL(expected,uint,8,8) [] = { 0xff, 0xff, 0xff, 0xff,
+					0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected,uint,16,4) [] = { 0xffff, 0xffff, 0xffff, 0xffff };
+VECT_VAR_DECL(expected,uint,32,2) [] = { 0xffffffff, 0xffffffff };
+VECT_VAR_DECL(expected,uint,64,1) [] = { 0xffffffffffffffff };
+VECT_VAR_DECL(expected,int,8,16) [] = { 0x80, 0x80, 0x80, 0x80,
+					0x80, 0x80, 0x80, 0x80,
+					0x80, 0x80, 0x80, 0x80,
+					0x80, 0x80, 0x80, 0x80 };
+VECT_VAR_DECL(expected,int,16,8) [] = { 0x8000, 0x8000, 0x8000, 0x8000,
+					0x8000, 0x8000, 0x8000, 0x8000 };
+VECT_VAR_DECL(expected,int,32,4) [] = { 0x80000000, 0x80000000,
+					0x80000000, 0x80000000 };
+VECT_VAR_DECL(expected,int,64,2) [] = { 0x8000000000000000, 0x8000000000000000 };
+VECT_VAR_DECL(expected,uint,8,16) [] = { 0xff, 0xff, 0xff, 0xff,
+					 0xff, 0xff, 0xff, 0xff,
+					 0xff, 0xff, 0xff, 0xff,
+					 0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected,uint,16,8) [] = { 0xffff, 0xffff, 0xffff, 0xffff,
+					 0xffff, 0xffff, 0xffff, 0xffff };
+VECT_VAR_DECL(expected,uint,32,4) [] = { 0xffffffff, 0xffffffff,
+					 0xffffffff, 0xffffffff };
+VECT_VAR_DECL(expected,uint,64,2) [] = { 0xffffffffffffffff,
+					 0xffffffffffffffff };
+
+/* Expected values of cumulative_saturation flag with negative shift
+   amount.  */
+int VECT_VAR(expected_cumulative_sat_neg,int,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,int,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,int,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,int,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,uint,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,uint,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,uint,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,uint,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,int,8,16) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,int,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,int,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,int,64,2) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,uint,8,16) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,uint,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,uint,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,uint,64,2) = 0;
+
+/* Expected results with negative shift amount.  */
+VECT_VAR_DECL(expected_neg,int,8,8) [] = { 0xfc, 0xfc, 0xfd, 0xfd,
+					   0xfd, 0xfd, 0xfe, 0xfe };
+VECT_VAR_DECL(expected_neg,int,16,4) [] = { 0xfffc, 0xfffc, 0xfffd, 0xfffd };
+VECT_VAR_DECL(expected_neg,int,32,2) [] = { 0xfffffffe, 0xfffffffe };
+VECT_VAR_DECL(expected_neg,int,64,1) [] = { 0xffffffffffffffff };
+VECT_VAR_DECL(expected_neg,uint,8,8) [] = { 0x3c, 0x3c, 0x3d, 0x3d,
+					    0x3d, 0x3d, 0x3e, 0x3e };
+VECT_VAR_DECL(expected_neg,uint,16,4) [] = { 0x3ffc, 0x3ffc, 0x3ffd, 0x3ffd };
+VECT_VAR_DECL(expected_neg,uint,32,2) [] = { 0x1ffffffe, 0x1ffffffe };
+VECT_VAR_DECL(expected_neg,uint,64,1) [] = { 0xfffffffffffffff };
+VECT_VAR_DECL(expected_neg,int,8,16) [] = { 0x0, 0x0, 0x0, 0x0,
+					    0x0, 0x0, 0x0, 0x0,
+					    0x0, 0x0, 0x0, 0x0,
+					    0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_neg,int,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					    0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_neg,int,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_neg,int,64,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_neg,uint,8,16) [] = { 0x2, 0x2, 0x2, 0x2,
+					     0x2, 0x2, 0x2, 0x2,
+					     0x2, 0x2, 0x2, 0x2,
+					     0x2, 0x2, 0x2, 0x2 };
+VECT_VAR_DECL(expected_neg,uint,16,8) [] = { 0x20, 0x20, 0x20, 0x20,
+					     0x20, 0x20, 0x20, 0x20 };
+VECT_VAR_DECL(expected_neg,uint,32,4) [] = { 0x80000, 0x80000,
+					     0x80000, 0x80000 };
+VECT_VAR_DECL(expected_neg,uint,64,2) [] = { 0x100000000000, 0x100000000000 };
+
+/* Expected values of cumulative_saturation flag with input=max and
+   shift by -1.  */
+int VECT_VAR(expected_cumulative_sat_minus1,int,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat_minus1,int,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat_minus1,int,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat_minus1,int,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat_minus1,uint,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat_minus1,uint,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat_minus1,uint,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat_minus1,uint,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat_minus1,int,8,16) = 0;
+int VECT_VAR(expected_cumulative_sat_minus1,int,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_minus1,int,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat_minus1,int,64,2) = 0;
+int VECT_VAR(expected_cumulative_sat_minus1,uint,8,16) = 0;
+int VECT_VAR(expected_cumulative_sat_minus1,uint,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_minus1,uint,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat_minus1,uint,64,2) = 0;
+
+/* Expected results with input=max and shift by -1.  */
+VECT_VAR_DECL(expected_minus1,int,8,8) [] = { 0x40, 0x40, 0x40, 0x40,
+					      0x40, 0x40, 0x40, 0x40 };
+VECT_VAR_DECL(expected_minus1,int,16,4) [] = { 0x4000, 0x4000, 0x4000, 0x4000 };
+VECT_VAR_DECL(expected_minus1,int,32,2) [] = { 0x40000000, 0x40000000 };
+VECT_VAR_DECL(expected_minus1,int,64,1) [] = { 0x4000000000000000 };
+VECT_VAR_DECL(expected_minus1,uint,8,8) [] = { 0x80, 0x80, 0x80, 0x80,
+					       0x80, 0x80, 0x80, 0x80 };
+VECT_VAR_DECL(expected_minus1,uint,16,4) [] = { 0x8000, 0x8000, 0x8000, 0x8000 };
+VECT_VAR_DECL(expected_minus1,uint,32,2) [] = { 0x80000000, 0x80000000 };
+VECT_VAR_DECL(expected_minus1,uint,64,1) [] = { 0x8000000000000000 };
+VECT_VAR_DECL(expected_minus1,int,8,16) [] = { 0x40, 0x40, 0x40, 0x40,
+					       0x40, 0x40, 0x40, 0x40,
+					       0x40, 0x40, 0x40, 0x40,
+					       0x40, 0x40, 0x40, 0x40 };
+VECT_VAR_DECL(expected_minus1,int,16,8) [] = { 0x4000, 0x4000, 0x4000, 0x4000,
+					       0x4000, 0x4000, 0x4000, 0x4000 };
+VECT_VAR_DECL(expected_minus1,int,32,4) [] = { 0x40000000, 0x40000000,
+					       0x40000000, 0x40000000 };
+VECT_VAR_DECL(expected_minus1,int,64,2) [] = { 0x4000000000000000,
+					       0x4000000000000000 };
+VECT_VAR_DECL(expected_minus1,uint,8,16) [] = { 0x80, 0x80, 0x80, 0x80,
+						0x80, 0x80, 0x80, 0x80,
+						0x80, 0x80, 0x80, 0x80,
+						0x80, 0x80, 0x80, 0x80 };
+VECT_VAR_DECL(expected_minus1,uint,16,8) [] = { 0x8000, 0x8000, 0x8000, 0x8000,
+						0x8000, 0x8000, 0x8000, 0x8000 };
+VECT_VAR_DECL(expected_minus1,uint,32,4) [] = { 0x80000000, 0x80000000,
+						0x80000000, 0x80000000 };
+VECT_VAR_DECL(expected_minus1,uint,64,2) [] = { 0x8000000000000000,
+						0x8000000000000000 };
+
+/* Expected values of cumulative_saturation flag with input=max and
+   shift by -3.  */
+int VECT_VAR(expected_cumulative_sat_minus3,int,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat_minus3,int,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat_minus3,int,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat_minus3,int,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat_minus3,uint,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat_minus3,uint,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat_minus3,uint,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat_minus3,uint,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat_minus3,int,8,16) = 0;
+int VECT_VAR(expected_cumulative_sat_minus3,int,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_minus3,int,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat_minus3,int,64,2) = 0;
+int VECT_VAR(expected_cumulative_sat_minus3,uint,8,16) = 0;
+int VECT_VAR(expected_cumulative_sat_minus3,uint,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_minus3,uint,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat_minus3,uint,64,2) = 0;
+
+/* Expected results with input=max and shift by -3.  */
+VECT_VAR_DECL(expected_minus3,int,8,8) [] = { 0x10, 0x10, 0x10, 0x10,
+					      0x10, 0x10, 0x10, 0x10 };
+VECT_VAR_DECL(expected_minus3,int,16,4) [] = { 0x1000, 0x1000, 0x1000, 0x1000 };
+VECT_VAR_DECL(expected_minus3,int,32,2) [] = { 0x10000000, 0x10000000 };
+VECT_VAR_DECL(expected_minus3,int,64,1) [] = { 0x1000000000000000 };
+VECT_VAR_DECL(expected_minus3,uint,8,8) [] = { 0x20, 0x20, 0x20, 0x20,
+					       0x20, 0x20, 0x20, 0x20 };
+VECT_VAR_DECL(expected_minus3,uint,16,4) [] = { 0x2000, 0x2000, 0x2000, 0x2000 };
+VECT_VAR_DECL(expected_minus3,uint,32,2) [] = { 0x20000000, 0x20000000 };
+VECT_VAR_DECL(expected_minus3,uint,64,1) [] = { 0x2000000000000000 };
+VECT_VAR_DECL(expected_minus3,int,8,16) [] = { 0x10, 0x10, 0x10, 0x10,
+					       0x10, 0x10, 0x10, 0x10,
+					       0x10, 0x10, 0x10, 0x10,
+					       0x10, 0x10, 0x10, 0x10 };
+VECT_VAR_DECL(expected_minus3,int,16,8) [] = { 0x1000, 0x1000, 0x1000, 0x1000,
+					       0x1000, 0x1000, 0x1000, 0x1000 };
+VECT_VAR_DECL(expected_minus3,int,32,4) [] = { 0x10000000, 0x10000000,
+					       0x10000000, 0x10000000 };
+VECT_VAR_DECL(expected_minus3,int,64,2) [] = { 0x1000000000000000,
+					       0x1000000000000000 };
+VECT_VAR_DECL(expected_minus3,uint,8,16) [] = { 0x20, 0x20, 0x20, 0x20,
+						0x20, 0x20, 0x20, 0x20,
+						0x20, 0x20, 0x20, 0x20,
+						0x20, 0x20, 0x20, 0x20 };
+VECT_VAR_DECL(expected_minus3,uint,16,8) [] = { 0x2000, 0x2000, 0x2000, 0x2000,
+						0x2000, 0x2000, 0x2000, 0x2000 };
+VECT_VAR_DECL(expected_minus3,uint,32,4) [] = { 0x20000000, 0x20000000,
+						0x20000000, 0x20000000 };
+VECT_VAR_DECL(expected_minus3,uint,64,2) [] = { 0x2000000000000000,
+						0x2000000000000000 };
+
+/* Expected values of cumulative_saturation flag with input=max and
+   large shift amount.  */
+int VECT_VAR(expected_cumulative_sat_large_sh,int,8,8) = 1;
+int VECT_VAR(expected_cumulative_sat_large_sh,int,16,4) = 1;
+int VECT_VAR(expected_cumulative_sat_large_sh,int,32,2) = 1;
+int VECT_VAR(expected_cumulative_sat_large_sh,int,64,1) = 1;
+int VECT_VAR(expected_cumulative_sat_large_sh,uint,8,8) = 1;
+int VECT_VAR(expected_cumulative_sat_large_sh,uint,16,4) = 1;
+int VECT_VAR(expected_cumulative_sat_large_sh,uint,32,2) = 1;
+int VECT_VAR(expected_cumulative_sat_large_sh,uint,64,1) = 1;
+int VECT_VAR(expected_cumulative_sat_large_sh,int,8,16) = 1;
+int VECT_VAR(expected_cumulative_sat_large_sh,int,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat_large_sh,int,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat_large_sh,int,64,2) = 1;
+int VECT_VAR(expected_cumulative_sat_large_sh,uint,8,16) = 1;
+int VECT_VAR(expected_cumulative_sat_large_sh,uint,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat_large_sh,uint,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat_large_sh,uint,64,2) = 1;
+
+/* Expected results with input=max and large shift amount.  */
+VECT_VAR_DECL(expected_large_sh,int,8,8) [] = { 0x7f, 0x7f, 0x7f, 0x7f,
+						0x7f, 0x7f, 0x7f, 0x7f };
+VECT_VAR_DECL(expected_large_sh,int,16,4) [] = { 0x7fff, 0x7fff,
+						 0x7fff, 0x7fff };
+VECT_VAR_DECL(expected_large_sh,int,32,2) [] = { 0x7fffffff, 0x7fffffff };
+VECT_VAR_DECL(expected_large_sh,int,64,1) [] = { 0x7fffffffffffffff };
+VECT_VAR_DECL(expected_large_sh,uint,8,8) [] = { 0xff, 0xff, 0xff, 0xff,
+						 0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_large_sh,uint,16,4) [] = { 0xffff, 0xffff,
+						  0xffff, 0xffff };
+VECT_VAR_DECL(expected_large_sh,uint,32,2) [] = { 0xffffffff, 0xffffffff };
+VECT_VAR_DECL(expected_large_sh,uint,64,1) [] = { 0xffffffffffffffff };
+VECT_VAR_DECL(expected_large_sh,int,8,16) [] = { 0x7f, 0x7f, 0x7f, 0x7f,
+						 0x7f, 0x7f, 0x7f, 0x7f,
+						 0x7f, 0x7f, 0x7f, 0x7f,
+						 0x7f, 0x7f, 0x7f, 0x7f };
+VECT_VAR_DECL(expected_large_sh,int,16,8) [] = { 0x7fff, 0x7fff,
+						 0x7fff, 0x7fff,
+						 0x7fff, 0x7fff,
+						 0x7fff, 0x7fff };
+VECT_VAR_DECL(expected_large_sh,int,32,4) [] = { 0x7fffffff, 0x7fffffff,
+						 0x7fffffff, 0x7fffffff };
+VECT_VAR_DECL(expected_large_sh,int,64,2) [] = { 0x7fffffffffffffff,
+						 0x7fffffffffffffff };
+VECT_VAR_DECL(expected_large_sh,uint,8,16) [] = { 0xff, 0xff, 0xff, 0xff,
+						  0xff, 0xff, 0xff, 0xff,
+						  0xff, 0xff, 0xff, 0xff,
+						  0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_large_sh,uint,16,8) [] = { 0xffff, 0xffff,
+						  0xffff, 0xffff,
+						  0xffff, 0xffff,
+						  0xffff, 0xffff };
+VECT_VAR_DECL(expected_large_sh,uint,32,4) [] = { 0xffffffff, 0xffffffff,
+						  0xffffffff, 0xffffffff };
+VECT_VAR_DECL(expected_large_sh,uint,64,2) [] = { 0xffffffffffffffff,
+						  0xffffffffffffffff };
+
+/* Expected values of cumulative_saturation flag with negative input and
+   large shift amount.  */
+int VECT_VAR(expected_cumulative_sat_neg_large_sh,int,8,8) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large_sh,int,16,4) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large_sh,int,32,2) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large_sh,int,64,1) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large_sh,uint,8,8) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large_sh,uint,16,4) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large_sh,uint,32,2) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large_sh,uint,64,1) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large_sh,int,8,16) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large_sh,int,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large_sh,int,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large_sh,int,64,2) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large_sh,uint,8,16) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large_sh,uint,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large_sh,uint,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large_sh,uint,64,2) = 1;
+
+/* Expected results with negative input and large shift amount.  */
+VECT_VAR_DECL(expected_neg_large_sh,int,8,8) [] = { 0x80, 0x80, 0x80, 0x80,
+						    0x80, 0x80, 0x80, 0x80 };
+VECT_VAR_DECL(expected_neg_large_sh,int,16,4) [] = { 0x8000, 0x8000,
+						     0x8000, 0x8000 };
+VECT_VAR_DECL(expected_neg_large_sh,int,32,2) [] = { 0x80000000, 0x80000000 };
+VECT_VAR_DECL(expected_neg_large_sh,int,64,1) [] = { 0x8000000000000000 };
+VECT_VAR_DECL(expected_neg_large_sh,uint,8,8) [] = { 0xff, 0xff, 0xff, 0xff,
+						     0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_neg_large_sh,uint,16,4) [] = { 0xffff, 0xffff,
+						      0xffff, 0xffff };
+VECT_VAR_DECL(expected_neg_large_sh,uint,32,2) [] = { 0xffffffff,
+						      0xffffffff };
+VECT_VAR_DECL(expected_neg_large_sh,uint,64,1) [] = { 0xffffffffffffffff };
+VECT_VAR_DECL(expected_neg_large_sh,int,8,16) [] = { 0x80, 0x80, 0x80, 0x80,
+						     0x80, 0x80, 0x80, 0x80,
+						     0x80, 0x80, 0x80, 0x80,
+						     0x80, 0x80, 0x80, 0x80 };
+VECT_VAR_DECL(expected_neg_large_sh,int,16,8) [] = { 0x8000, 0x8000,
+						     0x8000, 0x8000,
+						     0x8000, 0x8000,
+						     0x8000, 0x8000 };
+VECT_VAR_DECL(expected_neg_large_sh,int,32,4) [] = { 0x80000000, 0x80000000,
+						     0x80000000, 0x80000000 };
+VECT_VAR_DECL(expected_neg_large_sh,int,64,2) [] = { 0x8000000000000000,
+						     0x8000000000000000 };
+VECT_VAR_DECL(expected_neg_large_sh,uint,8,16) [] = { 0xff, 0xff, 0xff, 0xff,
+						      0xff, 0xff, 0xff, 0xff,
+						      0xff, 0xff, 0xff, 0xff,
+						      0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_neg_large_sh,uint,16,8) [] = { 0xffff, 0xffff,
+						      0xffff, 0xffff,
+						      0xffff, 0xffff,
+						      0xffff, 0xffff };
+VECT_VAR_DECL(expected_neg_large_sh,uint,32,4) [] = { 0xffffffff,
+						      0xffffffff,
+						      0xffffffff,
+						      0xffffffff };
+VECT_VAR_DECL(expected_neg_large_sh,uint,64,2) [] = { 0xffffffffffffffff,
+						      0xffffffffffffffff };
+
+/* Expected values of cumulative_saturation flag with max/min input and
+   large negative shift amount.  */
+int VECT_VAR(expected_cumulative_sat_large_neg_sh,int,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat_large_neg_sh,int,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat_large_neg_sh,int,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat_large_neg_sh,int,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat_large_neg_sh,uint,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat_large_neg_sh,uint,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat_large_neg_sh,uint,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat_large_neg_sh,uint,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat_large_neg_sh,int,8,16) = 0;
+int VECT_VAR(expected_cumulative_sat_large_neg_sh,int,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_large_neg_sh,int,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat_large_neg_sh,int,64,2) = 0;
+int VECT_VAR(expected_cumulative_sat_large_neg_sh,uint,8,16) = 0;
+int VECT_VAR(expected_cumulative_sat_large_neg_sh,uint,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_large_neg_sh,uint,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat_large_neg_sh,uint,64,2) = 0;
+
+/* Expected results with max/min input and large negative shift amount.  */
+VECT_VAR_DECL(expected_large_neg_sh,int,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+						    0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_large_neg_sh,int,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_large_neg_sh,int,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_large_neg_sh,int,64,1) [] = { 0x0 };
+VECT_VAR_DECL(expected_large_neg_sh,uint,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+						     0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_large_neg_sh,uint,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_large_neg_sh,uint,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_large_neg_sh,uint,64,1) [] = { 0x0 };
+VECT_VAR_DECL(expected_large_neg_sh,int,8,16) [] = { 0x0, 0x0, 0x0, 0x0,
+						     0x0, 0x0, 0x0, 0x0,
+						     0x0, 0x0, 0x0, 0x0,
+						     0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_large_neg_sh,int,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+						     0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_large_neg_sh,int,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_large_neg_sh,int,64,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_large_neg_sh,uint,8,16) [] = { 0x0, 0x0, 0x0, 0x0,
+						      0x0, 0x0, 0x0, 0x0,
+						      0x0, 0x0, 0x0, 0x0,
+						      0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_large_neg_sh,uint,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+						      0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_large_neg_sh,uint,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_large_neg_sh,uint,64,2) [] = { 0x0, 0x0 };
+
+/* Expected values of cumulative_saturation flag with input=0 and
+   large negative shift amount.  */
+int VECT_VAR(expected_cumulative_sat_0_large_neg_sh,int,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat_0_large_neg_sh,int,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat_0_large_neg_sh,int,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat_0_large_neg_sh,int,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat_0_large_neg_sh,uint,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat_0_large_neg_sh,uint,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat_0_large_neg_sh,uint,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat_0_large_neg_sh,uint,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat_0_large_neg_sh,int,8,16) = 0;
+int VECT_VAR(expected_cumulative_sat_0_large_neg_sh,int,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_0_large_neg_sh,int,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat_0_large_neg_sh,int,64,2) = 0;
+int VECT_VAR(expected_cumulative_sat_0_large_neg_sh,uint,8,16) = 0;
+int VECT_VAR(expected_cumulative_sat_0_large_neg_sh,uint,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_0_large_neg_sh,uint,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat_0_large_neg_sh,uint,64,2) = 0;
+
+/* Expected results with input=0 and large negative shift amount.  */
+VECT_VAR_DECL(expected_0_large_neg_sh,int,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+						      0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_large_neg_sh,int,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_large_neg_sh,int,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_large_neg_sh,int,64,1) [] = { 0x0 };
+VECT_VAR_DECL(expected_0_large_neg_sh,uint,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+						       0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_large_neg_sh,uint,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_large_neg_sh,uint,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_large_neg_sh,uint,64,1) [] = { 0x0 };
+VECT_VAR_DECL(expected_0_large_neg_sh,int,8,16) [] = { 0x0, 0x0, 0x0, 0x0,
+						       0x0, 0x0, 0x0, 0x0,
+						       0x0, 0x0, 0x0, 0x0,
+						       0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_large_neg_sh,int,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+						       0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_large_neg_sh,int,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_large_neg_sh,int,64,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_large_neg_sh,uint,8,16) [] = { 0x0, 0x0, 0x0, 0x0,
+							0x0, 0x0, 0x0, 0x0,
+							0x0, 0x0, 0x0, 0x0,
+							0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_large_neg_sh,uint,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+							0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_large_neg_sh,uint,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_large_neg_sh,uint,64,2) [] = { 0x0, 0x0 };
+
+#define INSN vqrshl
+#define TEST_MSG "VQRSHL/VQRSHLQ"
+
+#define FNNAME1(NAME) void exec_ ## NAME (void)
+#define FNNAME(NAME) FNNAME1(NAME)
+
+FNNAME (INSN)
+{
+  /* Basic test: v3=vqrshl(v1,v2), then store the result.  */
+#define TEST_VQRSHL2(INSN, T3, Q, T1, T2, W, N, EXPECTED_CUMULATIVE_SAT, CMT) \
+  Set_Neon_Cumulative_Sat(0, VECT_VAR(vector_res, T1, W, N));		\
+  VECT_VAR(vector_res, T1, W, N) =					\
+    INSN##Q##_##T2##W(VECT_VAR(vector, T1, W, N),			\
+		      VECT_VAR(vector_shift, T3, W, N));		\
+  vst1##Q##_##T2##W(VECT_VAR(result, T1, W, N),				\
+		    VECT_VAR(vector_res, T1, W, N));			\
+  CHECK_CUMULATIVE_SAT(TEST_MSG, T1, W, N, EXPECTED_CUMULATIVE_SAT, CMT)
+
+  /* Two auxliary macros are necessary to expand INSN */
+#define TEST_VQRSHL1(INSN, T3, Q, T1, T2, W, N, EXPECTED_CUMULATIVE_SAT, CMT) \
+  TEST_VQRSHL2(INSN, T3, Q, T1, T2, W, N, EXPECTED_CUMULATIVE_SAT, CMT)
+
+#define TEST_VQRSHL(T3, Q, T1, T2, W, N, EXPECTED_CUMULATIVE_SAT, CMT)	\
+  TEST_VQRSHL1(INSN, T3, Q, T1, T2, W, N, EXPECTED_CUMULATIVE_SAT, CMT)
+
+  DECL_VARIABLE_ALL_VARIANTS(vector);
+  DECL_VARIABLE_ALL_VARIANTS(vector_res);
+
+  DECL_VARIABLE_SIGNED_VARIANTS(vector_shift);
+
+  clean_results ();
+
+  /* Fill input vector with 0, to check saturation on limits.  */
+  VDUP(vector, , int, s, 8, 8, 0);
+  VDUP(vector, , int, s, 16, 4, 0);
+  VDUP(vector, , int, s, 32, 2, 0);
+  VDUP(vector, , int, s, 64, 1, 0);
+  VDUP(vector, , uint, u, 8, 8, 0);
+  VDUP(vector, , uint, u, 16, 4, 0);
+  VDUP(vector, , uint, u, 32, 2, 0);
+  VDUP(vector, , uint, u, 64, 1, 0);
+  VDUP(vector, q, int, s, 8, 16, 0);
+  VDUP(vector, q, int, s, 16, 8, 0);
+  VDUP(vector, q, int, s, 32, 4, 0);
+  VDUP(vector, q, int, s, 64, 2, 0);
+  VDUP(vector, q, uint, u, 8, 16, 0);
+  VDUP(vector, q, uint, u, 16, 8, 0);
+  VDUP(vector, q, uint, u, 32, 4, 0);
+  VDUP(vector, q, uint, u, 64, 2, 0);
+
+  /* Choose init value arbitrarily, will be used as shift amount */
+  /* Use values equal to or one-less-than the type width to check
+     behaviour on limits.  */
+  VDUP(vector_shift, , int, s, 8, 8, 7);
+  VDUP(vector_shift, , int, s, 16, 4, 15);
+  VDUP(vector_shift, , int, s, 32, 2, 31);
+  VDUP(vector_shift, , int, s, 64, 1, 63);
+  VDUP(vector_shift, q, int, s, 8, 16, 8);
+  VDUP(vector_shift, q, int, s, 16, 8, 16);
+  VDUP(vector_shift, q, int, s, 32, 4, 32);
+  VDUP(vector_shift, q, int, s, 64, 2, 64);
+
+#define CMT " (with input = 0)"
+  TEST_VQRSHL(int, , int, s, 8, 8, expected_cumulative_sat_0, CMT);
+  TEST_VQRSHL(int, , int, s, 16, 4, expected_cumulative_sat_0, CMT);
+  TEST_VQRSHL(int, , int, s, 32, 2, expected_cumulative_sat_0, CMT);
+  TEST_VQRSHL(int, , int, s, 64, 1, expected_cumulative_sat_0, CMT);
+  TEST_VQRSHL(int, , uint, u, 8, 8, expected_cumulative_sat_0, CMT);
+  TEST_VQRSHL(int, , uint, u, 16, 4, expected_cumulative_sat_0, CMT);
+  TEST_VQRSHL(int, , uint, u, 32, 2, expected_cumulative_sat_0, CMT);
+  TEST_VQRSHL(int, , uint, u, 64, 1, expected_cumulative_sat_0, CMT);
+  TEST_VQRSHL(int, q, int, s, 8, 16, expected_cumulative_sat_0, CMT);
+  TEST_VQRSHL(int, q, int, s, 16, 8, expected_cumulative_sat_0, CMT);
+  TEST_VQRSHL(int, q, int, s, 32, 4, expected_cumulative_sat_0, CMT);
+  TEST_VQRSHL(int, q, int, s, 64, 2, expected_cumulative_sat_0, CMT);
+  TEST_VQRSHL(int, q, uint, u, 8, 16, expected_cumulative_sat_0, CMT);
+  TEST_VQRSHL(int, q, uint, u, 16, 8, expected_cumulative_sat_0, CMT);
+  TEST_VQRSHL(int, q, uint, u, 32, 4, expected_cumulative_sat_0, CMT);
+  TEST_VQRSHL(int, q, uint, u, 64, 2, expected_cumulative_sat_0, CMT);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_0, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_0, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_0, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_0, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_0, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_0, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_0, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_0, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_0, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_0, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_0, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_0, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_0, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_0, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_0, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_0, CMT);
+
+
+  /* Use negative shift amounts.  */
+  VDUP(vector_shift, , int, s, 8, 8, -1);
+  VDUP(vector_shift, , int, s, 16, 4, -2);
+  VDUP(vector_shift, , int, s, 32, 2, -3);
+  VDUP(vector_shift, , int, s, 64, 1, -4);
+  VDUP(vector_shift, q, int, s, 8, 16, -7);
+  VDUP(vector_shift, q, int, s, 16, 8, -11);
+  VDUP(vector_shift, q, int, s, 32, 4, -13);
+  VDUP(vector_shift, q, int, s, 64, 2, -20);
+
+#undef CMT
+#define CMT " (input 0 and negative shift amount)"
+  TEST_VQRSHL(int, , int, s, 8, 8, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQRSHL(int, , int, s, 16, 4, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQRSHL(int, , int, s, 32, 2, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQRSHL(int, , int, s, 64, 1, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQRSHL(int, , uint, u, 8, 8, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQRSHL(int, , uint, u, 16, 4, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQRSHL(int, , uint, u, 32, 2, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQRSHL(int, , uint, u, 64, 1, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQRSHL(int, q, int, s, 8, 16, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQRSHL(int, q, int, s, 16, 8, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQRSHL(int, q, int, s, 32, 4, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQRSHL(int, q, int, s, 64, 2, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQRSHL(int, q, uint, u, 8, 16, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQRSHL(int, q, uint, u, 16, 8, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQRSHL(int, q, uint, u, 32, 4, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQRSHL(int, q, uint, u, 64, 2, expected_cumulative_sat_0_neg, CMT);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_0_neg, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_0_neg, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_0_neg, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_0_neg, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_0_neg, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_0_neg, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_0_neg, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_0_neg, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_0_neg, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_0_neg, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_0_neg, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_0_neg, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_0_neg, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_0_neg, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_0_neg, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_0_neg, CMT);
+
+
+  /* Test again, with predefined input values.  */
+  TEST_MACRO_ALL_VARIANTS_2_5(VLOAD, vector, buffer);
+
+  /* Choose init value arbitrarily, will be used as shift amount.  */
+  VDUP(vector_shift, , int, s, 8, 8, 1);
+  VDUP(vector_shift, , int, s, 16, 4, 3);
+  VDUP(vector_shift, , int, s, 32, 2, 8);
+  VDUP(vector_shift, , int, s, 64, 1, 3);
+  VDUP(vector_shift, q, int, s, 8, 16, 10);
+  VDUP(vector_shift, q, int, s, 16, 8, 12);
+  VDUP(vector_shift, q, int, s, 32, 4, 31);
+  VDUP(vector_shift, q, int, s, 64, 2, 63);
+
+#undef CMT
+#define CMT ""
+  TEST_VQRSHL(int, , int, s, 8, 8, expected_cumulative_sat, CMT);
+  TEST_VQRSHL(int, , int, s, 16, 4, expected_cumulative_sat, CMT);
+  TEST_VQRSHL(int, , int, s, 32, 2, expected_cumulative_sat, CMT);
+  TEST_VQRSHL(int, , int, s, 64, 1, expected_cumulative_sat, CMT);
+  TEST_VQRSHL(int, , uint, u, 8, 8, expected_cumulative_sat, CMT);
+  TEST_VQRSHL(int, , uint, u, 16, 4, expected_cumulative_sat, CMT);
+  TEST_VQRSHL(int, , uint, u, 32, 2, expected_cumulative_sat, CMT);
+  TEST_VQRSHL(int, , uint, u, 64, 1, expected_cumulative_sat, CMT);
+  TEST_VQRSHL(int, q, int, s, 8, 16, expected_cumulative_sat, CMT);
+  TEST_VQRSHL(int, q, int, s, 16, 8, expected_cumulative_sat, CMT);
+  TEST_VQRSHL(int, q, int, s, 32, 4, expected_cumulative_sat, CMT);
+  TEST_VQRSHL(int, q, int, s, 64, 2, expected_cumulative_sat, CMT);
+  TEST_VQRSHL(int, q, uint, u, 8, 16, expected_cumulative_sat, CMT);
+  TEST_VQRSHL(int, q, uint, u, 16, 8, expected_cumulative_sat, CMT);
+  TEST_VQRSHL(int, q, uint, u, 32, 4, expected_cumulative_sat, CMT);
+  TEST_VQRSHL(int, q, uint, u, 64, 2, expected_cumulative_sat, CMT);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected, CMT);
+
+
+  /* Use negative shift amounts.  */
+  VDUP(vector_shift, , int, s, 8, 8, -2);
+  VDUP(vector_shift, , int, s, 16, 4, -2);
+  VDUP(vector_shift, , int, s, 32, 2, -3);
+  VDUP(vector_shift, , int, s, 64, 1, -4);
+  VDUP(vector_shift, q, int, s, 8, 16, -7);
+  VDUP(vector_shift, q, int, s, 16, 8, -11);
+  VDUP(vector_shift, q, int, s, 32, 4, -13);
+  VDUP(vector_shift, q, int, s, 64, 2, -20);
+
+#undef CMT
+#define CMT " (negative shift amount)"
+  TEST_VQRSHL(int, , int, s, 8, 8, expected_cumulative_sat_neg, CMT);
+  TEST_VQRSHL(int, , int, s, 16, 4, expected_cumulative_sat_neg, CMT);
+  TEST_VQRSHL(int, , int, s, 32, 2, expected_cumulative_sat_neg, CMT);
+  TEST_VQRSHL(int, , int, s, 64, 1, expected_cumulative_sat_neg, CMT);
+  TEST_VQRSHL(int, , uint, u, 8, 8, expected_cumulative_sat_neg, CMT);
+  TEST_VQRSHL(int, , uint, u, 16, 4, expected_cumulative_sat_neg, CMT);
+  TEST_VQRSHL(int, , uint, u, 32, 2, expected_cumulative_sat_neg, CMT);
+  TEST_VQRSHL(int, , uint, u, 64, 1, expected_cumulative_sat_neg, CMT);
+  TEST_VQRSHL(int, q, int, s, 8, 16, expected_cumulative_sat_neg, CMT);
+  TEST_VQRSHL(int, q, int, s, 16, 8, expected_cumulative_sat_neg, CMT);
+  TEST_VQRSHL(int, q, int, s, 32, 4, expected_cumulative_sat_neg, CMT);
+  TEST_VQRSHL(int, q, int, s, 64, 2, expected_cumulative_sat_neg, CMT);
+  TEST_VQRSHL(int, q, uint, u, 8, 16, expected_cumulative_sat_neg, CMT);
+  TEST_VQRSHL(int, q, uint, u, 16, 8, expected_cumulative_sat_neg, CMT);
+  TEST_VQRSHL(int, q, uint, u, 32, 4, expected_cumulative_sat_neg, CMT);
+  TEST_VQRSHL(int, q, uint, u, 64, 2, expected_cumulative_sat_neg, CMT);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_neg, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_neg, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_neg, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_neg, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_neg, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_neg, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_neg, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_neg, CMT);
+
+
+  /* Fill input vector with max value, to check saturation on
+     limits.  */
+  VDUP(vector, , int, s, 8, 8, 0x7F);
+  VDUP(vector, , int, s, 16, 4, 0x7FFF);
+  VDUP(vector, , int, s, 32, 2, 0x7FFFFFFF);
+  VDUP(vector, , int, s, 64, 1, 0x7FFFFFFFFFFFFFFFLL);
+  VDUP(vector, , uint, u, 8, 8, 0xFF);
+  VDUP(vector, , uint, u, 16, 4, 0xFFFF);
+  VDUP(vector, , uint, u, 32, 2, 0xFFFFFFFF);
+  VDUP(vector, , uint, u, 64, 1, 0xFFFFFFFFFFFFFFFFULL);
+  VDUP(vector, q, int, s, 8, 16, 0x7F);
+  VDUP(vector, q, int, s, 16, 8, 0x7FFF);
+  VDUP(vector, q, int, s, 32, 4, 0x7FFFFFFF);
+  VDUP(vector, q, int, s, 64, 2, 0x7FFFFFFFFFFFFFFFLL);
+  VDUP(vector, q, uint, u, 8, 16, 0xFF);
+  VDUP(vector, q, uint, u, 16, 8, 0xFFFF);
+  VDUP(vector, q, uint, u, 32, 4, 0xFFFFFFFF);
+  VDUP(vector, q, uint, u, 64, 2, 0xFFFFFFFFFFFFFFFFULL);
+
+  /* Use -1 shift amount to check cumulative saturation with
+     round_const.  */
+  VDUP(vector_shift, , int, s, 8, 8, -1);
+  VDUP(vector_shift, , int, s, 16, 4, -1);
+  VDUP(vector_shift, , int, s, 32, 2, -1);
+  VDUP(vector_shift, , int, s, 64, 1, -1);
+  VDUP(vector_shift, q, int, s, 8, 16, -1);
+  VDUP(vector_shift, q, int, s, 16, 8, -1);
+  VDUP(vector_shift, q, int, s, 32, 4, -1);
+  VDUP(vector_shift, q, int, s, 64, 2, -1);
+
+#undef CMT
+#define CMT " (checking cumulative saturation: shift by -1)"
+  TEST_VQRSHL(int, , int, s, 8, 8, expected_cumulative_sat_minus1, CMT);
+  TEST_VQRSHL(int, , int, s, 16, 4, expected_cumulative_sat_minus1, CMT);
+  TEST_VQRSHL(int, , int, s, 32, 2, expected_cumulative_sat_minus1, CMT);
+  TEST_VQRSHL(int, , int, s, 64, 1, expected_cumulative_sat_minus1, CMT);
+  TEST_VQRSHL(int, , uint, u, 8, 8, expected_cumulative_sat_minus1, CMT);
+  TEST_VQRSHL(int, , uint, u, 16, 4, expected_cumulative_sat_minus1, CMT);
+  TEST_VQRSHL(int, , uint, u, 32, 2, expected_cumulative_sat_minus1, CMT);
+  TEST_VQRSHL(int, , uint, u, 64, 1, expected_cumulative_sat_minus1, CMT);
+  TEST_VQRSHL(int, q, int, s, 8, 16, expected_cumulative_sat_minus1, CMT);
+  TEST_VQRSHL(int, q, int, s, 16, 8, expected_cumulative_sat_minus1, CMT);
+  TEST_VQRSHL(int, q, int, s, 32, 4, expected_cumulative_sat_minus1, CMT);
+  TEST_VQRSHL(int, q, int, s, 64, 2, expected_cumulative_sat_minus1, CMT);
+  TEST_VQRSHL(int, q, uint, u, 8, 16, expected_cumulative_sat_minus1, CMT);
+  TEST_VQRSHL(int, q, uint, u, 16, 8, expected_cumulative_sat_minus1, CMT);
+  TEST_VQRSHL(int, q, uint, u, 32, 4, expected_cumulative_sat_minus1, CMT);
+  TEST_VQRSHL(int, q, uint, u, 64, 2, expected_cumulative_sat_minus1, CMT);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_minus1, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_minus1, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_minus1, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_minus1, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_minus1, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_minus1, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_minus1, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_minus1, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_minus1, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_minus1, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_minus1, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_minus1, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_minus1, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_minus1, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_minus1, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_minus1, CMT);
+
+
+  /* Use -3 shift amount to check cumulative saturation with
+     round_const. */
+  VDUP(vector_shift, , int, s, 8, 8, -3);
+  VDUP(vector_shift, , int, s, 16, 4, -3);
+  VDUP(vector_shift, , int, s, 32, 2, -3);
+  VDUP(vector_shift, , int, s, 64, 1, -3);
+  VDUP(vector_shift, q, int, s, 8, 16, -3);
+  VDUP(vector_shift, q, int, s, 16, 8, -3);
+  VDUP(vector_shift, q, int, s, 32, 4, -3);
+  VDUP(vector_shift, q, int, s, 64, 2, -3);
+
+#undef CMT
+#define CMT " (checking cumulative saturation: shift by -3)"
+  TEST_VQRSHL(int, , int, s, 8, 8, expected_cumulative_sat_minus3, CMT);
+  TEST_VQRSHL(int, , int, s, 16, 4, expected_cumulative_sat_minus3, CMT);
+  TEST_VQRSHL(int, , int, s, 32, 2, expected_cumulative_sat_minus3, CMT);
+  TEST_VQRSHL(int, , int, s, 64, 1, expected_cumulative_sat_minus3, CMT);
+  TEST_VQRSHL(int, , uint, u, 8, 8, expected_cumulative_sat_minus3, CMT);
+  TEST_VQRSHL(int, , uint, u, 16, 4, expected_cumulative_sat_minus3, CMT);
+  TEST_VQRSHL(int, , uint, u, 32, 2, expected_cumulative_sat_minus3, CMT);
+  TEST_VQRSHL(int, , uint, u, 64, 1, expected_cumulative_sat_minus3, CMT);
+  TEST_VQRSHL(int, q, int, s, 8, 16, expected_cumulative_sat_minus3, CMT);
+  TEST_VQRSHL(int, q, int, s, 16, 8, expected_cumulative_sat_minus3, CMT);
+  TEST_VQRSHL(int, q, int, s, 32, 4, expected_cumulative_sat_minus3, CMT);
+  TEST_VQRSHL(int, q, int, s, 64, 2, expected_cumulative_sat_minus3, CMT);
+  TEST_VQRSHL(int, q, uint, u, 8, 16, expected_cumulative_sat_minus3, CMT);
+  TEST_VQRSHL(int, q, uint, u, 16, 8, expected_cumulative_sat_minus3, CMT);
+  TEST_VQRSHL(int, q, uint, u, 32, 4, expected_cumulative_sat_minus3, CMT);
+  TEST_VQRSHL(int, q, uint, u, 64, 2, expected_cumulative_sat_minus3, CMT);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_minus3, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_minus3, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_minus3, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_minus3, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_minus3, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_minus3, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_minus3, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_minus3, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_minus3, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_minus3, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_minus3, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_minus3, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_minus3, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_minus3, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_minus3, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_minus3, CMT);
+
+
+  /* Use large shift amount.  */
+  VDUP(vector_shift, , int, s, 8, 8, 10);
+  VDUP(vector_shift, , int, s, 16, 4, 20);
+  VDUP(vector_shift, , int, s, 32, 2, 40);
+  VDUP(vector_shift, , int, s, 64, 1, 70);
+  VDUP(vector_shift, q, int, s, 8, 16, 10);
+  VDUP(vector_shift, q, int, s, 16, 8, 20);
+  VDUP(vector_shift, q, int, s, 32, 4, 40);
+  VDUP(vector_shift, q, int, s, 64, 2, 70);
+
+#undef CMT
+#define CMT " (checking cumulative saturation: large shift amount)"
+  TEST_VQRSHL(int, , int, s, 8, 8, expected_cumulative_sat_large_sh, CMT);
+  TEST_VQRSHL(int, , int, s, 16, 4, expected_cumulative_sat_large_sh, CMT);
+  TEST_VQRSHL(int, , int, s, 32, 2, expected_cumulative_sat_large_sh, CMT);
+  TEST_VQRSHL(int, , int, s, 64, 1, expected_cumulative_sat_large_sh, CMT);
+  TEST_VQRSHL(int, , uint, u, 8, 8, expected_cumulative_sat_large_sh, CMT);
+  TEST_VQRSHL(int, , uint, u, 16, 4, expected_cumulative_sat_large_sh, CMT);
+  TEST_VQRSHL(int, , uint, u, 32, 2, expected_cumulative_sat_large_sh, CMT);
+  TEST_VQRSHL(int, , uint, u, 64, 1, expected_cumulative_sat_large_sh, CMT);
+  TEST_VQRSHL(int, q, int, s, 8, 16, expected_cumulative_sat_large_sh, CMT);
+  TEST_VQRSHL(int, q, int, s, 16, 8, expected_cumulative_sat_large_sh, CMT);
+  TEST_VQRSHL(int, q, int, s, 32, 4, expected_cumulative_sat_large_sh, CMT);
+  TEST_VQRSHL(int, q, int, s, 64, 2, expected_cumulative_sat_large_sh, CMT);
+  TEST_VQRSHL(int, q, uint, u, 8, 16, expected_cumulative_sat_large_sh, CMT);
+  TEST_VQRSHL(int, q, uint, u, 16, 8, expected_cumulative_sat_large_sh, CMT);
+  TEST_VQRSHL(int, q, uint, u, 32, 4, expected_cumulative_sat_large_sh, CMT);
+  TEST_VQRSHL(int, q, uint, u, 64, 2, expected_cumulative_sat_large_sh, CMT);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_large_sh, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_large_sh, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_large_sh, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_large_sh, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_large_sh, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_large_sh, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_large_sh, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_large_sh, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_large_sh, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_large_sh, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_large_sh, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_large_sh, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_large_sh, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_large_sh, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_large_sh, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_large_sh, CMT);
+
+
+  /* Fill input vector with negative values, to check saturation on
+     limits.  */
+  VDUP(vector, , int, s, 8, 8, 0x80);
+  VDUP(vector, , int, s, 16, 4, 0x8000);
+  VDUP(vector, , int, s, 32, 2, 0x80000000);
+  VDUP(vector, , int, s, 64, 1, 0x8000000000000000LL);
+  VDUP(vector, q, int, s, 8, 16, 0x80);
+  VDUP(vector, q, int, s, 16, 8, 0x8000);
+  VDUP(vector, q, int, s, 32, 4, 0x80000000);
+  VDUP(vector, q, int, s, 64, 2, 0x8000000000000000LL);
+
+  /* Use large shift amount.  */
+  VDUP(vector_shift, , int, s, 8, 8, 10);
+  VDUP(vector_shift, , int, s, 16, 4, 20);
+  VDUP(vector_shift, , int, s, 32, 2, 40);
+  VDUP(vector_shift, , int, s, 64, 1, 70);
+  VDUP(vector_shift, q, int, s, 8, 16, 10);
+  VDUP(vector_shift, q, int, s, 16, 8, 20);
+  VDUP(vector_shift, q, int, s, 32, 4, 40);
+  VDUP(vector_shift, q, int, s, 64, 2, 70);
+
+#undef CMT
+#define CMT " (checking cumulative saturation: large shift amount with negative input)"
+  TEST_VQRSHL(int, , int, s, 8, 8, expected_cumulative_sat_neg_large_sh, CMT);
+  TEST_VQRSHL(int, , int, s, 16, 4, expected_cumulative_sat_neg_large_sh, CMT);
+  TEST_VQRSHL(int, , int, s, 32, 2, expected_cumulative_sat_neg_large_sh, CMT);
+  TEST_VQRSHL(int, , int, s, 64, 1, expected_cumulative_sat_neg_large_sh, CMT);
+  TEST_VQRSHL(int, , uint, u, 8, 8, expected_cumulative_sat_neg_large_sh, CMT);
+  TEST_VQRSHL(int, , uint, u, 16, 4, expected_cumulative_sat_neg_large_sh, CMT);
+  TEST_VQRSHL(int, , uint, u, 32, 2, expected_cumulative_sat_neg_large_sh, CMT);
+  TEST_VQRSHL(int, , uint, u, 64, 1, expected_cumulative_sat_neg_large_sh, CMT);
+  TEST_VQRSHL(int, q, int, s, 8, 16, expected_cumulative_sat_neg_large_sh, CMT);
+  TEST_VQRSHL(int, q, int, s, 16, 8, expected_cumulative_sat_neg_large_sh, CMT);
+  TEST_VQRSHL(int, q, int, s, 32, 4, expected_cumulative_sat_neg_large_sh, CMT);
+  TEST_VQRSHL(int, q, int, s, 64, 2, expected_cumulative_sat_neg_large_sh, CMT);
+  TEST_VQRSHL(int, q, uint, u, 8, 16, expected_cumulative_sat_neg_large_sh, CMT);
+  TEST_VQRSHL(int, q, uint, u, 16, 8, expected_cumulative_sat_neg_large_sh, CMT);
+  TEST_VQRSHL(int, q, uint, u, 32, 4, expected_cumulative_sat_neg_large_sh, CMT);
+  TEST_VQRSHL(int, q, uint, u, 64, 2, expected_cumulative_sat_neg_large_sh, CMT);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_neg_large_sh, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_neg_large_sh, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_neg_large_sh, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_neg_large_sh, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_neg_large_sh, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_neg_large_sh, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_neg_large_sh, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_neg_large_sh, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_neg_large_sh, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_neg_large_sh, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_neg_large_sh, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_neg_large_sh, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_neg_large_sh, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_neg_large_sh, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_neg_large_sh, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_neg_large_sh, CMT);
+
+
+  /* Fill input vector with negative and positive values, to check
+   * saturation on limits */
+  VDUP(vector, , int, s, 8, 8, 0x7F);
+  VDUP(vector, , int, s, 16, 4, 0x7FFF);
+  VDUP(vector, , int, s, 32, 2, 0x7FFFFFFF);
+  VDUP(vector, , int, s, 64, 1, 0x7FFFFFFFFFFFFFFFLL);
+  VDUP(vector, q, int, s, 8, 16, 0x80);
+  VDUP(vector, q, int, s, 16, 8, 0x8000);
+  VDUP(vector, q, int, s, 32, 4, 0x80000000);
+  VDUP(vector, q, int, s, 64, 2, 0x8000000000000000LL);
+
+  /* Use large negative shift amount  */
+  VDUP(vector_shift, , int, s, 8, 8, -10);
+  VDUP(vector_shift, , int, s, 16, 4, -20);
+  VDUP(vector_shift, , int, s, 32, 2, -40);
+  VDUP(vector_shift, , int, s, 64, 1, -70);
+  VDUP(vector_shift, q, int, s, 8, 16, -10);
+  VDUP(vector_shift, q, int, s, 16, 8, -20);
+  VDUP(vector_shift, q, int, s, 32, 4, -40);
+  VDUP(vector_shift, q, int, s, 64, 2, -70);
+
+#undef CMT
+#define CMT " (checking cumulative saturation: large negative shift amount)"
+  TEST_VQRSHL(int, , int, s, 8, 8, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, , int, s, 16, 4, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, , int, s, 32, 2, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, , int, s, 64, 1, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, , uint, u, 8, 8, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, , uint, u, 16, 4, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, , uint, u, 32, 2, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, , uint, u, 64, 1, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, q, int, s, 8, 16, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, q, int, s, 16, 8, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, q, int, s, 32, 4, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, q, int, s, 64, 2, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, q, uint, u, 8, 16, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, q, uint, u, 16, 8, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, q, uint, u, 32, 4, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, q, uint, u, 64, 2, expected_cumulative_sat_large_neg_sh, CMT);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_large_neg_sh, CMT);
+
+
+  /* Fill input vector with 0, to check saturation in case of large
+   * shift amount */
+  VDUP(vector, , int, s, 8, 8, 0);
+  VDUP(vector, , int, s, 16, 4, 0);
+  VDUP(vector, , int, s, 32, 2, 0);
+  VDUP(vector, , int, s, 64, 1, 0);
+  VDUP(vector, q, int, s, 8, 16, 0);
+  VDUP(vector, q, int, s, 16, 8, 0);
+  VDUP(vector, q, int, s, 32, 4, 0);
+  VDUP(vector, q, int, s, 64, 2, 0);
+
+  /* Use large shift amount  */
+  VDUP(vector_shift, , int, s, 8, 8, -10);
+  VDUP(vector_shift, , int, s, 16, 4, -20);
+  VDUP(vector_shift, , int, s, 32, 2, -40);
+  VDUP(vector_shift, , int, s, 64, 1, -70);
+  VDUP(vector_shift, q, int, s, 8, 16, -10);
+  VDUP(vector_shift, q, int, s, 16, 8, -20);
+  VDUP(vector_shift, q, int, s, 32, 4, -40);
+  VDUP(vector_shift, q, int, s, 64, 2, -70);
+
+#undef CMT
+#define CMT " (checking cumulative saturation: large negative shift amount with 0 input)"
+  TEST_VQRSHL(int, , int, s, 8, 8, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, , int, s, 16, 4, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, , int, s, 32, 2, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, , int, s, 64, 1, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, , uint, u, 8, 8, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, , uint, u, 16, 4, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, , uint, u, 32, 2, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, , uint, u, 64, 1, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, q, int, s, 8, 16, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, q, int, s, 16, 8, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, q, int, s, 32, 4, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, q, int, s, 64, 2, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, q, uint, u, 8, 16, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, q, uint, u, 16, 8, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, q, uint, u, 32, 4, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, q, uint, u, 64, 2, expected_cumulative_sat_large_neg_sh, CMT);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_large_neg_sh, CMT);
+}
+
+int main (void)
+{
+  exec_vqrshl ();
+  return 0;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vqrshrn_n.c
@@ -0,0 +1,174 @@
+#include <arm_neon.h>
+#include "arm-neon-ref.h"
+#include "compute-ref-data.h"
+
+/* Expected values of cumulative_saturation flag.  */
+int VECT_VAR(expected_cumulative_sat,int,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat,int,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat,int,64,2) = 0;
+int VECT_VAR(expected_cumulative_sat,uint,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat,uint,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat,uint,64,2) = 1;
+
+/* Expected results.  */
+VECT_VAR_DECL(expected,int,8,8) [] = { 0xf8, 0xf9, 0xf9, 0xfa,
+				       0xfa, 0xfb, 0xfb, 0xfc };
+VECT_VAR_DECL(expected,int,16,4) [] = { 0xfff8, 0xfff9, 0xfff9, 0xfffa };
+VECT_VAR_DECL(expected,int,32,2) [] = { 0xfffffffc, 0xfffffffc };
+VECT_VAR_DECL(expected,uint,8,8) [] = { 0xff, 0xff, 0xff, 0xff,
+					0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected,uint,16,4) [] = { 0xffff, 0xffff, 0xffff, 0xffff };
+VECT_VAR_DECL(expected,uint,32,2) [] = { 0xffffffff, 0xffffffff };
+
+/* Expected values of cumulative_saturation flag with shift by 3.  */
+int VECT_VAR(expected_cumulative_sat_sh3,int,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat_sh3,int,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat_sh3,int,64,2) = 1;
+int VECT_VAR(expected_cumulative_sat_sh3,uint,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat_sh3,uint,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat_sh3,uint,64,2) = 1;
+
+/* Expected results with shift by 3.  */
+VECT_VAR_DECL(expected_sh3,int,8,8) [] = { 0x7f, 0x7f, 0x7f, 0x7f,
+					   0x7f, 0x7f, 0x7f, 0x7f };
+VECT_VAR_DECL(expected_sh3,int,16,4) [] = { 0x7fff, 0x7fff, 0x7fff, 0x7fff };
+VECT_VAR_DECL(expected_sh3,int,32,2) [] = { 0x7fffffff, 0x7fffffff };
+VECT_VAR_DECL(expected_sh3,uint,8,8) [] = { 0xff, 0xff, 0xff, 0xff,
+					    0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_sh3,uint,16,4) [] = { 0xffff, 0xffff, 0xffff, 0xffff };
+VECT_VAR_DECL(expected_sh3,uint,32,2) [] = { 0xffffffff, 0xffffffff };
+
+/* Expected values of cumulative_saturation flag with shift by max
+   amount.  */
+int VECT_VAR(expected_cumulative_sat_shmax,int,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat_shmax,int,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat_shmax,int,64,2) = 1;
+int VECT_VAR(expected_cumulative_sat_shmax,uint,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat_shmax,uint,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat_shmax,uint,64,2) = 1;
+
+/* Expected results with shift by max amount.  */
+VECT_VAR_DECL(expected_shmax,int,8,8) [] = { 0x7f, 0x7f, 0x7f, 0x7f,
+					     0x7f, 0x7f, 0x7f, 0x7f };
+VECT_VAR_DECL(expected_shmax,int,16,4) [] = { 0x7fff, 0x7fff, 0x7fff, 0x7fff };
+VECT_VAR_DECL(expected_shmax,int,32,2) [] = { 0x7fffffff, 0x7fffffff };
+VECT_VAR_DECL(expected_shmax,uint,8,8) [] = { 0xff, 0xff, 0xff, 0xff,
+					      0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_shmax,uint,16,4) [] = { 0xffff, 0xffff, 0xffff, 0xffff };
+VECT_VAR_DECL(expected_shmax,uint,32,2) [] = { 0xffffffff, 0xffffffff };
+
+#define INSN vqrshrn_n
+#define TEST_MSG "VQRSHRN_N"
+
+#define FNNAME1(NAME) void exec_ ## NAME (void)
+#define FNNAME(NAME) FNNAME1(NAME)
+
+FNNAME (INSN)
+{
+  /* Basic test: y=vqrshrn_n(x,v), then store the result.  */
+#define TEST_VQRSHRN_N2(INSN, T1, T2, W, W2, N, V, EXPECTED_CUMULATIVE_SAT, CMT) \
+  Set_Neon_Cumulative_Sat(0, VECT_VAR(vector_res, T1, W2, N));		\
+  VECT_VAR(vector_res, T1, W2, N) =					\
+    INSN##_##T2##W(VECT_VAR(vector, T1, W, N),				\
+		   V);							\
+  vst1_##T2##W2(VECT_VAR(result, T1, W2, N),				\
+		VECT_VAR(vector_res, T1, W2, N));			\
+  CHECK_CUMULATIVE_SAT(TEST_MSG, T1, W, N, EXPECTED_CUMULATIVE_SAT, CMT)
+
+  /* Two auxliary macros are necessary to expand INSN */
+#define TEST_VQRSHRN_N1(INSN, T1, T2, W, W2, N, V, EXPECTED_CUMULATIVE_SAT, CMT) \
+  TEST_VQRSHRN_N2(INSN, T1, T2, W, W2, N, V, EXPECTED_CUMULATIVE_SAT, CMT)
+
+#define TEST_VQRSHRN_N(T1, T2, W, W2, N, V, EXPECTED_CUMULATIVE_SAT, CMT) \
+  TEST_VQRSHRN_N1(INSN, T1, T2, W, W2, N, V, EXPECTED_CUMULATIVE_SAT, CMT)
+
+
+  /* vector is twice as large as vector_res.  */
+  DECL_VARIABLE(vector, int, 16, 8);
+  DECL_VARIABLE(vector, int, 32, 4);
+  DECL_VARIABLE(vector, int, 64, 2);
+  DECL_VARIABLE(vector, uint, 16, 8);
+  DECL_VARIABLE(vector, uint, 32, 4);
+  DECL_VARIABLE(vector, uint, 64, 2);
+
+  DECL_VARIABLE(vector_res, int, 8, 8);
+  DECL_VARIABLE(vector_res, int, 16, 4);
+  DECL_VARIABLE(vector_res, int, 32, 2);
+  DECL_VARIABLE(vector_res, uint, 8, 8);
+  DECL_VARIABLE(vector_res, uint, 16, 4);
+  DECL_VARIABLE(vector_res, uint, 32, 2);
+
+  clean_results ();
+
+  VLOAD(vector, buffer, q, int, s, 16, 8);
+  VLOAD(vector, buffer, q, int, s, 32, 4);
+  VLOAD(vector, buffer, q, int, s, 64, 2);
+  VLOAD(vector, buffer, q, uint, u, 16, 8);
+  VLOAD(vector, buffer, q, uint, u, 32, 4);
+  VLOAD(vector, buffer, q, uint, u, 64, 2);
+
+  /* Choose shift amount arbitrarily.  */
+#define CMT ""
+  TEST_VQRSHRN_N(int, s, 16, 8, 8, 1, expected_cumulative_sat, CMT);
+  TEST_VQRSHRN_N(int, s, 32, 16, 4, 1, expected_cumulative_sat, CMT);
+  TEST_VQRSHRN_N(int, s, 64, 32, 2, 2, expected_cumulative_sat, CMT);
+  TEST_VQRSHRN_N(uint, u, 16, 8, 8, 2, expected_cumulative_sat, CMT);
+  TEST_VQRSHRN_N(uint, u, 32, 16, 4, 3, expected_cumulative_sat, CMT);
+  TEST_VQRSHRN_N(uint, u, 64, 32, 2, 3, expected_cumulative_sat, CMT);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected, CMT);
+
+
+  /* Another set of tests, shifting max value by 3.  */
+  VDUP(vector, q, int, s, 16, 8, 0x7FFF);
+  VDUP(vector, q, int, s, 32, 4, 0x7FFFFFFF);
+  VDUP(vector, q, int, s, 64, 2, 0x7FFFFFFFFFFFFFFFLL);
+  VDUP(vector, q, uint, u, 16, 8, 0xFFFF);
+  VDUP(vector, q, uint, u, 32, 4, 0xFFFFFFFF);
+  VDUP(vector, q, uint, u, 64, 2, 0xFFFFFFFFFFFFFFFFULL);
+
+#undef CMT
+#define CMT " (check saturation: shift by 3)"
+  TEST_VQRSHRN_N(int, s, 16, 8, 8, 3, expected_cumulative_sat_sh3, CMT);
+  TEST_VQRSHRN_N(int, s, 32, 16, 4, 3, expected_cumulative_sat_sh3, CMT);
+  TEST_VQRSHRN_N(int, s, 64, 32, 2, 3, expected_cumulative_sat_sh3, CMT);
+  TEST_VQRSHRN_N(uint, u, 16, 8, 8, 3, expected_cumulative_sat_sh3, CMT);
+  TEST_VQRSHRN_N(uint, u, 32, 16, 4, 3, expected_cumulative_sat_sh3, CMT);
+  TEST_VQRSHRN_N(uint, u, 64, 32, 2, 3, expected_cumulative_sat_sh3, CMT);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_sh3, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_sh3, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_sh3, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_sh3, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_sh3, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_sh3, CMT);
+
+
+  /* Shift by max amount.  */
+#undef CMT
+#define CMT " (check saturation: shift by max)"
+  TEST_VQRSHRN_N(int, s, 16, 8, 8, 8, expected_cumulative_sat_shmax, CMT);
+  TEST_VQRSHRN_N(int, s, 32, 16, 4, 16, expected_cumulative_sat_shmax, CMT);
+  TEST_VQRSHRN_N(int, s, 64, 32, 2, 32, expected_cumulative_sat_shmax, CMT);
+  TEST_VQRSHRN_N(uint, u, 16, 8, 8, 8, expected_cumulative_sat_shmax, CMT);
+  TEST_VQRSHRN_N(uint, u, 32, 16, 4, 16, expected_cumulative_sat_shmax, CMT);
+  TEST_VQRSHRN_N(uint, u, 64, 32, 2, 32, expected_cumulative_sat_shmax, CMT);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_shmax, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_shmax, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_shmax, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_shmax, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_shmax, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_shmax, CMT);
+}
+
+int main (void)
+{
+  exec_vqrshrn_n ();
+  return 0;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vqrshrun_n.c
@@ -0,0 +1,189 @@
+#include <arm_neon.h>
+#include "arm-neon-ref.h"
+#include "compute-ref-data.h"
+
+/* Expected values of cumulative_saturation flag with negative unput.  */
+int VECT_VAR(expected_cumulative_sat_neg,int,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,int,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,int,64,2) = 1;
+
+/* Expected results with negative input.  */
+VECT_VAR_DECL(expected_neg,uint,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					    0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_neg,uint,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_neg,uint,32,2) [] = { 0x0, 0x0 };
+
+/* Expected values of cumulative_saturation flag with max input value
+   shifted by 1.  */
+int VECT_VAR(expected_cumulative_sat_max_sh1,int,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat_max_sh1,int,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat_max_sh1,int,64,2) = 1;
+
+/* Expected results with max input value shifted by 1.  */
+VECT_VAR_DECL(expected_max_sh1,uint,8,8) [] = { 0xff, 0xff, 0xff, 0xff,
+						0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_max_sh1,uint,16,4) [] = { 0xffff, 0xffff,
+						 0xffff, 0xffff };
+VECT_VAR_DECL(expected_max_sh1,uint,32,2) [] = { 0xffffffff, 0xffffffff };
+VECT_VAR_DECL(expected_max_sh1,uint,64,1) [] = { 0x3333333333333333 };
+
+/* Expected values of cumulative_saturation flag with max input value
+   shifted by max amount.  */
+int VECT_VAR(expected_cumulative_sat_max_shmax,int,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_max_shmax,int,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat_max_shmax,int,64,2) = 0;
+
+/* Expected results with max input value shifted by max amount.  */
+VECT_VAR_DECL(expected_max_shmax,uint,8,8) [] = { 0x80, 0x80, 0x80, 0x80,
+						  0x80, 0x80, 0x80, 0x80 };
+VECT_VAR_DECL(expected_max_shmax,uint,16,4) [] = { 0x8000, 0x8000,
+						   0x8000, 0x8000 };
+VECT_VAR_DECL(expected_max_shmax,uint,32,2) [] = { 0x80000000, 0x80000000 };
+
+/* Expected values of cumulative_saturation flag with min input value
+   shifted by max amount.  */
+int VECT_VAR(expected_cumulative_sat_min_shmax,int,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat_min_shmax,int,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat_min_shmax,int,64,2) = 1;
+
+/* Expected results with min input value shifted by max amount.  */
+VECT_VAR_DECL(expected_min_shmax,uint,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+						  0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_min_shmax,uint,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_min_shmax,uint,32,2) [] = { 0x0, 0x0 };
+
+/* Expected values of cumulative_saturation flag with inputs in usual
+   range.  */
+int VECT_VAR(expected_cumulative_sat,int,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat,int,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat,int,64,2) = 0;
+
+/* Expected results with inputs in usual range.  */
+VECT_VAR_DECL(expected,uint,8,8) [] = { 0x49, 0x49, 0x49, 0x49,
+					0x49, 0x49, 0x49, 0x49 };
+VECT_VAR_DECL(expected,uint,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected,uint,32,2) [] = { 0xdeadbf, 0xdeadbf };
+
+#define INSN vqrshrun_n
+#define TEST_MSG "VQRSHRUN_N"
+
+#define FNNAME1(NAME) void exec_ ## NAME (void)
+#define FNNAME(NAME) FNNAME1(NAME)
+
+FNNAME (INSN)
+{
+  /* Basic test: y=vqrshrun_n(x,v), then store the result.  */
+#define TEST_VQRSHRUN_N2(INSN, T1, T2, W, W2, N, V, EXPECTED_CUMULATIVE_SAT, CMT) \
+  Set_Neon_Cumulative_Sat(0, VECT_VAR(vector_res, uint, W2, N));	\
+  VECT_VAR(vector_res, uint, W2, N) =					\
+    INSN##_##T2##W(VECT_VAR(vector, T1, W, N),				\
+		   V);							\
+  vst1_u##W2(VECT_VAR(result, uint, W2, N),				\
+	     VECT_VAR(vector_res, uint, W2, N));			\
+  CHECK_CUMULATIVE_SAT(TEST_MSG, T1, W, N, EXPECTED_CUMULATIVE_SAT, CMT)
+
+  /* Two auxliary macros are necessary to expand INSN */
+#define TEST_VQRSHRUN_N1(INSN, T1, T2, W, W2, N, V, EXPECTED_CUMULATIVE_SAT, CMT) \
+  TEST_VQRSHRUN_N2(INSN, T1, T2, W, W2, N, V, EXPECTED_CUMULATIVE_SAT, CMT)
+
+#define TEST_VQRSHRUN_N(T1, T2, W, W2, N, V, EXPECTED_CUMULATIVE_SAT, CMT) \
+  TEST_VQRSHRUN_N1(INSN, T1, T2, W, W2, N, V, EXPECTED_CUMULATIVE_SAT, CMT)
+
+
+  /* vector is twice as large as vector_res.  */
+  DECL_VARIABLE(vector, int, 16, 8);
+  DECL_VARIABLE(vector, int, 32, 4);
+  DECL_VARIABLE(vector, int, 64, 2);
+
+  DECL_VARIABLE(vector_res, uint, 8, 8);
+  DECL_VARIABLE(vector_res, uint, 16, 4);
+  DECL_VARIABLE(vector_res, uint, 32, 2);
+
+  clean_results ();
+
+  /* Fill input vector with negative values, to check saturation on
+     limits.  */
+  VDUP(vector, q, int, s, 16, 8, -2);
+  VDUP(vector, q, int, s, 32, 4, -3);
+  VDUP(vector, q, int, s, 64, 2, -4);
+
+  /* Choose shift amount arbitrarily.   */
+#define CMT " (negative input)"
+  TEST_VQRSHRUN_N(int, s, 16, 8, 8, 3, expected_cumulative_sat_neg, CMT);
+  TEST_VQRSHRUN_N(int, s, 32, 16, 4, 4, expected_cumulative_sat_neg, CMT);
+  TEST_VQRSHRUN_N(int, s, 64, 32, 2, 2, expected_cumulative_sat_neg, CMT);
+
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_neg, CMT);
+
+
+  /* Fill input vector with max value, to check saturation on
+     limits.  */
+  VDUP(vector, q, int, s, 16, 8, 0x7FFF);
+  VDUP(vector, q, int, s, 32, 4, 0x7FFFFFFF);
+  VDUP(vector, q, int, s, 64, 2, 0x7FFFFFFFFFFFFFFFLL);
+
+  /* shift by 1.  */
+#undef CMT
+#define CMT " (check cumulative saturation: shift by 1)"
+  TEST_VQRSHRUN_N(int, s, 16, 8, 8, 1, expected_cumulative_sat_max_sh1, CMT);
+  TEST_VQRSHRUN_N(int, s, 32, 16, 4, 1, expected_cumulative_sat_max_sh1, CMT);
+  TEST_VQRSHRUN_N(int, s, 64, 32, 2, 1, expected_cumulative_sat_max_sh1, CMT);
+
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_max_sh1, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_max_sh1, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_max_sh1, CMT);
+
+
+  /* shift by max.  */
+#undef CMT
+#define CMT " (check cumulative saturation: shift by max, positive input)"
+  TEST_VQRSHRUN_N(int, s, 16, 8, 8, 8, expected_cumulative_sat_max_shmax, CMT);
+  TEST_VQRSHRUN_N(int, s, 32, 16, 4, 16, expected_cumulative_sat_max_shmax, CMT);
+  TEST_VQRSHRUN_N(int, s, 64, 32, 2, 32, expected_cumulative_sat_max_shmax, CMT);
+
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_max_shmax, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_max_shmax, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_max_shmax, CMT);
+
+
+  /* Fill input vector with min value, to check saturation on limits.  */
+  VDUP(vector, q, int, s, 16, 8, 0x8000);
+  VDUP(vector, q, int, s, 32, 4, 0x80000000);
+  VDUP(vector, q, int, s, 64, 2, 0x8000000000000000LL);
+
+  /* shift by max  */
+#undef CMT
+#define CMT " (check cumulative saturation: shift by max, negative input)"
+  TEST_VQRSHRUN_N(int, s, 16, 8, 8, 8, expected_cumulative_sat_min_shmax, CMT);
+  TEST_VQRSHRUN_N(int, s, 32, 16, 4, 16, expected_cumulative_sat_min_shmax, CMT);
+  TEST_VQRSHRUN_N(int, s, 64, 32, 2, 32, expected_cumulative_sat_min_shmax, CMT);
+
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_min_shmax, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_min_shmax, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_min_shmax, CMT);
+
+
+  /* Fill input vector with positive values, to check normal case.  */
+  VDUP(vector, q, int, s, 16, 8, 0x1234);
+  VDUP(vector, q, int, s, 32, 4, 0x87654321);
+  VDUP(vector, q, int, s, 64, 2, 0xDEADBEEF);
+
+  /* shift arbitrary amount.  */
+#undef CMT
+#define CMT ""
+  TEST_VQRSHRUN_N(int, s, 16, 8, 8, 6, expected_cumulative_sat, CMT);
+  TEST_VQRSHRUN_N(int, s, 32, 16, 4, 7, expected_cumulative_sat, CMT);
+  TEST_VQRSHRUN_N(int, s, 64, 32, 2, 8, expected_cumulative_sat, CMT);
+
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected, CMT);
+}
+
+int main (void)
+{
+  exec_vqrshrun_n ();
+  return 0;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vqshl.c
@@ -0,0 +1,829 @@
+#include <arm_neon.h>
+#include "arm-neon-ref.h"
+#include "compute-ref-data.h"
+
+/* Expected values of cumulative_saturation flag with input=0.  */
+int VECT_VAR(expected_cumulative_sat_0,int,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat_0,int,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat_0,int,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat_0,int,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat_0,uint,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat_0,uint,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat_0,uint,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat_0,uint,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat_0,int,8,16) = 0;
+int VECT_VAR(expected_cumulative_sat_0,int,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_0,int,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat_0,int,64,2) = 0;
+int VECT_VAR(expected_cumulative_sat_0,uint,8,16) = 0;
+int VECT_VAR(expected_cumulative_sat_0,uint,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_0,uint,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat_0,uint,64,2) = 0;
+
+/* Expected results with input=0.  */
+VECT_VAR_DECL(expected_0,int,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,int,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,int,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,int,64,1) [] = { 0x0 };
+VECT_VAR_DECL(expected_0,uint,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					  0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,uint,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,uint,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,uint,64,1) [] = { 0x0 };
+VECT_VAR_DECL(expected_0,int,8,16) [] = { 0x0, 0x0, 0x0, 0x0,
+					  0x0, 0x0, 0x0, 0x0,
+					  0x0, 0x0, 0x0, 0x0,
+					  0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,int,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					  0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,int,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,int,64,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,uint,8,16) [] = { 0x0, 0x0, 0x0, 0x0,
+					   0x0, 0x0, 0x0, 0x0,
+					   0x0, 0x0, 0x0, 0x0,
+					   0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,uint,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					   0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,uint,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,uint,64,2) [] = { 0x0, 0x0 };
+
+/* Expected values of cumulative_saturation flag with input=0 and
+   negative shift amount.  */
+int VECT_VAR(expected_cumulative_sat_0_neg,int,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,int,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,int,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,int,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,uint,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,uint,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,uint,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,uint,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,int,8,16) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,int,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,int,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,int,64,2) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,uint,8,16) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,uint,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,uint,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,uint,64,2) = 0;
+
+/* Expected results with input=0 and negative shift amount.  */
+VECT_VAR_DECL(expected_0_neg,int,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					     0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_neg,int,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_neg,int,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_neg,int,64,1) [] = { 0x0 };
+VECT_VAR_DECL(expected_0_neg,uint,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					      0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_neg,uint,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_neg,uint,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_neg,uint,64,1) [] = { 0x0 };
+VECT_VAR_DECL(expected_0_neg,int,8,16) [] = { 0x0, 0x0, 0x0, 0x0,
+					      0x0, 0x0, 0x0, 0x0,
+					      0x0, 0x0, 0x0, 0x0,
+					      0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_neg,int,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					      0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_neg,int,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_neg,int,64,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_neg,uint,8,16) [] = { 0x0, 0x0, 0x0, 0x0,
+					       0x0, 0x0, 0x0, 0x0,
+					       0x0, 0x0, 0x0, 0x0,
+					       0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_neg,uint,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					       0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_neg,uint,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_neg,uint,64,2) [] = { 0x0, 0x0 };
+
+/* Expected values of cumulative_saturation flag.  */
+int VECT_VAR(expected_cumulative_sat,int,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat,int,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat,int,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat,int,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat,uint,8,8) = 1;
+int VECT_VAR(expected_cumulative_sat,uint,16,4) = 1;
+int VECT_VAR(expected_cumulative_sat,uint,32,2) = 1;
+int VECT_VAR(expected_cumulative_sat,uint,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat,int,8,16) = 1;
+int VECT_VAR(expected_cumulative_sat,int,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat,int,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat,int,64,2) = 1;
+int VECT_VAR(expected_cumulative_sat,uint,8,16) = 1;
+int VECT_VAR(expected_cumulative_sat,uint,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat,uint,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat,uint,64,2) = 1;
+
+/* Expected results.  */
+VECT_VAR_DECL(expected,int,8,8) [] = { 0xe0, 0xe2, 0xe4, 0xe6,
+				       0xe8, 0xea, 0xec, 0xee };
+VECT_VAR_DECL(expected,int,16,4) [] = { 0xff80, 0xff88, 0xff90, 0xff98 };
+VECT_VAR_DECL(expected,int,32,2) [] = { 0xfffff000, 0xfffff100 };
+VECT_VAR_DECL(expected,int,64,1) [] = { 0xfffffffffffffffe };
+VECT_VAR_DECL(expected,uint,8,8) [] = { 0xff, 0xff, 0xff, 0xff,
+					0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected,uint,16,4) [] = { 0xffff, 0xffff, 0xffff, 0xffff };
+VECT_VAR_DECL(expected,uint,32,2) [] = { 0xffffffff, 0xffffffff };
+VECT_VAR_DECL(expected,uint,64,1) [] = { 0x1ffffffffffffffe };
+VECT_VAR_DECL(expected,int,8,16) [] = { 0x80, 0x80, 0x80, 0x80,
+					0x80, 0x80, 0x80, 0x80,
+					0x80, 0x80, 0x80, 0x80,
+					0x80, 0x80, 0x80, 0x80 };
+VECT_VAR_DECL(expected,int,16,8) [] = { 0x8000, 0x8000, 0x8000, 0x8000,
+					0x8000, 0x8000, 0x8000, 0x8000 };
+VECT_VAR_DECL(expected,int,32,4) [] = { 0x80000000, 0x80000000,
+					0x80000000, 0x80000000 };
+VECT_VAR_DECL(expected,int,64,2) [] = { 0x8000000000000000,
+					0x8000000000000000 };
+VECT_VAR_DECL(expected,uint,8,16) [] = { 0xff, 0xff, 0xff, 0xff,
+					 0xff, 0xff, 0xff, 0xff,
+					 0xff, 0xff, 0xff, 0xff,
+					 0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected,uint,16,8) [] = { 0xffff, 0xffff, 0xffff, 0xffff,
+					 0xffff, 0xffff, 0xffff, 0xffff };
+VECT_VAR_DECL(expected,uint,32,4) [] = { 0xffffffff, 0xffffffff,
+					 0xffffffff, 0xffffffff };
+VECT_VAR_DECL(expected,uint,64,2) [] = { 0xffffffffffffffff,
+					 0xffffffffffffffff };
+
+/* Expected values of cumulative_sat_saturation flag with negative shift
+   amount.  */
+int VECT_VAR(expected_cumulative_sat_neg,int,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,int,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,int,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,int,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,uint,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,uint,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,uint,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,uint,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,int,8,16) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,int,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,int,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,int,64,2) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,uint,8,16) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,uint,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,uint,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,uint,64,2) = 0;
+
+/* Expected results with negative shift amount.  */
+VECT_VAR_DECL(expected_neg,int,8,8) [] = { 0xf8, 0xf8, 0xf9, 0xf9,
+					   0xfa, 0xfa, 0xfb, 0xfb };
+VECT_VAR_DECL(expected_neg,int,16,4) [] = { 0xfffc, 0xfffc, 0xfffc, 0xfffc };
+VECT_VAR_DECL(expected_neg,int,32,2) [] = { 0xfffffffe, 0xfffffffe };
+VECT_VAR_DECL(expected_neg,int,64,1) [] = { 0xffffffffffffffff };
+VECT_VAR_DECL(expected_neg,uint,8,8) [] = { 0x78, 0x78, 0x79, 0x79,
+					    0x7a, 0x7a, 0x7b, 0x7b };
+VECT_VAR_DECL(expected_neg,uint,16,4) [] = { 0x3ffc, 0x3ffc, 0x3ffc, 0x3ffc };
+VECT_VAR_DECL(expected_neg,uint,32,2) [] = { 0x1ffffffe, 0x1ffffffe };
+VECT_VAR_DECL(expected_neg,uint,64,1) [] = { 0xfffffffffffffff };
+VECT_VAR_DECL(expected_neg,int,8,16) [] = { 0xff, 0xff, 0xff, 0xff,
+					    0xff, 0xff, 0xff, 0xff,
+					    0xff, 0xff, 0xff, 0xff,
+					    0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_neg,int,16,8) [] = { 0xffff, 0xffff, 0xffff, 0xffff,
+					    0xffff, 0xffff, 0xffff, 0xffff };
+VECT_VAR_DECL(expected_neg,int,32,4) [] = { 0xffffffff, 0xffffffff,
+					    0xffffffff, 0xffffffff };
+VECT_VAR_DECL(expected_neg,int,64,2) [] = { 0xffffffffffffffff,
+					    0xffffffffffffffff };
+VECT_VAR_DECL(expected_neg,uint,8,16) [] = { 0x1, 0x1, 0x1, 0x1,
+					     0x1, 0x1, 0x1, 0x1,
+					     0x1, 0x1, 0x1, 0x1,
+					     0x1, 0x1, 0x1, 0x1 };
+VECT_VAR_DECL(expected_neg,uint,16,8) [] = { 0x1f, 0x1f, 0x1f, 0x1f,
+					     0x1f, 0x1f, 0x1f, 0x1f };
+VECT_VAR_DECL(expected_neg,uint,32,4) [] = { 0x7ffff, 0x7ffff,
+					     0x7ffff, 0x7ffff };
+VECT_VAR_DECL(expected_neg,uint,64,2) [] = { 0xfffffffffff, 0xfffffffffff };
+
+/* Expected values of cumulative_sat_saturation flag with negative
+   input and large shift amount.  */
+int VECT_VAR(expected_cumulative_sat_neg_large,int,8,8) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large,int,16,4) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large,int,32,2) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large,int,64,1) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large,uint,8,8) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large,uint,16,4) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large,uint,32,2) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large,uint,64,1) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large,int,8,16) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large,int,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large,int,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large,int,64,2) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large,uint,8,16) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large,uint,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large,uint,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large,uint,64,2) = 1;
+
+/* Expected results with negative input and large shift amount.  */
+VECT_VAR_DECL(expected_neg_large,int,8,8) [] = { 0x80, 0x80, 0x80, 0x80,
+						 0x80, 0x80, 0x80, 0x80 };
+VECT_VAR_DECL(expected_neg_large,int,16,4) [] = { 0x8000, 0x8000,
+						  0x8000, 0x8000 };
+VECT_VAR_DECL(expected_neg_large,int,32,2) [] = { 0x80000000, 0x80000000 };
+VECT_VAR_DECL(expected_neg_large,int,64,1) [] = { 0x8000000000000000 };
+VECT_VAR_DECL(expected_neg_large,uint,8,8) [] = { 0xff, 0xff, 0xff, 0xff,
+						  0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_neg_large,uint,16,4) [] = { 0xffff, 0xffff,
+						   0xffff, 0xffff };
+VECT_VAR_DECL(expected_neg_large,uint,32,2) [] = { 0xffffffff, 0xffffffff };
+VECT_VAR_DECL(expected_neg_large,uint,64,1) [] = { 0xffffffffffffffff };
+VECT_VAR_DECL(expected_neg_large,int,8,16) [] = { 0x80, 0x80, 0x80, 0x80,
+						  0x80, 0x80, 0x80, 0x80,
+						  0x80, 0x80, 0x80, 0x80,
+						  0x80, 0x80, 0x80, 0x80 };
+VECT_VAR_DECL(expected_neg_large,int,16,8) [] = { 0x8000, 0x8000,
+						  0x8000, 0x8000,
+						  0x8000, 0x8000,
+						  0x8000, 0x8000 };
+VECT_VAR_DECL(expected_neg_large,int,32,4) [] = { 0x80000000, 0x80000000,
+						  0x80000000, 0x80000000 };
+VECT_VAR_DECL(expected_neg_large,int,64,2) [] = { 0x8000000000000000,
+						  0x8000000000000000 };
+VECT_VAR_DECL(expected_neg_large,uint,8,16) [] = { 0xff, 0xff, 0xff, 0xff,
+						   0xff, 0xff, 0xff, 0xff,
+						   0xff, 0xff, 0xff, 0xff,
+						   0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_neg_large,uint,16,8) [] = { 0xffff, 0xffff,
+						   0xffff, 0xffff,
+						   0xffff, 0xffff,
+						   0xffff, 0xffff };
+VECT_VAR_DECL(expected_neg_large,uint,32,4) [] = { 0xffffffff, 0xffffffff,
+						   0xffffffff, 0xffffffff };
+VECT_VAR_DECL(expected_neg_large,uint,64,2) [] = { 0xffffffffffffffff,
+						   0xffffffffffffffff };
+
+/* Expected values of cumulative_sat_saturation flag with max input
+   and shift by -1.  */
+int VECT_VAR(expected_cumulative_sat_max_minus1,int,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat_max_minus1,int,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat_max_minus1,int,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat_max_minus1,int,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat_max_minus1,uint,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat_max_minus1,uint,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat_max_minus1,uint,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat_max_minus1,uint,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat_max_minus1,int,8,16) = 0;
+int VECT_VAR(expected_cumulative_sat_max_minus1,int,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_max_minus1,int,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat_max_minus1,int,64,2) = 0;
+int VECT_VAR(expected_cumulative_sat_max_minus1,uint,8,16) = 0;
+int VECT_VAR(expected_cumulative_sat_max_minus1,uint,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_max_minus1,uint,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat_max_minus1,uint,64,2) = 0;
+
+/* Expected results with max input and shift by -1.  */
+VECT_VAR_DECL(expected_max_minus1,int,8,8) [] = { 0x3f, 0x3f, 0x3f, 0x3f,
+						  0x3f, 0x3f, 0x3f, 0x3f };
+VECT_VAR_DECL(expected_max_minus1,int,16,4) [] = { 0x3fff, 0x3fff,
+						   0x3fff, 0x3fff };
+VECT_VAR_DECL(expected_max_minus1,int,32,2) [] = { 0x3fffffff, 0x3fffffff };
+VECT_VAR_DECL(expected_max_minus1,int,64,1) [] = { 0x3fffffffffffffff };
+VECT_VAR_DECL(expected_max_minus1,uint,8,8) [] = { 0x7f, 0x7f, 0x7f, 0x7f,
+						   0x7f, 0x7f, 0x7f, 0x7f };
+VECT_VAR_DECL(expected_max_minus1,uint,16,4) [] = { 0x7fff, 0x7fff,
+						    0x7fff, 0x7fff };
+VECT_VAR_DECL(expected_max_minus1,uint,32,2) [] = { 0x7fffffff, 0x7fffffff };
+VECT_VAR_DECL(expected_max_minus1,uint,64,1) [] = { 0x7fffffffffffffff };
+VECT_VAR_DECL(expected_max_minus1,int,8,16) [] = { 0x3f, 0x3f, 0x3f, 0x3f,
+						   0x3f, 0x3f, 0x3f, 0x3f,
+						   0x3f, 0x3f, 0x3f, 0x3f,
+						   0x3f, 0x3f, 0x3f, 0x3f };
+VECT_VAR_DECL(expected_max_minus1,int,16,8) [] = { 0x3fff, 0x3fff,
+						   0x3fff, 0x3fff,
+						   0x3fff, 0x3fff,
+						   0x3fff, 0x3fff };
+VECT_VAR_DECL(expected_max_minus1,int,32,4) [] = { 0x3fffffff, 0x3fffffff,
+						   0x3fffffff, 0x3fffffff };
+VECT_VAR_DECL(expected_max_minus1,int,64,2) [] = { 0x3fffffffffffffff,
+						   0x3fffffffffffffff };
+VECT_VAR_DECL(expected_max_minus1,uint,8,16) [] = { 0x7f, 0x7f, 0x7f, 0x7f,
+						    0x7f, 0x7f, 0x7f, 0x7f,
+						    0x7f, 0x7f, 0x7f, 0x7f,
+						    0x7f, 0x7f, 0x7f, 0x7f };
+VECT_VAR_DECL(expected_max_minus1,uint,16,8) [] = { 0x7fff, 0x7fff,
+						    0x7fff, 0x7fff,
+						    0x7fff, 0x7fff,
+						    0x7fff, 0x7fff };
+VECT_VAR_DECL(expected_max_minus1,uint,32,4) [] = { 0x7fffffff, 0x7fffffff,
+						    0x7fffffff, 0x7fffffff };
+VECT_VAR_DECL(expected_max_minus1,uint,64,2) [] = { 0x7fffffffffffffff,
+						    0x7fffffffffffffff };
+
+/* Expected values of cumulative_sat_saturation flag with max input
+   and large shift amount.  */
+int VECT_VAR(expected_cumulative_sat_max_large,int,8,8) = 1;
+int VECT_VAR(expected_cumulative_sat_max_large,int,16,4) = 1;
+int VECT_VAR(expected_cumulative_sat_max_large,int,32,2) = 1;
+int VECT_VAR(expected_cumulative_sat_max_large,int,64,1) = 1;
+int VECT_VAR(expected_cumulative_sat_max_large,uint,8,8) = 1;
+int VECT_VAR(expected_cumulative_sat_max_large,uint,16,4) = 1;
+int VECT_VAR(expected_cumulative_sat_max_large,uint,32,2) = 1;
+int VECT_VAR(expected_cumulative_sat_max_large,uint,64,1) = 1;
+int VECT_VAR(expected_cumulative_sat_max_large,int,8,16) = 1;
+int VECT_VAR(expected_cumulative_sat_max_large,int,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat_max_large,int,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat_max_large,int,64,2) = 1;
+int VECT_VAR(expected_cumulative_sat_max_large,uint,8,16) = 1;
+int VECT_VAR(expected_cumulative_sat_max_large,uint,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat_max_large,uint,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat_max_large,uint,64,2) = 1;
+
+/* Expected results with max input and large shift amount.  */
+VECT_VAR_DECL(expected_max_large,int,8,8) [] = { 0x7f, 0x7f, 0x7f, 0x7f,
+					       0x7f, 0x7f, 0x7f, 0x7f };
+VECT_VAR_DECL(expected_max_large,int,16,4) [] = { 0x7fff, 0x7fff,
+						0x7fff, 0x7fff };
+VECT_VAR_DECL(expected_max_large,int,32,2) [] = { 0x7fffffff, 0x7fffffff };
+VECT_VAR_DECL(expected_max_large,int,64,1) [] = { 0x7fffffffffffffff };
+VECT_VAR_DECL(expected_max_large,uint,8,8) [] = { 0xff, 0xff, 0xff, 0xff,
+						0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_max_large,uint,16,4) [] = { 0xffff, 0xffff,
+						 0xffff, 0xffff };
+VECT_VAR_DECL(expected_max_large,uint,32,2) [] = { 0xffffffff, 0xffffffff };
+VECT_VAR_DECL(expected_max_large,uint,64,1) [] = { 0xffffffffffffffff };
+VECT_VAR_DECL(expected_max_large,int,8,16) [] = { 0x7f, 0x7f, 0x7f, 0x7f,
+						0x7f, 0x7f, 0x7f, 0x7f,
+						0x7f, 0x7f, 0x7f, 0x7f,
+						0x7f, 0x7f, 0x7f, 0x7f };
+VECT_VAR_DECL(expected_max_large,int,16,8) [] = { 0x7fff, 0x7fff,
+						0x7fff, 0x7fff,
+						0x7fff, 0x7fff,
+						0x7fff, 0x7fff };
+VECT_VAR_DECL(expected_max_large,int,32,4) [] = { 0x7fffffff, 0x7fffffff,
+						0x7fffffff, 0x7fffffff };
+VECT_VAR_DECL(expected_max_large,int,64,2) [] = { 0x7fffffffffffffff,
+						0x7fffffffffffffff };
+VECT_VAR_DECL(expected_max_large,uint,8,16) [] = { 0xff, 0xff, 0xff, 0xff,
+						 0xff, 0xff, 0xff, 0xff,
+						 0xff, 0xff, 0xff, 0xff,
+						 0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_max_large,uint,16,8) [] = { 0xffff, 0xffff,
+						 0xffff, 0xffff,
+						 0xffff, 0xffff,
+						 0xffff, 0xffff };
+VECT_VAR_DECL(expected_max_large,uint,32,4) [] = { 0xffffffff, 0xffffffff,
+						 0xffffffff, 0xffffffff };
+VECT_VAR_DECL(expected_max_large,uint,64,2) [] = { 0xffffffffffffffff,
+						 0xffffffffffffffff };
+
+/* Expected values of cumulative_sat_saturation flag with saturation
+   on 64-bits values.  */
+int VECT_VAR(expected_cumulative_sat_64,int,64,1) = 1;
+int VECT_VAR(expected_cumulative_sat_64,int,64,2) = 1;
+
+/* Expected results with saturation on 64-bits values..  */
+VECT_VAR_DECL(expected_64,int,64,1) [] = { 0x8000000000000000 };
+VECT_VAR_DECL(expected_64,int,64,2) [] = { 0x7fffffffffffffff,
+					   0x7fffffffffffffff };
+
+#define INSN vqshl
+#define TEST_MSG "VQSHL/VQSHLQ"
+
+#define FNNAME1(NAME) void exec_ ## NAME (void)
+#define FNNAME(NAME) FNNAME1(NAME)
+
+FNNAME (INSN)
+{
+  /* Basic test: v3=vqshl(v1,v2), then store the result.  */
+#define TEST_VQSHL2(INSN, T3, Q, T1, T2, W, N, EXPECTED_CUMULATIVE_SAT, CMT) \
+  Set_Neon_Cumulative_Sat(0, VECT_VAR(vector_res, T1, W, N));		\
+  VECT_VAR(vector_res, T1, W, N) =					\
+    INSN##Q##_##T2##W(VECT_VAR(vector, T1, W, N),			\
+		      VECT_VAR(vector_shift, T3, W, N));		\
+  vst1##Q##_##T2##W(VECT_VAR(result, T1, W, N),				\
+		    VECT_VAR(vector_res, T1, W, N));			\
+  CHECK_CUMULATIVE_SAT(TEST_MSG, T1, W, N, EXPECTED_CUMULATIVE_SAT, CMT)
+
+  /* Two auxliary macros are necessary to expand INSN */
+#define TEST_VQSHL1(INSN, T3, Q, T1, T2, W, N, EXPECTED_CUMULATIVE_SAT, CMT) \
+  TEST_VQSHL2(INSN, T3, Q, T1, T2, W, N, EXPECTED_CUMULATIVE_SAT, CMT)
+
+#define TEST_VQSHL(T3, Q, T1, T2, W, N, EXPECTED_CUMULATIVE_SAT, CMT)	\
+  TEST_VQSHL1(INSN, T3, Q, T1, T2, W, N, EXPECTED_CUMULATIVE_SAT, CMT)
+
+
+  DECL_VARIABLE_ALL_VARIANTS(vector);
+  DECL_VARIABLE_ALL_VARIANTS(vector_res);
+
+  DECL_VARIABLE_SIGNED_VARIANTS(vector_shift);
+
+  clean_results ();
+
+  /* Fill input vector with 0, to check saturation on limits.  */
+  VDUP(vector, , int, s, 8, 8, 0);
+  VDUP(vector, , int, s, 16, 4, 0);
+  VDUP(vector, , int, s, 32, 2, 0);
+  VDUP(vector, , int, s, 64, 1, 0);
+  VDUP(vector, , uint, u, 8, 8, 0);
+  VDUP(vector, , uint, u, 16, 4, 0);
+  VDUP(vector, , uint, u, 32, 2, 0);
+  VDUP(vector, , uint, u, 64, 1, 0);
+  VDUP(vector, q, int, s, 8, 16, 0);
+  VDUP(vector, q, int, s, 16, 8, 0);
+  VDUP(vector, q, int, s, 32, 4, 0);
+  VDUP(vector, q, int, s, 64, 2, 0);
+  VDUP(vector, q, uint, u, 8, 16, 0);
+  VDUP(vector, q, uint, u, 16, 8, 0);
+  VDUP(vector, q, uint, u, 32, 4, 0);
+  VDUP(vector, q, uint, u, 64, 2, 0);
+
+  /* Choose init value arbitrarily, will be used as shift amount */
+  /* Use values equal or one-less-than the type width to check
+     behaviour on limits.  */
+
+  /* 64-bits vectors first.  */
+  /* Shift 8-bits lanes by 7...  */
+  VDUP(vector_shift, , int, s, 8, 8, 7);
+  /* ... except: lane 0 (by 6), lane 1 (by 8) and lane 2 (by 9).  */
+  VSET_LANE(vector_shift, , int, s, 8, 8, 0, 6);
+  VSET_LANE(vector_shift, , int, s, 8, 8, 1, 8);
+  VSET_LANE(vector_shift, , int, s, 8, 8, 2, 9);
+
+  /* Shift 16-bits lanes by 15... */
+  VDUP(vector_shift, , int, s, 16, 4, 15);
+  /* ... except: lane 0 (by 14), lane 1 (by 16), and lane 2 (by 17).  */
+  VSET_LANE(vector_shift, , int, s, 16, 4, 0, 14);
+  VSET_LANE(vector_shift, , int, s, 16, 4, 1, 16);
+  VSET_LANE(vector_shift, , int, s, 16, 4, 2, 17);
+
+  /* Shift 32-bits lanes by 31... */
+  VDUP(vector_shift, , int, s, 32, 2, 31);
+  /* ... except lane 1 (by 30).  */
+  VSET_LANE(vector_shift, , int, s, 32, 2, 1, 30);
+
+  /* Shift 64 bits lane by 63.  */
+  VDUP(vector_shift, , int, s, 64, 1, 63);
+
+  /* 128-bits vectors.  */
+  /* Shift 8-bits lanes by 8.  */
+  VDUP(vector_shift, q, int, s, 8, 16, 8);
+  /* Shift 16-bits lanes by 16.  */
+  VDUP(vector_shift, q, int, s, 16, 8, 16);
+  /* Shift 32-bits lanes by 32...  */
+  VDUP(vector_shift, q, int, s, 32, 4, 32);
+  /* ... except lane 1 (by 33).  */
+  VSET_LANE(vector_shift, q, int, s, 32, 4, 1, 33);
+
+  /* Shift 64-bits lanes by 64... */
+  VDUP(vector_shift, q, int, s, 64, 2, 64);
+  /* ... except lane 1 (by 62).  */
+  VSET_LANE(vector_shift, q, int, s, 64, 2, 1, 62);
+
+#define CMT " (with input = 0)"
+  TEST_VQSHL(int, , int, s, 8, 8, expected_cumulative_sat_0, CMT);
+  TEST_VQSHL(int, , int, s, 16, 4, expected_cumulative_sat_0, CMT);
+  TEST_VQSHL(int, , int, s, 32, 2, expected_cumulative_sat_0, CMT);
+  TEST_VQSHL(int, , int, s, 64, 1, expected_cumulative_sat_0, CMT);
+  TEST_VQSHL(int, , uint, u, 8, 8, expected_cumulative_sat_0, CMT);
+  TEST_VQSHL(int, , uint, u, 16, 4, expected_cumulative_sat_0, CMT);
+  TEST_VQSHL(int, , uint, u, 32, 2, expected_cumulative_sat_0, CMT);
+  TEST_VQSHL(int, , uint, u, 64, 1, expected_cumulative_sat_0, CMT);
+  TEST_VQSHL(int, q, int, s, 8, 16, expected_cumulative_sat_0, CMT);
+  TEST_VQSHL(int, q, int, s, 16, 8, expected_cumulative_sat_0, CMT);
+  TEST_VQSHL(int, q, int, s, 32, 4, expected_cumulative_sat_0, CMT);
+  TEST_VQSHL(int, q, int, s, 64, 2, expected_cumulative_sat_0, CMT);
+  TEST_VQSHL(int, q, uint, u, 8, 16, expected_cumulative_sat_0, CMT);
+  TEST_VQSHL(int, q, uint, u, 16, 8, expected_cumulative_sat_0, CMT);
+  TEST_VQSHL(int, q, uint, u, 32, 4, expected_cumulative_sat_0, CMT);
+  TEST_VQSHL(int, q, uint, u, 64, 2, expected_cumulative_sat_0, CMT);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_0, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_0, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_0, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_0, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_0, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_0, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_0, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_0, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_0, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_0, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_0, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_0, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_0, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_0, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_0, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_0, CMT);
+
+
+  /* Use negative shift amounts */
+  VDUP(vector_shift, , int, s, 8, 8, -1);
+  VDUP(vector_shift, , int, s, 16, 4, -2);
+  VDUP(vector_shift, , int, s, 32, 2, -3);
+  VDUP(vector_shift, , int, s, 64, 1, -4);
+  VDUP(vector_shift, q, int, s, 8, 16, -7);
+  VDUP(vector_shift, q, int, s, 16, 8, -11);
+  VDUP(vector_shift, q, int, s, 32, 4, -13);
+  VDUP(vector_shift, q, int, s, 64, 2, -20);
+
+#undef CMT
+#define CMT " (input 0 and negative shift amount)"
+  TEST_VQSHL(int, , int, s, 8, 8, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQSHL(int, , int, s, 16, 4, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQSHL(int, , int, s, 32, 2, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQSHL(int, , int, s, 64, 1, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQSHL(int, , uint, u, 8, 8, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQSHL(int, , uint, u, 16, 4, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQSHL(int, , uint, u, 32, 2, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQSHL(int, , uint, u, 64, 1, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQSHL(int, q, int, s, 8, 16, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQSHL(int, q, int, s, 16, 8, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQSHL(int, q, int, s, 32, 4, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQSHL(int, q, int, s, 64, 2, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQSHL(int, q, uint, u, 8, 16, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQSHL(int, q, uint, u, 16, 8, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQSHL(int, q, uint, u, 32, 4, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQSHL(int, q, uint, u, 64, 2, expected_cumulative_sat_0_neg, CMT);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_0_neg, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_0_neg, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_0_neg, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_0_neg, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_0_neg, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_0_neg, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_0_neg, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_0_neg, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_0_neg, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_0_neg, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_0_neg, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_0_neg, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_0_neg, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_0_neg, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_0_neg, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_0_neg, CMT);
+
+  /* Test again, with predefined input values.  */
+  TEST_MACRO_ALL_VARIANTS_2_5(VLOAD, vector, buffer);
+
+  /* Choose init value arbitrarily, will be used as shift amount.  */
+  VDUP(vector_shift, , int, s, 8, 8, 1);
+  VDUP(vector_shift, , int, s, 16, 4, 3);
+  VDUP(vector_shift, , int, s, 32, 2, 8);
+  VDUP(vector_shift, , int, s, 64, 1, -3);
+  VDUP(vector_shift, q, int, s, 8, 16, 10);
+  VDUP(vector_shift, q, int, s, 16, 8, 12);
+  VDUP(vector_shift, q, int, s, 32, 4, 32);
+  VDUP(vector_shift, q, int, s, 64, 2, 63);
+
+#undef CMT
+#define CMT ""
+  TEST_VQSHL(int, , int, s, 8, 8, expected_cumulative_sat, CMT);
+  TEST_VQSHL(int, , int, s, 16, 4, expected_cumulative_sat, CMT);
+  TEST_VQSHL(int, , int, s, 32, 2, expected_cumulative_sat, CMT);
+  TEST_VQSHL(int, , int, s, 64, 1, expected_cumulative_sat, CMT);
+  TEST_VQSHL(int, , uint, u, 8, 8, expected_cumulative_sat, CMT);
+  TEST_VQSHL(int, , uint, u, 16, 4, expected_cumulative_sat, CMT);
+  TEST_VQSHL(int, , uint, u, 32, 2, expected_cumulative_sat, CMT);
+  TEST_VQSHL(int, , uint, u, 64, 1, expected_cumulative_sat, CMT);
+  TEST_VQSHL(int, q, int, s, 8, 16, expected_cumulative_sat, CMT);
+  TEST_VQSHL(int, q, int, s, 16, 8, expected_cumulative_sat, CMT);
+  TEST_VQSHL(int, q, int, s, 32, 4, expected_cumulative_sat, CMT);
+  TEST_VQSHL(int, q, int, s, 64, 2, expected_cumulative_sat, CMT);
+  TEST_VQSHL(int, q, uint, u, 8, 16, expected_cumulative_sat, CMT);
+  TEST_VQSHL(int, q, uint, u, 16, 8, expected_cumulative_sat, CMT);
+  TEST_VQSHL(int, q, uint, u, 32, 4, expected_cumulative_sat, CMT);
+  TEST_VQSHL(int, q, uint, u, 64, 2, expected_cumulative_sat, CMT);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected, CMT);
+
+
+  /* Use negative shift amounts */
+  VDUP(vector_shift, , int, s, 8, 8, -1);
+  VDUP(vector_shift, , int, s, 16, 4, -2);
+  VDUP(vector_shift, , int, s, 32, 2, -3);
+  VDUP(vector_shift, , int, s, 64, 1, -4);
+  VDUP(vector_shift, q, int, s, 8, 16, -7);
+  VDUP(vector_shift, q, int, s, 16, 8, -11);
+  VDUP(vector_shift, q, int, s, 32, 4, -13);
+  VDUP(vector_shift, q, int, s, 64, 2, -20);
+
+#undef CMT
+#define CMT " (negative shift amount)"
+  TEST_VQSHL(int, , int, s, 8, 8, expected_cumulative_sat_neg, CMT);
+  TEST_VQSHL(int, , int, s, 16, 4, expected_cumulative_sat_neg, CMT);
+  TEST_VQSHL(int, , int, s, 32, 2, expected_cumulative_sat_neg, CMT);
+  TEST_VQSHL(int, , int, s, 64, 1, expected_cumulative_sat_neg, CMT);
+  TEST_VQSHL(int, , uint, u, 8, 8, expected_cumulative_sat_neg, CMT);
+  TEST_VQSHL(int, , uint, u, 16, 4, expected_cumulative_sat_neg, CMT);
+  TEST_VQSHL(int, , uint, u, 32, 2, expected_cumulative_sat_neg, CMT);
+  TEST_VQSHL(int, , uint, u, 64, 1, expected_cumulative_sat_neg, CMT);
+  TEST_VQSHL(int, q, int, s, 8, 16, expected_cumulative_sat_neg, CMT);
+  TEST_VQSHL(int, q, int, s, 16, 8, expected_cumulative_sat_neg, CMT);
+  TEST_VQSHL(int, q, int, s, 32, 4, expected_cumulative_sat_neg, CMT);
+  TEST_VQSHL(int, q, int, s, 64, 2, expected_cumulative_sat_neg, CMT);
+  TEST_VQSHL(int, q, uint, u, 8, 16, expected_cumulative_sat_neg, CMT);
+  TEST_VQSHL(int, q, uint, u, 16, 8, expected_cumulative_sat_neg, CMT);
+  TEST_VQSHL(int, q, uint, u, 32, 4, expected_cumulative_sat_neg, CMT);
+  TEST_VQSHL(int, q, uint, u, 64, 2, expected_cumulative_sat_neg, CMT);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_neg, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_neg, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_neg, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_neg, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_neg, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_neg, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_neg, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_neg, CMT);
+
+
+  /* Use large shift amounts.  */
+  VDUP(vector_shift, , int, s, 8, 8, 8);
+  VDUP(vector_shift, , int, s, 16, 4, 16);
+  VDUP(vector_shift, , int, s, 32, 2, 32);
+  VDUP(vector_shift, , int, s, 64, 1, 64);
+  VDUP(vector_shift, q, int, s, 8, 16, 8);
+  VDUP(vector_shift, q, int, s, 16, 8, 16);
+  VDUP(vector_shift, q, int, s, 32, 4, 32);
+  VDUP(vector_shift, q, int, s, 64, 2, 64);
+
+#undef CMT
+#define CMT " (large shift amount, negative input)"
+  TEST_VQSHL(int, , int, s, 8, 8, expected_cumulative_sat_neg_large, CMT);
+  TEST_VQSHL(int, , int, s, 16, 4, expected_cumulative_sat_neg_large, CMT);
+  TEST_VQSHL(int, , int, s, 32, 2, expected_cumulative_sat_neg_large, CMT);
+  TEST_VQSHL(int, , int, s, 64, 1, expected_cumulative_sat_neg_large, CMT);
+  TEST_VQSHL(int, , uint, u, 8, 8, expected_cumulative_sat_neg_large, CMT);
+  TEST_VQSHL(int, , uint, u, 16, 4, expected_cumulative_sat_neg_large, CMT);
+  TEST_VQSHL(int, , uint, u, 32, 2, expected_cumulative_sat_neg_large, CMT);
+  TEST_VQSHL(int, , uint, u, 64, 1, expected_cumulative_sat_neg_large, CMT);
+  TEST_VQSHL(int, q, int, s, 8, 16, expected_cumulative_sat_neg_large, CMT);
+  TEST_VQSHL(int, q, int, s, 16, 8, expected_cumulative_sat_neg_large, CMT);
+  TEST_VQSHL(int, q, int, s, 32, 4, expected_cumulative_sat_neg_large, CMT);
+  TEST_VQSHL(int, q, int, s, 64, 2, expected_cumulative_sat_neg_large, CMT);
+  TEST_VQSHL(int, q, uint, u, 8, 16, expected_cumulative_sat_neg_large, CMT);
+  TEST_VQSHL(int, q, uint, u, 16, 8, expected_cumulative_sat_neg_large, CMT);
+  TEST_VQSHL(int, q, uint, u, 32, 4, expected_cumulative_sat_neg_large, CMT);
+  TEST_VQSHL(int, q, uint, u, 64, 2, expected_cumulative_sat_neg_large, CMT);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_neg_large, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_neg_large, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_neg_large, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_neg_large, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_neg_large, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_neg_large, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_neg_large, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_neg_large, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_neg_large, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_neg_large, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_neg_large, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_neg_large, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_neg_large, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_neg_large, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_neg_large, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_neg_large, CMT);
+
+
+  /* Fill input vector with max value, to check saturation on limits */
+  VDUP(vector, , int, s, 8, 8, 0x7F);
+  VDUP(vector, , int, s, 16, 4, 0x7FFF);
+  VDUP(vector, , int, s, 32, 2, 0x7FFFFFFF);
+  VDUP(vector, , int, s, 64, 1, 0x7FFFFFFFFFFFFFFFLL);
+  VDUP(vector, , uint, u, 8, 8, 0xFF);
+  VDUP(vector, , uint, u, 16, 4, 0xFFFF);
+  VDUP(vector, , uint, u, 32, 2, 0xFFFFFFFF);
+  VDUP(vector, , uint, u, 64, 1, 0xFFFFFFFFFFFFFFFFULL);
+  VDUP(vector, q, int, s, 8, 16, 0x7F);
+  VDUP(vector, q, int, s, 16, 8, 0x7FFF);
+  VDUP(vector, q, int, s, 32, 4, 0x7FFFFFFF);
+  VDUP(vector, q, int, s, 64, 2, 0x7FFFFFFFFFFFFFFFLL);
+  VDUP(vector, q, uint, u, 8, 16, 0xFF);
+  VDUP(vector, q, uint, u, 16, 8, 0xFFFF);
+  VDUP(vector, q, uint, u, 32, 4, 0xFFFFFFFF);
+  VDUP(vector, q, uint, u, 64, 2, 0xFFFFFFFFFFFFFFFFULL);
+
+  /* Shift by -1 */
+  VDUP(vector_shift, , int, s, 8, 8, -1);
+  VDUP(vector_shift, , int, s, 16, 4, -1);
+  VDUP(vector_shift, , int, s, 32, 2, -1);
+  VDUP(vector_shift, , int, s, 64, 1, -1);
+  VDUP(vector_shift, q, int, s, 8, 16, -1);
+  VDUP(vector_shift, q, int, s, 16, 8, -1);
+  VDUP(vector_shift, q, int, s, 32, 4, -1);
+  VDUP(vector_shift, q, int, s, 64, 2, -1);
+
+#undef CMT
+#define CMT " (max input, shift by -1)"
+  TEST_VQSHL(int, , int, s, 8, 8, expected_cumulative_sat_max_minus1, CMT);
+  TEST_VQSHL(int, , int, s, 16, 4, expected_cumulative_sat_max_minus1, CMT);
+  TEST_VQSHL(int, , int, s, 32, 2, expected_cumulative_sat_max_minus1, CMT);
+  TEST_VQSHL(int, , int, s, 64, 1, expected_cumulative_sat_max_minus1, CMT);
+  TEST_VQSHL(int, , uint, u, 8, 8, expected_cumulative_sat_max_minus1, CMT);
+  TEST_VQSHL(int, , uint, u, 16, 4, expected_cumulative_sat_max_minus1, CMT);
+  TEST_VQSHL(int, , uint, u, 32, 2, expected_cumulative_sat_max_minus1, CMT);
+  TEST_VQSHL(int, , uint, u, 64, 1, expected_cumulative_sat_max_minus1, CMT);
+  TEST_VQSHL(int, q, int, s, 8, 16, expected_cumulative_sat_max_minus1, CMT);
+  TEST_VQSHL(int, q, int, s, 16, 8, expected_cumulative_sat_max_minus1, CMT);
+  TEST_VQSHL(int, q, int, s, 32, 4, expected_cumulative_sat_max_minus1, CMT);
+  TEST_VQSHL(int, q, int, s, 64, 2, expected_cumulative_sat_max_minus1, CMT);
+  TEST_VQSHL(int, q, uint, u, 8, 16, expected_cumulative_sat_max_minus1, CMT);
+  TEST_VQSHL(int, q, uint, u, 16, 8, expected_cumulative_sat_max_minus1, CMT);
+  TEST_VQSHL(int, q, uint, u, 32, 4, expected_cumulative_sat_max_minus1, CMT);
+  TEST_VQSHL(int, q, uint, u, 64, 2, expected_cumulative_sat_max_minus1, CMT);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_max_minus1, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_max_minus1, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_max_minus1, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_max_minus1, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_max_minus1, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_max_minus1, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_max_minus1, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_max_minus1, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_max_minus1, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_max_minus1, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_max_minus1, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_max_minus1, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_max_minus1, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_max_minus1, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_max_minus1, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_max_minus1, CMT);
+
+
+  /* Use large shift amounts */
+  VDUP(vector_shift, , int, s, 8, 8, 8);
+  VDUP(vector_shift, , int, s, 16, 4, 16);
+  VDUP(vector_shift, , int, s, 32, 2, 32);
+  VDUP(vector_shift, , int, s, 64, 1, 64);
+  VDUP(vector_shift, q, int, s, 8, 16, 8);
+  VDUP(vector_shift, q, int, s, 16, 8, 16);
+  VDUP(vector_shift, q, int, s, 32, 4, 32);
+  VDUP(vector_shift, q, int, s, 64, 2, 64);
+
+#undef CMT
+#define CMT " (max input, large shift amount)"
+  TEST_VQSHL(int, , int, s, 8, 8, expected_cumulative_sat_max_large, CMT);
+  TEST_VQSHL(int, , int, s, 16, 4, expected_cumulative_sat_max_large, CMT);
+  TEST_VQSHL(int, , int, s, 32, 2, expected_cumulative_sat_max_large, CMT);
+  TEST_VQSHL(int, , int, s, 64, 1, expected_cumulative_sat_max_large, CMT);
+  TEST_VQSHL(int, , uint, u, 8, 8, expected_cumulative_sat_max_large, CMT);
+  TEST_VQSHL(int, , uint, u, 16, 4, expected_cumulative_sat_max_large, CMT);
+  TEST_VQSHL(int, , uint, u, 32, 2, expected_cumulative_sat_max_large, CMT);
+  TEST_VQSHL(int, , uint, u, 64, 1, expected_cumulative_sat_max_large, CMT);
+  TEST_VQSHL(int, q, int, s, 8, 16, expected_cumulative_sat_max_large, CMT);
+  TEST_VQSHL(int, q, int, s, 16, 8, expected_cumulative_sat_max_large, CMT);
+  TEST_VQSHL(int, q, int, s, 32, 4, expected_cumulative_sat_max_large, CMT);
+  TEST_VQSHL(int, q, int, s, 64, 2, expected_cumulative_sat_max_large, CMT);
+  TEST_VQSHL(int, q, uint, u, 8, 16, expected_cumulative_sat_max_large, CMT);
+  TEST_VQSHL(int, q, uint, u, 16, 8, expected_cumulative_sat_max_large, CMT);
+  TEST_VQSHL(int, q, uint, u, 32, 4, expected_cumulative_sat_max_large, CMT);
+  TEST_VQSHL(int, q, uint, u, 64, 2, expected_cumulative_sat_max_large, CMT);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_max_large, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_max_large, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_max_large, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_max_large, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_max_large, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_max_large, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_max_large, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_max_large, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_max_large, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_max_large, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_max_large, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_max_large, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_max_large, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_max_large, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_max_large, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_max_large, CMT);
+
+
+  /* Check 64 bits saturation.  */
+  VDUP(vector, , int, s, 64, 1, -10);
+  VDUP(vector_shift, , int, s, 64, 1, 64);
+  VDUP(vector, q, int, s, 64, 2, 10);
+  VDUP(vector_shift, q, int, s, 64, 2, 64);
+
+#undef CMT
+#define CMT " (check saturation on 64 bits)"
+  TEST_VQSHL(int, , int, s, 64, 1, expected_cumulative_sat_64, CMT);
+  TEST_VQSHL(int, q, int, s, 64, 2, expected_cumulative_sat_64, CMT);
+
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_64, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_64, CMT);
+}
+
+int main (void)
+{
+  exec_vqshl ();
+  return 0;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vqshl_n.c
@@ -0,0 +1,234 @@
+#include <arm_neon.h>
+#include "arm-neon-ref.h"
+#include "compute-ref-data.h"
+
+/* Expected values of cumulative_saturation flag.  */
+int VECT_VAR(expected_cumulative_sat,int,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat,int,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat,int,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat,int,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat,uint,8,8) = 1;
+int VECT_VAR(expected_cumulative_sat,uint,16,4) = 1;
+int VECT_VAR(expected_cumulative_sat,uint,32,2) = 1;
+int VECT_VAR(expected_cumulative_sat,uint,64,1) = 1;
+int VECT_VAR(expected_cumulative_sat,int,8,16) = 0;
+int VECT_VAR(expected_cumulative_sat,int,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat,int,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat,int,64,2) = 0;
+int VECT_VAR(expected_cumulative_sat,uint,8,16) = 1;
+int VECT_VAR(expected_cumulative_sat,uint,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat,uint,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat,uint,64,2) = 1;
+
+/* Expected results.  */
+VECT_VAR_DECL(expected,int,8,8) [] = { 0xc0, 0xc4, 0xc8, 0xcc,
+				       0xd0, 0xd4, 0xd8, 0xdc };
+VECT_VAR_DECL(expected,int,16,4) [] = { 0xffe0, 0xffe2, 0xffe4, 0xffe6 };
+VECT_VAR_DECL(expected,int,32,2) [] = { 0xffffffe0, 0xffffffe2 };
+VECT_VAR_DECL(expected,int,64,1) [] = { 0xffffffffffffffc0 };
+VECT_VAR_DECL(expected,uint,8,8) [] = { 0xff, 0xff, 0xff, 0xff,
+					0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected,uint,16,4) [] = { 0xffff, 0xffff, 0xffff, 0xffff };
+VECT_VAR_DECL(expected,uint,32,2) [] = { 0xffffffff, 0xffffffff };
+VECT_VAR_DECL(expected,uint,64,1) [] = { 0xffffffffffffffff };
+VECT_VAR_DECL(expected,int,8,16) [] = { 0xc0, 0xc4, 0xc8, 0xcc,
+					0xd0, 0xd4, 0xd8, 0xdc,
+					0xe0, 0xe4, 0xe8, 0xec,
+					0xf0, 0xf4, 0xf8, 0xfc };
+VECT_VAR_DECL(expected,int,16,8) [] = { 0xffe0, 0xffe2, 0xffe4, 0xffe6,
+					0xffe8, 0xffea, 0xffec, 0xffee };
+VECT_VAR_DECL(expected,int,32,4) [] = { 0xffffffe0, 0xffffffe2,
+					0xffffffe4, 0xffffffe6 };
+VECT_VAR_DECL(expected,int,64,2) [] = { 0xffffffffffffffc0, 0xffffffffffffffc4 };
+VECT_VAR_DECL(expected,uint,8,16) [] = { 0xff, 0xff, 0xff, 0xff,
+					 0xff, 0xff, 0xff, 0xff,
+					 0xff, 0xff, 0xff, 0xff,
+					 0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected,uint,16,8) [] = { 0xffff, 0xffff, 0xffff, 0xffff,
+					 0xffff, 0xffff, 0xffff, 0xffff };
+VECT_VAR_DECL(expected,uint,32,4) [] = { 0xffffffff, 0xffffffff,
+					 0xffffffff, 0xffffffff };
+VECT_VAR_DECL(expected,uint,64,2) [] = { 0xffffffffffffffff,
+					 0xffffffffffffffff };
+
+/* Expected values of cumulative_saturation flag with max positive input.  */
+int VECT_VAR(expected_cumulative_sat_max,int,8,8) = 1;
+int VECT_VAR(expected_cumulative_sat_max,int,16,4) = 1;
+int VECT_VAR(expected_cumulative_sat_max,int,32,2) = 1;
+int VECT_VAR(expected_cumulative_sat_max,int,64,1) = 1;
+int VECT_VAR(expected_cumulative_sat_max,uint,8,8) = 1;
+int VECT_VAR(expected_cumulative_sat_max,uint,16,4) = 1;
+int VECT_VAR(expected_cumulative_sat_max,uint,32,2) = 1;
+int VECT_VAR(expected_cumulative_sat_max,uint,64,1) = 1;
+int VECT_VAR(expected_cumulative_sat_max,int,8,16) = 1;
+int VECT_VAR(expected_cumulative_sat_max,int,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat_max,int,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat_max,int,64,2) = 1;
+int VECT_VAR(expected_cumulative_sat_max,uint,8,16) = 1;
+int VECT_VAR(expected_cumulative_sat_max,uint,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat_max,uint,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat_max,uint,64,2) = 1;
+
+/* Expected results with max positive input.  */
+VECT_VAR_DECL(expected_max,int,8,8) [] = { 0x7f, 0x7f, 0x7f, 0x7f,
+					   0x7f, 0x7f, 0x7f, 0x7f };
+VECT_VAR_DECL(expected_max,int,16,4) [] = { 0x7fff, 0x7fff, 0x7fff, 0x7fff };
+VECT_VAR_DECL(expected_max,int,32,2) [] = { 0x7fffffff, 0x7fffffff };
+VECT_VAR_DECL(expected_max,int,64,1) [] = { 0x7fffffffffffffff };
+VECT_VAR_DECL(expected_max,uint,8,8) [] = { 0xff, 0xff, 0xff, 0xff,
+					    0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_max,uint,16,4) [] = { 0xffff, 0xffff, 0xffff, 0xffff };
+VECT_VAR_DECL(expected_max,uint,32,2) [] = { 0xffffffff, 0xffffffff };
+VECT_VAR_DECL(expected_max,uint,64,1) [] = { 0xffffffffffffffff };
+VECT_VAR_DECL(expected_max,int,8,16) [] = { 0x7f, 0x7f, 0x7f, 0x7f,
+					    0x7f, 0x7f, 0x7f, 0x7f,
+					    0x7f, 0x7f, 0x7f, 0x7f,
+					    0x7f, 0x7f, 0x7f, 0x7f };
+VECT_VAR_DECL(expected_max,int,16,8) [] = { 0x7fff, 0x7fff, 0x7fff, 0x7fff,
+					    0x7fff, 0x7fff, 0x7fff, 0x7fff };
+VECT_VAR_DECL(expected_max,int,32,4) [] = { 0x7fffffff, 0x7fffffff,
+					    0x7fffffff, 0x7fffffff };
+VECT_VAR_DECL(expected_max,int,64,2) [] = { 0x7fffffffffffffff,
+					    0x7fffffffffffffff };
+VECT_VAR_DECL(expected_max,uint,8,16) [] = { 0xff, 0xff, 0xff, 0xff,
+					     0xff, 0xff, 0xff, 0xff,
+					     0xff, 0xff, 0xff, 0xff,
+					     0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_max,uint,16,8) [] = { 0xffff, 0xffff, 0xffff, 0xffff,
+					     0xffff, 0xffff, 0xffff, 0xffff };
+VECT_VAR_DECL(expected_max,uint,32,4) [] = { 0xffffffff, 0xffffffff,
+					     0xffffffff, 0xffffffff };
+VECT_VAR_DECL(expected_max,uint,64,2) [] = { 0xffffffffffffffff,
+					     0xffffffffffffffff };
+
+#define INSN vqshl
+#define TEST_MSG "VQSHL_N/VQSHLQ_N"
+
+#define FNNAME1(NAME) void exec_ ## NAME ##_n (void)
+#define FNNAME(NAME) FNNAME1(NAME)
+
+FNNAME (INSN)
+{
+  /* Basic test: v2=vqshl_n(v1,v), then store the result.  */
+#define TEST_VQSHL_N2(INSN, Q, T1, T2, W, N, V, EXPECTED_CUMULATIVE_SAT, CMT) \
+  Set_Neon_Cumulative_Sat(0, VECT_VAR(vector_res, T1, W, N));		\
+  VECT_VAR(vector_res, T1, W, N) =					\
+    INSN##Q##_n_##T2##W(VECT_VAR(vector, T1, W, N),			\
+			V);						\
+  vst1##Q##_##T2##W(VECT_VAR(result, T1, W, N),				\
+		    VECT_VAR(vector_res, T1, W, N));			\
+  CHECK_CUMULATIVE_SAT(TEST_MSG, T1, W, N, EXPECTED_CUMULATIVE_SAT, CMT)
+
+  /* Two auxliary macros are necessary to expand INSN */
+#define TEST_VQSHL_N1(INSN, T3, Q, T1, T2, W, N, EXPECTED_CUMULATIVE_SAT, CMT) \
+  TEST_VQSHL_N2(INSN, T3, Q, T1, T2, W, N, EXPECTED_CUMULATIVE_SAT, CMT)
+
+#define TEST_VQSHL_N(T3, Q, T1, T2, W, N, EXPECTED_CUMULATIVE_SAT, CMT)	\
+  TEST_VQSHL_N1(INSN, T3, Q, T1, T2, W, N, EXPECTED_CUMULATIVE_SAT, CMT)
+
+  DECL_VARIABLE_ALL_VARIANTS(vector);
+  DECL_VARIABLE_ALL_VARIANTS(vector_res);
+
+  clean_results ();
+
+  TEST_MACRO_ALL_VARIANTS_2_5(VLOAD, vector, buffer);
+
+  /* Choose shift amount arbitrarily.  */
+#define CMT ""
+  TEST_VQSHL_N(, int, s, 8, 8, 2, expected_cumulative_sat, CMT);
+  TEST_VQSHL_N(, int, s, 16, 4, 1, expected_cumulative_sat, CMT);
+  TEST_VQSHL_N(, int, s, 32, 2, 1, expected_cumulative_sat, CMT);
+  TEST_VQSHL_N(, int, s, 64, 1, 2, expected_cumulative_sat, CMT);
+  TEST_VQSHL_N(, uint, u, 8, 8, 3, expected_cumulative_sat, CMT);
+  TEST_VQSHL_N(, uint, u, 16, 4, 2, expected_cumulative_sat, CMT);
+  TEST_VQSHL_N(, uint, u, 32, 2, 3, expected_cumulative_sat, CMT);
+  TEST_VQSHL_N(, uint, u, 64, 1, 3, expected_cumulative_sat, CMT);
+
+  TEST_VQSHL_N(q, int, s, 8, 16, 2, expected_cumulative_sat, CMT);
+  TEST_VQSHL_N(q, int, s, 16, 8, 1, expected_cumulative_sat, CMT);
+  TEST_VQSHL_N(q, int, s, 32, 4, 1, expected_cumulative_sat, CMT);
+  TEST_VQSHL_N(q, int, s, 64, 2, 2, expected_cumulative_sat, CMT);
+  TEST_VQSHL_N(q, uint, u, 8, 16, 3, expected_cumulative_sat, CMT);
+  TEST_VQSHL_N(q, uint, u, 16, 8, 2, expected_cumulative_sat, CMT);
+  TEST_VQSHL_N(q, uint, u, 32, 4, 3, expected_cumulative_sat, CMT);
+  TEST_VQSHL_N(q, uint, u, 64, 2, 3, expected_cumulative_sat, CMT);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected, CMT);
+
+
+  /* Fill input vector with max value, to check saturation on limits.  */
+  VDUP(vector, , int, s, 8, 8, 0x7F);
+  VDUP(vector, , int, s, 16, 4, 0x7FFF);
+  VDUP(vector, , int, s, 32, 2, 0x7FFFFFFF);
+  VDUP(vector, , int, s, 64, 1, 0x7FFFFFFFFFFFFFFFLL);
+  VDUP(vector, , uint, u, 8, 8, 0xFF);
+  VDUP(vector, , uint, u, 16, 4, 0xFFFF);
+  VDUP(vector, , uint, u, 32, 2, 0xFFFFFFFF);
+  VDUP(vector, , uint, u, 64, 1, 0xFFFFFFFFFFFFFFFFULL);
+  VDUP(vector, q, int, s, 8, 16, 0x7F);
+  VDUP(vector, q, int, s, 16, 8, 0x7FFF);
+  VDUP(vector, q, int, s, 32, 4, 0x7FFFFFFF);
+  VDUP(vector, q, int, s, 64, 2, 0x7FFFFFFFFFFFFFFFLL);
+  VDUP(vector, q, uint, u, 8, 16, 0xFF);
+  VDUP(vector, q, uint, u, 16, 8, 0xFFFF);
+  VDUP(vector, q, uint, u, 32, 4, 0xFFFFFFFF);
+  VDUP(vector, q, uint, u, 64, 2, 0xFFFFFFFFFFFFFFFFULL);
+
+#undef CMT
+#define CMT " (with max input)"
+  TEST_VQSHL_N(, int, s, 8, 8, 2, expected_cumulative_sat_max, CMT);
+  TEST_VQSHL_N(, int, s, 16, 4, 1, expected_cumulative_sat_max, CMT);
+  TEST_VQSHL_N(, int, s, 32, 2, 1, expected_cumulative_sat_max, CMT);
+  TEST_VQSHL_N(, int, s, 64, 1, 2, expected_cumulative_sat_max, CMT);
+  TEST_VQSHL_N(, uint, u, 8, 8, 3, expected_cumulative_sat_max, CMT);
+  TEST_VQSHL_N(, uint, u, 16, 4, 2, expected_cumulative_sat_max, CMT);
+  TEST_VQSHL_N(, uint, u, 32, 2, 3, expected_cumulative_sat_max, CMT);
+  TEST_VQSHL_N(, uint, u, 64, 1, 3, expected_cumulative_sat_max, CMT);
+
+  TEST_VQSHL_N(q, int, s, 8, 16, 2, expected_cumulative_sat_max, CMT);
+  TEST_VQSHL_N(q, int, s, 16, 8, 1, expected_cumulative_sat_max, CMT);
+  TEST_VQSHL_N(q, int, s, 32, 4, 1, expected_cumulative_sat_max, CMT);
+  TEST_VQSHL_N(q, int, s, 64, 2, 2, expected_cumulative_sat_max, CMT);
+  TEST_VQSHL_N(q, uint, u, 8, 16, 3, expected_cumulative_sat_max, CMT);
+  TEST_VQSHL_N(q, uint, u, 16, 8, 2, expected_cumulative_sat_max, CMT);
+  TEST_VQSHL_N(q, uint, u, 32, 4, 3, expected_cumulative_sat_max, CMT);
+  TEST_VQSHL_N(q, uint, u, 64, 2, 3, expected_cumulative_sat_max, CMT);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_max, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_max, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_max, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_max, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_max, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_max, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_max, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_max, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_max, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_max, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_max, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_max, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_max, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_max, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_max, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_max, CMT);
+}
+
+int main (void)
+{
+  exec_vqshl_n ();
+  return 0;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vqshlu_n.c
@@ -0,0 +1,263 @@
+#include <arm_neon.h>
+#include "arm-neon-ref.h"
+#include "compute-ref-data.h"
+
+/* Expected values of cumulative_saturation flag with negative
+   input.  */
+int VECT_VAR(expected_cumulative_sat_neg,int,8,8) = 1;
+int VECT_VAR(expected_cumulative_sat_neg,int,16,4) = 1;
+int VECT_VAR(expected_cumulative_sat_neg,int,32,2) = 1;
+int VECT_VAR(expected_cumulative_sat_neg,int,64,1) = 1;
+int VECT_VAR(expected_cumulative_sat_neg,int,8,16) = 1;
+int VECT_VAR(expected_cumulative_sat_neg,int,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat_neg,int,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat_neg,int,64,2) = 1;
+
+/* Expected results with negative input.  */
+VECT_VAR_DECL(expected_neg,uint,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					    0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_neg,uint,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_neg,uint,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_neg,uint,64,1) [] = { 0x0 };
+VECT_VAR_DECL(expected_neg,uint,8,16) [] = { 0x0, 0x0, 0x0, 0x0,
+					     0x0, 0x0, 0x0, 0x0,
+					     0x0, 0x0, 0x0, 0x0,
+					     0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_neg,uint,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					     0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_neg,uint,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_neg,uint,64,2) [] = { 0x0, 0x0 };
+
+/* Expected values of cumulative_saturation flag with shift by 1.  */
+int VECT_VAR(expected_cumulative_sat_sh1,int,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat_sh1,int,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat_sh1,int,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat_sh1,int,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat_sh1,int,8,16) = 0;
+int VECT_VAR(expected_cumulative_sat_sh1,int,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_sh1,int,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat_sh1,int,64,2) = 0;
+
+/* Expected results with shift by 1.  */
+VECT_VAR_DECL(expected_sh1,uint,8,8) [] = { 0xfe, 0xfe, 0xfe, 0xfe,
+					    0xfe, 0xfe, 0xfe, 0xfe };
+VECT_VAR_DECL(expected_sh1,uint,16,4) [] = { 0xfffe, 0xfffe, 0xfffe, 0xfffe };
+VECT_VAR_DECL(expected_sh1,uint,32,2) [] = { 0xfffffffe, 0xfffffffe };
+VECT_VAR_DECL(expected_sh1,uint,64,1) [] = { 0xfffffffffffffffe };
+VECT_VAR_DECL(expected_sh1,uint,8,16) [] = { 0xfe, 0xfe, 0xfe, 0xfe,
+					     0xfe, 0xfe, 0xfe, 0xfe,
+					     0xfe, 0xfe, 0xfe, 0xfe,
+					     0xfe, 0xfe, 0xfe, 0xfe };
+VECT_VAR_DECL(expected_sh1,uint,16,8) [] = { 0xfffe, 0xfffe, 0xfffe, 0xfffe,
+					     0xfffe, 0xfffe, 0xfffe, 0xfffe };
+VECT_VAR_DECL(expected_sh1,uint,32,4) [] = { 0xfffffffe, 0xfffffffe,
+					     0xfffffffe, 0xfffffffe };
+VECT_VAR_DECL(expected_sh1,uint,64,2) [] = { 0xfffffffffffffffe,
+					     0xfffffffffffffffe };
+
+/* Expected values of cumulative_saturation flag with shift by 2.  */
+int VECT_VAR(expected_cumulative_sat_sh2,int,8,8) = 1;
+int VECT_VAR(expected_cumulative_sat_sh2,int,16,4) = 1;
+int VECT_VAR(expected_cumulative_sat_sh2,int,32,2) = 1;
+int VECT_VAR(expected_cumulative_sat_sh2,int,64,1) = 1;
+int VECT_VAR(expected_cumulative_sat_sh2,int,8,16) = 1;
+int VECT_VAR(expected_cumulative_sat_sh2,int,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat_sh2,int,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat_sh2,int,64,2) = 1;
+
+/* Expected results with shift by 2.  */
+VECT_VAR_DECL(expected_sh2,uint,8,8) [] = { 0xff, 0xff, 0xff, 0xff,
+					    0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_sh2,uint,16,4) [] = { 0xffff, 0xffff, 0xffff, 0xffff };
+VECT_VAR_DECL(expected_sh2,uint,32,2) [] = { 0xffffffff, 0xffffffff };
+VECT_VAR_DECL(expected_sh2,uint,64,1) [] = { 0xffffffffffffffff };
+VECT_VAR_DECL(expected_sh2,uint,8,16) [] = { 0xff, 0xff, 0xff, 0xff,
+					     0xff, 0xff, 0xff, 0xff,
+					     0xff, 0xff, 0xff, 0xff,
+					     0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_sh2,uint,16,8) [] = { 0xffff, 0xffff, 0xffff, 0xffff,
+					     0xffff, 0xffff, 0xffff, 0xffff };
+VECT_VAR_DECL(expected_sh2,uint,32,4) [] = { 0xffffffff, 0xffffffff,
+					     0xffffffff, 0xffffffff };
+VECT_VAR_DECL(expected_sh2,uint,64,2) [] = { 0xffffffffffffffff,
+					     0xffffffffffffffff };
+
+/* Expected values of cumulative_saturation flag.  */
+int VECT_VAR(expected_cumulative_sat,int,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat,int,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat,int,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat,int,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat,int,8,16) = 0;
+int VECT_VAR(expected_cumulative_sat,int,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat,int,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat,int,64,2) = 0;
+
+/* Expected results.  */
+VECT_VAR_DECL(expected,uint,8,8) [] = { 0x2, 0x2, 0x2, 0x2, 0x2, 0x2, 0x2, 0x2 };
+VECT_VAR_DECL(expected,uint,16,4) [] = { 0x8, 0x8, 0x8, 0x8 };
+VECT_VAR_DECL(expected,uint,32,2) [] = { 0x18, 0x18 };
+VECT_VAR_DECL(expected,uint,64,1) [] = { 0x40 };
+VECT_VAR_DECL(expected,uint,8,16) [] = { 0xa0, 0xa0, 0xa0, 0xa0,
+					 0xa0, 0xa0, 0xa0, 0xa0,
+					 0xa0, 0xa0, 0xa0, 0xa0,
+					 0xa0, 0xa0, 0xa0, 0xa0 };
+VECT_VAR_DECL(expected,uint,16,8) [] = { 0x180, 0x180, 0x180, 0x180,
+					 0x180, 0x180, 0x180, 0x180 };
+VECT_VAR_DECL(expected,uint,32,4) [] = { 0x380, 0x380, 0x380, 0x380 };
+VECT_VAR_DECL(expected,uint,64,2) [] = { 0x800, 0x800 };
+
+
+#define INSN vqshlu
+#define TEST_MSG "VQSHLU_N/VQSHLUQ_N"
+
+#define FNNAME1(NAME) void exec_ ## NAME ## _n(void)
+#define FNNAME(NAME) FNNAME1(NAME)
+
+FNNAME (INSN)
+{
+  /* Basic test: v2=vqshlu_n(v1,v), then store the result.  */
+#define TEST_VQSHLU_N2(INSN, Q, T1, T2, T3, T4, W, N, V, EXPECTED_CUMULATIVE_SAT, CMT) \
+  Set_Neon_Cumulative_Sat(0, VECT_VAR(vector_res, T3, W, N));		\
+  VECT_VAR(vector_res, T3, W, N) =					\
+    INSN##Q##_n_##T2##W(VECT_VAR(vector, T1, W, N),			\
+			V);						\
+  vst1##Q##_##T4##W(VECT_VAR(result, T3, W, N),				\
+		    VECT_VAR(vector_res, T3, W, N));			\
+  CHECK_CUMULATIVE_SAT(TEST_MSG, T1, W, N, EXPECTED_CUMULATIVE_SAT, CMT)
+
+  /* Two auxliary macros are necessary to expand INSN */
+#define TEST_VQSHLU_N1(INSN, Q, T1, T2, T3, T4, W, N, V, EXPECTED_CUMULATIVE_SAT, CMT) \
+  TEST_VQSHLU_N2(INSN, Q, T1, T2, T3, T4, W, N, V, EXPECTED_CUMULATIVE_SAT, CMT)
+
+#define TEST_VQSHLU_N(Q, T1, T2, T3, T4, W, N, V, EXPECTED_CUMULATIVE_SAT, CMT) \
+  TEST_VQSHLU_N1(INSN, Q, T1, T2, T3, T4, W, N, V, EXPECTED_CUMULATIVE_SAT, CMT)
+
+
+  DECL_VARIABLE_ALL_VARIANTS(vector);
+  DECL_VARIABLE_ALL_VARIANTS(vector_res);
+
+  clean_results ();
+
+  /* Fill input vector with negative values, to check saturation on
+     limits.  */
+  VDUP(vector, , int, s, 8, 8, -1);
+  VDUP(vector, , int, s, 16, 4, -2);
+  VDUP(vector, , int, s, 32, 2, -3);
+  VDUP(vector, , int, s, 64, 1, -4);
+  VDUP(vector, q, int, s, 8, 16, -1);
+  VDUP(vector, q, int, s, 16, 8, -2);
+  VDUP(vector, q, int, s, 32, 4, -3);
+  VDUP(vector, q, int, s, 64, 2, -4);
+
+  /* Choose shift amount arbitrarily.  */
+#define CMT " (negative input)"
+  TEST_VQSHLU_N(, int, s, uint, u, 8, 8, 2, expected_cumulative_sat_neg, CMT);
+  TEST_VQSHLU_N(, int, s, uint, u, 16, 4, 1, expected_cumulative_sat_neg, CMT);
+  TEST_VQSHLU_N(, int, s, uint, u, 32, 2, 1, expected_cumulative_sat_neg, CMT);
+  TEST_VQSHLU_N(, int, s, uint, u, 64, 1, 2, expected_cumulative_sat_neg, CMT);
+  TEST_VQSHLU_N(q, int, s, uint, u, 8, 16, 2, expected_cumulative_sat_neg, CMT);
+  TEST_VQSHLU_N(q, int, s, uint, u, 16, 8, 1, expected_cumulative_sat_neg, CMT);
+  TEST_VQSHLU_N(q, int, s, uint, u, 32, 4, 1, expected_cumulative_sat_neg, CMT);
+  TEST_VQSHLU_N(q, int, s, uint, u, 64, 2, 2, expected_cumulative_sat_neg, CMT);
+
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_neg, CMT);
+
+  
+  /* Fill input vector with max value, to check saturation on
+     limits.  */
+  VDUP(vector, , int, s, 8, 8, 0x7F);
+  VDUP(vector, , int, s, 16, 4, 0x7FFF);
+  VDUP(vector, , int, s, 32, 2, 0x7FFFFFFF);
+  VDUP(vector, , int, s, 64, 1, 0x7FFFFFFFFFFFFFFFLL);
+  VDUP(vector, q, int, s, 8, 16, 0x7F);
+  VDUP(vector, q, int, s, 16, 8, 0x7FFF);
+  VDUP(vector, q, int, s, 32, 4, 0x7FFFFFFF);
+  VDUP(vector, q, int, s, 64, 2, 0x7FFFFFFFFFFFFFFFULL);
+
+  /* shift by 1.  */
+#undef CMT
+#define CMT " (shift by 1)"
+  TEST_VQSHLU_N(, int, s, uint, u, 8, 8, 1, expected_cumulative_sat_sh1, CMT);
+  TEST_VQSHLU_N(, int, s, uint, u, 16, 4, 1, expected_cumulative_sat_sh1, CMT);
+  TEST_VQSHLU_N(, int, s, uint, u, 32, 2, 1, expected_cumulative_sat_sh1, CMT);
+  TEST_VQSHLU_N(, int, s, uint, u, 64, 1, 1, expected_cumulative_sat_sh1, CMT);
+  TEST_VQSHLU_N(q, int, s, uint, u, 8, 16, 1, expected_cumulative_sat_sh1, CMT);
+  TEST_VQSHLU_N(q, int, s, uint, u, 16, 8, 1, expected_cumulative_sat_sh1, CMT);
+  TEST_VQSHLU_N(q, int, s, uint, u, 32, 4, 1, expected_cumulative_sat_sh1, CMT);
+  TEST_VQSHLU_N(q, int, s, uint, u, 64, 2, 1, expected_cumulative_sat_sh1, CMT);
+
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_sh1, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_sh1, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_sh1, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_sh1, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_sh1, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_sh1, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_sh1, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_sh1, CMT);
+
+  /* shift by 2 to force saturation.  */
+#undef CMT
+#define CMT " (shift by 2)"
+  TEST_VQSHLU_N(, int, s, uint, u, 8, 8, 2, expected_cumulative_sat_sh2, CMT);
+  TEST_VQSHLU_N(, int, s, uint, u, 16, 4, 2, expected_cumulative_sat_sh2, CMT);
+  TEST_VQSHLU_N(, int, s, uint, u, 32, 2, 2, expected_cumulative_sat_sh2, CMT);
+  TEST_VQSHLU_N(, int, s, uint, u, 64, 1, 2, expected_cumulative_sat_sh2, CMT);
+  TEST_VQSHLU_N(q, int, s, uint, u, 8, 16, 2, expected_cumulative_sat_sh2, CMT);
+  TEST_VQSHLU_N(q, int, s, uint, u, 16, 8, 2, expected_cumulative_sat_sh2, CMT);
+  TEST_VQSHLU_N(q, int, s, uint, u, 32, 4, 2, expected_cumulative_sat_sh2, CMT);
+  TEST_VQSHLU_N(q, int, s, uint, u, 64, 2, 2, expected_cumulative_sat_sh2, CMT);
+
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_sh2, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_sh2, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_sh2, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_sh2, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_sh2, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_sh2, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_sh2, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_sh2, CMT);
+
+  
+  /* Fill input vector with positive values, to check normal case.  */
+  VDUP(vector, , int, s, 8, 8, 1);
+  VDUP(vector, , int, s, 16, 4, 2);
+  VDUP(vector, , int, s, 32, 2, 3);
+  VDUP(vector, , int, s, 64, 1, 4);
+  VDUP(vector, q, int, s, 8, 16, 5);
+  VDUP(vector, q, int, s, 16, 8, 6);
+  VDUP(vector, q, int, s, 32, 4, 7);
+  VDUP(vector, q, int, s, 64, 2, 8);
+
+  /* Arbitrary shift amount.  */
+#undef CMT
+#define CMT ""
+  TEST_VQSHLU_N(, int, s, uint, u, 8, 8, 1, expected_cumulative_sat, CMT);
+  TEST_VQSHLU_N(, int, s, uint, u, 16, 4, 2, expected_cumulative_sat, CMT);
+  TEST_VQSHLU_N(, int, s, uint, u, 32, 2, 3, expected_cumulative_sat, CMT);
+  TEST_VQSHLU_N(, int, s, uint, u, 64, 1, 4, expected_cumulative_sat, CMT);
+  TEST_VQSHLU_N(q, int, s, uint, u, 8, 16, 5, expected_cumulative_sat, CMT);
+  TEST_VQSHLU_N(q, int, s, uint, u, 16, 8, 6, expected_cumulative_sat, CMT);
+  TEST_VQSHLU_N(q, int, s, uint, u, 32, 4, 7, expected_cumulative_sat, CMT);
+  TEST_VQSHLU_N(q, int, s, uint, u, 64, 2, 8, expected_cumulative_sat, CMT);
+
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected, CMT);
+}
+
+int main (void)
+{
+  exec_vqshlu_n ();
+  return 0;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vqshrn_n.c
@@ -0,0 +1,177 @@
+#include <arm_neon.h>
+#include "arm-neon-ref.h"
+#include "compute-ref-data.h"
+
+/* Expected values of cumulative_saturation flag.  */
+int VECT_VAR(expected_cumulative_sat,int,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat,int,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat,int,64,2) = 0;
+int VECT_VAR(expected_cumulative_sat,uint,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat,uint,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat,uint,64,2) = 1;
+
+/* Expected results.  */
+VECT_VAR_DECL(expected,int,8,8) [] = { 0xf8, 0xf8, 0xf9, 0xf9,
+				       0xfa, 0xfa, 0xfb, 0xfb };
+VECT_VAR_DECL(expected,int,16,4) [] = { 0xfff8, 0xfff8, 0xfff9, 0xfff9 };
+VECT_VAR_DECL(expected,int,32,2) [] = { 0xfffffffc, 0xfffffffc };
+VECT_VAR_DECL(expected,uint,8,8) [] = { 0xff, 0xff, 0xff, 0xff,
+					0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected,uint,16,4) [] = { 0xffff, 0xffff, 0xffff, 0xffff };
+VECT_VAR_DECL(expected,uint,32,2) [] = { 0xffffffff, 0xffffffff };
+
+/* Expected values of cumulative_saturation flag with max input value
+   shifted by 3.  */
+int VECT_VAR(expected_cumulative_sat_max_sh3,int,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat_max_sh3,int,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat_max_sh3,int,64,2) = 1;
+int VECT_VAR(expected_cumulative_sat_max_sh3,uint,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat_max_sh3,uint,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat_max_sh3,uint,64,2) = 1;
+
+/* Expected results with max input value shifted by 3.  */
+VECT_VAR_DECL(expected_max_sh3,int,8,8) [] = { 0x7f, 0x7f, 0x7f, 0x7f,
+					       0x7f, 0x7f, 0x7f, 0x7f };
+VECT_VAR_DECL(expected_max_sh3,int,16,4) [] = { 0x7fff, 0x7fff, 0x7fff, 0x7fff };
+VECT_VAR_DECL(expected_max_sh3,int,32,2) [] = { 0x7fffffff, 0x7fffffff };
+VECT_VAR_DECL(expected_max_sh3,uint,8,8) [] = { 0xff, 0xff, 0xff, 0xff,
+						0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_max_sh3,uint,16,4) [] = { 0xffff, 0xffff,
+						 0xffff, 0xffff };
+VECT_VAR_DECL(expected_max_sh3,uint,32,2) [] = { 0xffffffff, 0xffffffff };
+
+/* Expected values of cumulative_saturation flag with max input value
+   shifted by type size.  */
+int VECT_VAR(expected_cumulative_sat_max_shmax,int,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_max_shmax,int,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat_max_shmax,int,64,2) = 0;
+int VECT_VAR(expected_cumulative_sat_max_shmax,uint,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_max_shmax,uint,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat_max_shmax,uint,64,2) = 0;
+
+/* Expected results with max input value shifted by type size.  */
+VECT_VAR_DECL(expected_max_shmax,int,8,8) [] = { 0x7f, 0x7f, 0x7f, 0x7f,
+						 0x7f, 0x7f, 0x7f, 0x7f };
+VECT_VAR_DECL(expected_max_shmax,int,16,4) [] = { 0x7fff, 0x7fff,
+						  0x7fff, 0x7fff };
+VECT_VAR_DECL(expected_max_shmax,int,32,2) [] = { 0x7fffffff, 0x7fffffff };
+VECT_VAR_DECL(expected_max_shmax,uint,8,8) [] = { 0xff, 0xff, 0xff, 0xff,
+						  0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_max_shmax,uint,16,4) [] = { 0xffff, 0xffff,
+						   0xffff, 0xffff };
+VECT_VAR_DECL(expected_max_shmax,uint,32,2) [] = { 0xffffffff, 0xffffffff };
+
+#define INSN vqshrn_n
+#define TEST_MSG "VQSHRN_N"
+
+#define FNNAME1(NAME) void exec_ ## NAME (void)
+#define FNNAME(NAME) FNNAME1(NAME)
+
+FNNAME (INSN)
+{
+  /* Basic test: y=vqshrn_n(x,v), then store the result.  */
+#define TEST_VQSHRN_N2(INSN, T1, T2, W, W2, N, V, EXPECTED_CUMULATIVE_SAT, CMT) \
+  Set_Neon_Cumulative_Sat(0, VECT_VAR(vector_res, T1, W2, N));		\
+  VECT_VAR(vector_res, T1, W2, N) =					\
+    INSN##_##T2##W(VECT_VAR(vector, T1, W, N),				\
+		   V);							\
+  vst1_##T2##W2(VECT_VAR(result, T1, W2, N),				\
+		VECT_VAR(vector_res, T1, W2, N));			\
+  CHECK_CUMULATIVE_SAT(TEST_MSG, T1, W, N, EXPECTED_CUMULATIVE_SAT, CMT)
+
+  /* Two auxliary macros are necessary to expand INSN */
+#define TEST_VQSHRN_N1(INSN, T1, T2, W, W2, N, V, EXPECTED_CUMULATIVE_SAT, CMT) \
+  TEST_VQSHRN_N2(INSN, T1, T2, W, W2, N, V, EXPECTED_CUMULATIVE_SAT, CMT)
+
+#define TEST_VQSHRN_N(T1, T2, W, W2, N, V, EXPECTED_CUMULATIVE_SAT, CMT) \
+  TEST_VQSHRN_N1(INSN, T1, T2, W, W2, N, V, EXPECTED_CUMULATIVE_SAT, CMT)
+
+
+  /* vector is twice as large as vector_res.  */
+  DECL_VARIABLE(vector, int, 16, 8);
+  DECL_VARIABLE(vector, int, 32, 4);
+  DECL_VARIABLE(vector, int, 64, 2);
+  DECL_VARIABLE(vector, uint, 16, 8);
+  DECL_VARIABLE(vector, uint, 32, 4);
+  DECL_VARIABLE(vector, uint, 64, 2);
+
+  DECL_VARIABLE(vector_res, int, 8, 8);
+  DECL_VARIABLE(vector_res, int, 16, 4);
+  DECL_VARIABLE(vector_res, int, 32, 2);
+  DECL_VARIABLE(vector_res, uint, 8, 8);
+  DECL_VARIABLE(vector_res, uint, 16, 4);
+  DECL_VARIABLE(vector_res, uint, 32, 2);
+
+  clean_results ();
+
+  VLOAD(vector, buffer, q, int, s, 16, 8);
+  VLOAD(vector, buffer, q, int, s, 32, 4);
+  VLOAD(vector, buffer, q, int, s, 64, 2);
+  VLOAD(vector, buffer, q, uint, u, 16, 8);
+  VLOAD(vector, buffer, q, uint, u, 32, 4);
+  VLOAD(vector, buffer, q, uint, u, 64, 2);
+
+  /* Choose shift amount arbitrarily.  */
+#define CMT ""
+  TEST_VQSHRN_N(int, s, 16, 8, 8, 1, expected_cumulative_sat, CMT);
+  TEST_VQSHRN_N(int, s, 32, 16, 4, 1, expected_cumulative_sat, CMT);
+  TEST_VQSHRN_N(int, s, 64, 32, 2, 2, expected_cumulative_sat, CMT);
+  TEST_VQSHRN_N(uint, u, 16, 8, 8, 2, expected_cumulative_sat, CMT);
+  TEST_VQSHRN_N(uint, u, 32, 16, 4, 3, expected_cumulative_sat, CMT);
+  TEST_VQSHRN_N(uint, u, 64, 32, 2, 3, expected_cumulative_sat, CMT);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected, CMT);
+
+
+  /* Use max possible value as input.  */
+  VDUP(vector, q, int, s, 16, 8, 0x7FFF);
+  VDUP(vector, q, int, s, 32, 4, 0x7FFFFFFF);
+  VDUP(vector, q, int, s, 64, 2, 0x7FFFFFFFFFFFFFFFLL);
+  VDUP(vector, q, uint, u, 16, 8, 0xFFFF);
+  VDUP(vector, q, uint, u, 32, 4, 0xFFFFFFFF);
+  VDUP(vector, q, uint, u, 64, 2, 0xFFFFFFFFFFFFFFFFULL);
+
+#undef CMT
+#define CMT " (check saturation: shift by 3)"
+  TEST_VQSHRN_N(int, s, 16, 8, 8, 3, expected_cumulative_sat_max_sh3, CMT);
+  TEST_VQSHRN_N(int, s, 32, 16, 4, 3, expected_cumulative_sat_max_sh3, CMT);
+  TEST_VQSHRN_N(int, s, 64, 32, 2, 3, expected_cumulative_sat_max_sh3, CMT);
+  TEST_VQSHRN_N(uint, u, 16, 8, 8, 3, expected_cumulative_sat_max_sh3, CMT);
+  TEST_VQSHRN_N(uint, u, 32, 16, 4, 3, expected_cumulative_sat_max_sh3, CMT);
+  TEST_VQSHRN_N(uint, u, 64, 32, 2, 3, expected_cumulative_sat_max_sh3, CMT);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_max_sh3, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_max_sh3, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_max_sh3, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_max_sh3, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_max_sh3, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_max_sh3, CMT);
+
+
+#undef CMT
+#define CMT " (check saturation: shift by max)"
+  TEST_VQSHRN_N(int, s, 16, 8, 8, 8, expected_cumulative_sat_max_shmax, CMT);
+  TEST_VQSHRN_N(int, s, 32, 16, 4, 16, expected_cumulative_sat_max_shmax, CMT);
+  TEST_VQSHRN_N(int, s, 64, 32, 2, 32, expected_cumulative_sat_max_shmax, CMT);
+  TEST_VQSHRN_N(uint, u, 16, 8, 8, 8, expected_cumulative_sat_max_shmax, CMT);
+  TEST_VQSHRN_N(uint, u, 32, 16, 4, 16, expected_cumulative_sat_max_shmax, CMT);
+  TEST_VQSHRN_N(uint, u, 64, 32, 2, 32, expected_cumulative_sat_max_shmax, CMT);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_max_shmax, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_max_shmax, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_max_shmax, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_max_shmax, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_max_shmax, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_max_shmax, CMT);
+}
+
+int main (void)
+{
+  exec_vqshrn_n ();
+  return 0;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vqshrun_n.c
@@ -0,0 +1,133 @@
+#include <arm_neon.h>
+#include "arm-neon-ref.h"
+#include "compute-ref-data.h"
+
+/* Expected values of cumulative_saturation flag with negative input.  */
+int VECT_VAR(expected_cumulative_sat_neg,int,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat_neg,int,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat_neg,int,64,2) = 1;
+
+/* Expected results with negative input.  */
+VECT_VAR_DECL(expected_neg,uint,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					    0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_neg,uint,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_neg,uint,32,2) [] = { 0x0, 0x0 };
+
+/* Expected values of cumulative_saturation flag with max input value
+   shifted by 1.  */
+int VECT_VAR(expected_cumulative_sat_max_sh1,int,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat_max_sh1,int,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat_max_sh1,int,64,2) = 1;
+
+/* Expected results with max input value shifted by 1.  */
+VECT_VAR_DECL(expected_max_sh1,uint,8,8) [] = { 0xff, 0xff, 0xff, 0xff,
+						0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_max_sh1,uint,16,4) [] = { 0xffff, 0xffff,
+						 0xffff, 0xffff };
+VECT_VAR_DECL(expected_max_sh1,uint,32,2) [] = { 0xffffffff, 0xffffffff };
+VECT_VAR_DECL(expected_max_sh1,uint,64,1) [] = { 0x3333333333333333 };
+
+/* Expected values of cumulative_saturation flag.  */
+int VECT_VAR(expected_cumulative_sat,int,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat,int,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat,int,64,2) = 0;
+
+/* Expected results.  */
+VECT_VAR_DECL(expected,uint,8,8) [] = { 0x48, 0x48, 0x48, 0x48,
+					0x48, 0x48, 0x48, 0x48 };
+VECT_VAR_DECL(expected,uint,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected,uint,32,2) [] = { 0xdeadbe, 0xdeadbe };
+
+
+#define INSN vqshrun_n
+#define TEST_MSG "VQSHRUN_N"
+
+#define FNNAME1(NAME) void exec_ ## NAME (void)
+#define FNNAME(NAME) FNNAME1(NAME)
+
+FNNAME (INSN)
+{
+  /* Basic test: y=vqshrun_n(x,v), then store the result.  */
+#define TEST_VQSHRUN_N2(INSN, T1, T2, W, W2, N, V, EXPECTED_CUMULATIVE_SAT, CMT) \
+  Set_Neon_Cumulative_Sat(0, VECT_VAR(vector_res, uint, W2, N));	\
+  VECT_VAR(vector_res, uint, W2, N) =					\
+    INSN##_##T2##W(VECT_VAR(vector, T1, W, N),				\
+		   V);							\
+  vst1_u##W2(VECT_VAR(result, uint, W2, N),				\
+	     VECT_VAR(vector_res, uint, W2, N));			\
+  CHECK_CUMULATIVE_SAT(TEST_MSG, T1, W, N, EXPECTED_CUMULATIVE_SAT, CMT)
+
+  /* Two auxliary macros are necessary to expand INSN */
+#define TEST_VQSHRUN_N1(INSN, T1, T2, W, W2, N, V, EXPECTED_CUMULATIVE_SAT, CMT) \
+  TEST_VQSHRUN_N2(INSN, T1, T2, W, W2, N, V, EXPECTED_CUMULATIVE_SAT, CMT)
+
+#define TEST_VQSHRUN_N(T1, T2, W, W2, N, V, EXPECTED_CUMULATIVE_SAT, CMT) \
+  TEST_VQSHRUN_N1(INSN, T1, T2, W, W2, N, V, EXPECTED_CUMULATIVE_SAT, CMT)
+
+
+  /* vector is twice as large as vector_res.  */
+  DECL_VARIABLE(vector, int, 16, 8);
+  DECL_VARIABLE(vector, int, 32, 4);
+  DECL_VARIABLE(vector, int, 64, 2);
+
+  DECL_VARIABLE(vector_res, uint, 8, 8);
+  DECL_VARIABLE(vector_res, uint, 16, 4);
+  DECL_VARIABLE(vector_res, uint, 32, 2);
+
+  clean_results ();
+
+  /* Fill input vector with negative values, to check saturation on
+     limits.  */
+  VDUP(vector, q, int, s, 16, 8, -2);
+  VDUP(vector, q, int, s, 32, 4, -3);
+  VDUP(vector, q, int, s, 64, 2, -4);
+
+  /* Choose shift amount arbitrarily.  */
+#define CMT " (negative input)"
+  TEST_VQSHRUN_N(int, s, 16, 8, 8, 3, expected_cumulative_sat_neg, CMT);
+  TEST_VQSHRUN_N(int, s, 32, 16, 4, 4, expected_cumulative_sat_neg, CMT);
+  TEST_VQSHRUN_N(int, s, 64, 32, 2, 2, expected_cumulative_sat_neg, CMT);
+
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_neg, CMT);
+
+  
+  /* Fill input vector with max value, to check saturation on
+     limits.  */
+  VDUP(vector, q, int, s, 16, 8, 0x7FFF);
+  VDUP(vector, q, int, s, 32, 4, 0x7FFFFFFF);
+  VDUP(vector, q, int, s, 64, 2, 0x7FFFFFFFFFFFFFFFLL);
+
+#undef CMT
+#define CMT " (check cumulative saturation)"
+  TEST_VQSHRUN_N(int, s, 16, 8, 8, 1, expected_cumulative_sat_max_sh1, CMT);
+  TEST_VQSHRUN_N(int, s, 32, 16, 4, 1, expected_cumulative_sat_max_sh1, CMT);
+  TEST_VQSHRUN_N(int, s, 64, 32, 2, 1, expected_cumulative_sat_max_sh1, CMT);
+
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_max_sh1, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_max_sh1, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_max_sh1, CMT);
+
+  
+  /* Fill input vector with positive values, to check normal case.  */
+  VDUP(vector, q, int, s, 16, 8, 0x1234);
+  VDUP(vector, q, int, s, 32, 4, 0x87654321);
+  VDUP(vector, q, int, s, 64, 2, 0xDEADBEEF);
+
+#undef CMT
+#define CMT ""
+  TEST_VQSHRUN_N(int, s, 16, 8, 8, 6, expected_cumulative_sat, CMT);
+  TEST_VQSHRUN_N(int, s, 32, 16, 4, 7, expected_cumulative_sat, CMT);
+  TEST_VQSHRUN_N(int, s, 64, 32, 2, 8, expected_cumulative_sat, CMT);
+
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected, CMT);
+}
+
+int main (void)
+{
+  exec_vqshrun_n ();
+  return 0;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/c-output-template-4.c
@@ -0,0 +1,10 @@
+/* { dg-do compile } */
+/* { dg-options "-O0" } */
+
+void
+test (void)
+{
+    __asm__ ("@ %c0" : : "S" (&test + 4));
+}
+
+/* { dg-final { scan-assembler "@ test\\+4" } } */
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/pow-sqrt-synth-1.c
@@ -0,0 +1,38 @@
+/* { dg-do compile } */
+/* { dg-options "-fdump-tree-sincos -Ofast --param max-pow-sqrt-depth=8" } */
+
+
+double
+foo (double a)
+{
+  return __builtin_pow (a, -5.875);
+}
+
+double
+foof (double a)
+{
+  return __builtin_pow (a, 0.75f);
+}
+
+double
+bar (double a)
+{
+  return __builtin_pow (a, 1.0 + 0.00390625);
+}
+
+double
+baz (double a)
+{
+  return __builtin_pow (a, -1.25) + __builtin_pow (a, 5.75) - __builtin_pow (a, 3.375);
+}
+
+#define N 256
+void
+vecfoo (double *a)
+{
+  for (int i = 0; i < N; i++)
+    a[i] = __builtin_pow (a[i], 1.25);
+}
+
+/* { dg-final { scan-tree-dump-times "synthesizing" 7 "sincos" } } */
+/* { dg-final { cleanup-tree-dump "sincos" } } */
\ No newline at end of file
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/pr65491_1.c
@@ -0,0 +1,11 @@
+/* { dg-do compile } */
+/* { dg-options "-O2" } */
+
+typedef long double a __attribute__((vector_size (16)));
+
+a
+sum (a first, a second)
+{
+  return first + second;
+}
+
--- a/src/gcc/testsuite/gcc.target/aarch64/singleton_intrinsics_1.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/singleton_intrinsics_1.c
@@ -235,8 +235,8 @@ test_vrshl_u64 (uint64x1_t a, int64x1_t b)
   return vrshl_u64 (a, b);
 }
 
-/* For int64x1_t, sshr...#63 is output instead of the equivalent cmlt...#0.  */
-/* { dg-final { scan-assembler-times "\\tsshr\\td\[0-9\]+" 2 } } */
+/* For int64x1_t, sshr...#63 is equivalent to cmlt...#0.  */
+/* { dg-final { scan-assembler-times "\\t(?:sshr|cmlt)\\td\[0-9\]+" 2 } } */
 
 int64x1_t
 test_vshr_n_s64 (int64x1_t a)
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/unsigned-float.c
@@ -0,0 +1,18 @@
+/* { dg-do compile } */
+/* { dg-options "-O1" } */
+
+#include <stdint.h>
+
+double
+f1 (uint16_t x)
+{
+  return (double)(float)x;
+}
+
+float
+f2 (uint16_t x)
+{
+  return (float)(double)x;
+}
+
+/* { dg-final { scan-assembler-not "fcvt" } } */
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/vec_init_1.c
@@ -0,0 +1,34 @@
+/* { dg-do run } */
+/* { dg-options "-O2 -fomit-frame-pointer --save-temps -fno-inline" } */
+
+extern void abort (void);
+
+typedef float float16x4_t __attribute__ ((vector_size ((16))));
+
+float a;
+float b;
+
+float16x4_t
+make_vector ()
+{
+  return (float16x4_t) { 0, 0, a, b };
+}
+
+int
+main (int argc, char **argv)
+{
+  a = 4.0;
+  b = 3.0;
+  float16x4_t vec = make_vector ();
+  if (vec[0] != 0 || vec[1] != 0 || vec[2] != a || vec[3] != b)
+    abort ();
+  return 0;
+}
+
+/* { dg-final { scan-assembler-times "ins\\t" 2 } } */
+/* What we want to check, is that make_vector does not stp the whole vector
+   to the stack.  Unfortunately here we scan the body of main() too, which may
+   be a bit fragile - the test is currently passing only because of the option
+   -fomit-frame-pointer which avoids use of stp in the prologue to main().  */
+/* { dg-final { scan-assembler-not "stp\\t" } } */
+/* { dg-final { cleanup-saved-temps } } */
--- a/src/gcc/testsuite/gcc.target/aarch64/vldN_lane_1.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/vldN_lane_1.c
@@ -54,11 +54,11 @@ test_vld##STRUCT##Q##_lane##SUFFIX (const BASE##_t *data,		\
 }
 
 
-/* Tests of vld2_dup and vld2q_dup.  */
+/* Tests of vld2_lane and vld2q_lane.  */
 VARIANTS (TESTMETH, 2)
-/* Tests of vld3_dup and vld3q_dup.  */
+/* Tests of vld3_lane and vld3q_lane.  */
 VARIANTS (TESTMETH, 3)
-/* Tests of vld4_dup and vld4q_dup.  */
+/* Tests of vld4_lane and vld4q_lane.  */
 VARIANTS (TESTMETH, 4)
 
 #define CHECK(BASE, Q, ELTS, SUFFIX, LANE, STRUCT)			\
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/vstN_lane_1.c
@@ -0,0 +1,75 @@
+/* { dg-do run } */
+/* { dg-options "-O3 -fno-inline" } */
+
+#include <arm_neon.h>
+
+extern void abort (void);
+
+#define VARIANTS(VARIANT, STRUCT)	\
+VARIANT (uint8, , 8, _u8, 6, STRUCT)	\
+VARIANT (uint16, , 4, _u16, 3, STRUCT)	\
+VARIANT (uint32, , 2, _u32, 1, STRUCT)	\
+VARIANT (uint64, , 1, _u64, 0, STRUCT)	\
+VARIANT (int8, , 8, _s8, 5, STRUCT)	\
+VARIANT (int16, , 4, _s16, 2, STRUCT)	\
+VARIANT (int32, , 2, _s32, 0, STRUCT)	\
+VARIANT (int64, , 1, _s64, 0, STRUCT)	\
+VARIANT (poly8, , 8, _p8, 7, STRUCT)	\
+VARIANT (poly16, , 4, _p16, 1, STRUCT)	\
+VARIANT (float32, , 2, _f32, 1, STRUCT)	\
+VARIANT (float64, , 1, _f64, 0, STRUCT)	\
+VARIANT (uint8, q, 16, _u8, 14, STRUCT)	\
+VARIANT (uint16, q, 8, _u16, 4, STRUCT)	\
+VARIANT (uint32, q, 4, _u32, 3, STRUCT)	\
+VARIANT (uint64, q, 2, _u64, 0, STRUCT)	\
+VARIANT (int8, q, 16, _s8, 13, STRUCT)	\
+VARIANT (int16, q, 8, _s16, 6, STRUCT)	\
+VARIANT (int32, q, 4, _s32, 2, STRUCT)	\
+VARIANT (int64, q, 2, _s64, 1, STRUCT)	\
+VARIANT (poly8, q, 16, _p8, 12, STRUCT)	\
+VARIANT (poly16, q, 8, _p16, 5, STRUCT)	\
+VARIANT (float32, q, 4, _f32, 1, STRUCT)\
+VARIANT (float64, q, 2, _f64, 0, STRUCT)
+
+#define TESTMETH(BASE, Q, ELTS, SUFFIX, LANE, STRUCT)			\
+int									\
+test_vst##STRUCT##Q##_lane##SUFFIX (const BASE##_t *data)		\
+{									\
+  BASE##x##ELTS##x##STRUCT##_t vectors;					\
+  for (int i = 0; i < STRUCT; i++, data += ELTS)			\
+    vectors.val[i] = vld1##Q##SUFFIX (data);				\
+  BASE##_t temp[STRUCT];						\
+  vst##STRUCT##Q##_lane##SUFFIX (temp, vectors, LANE);			\
+  for (int i = 0; i < STRUCT; i++)					\
+    {									\
+      if (temp[i] != vget##Q##_lane##SUFFIX (vectors.val[i], LANE))	\
+	return 1;							\
+    }									\
+  return 0;								\
+}
+
+/* Tests of vst2_lane and vst2q_lane.  */
+VARIANTS (TESTMETH, 2)
+/* Tests of vst3_lane and vst3q_lane.  */
+VARIANTS (TESTMETH, 3)
+/* Tests of vst4_lane and vst4q_lane.  */
+VARIANTS (TESTMETH, 4)
+
+#define CHECK(BASE, Q, ELTS, SUFFIX, LANE, STRUCT)			\
+  if (test_vst##STRUCT##Q##_lane##SUFFIX ((const BASE##_t *)orig_data))	\
+    abort ();
+
+int
+main (int argc, char **argv)
+{
+  /* Original data for all vector formats.  */
+  uint64_t orig_data[8] = {0x1234567890abcdefULL, 0x13579bdf02468aceULL,
+			   0x012389ab4567cdefULL, 0xfeeddadacafe0431ULL,
+			   0x1032547698badcfeULL, 0xbadbadbadbad0badULL,
+			   0x0102030405060708ULL, 0x0f0e0d0c0b0a0908ULL};
+
+  VARIANTS (CHECK, 2);
+  VARIANTS (CHECK, 3);
+  VARIANTS (CHECK, 4);
+  return 0;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/bics_1.c
@@ -0,0 +1,54 @@
+/* { dg-do run } */
+/* { dg-options "-O2 --save-temps -fno-inline" } */
+/* { dg-require-effective-target arm32 } */
+
+extern void abort (void);
+
+int
+bics_si_test1 (int a, int b, int c)
+{
+  int d = a & ~b;
+
+  /* { dg-final { scan-assembler-times "bics\tr\[0-9\]+, r\[0-9\]+, r\[0-9\]+" 2 } } */
+  if (d == 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+int
+bics_si_test2 (int a, int b, int c)
+{
+  int d = a & ~(b << 3);
+
+  /* { dg-final { scan-assembler-times "bics\tr\[0-9\]+, r\[0-9\]+, r\[0-9\]+, .sl \#3" 1 } } */
+  if (d == 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+int
+main ()
+{
+  int x;
+
+  x = bics_si_test1 (29, ~4, 5);
+  if (x != ((29 & 4) + ~4 + 5))
+    abort ();
+
+  x = bics_si_test1 (5, ~2, 20);
+  if (x != 25)
+    abort ();
+
+    x = bics_si_test2 (35, ~4, 5);
+  if (x != ((35 & ~(~4 << 3)) + ~4 + 5))
+    abort ();
+
+  x = bics_si_test2 (96, ~2, 20);
+  if (x != 116)
+  abort ();
+
+  return 0;
+}
+/* { dg-final { cleanup-saved-temps } } */
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/bics_2.c
@@ -0,0 +1,57 @@
+/* { dg-do run } */
+/* { dg-options "-O2 --save-temps -fno-inline" } */
+/* { dg-require-effective-target arm32 } */
+
+extern void abort (void);
+
+int
+bics_si_test1 (int a, int b, int c)
+{
+  int d = a & ~b;
+
+  /* { dg-final { scan-assembler-not "bics\tr\[0-9\]+, r\[0-9\]+, r\[0-9\]+" } } */
+  /* { dg-final { scan-assembler-times "bic\tr\[0-9\]+, r\[0-9\]+, r\[0-9\]+" 2 } } */
+  if (d <= 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+int
+bics_si_test2 (int a, int b, int c)
+{
+  int d = a & ~(b << 3);
+
+  /* { dg-final { scan-assembler-not "bics\tr\[0-9\]+, r\[0-9\]+, r\[0-9\]+, .sl \#3" } } */
+  /* { dg-final { scan-assembler "bic\tr\[0-9\]+, r\[0-9\]+, r\[0-9\]+, .sl \#3" } } */
+  if (d <= 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+int
+main ()
+{
+  int x;
+
+  x = bics_si_test1 (29, ~4, 5);
+  if (x != ((29 & 4) + ~4 + 5))
+    abort ();
+
+  x = bics_si_test1 (5, ~2, 20);
+  if (x != 25)
+    abort ();
+
+  x = bics_si_test2 (35, ~4, 5);
+  if (x != ((35 & ~(~4 << 3)) + ~4 + 5))
+    abort ();
+
+  x = bics_si_test2 (96, ~2, 20);
+  if (x != 116)
+    abort ();
+
+  return 0;
+}
+
+/* { dg-final { cleanup-saved-temps } } */
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/bics_3.c
@@ -0,0 +1,41 @@
+/* { dg-do run } */
+/* { dg-options "-O2 --save-temps -fno-inline" } */
+/* { dg-require-effective-target arm32 } */
+
+extern void abort (void);
+
+int
+bics_si_test (int a, int b)
+{
+  if (a & ~b)
+    return 1;
+  else
+    return 0;
+}
+
+int
+bics_si_test2 (int a, int b)
+{
+  if (a & ~ (b << 2))
+    return 1;
+  else
+    return 0;
+}
+
+int
+main (void)
+{
+  int a = 5;
+  int b = 5;
+  int c = 20;
+  if (bics_si_test (a, b))
+    abort ();
+  if (bics_si_test2 (c, b))
+    abort ();
+  return 0;
+}
+
+/* { dg-final { scan-assembler-times "bics\tr\[0-9\]+, r\[0-9\]+, r\[0-9\]+" 2 } } */
+/* { dg-final { scan-assembler-times "bics\tr\[0-9\]+, r\[0-9\]+, r\[0-9\]+, .sl #2" 1 } } */
+
+/* { dg-final { cleanup-saved-temps } } */
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/bics_4.c
@@ -0,0 +1,49 @@
+/* { dg-do run } */
+/* { dg-options "-O2 --save-temps -fno-inline" } */
+/* { dg-require-effective-target arm32 } */
+
+extern void abort (void);
+
+int
+bics_si_test1 (int a, int b, int c)
+{
+  if ((a & b) == a)
+    return a;
+  else
+    return c;
+}
+
+int
+bics_si_test2 (int a, int b, int c)
+{
+  if ((a & b) == b)
+    return b;
+  else
+    return c;
+}
+
+int
+main ()
+{
+  int x;
+  x = bics_si_test1 (0xf00d, 0xf11f, 0);
+  if (x != 0xf00d)
+    abort ();
+
+  x = bics_si_test1 (0xf11f, 0xf00d, 0);
+  if (x != 0)
+    abort ();
+
+  x = bics_si_test2 (0xf00d, 0xf11f, 0);
+  if (x != 0)
+    abort ();
+
+  x = bics_si_test2 (0xf11f, 0xf00d, 0);
+  if (x != 0xf00d)
+    abort ();
+
+  return 0;
+}
+
+/* { dg-final { scan-assembler-times "bics\tr\[0-9\]+, r\[0-9\]+, r\[0-9\]+" 2 } } */
+/* { dg-final { cleanup-saved-temps } } */
--- a/src/gcc/testsuite/gcc.target/arm/neon/pr51534.c
+++ b/src/gcc/testsuite/gcc.target/arm/neon/pr51534.c
@@ -58,18 +58,18 @@ GEN_COND_TESTS(vceq)
 /* { dg-final { scan-assembler-times "vcge\.u16\[ 	\]+\[qQ\]\[0-9\]+, \[qQ\]\[0-9\]+, \[qQ\]\[0-9\]+" 2 } } */
 /* { dg-final { scan-assembler "vcge\.s32\[ 	\]+\[qQ\]\[0-9\]+, \[qQ\]\[0-9\]+, #0" } } */
 /* { dg-final { scan-assembler-times "vcge\.u32\[ 	\]+\[qQ\]\[0-9\]+, \[qQ\]\[0-9\]+, \[qQ\]\[0-9\]+" 2 } } */
-/* { dg-final { scan-assembler "vcgt\.s8\[ 	\]+\[dD\]\[0-9\]+, \[dD\]\[0-9\]+, \[dD\]\[0-9\]+" } } */
-/* { dg-final { scan-assembler "vcgt\.s16\[ 	\]+\[dD\]\[0-9\]+, \[dD\]\[0-9\]+, \[dD\]\[0-9\]+" } } */
-/* { dg-final { scan-assembler "vcgt\.s32\[ 	\]+\[dD\]\[0-9\]+, \[dD\]\[0-9\]+, \[dD\]\[0-9\]+" } } */
-/* { dg-final { scan-assembler "vcgt\.s8\[ 	\]+\[qQ\]\[0-9\]+, \[qQ\]\[0-9\]+, \[qQ\]\[0-9\]+" } } */
-/* { dg-final { scan-assembler "vcgt\.s16\[ 	\]+\[qQ\]\[0-9\]+, \[qQ\]\[0-9\]+, \[qQ\]\[0-9\]+" } } */
-/* { dg-final { scan-assembler "vcgt\.s32\[ 	\]+\[qQ\]\[0-9\]+, \[qQ\]\[0-9\]+, \[qQ\]\[0-9\]+" } } */
-/* { dg-final { scan-assembler "vcge\.s8\[ 	\]+\[dD\]\[0-9\]+, \[dD\]\[0-9\]+, \[dD\]\[0-9\]+" } } */
-/* { dg-final { scan-assembler "vcge\.s16\[ 	\]+\[dD\]\[0-9\]+, \[dD\]\[0-9\]+, \[dD\]\[0-9\]+" } } */
-/* { dg-final { scan-assembler "vcge\.s32\[ 	\]+\[dD\]\[0-9\]+, \[dD\]\[0-9\]+, \[dD\]\[0-9\]+" } } */
-/* { dg-final { scan-assembler "vcge\.s8\[ 	\]+\[qQ\]\[0-9\]+, \[qQ\]\[0-9\]+, \[qQ\]\[0-9\]+" } } */
-/* { dg-final { scan-assembler "vcge\.s16\[ 	\]+\[qQ\]\[0-9\]+, \[qQ\]\[0-9\]+, \[qQ\]\[0-9\]+" } } */
-/* { dg-final { scan-assembler "vcge\.s32\[ 	\]+\[qQ\]\[0-9\]+, \[qQ\]\[0-9\]+, \[qQ\]\[0-9\]+" } } */
+/* { dg-final { scan-assembler "vclt\.s8\[ 	\]+\[dD\]\[0-9\]+, \[dD\]\[0-9\]+, #0" } } */
+/* { dg-final { scan-assembler "vclt\.s16\[ 	\]+\[dD\]\[0-9\]+, \[dD\]\[0-9\]+, #0" } } */
+/* { dg-final { scan-assembler "vclt\.s32\[ 	\]+\[dD\]\[0-9\]+, \[dD\]\[0-9\]+, #0" } } */
+/* { dg-final { scan-assembler "vclt\.s8\[ 	\]+\[qQ\]\[0-9\]+, \[qQ\]\[0-9\]+, #0" } } */
+/* { dg-final { scan-assembler "vclt\.s16\[ 	\]+\[qQ\]\[0-9\]+, \[qQ\]\[0-9\]+, #0" } } */
+/* { dg-final { scan-assembler "vclt\.s32\[ 	\]+\[qQ\]\[0-9\]+, \[qQ\]\[0-9\]+, #0" } } */
+/* { dg-final { scan-assembler "vcle\.s8\[ 	\]+\[dD\]\[0-9\]+, \[dD\]\[0-9\]+, #0" } } */
+/* { dg-final { scan-assembler "vcle\.s16\[ 	\]+\[dD\]\[0-9\]+, \[dD\]\[0-9\]+, #0" } } */
+/* { dg-final { scan-assembler "vcle\.s32\[ 	\]+\[dD\]\[0-9\]+, \[dD\]\[0-9\]+, #0" } } */
+/* { dg-final { scan-assembler "vcle\.s8\[ 	\]+\[qQ\]\[0-9\]+, \[qQ\]\[0-9\]+, #0" } } */
+/* { dg-final { scan-assembler "vcle\.s16\[ 	\]+\[qQ\]\[0-9\]+, \[qQ\]\[0-9\]+, #0" } } */
+/* { dg-final { scan-assembler "vcle\.s32\[ 	\]+\[qQ\]\[0-9\]+, \[qQ\]\[0-9\]+, #0" } } */
 /* { dg-final { scan-assembler-times "vceq\.i8\[ 	\]+\[dD\]\[0-9\]+, \[dD\]\[0-9\]+, #0" 2 } } */
 /* { dg-final { scan-assembler-times "vceq\.i16\[ 	\]+\[dD\]\[0-9\]+, \[dD\]\[0-9\]+, #0" 2 } } */
 /* { dg-final { scan-assembler-times "vceq\.i32\[ 	\]+\[dD\]\[0-9\]+, \[dD\]\[0-9\]+, #0" 2 } } */
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/pr26702.c
@@ -0,0 +1,4 @@
+/* { dg-do compile { target arm*-*-eabi* } } */
+/* { dg-final { scan-assembler "\\.size\[\\t \]+static_foo, 4" } } */
+int foo;
+static int static_foo;
--- a/src/gcc/testsuite/gcc.target/arm/pr42172-1.c
+++ b/src/gcc/testsuite/gcc.target/arm/pr42172-1.c
@@ -16,4 +16,4 @@ void init_A (struct A *this)
   this->f4 = 0;
 }
 
-/* { dg-final { scan-assembler-times "ldr" 1 } } */
+/* { dg-final { scan-assembler-times "str" 1 } } */
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/pr64208.c
@@ -0,0 +1,25 @@
+/* { dg-do compile } */
+/* { dg-skip-if "Test is specific to the iWMMXt" { arm*-*-* } { "-mcpu=*" } { "-mcpu=iwmmxt" } } */
+/* { dg-skip-if "Test is specific to the iWMMXt" { arm*-*-* } { "-mabi=*" } { "-mabi=iwmmxt" } } */
+/* { dg-skip-if "Test is specific to the iWMMXt" { arm*-*-* } { "-march=*" } { "-march=iwmmxt" } } */
+/* { dg-skip-if "Test is specific to ARM mode" { arm*-*-* } { "-mthumb" } { "" } } */
+/* { dg-require-effective-target arm32 } */
+/* { dg-require-effective-target arm_iwmmxt_ok } */
+/* { dg-options "-O1 -mcpu=iwmmxt" } */
+
+long long x6(void);
+void x7(long long, long long);
+void x8(long long);
+
+int x0;
+long long *x1;
+
+void x2(void) {
+  long long *x3 = x1;
+  while (x1) {
+    long long x4 = x0, x5 = x6();
+    x7(x4, x5);
+    x8(x5);
+    *x3 = 0;
+  }
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/pr64616.c
@@ -0,0 +1,15 @@
+/* { dg-do compile } */
+/* { dg-options "-O2 -fdump-rtl-cprop2" } */
+
+int f (int);
+unsigned int glob;
+
+void
+g ()
+{
+  while (f (glob));
+  glob = 5;
+}
+
+/* { dg-final { scan-rtl-dump "GLOBAL COPY-PROP" "cprop2" } } */
+/* { dg-final { cleanup-rtl-dump "cprop2" } } */
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/pr64818.c
@@ -0,0 +1,30 @@
+/* { dg-do compile } */
+/* { dg-options "-O1" } */
+
+char temp[16];
+extern int foo1 (void);
+
+void foo (void)
+{
+  int i;
+  int len;
+
+  while (1)
+  {
+    len = foo1 ();
+    register int a asm ("r0") = 5;
+    register char *b asm ("r1") = temp;
+    register int c asm ("r2") = len;
+    asm volatile ("mov %[r0], %[r0]\n  mov %[r1], %[r1]\n  mov %[r2], %[r2]\n"
+		   : "+m"(*b)
+		   : [r0]"r"(a), [r1]"r"(b), [r2]"r"(c));
+
+    for (i = 0; i < len; i++)
+    {
+      if (temp[i] == 10)
+      return;
+    }
+  }
+}
+
+/* { dg-final { scan-assembler "\[\\t \]+mov\ r1,\ r1" } } */
--- a/src/gcc/testsuite/gcc.target/arm/pr65067.c
+++ b/src/gcc/testsuite/gcc.target/arm/pr65067.c
@@ -1,4 +1,5 @@
 /* { dg-do compile } */
+/* { dg-require-effective-target arm_thumb2_ok } */
 /* { dg-options "-mthumb -mcpu=cortex-m3 -O2" } */
 
 struct tmp {
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/pr65710.c
@@ -0,0 +1,120 @@
+/* { dg-do compile } */
+/* { dg-skip-if "do not override -mfloat-abi" { *-*-* } { "-mfloat-abi=*" } {"-mfloat-abi=soft" } } */
+/* { dg-options "-mthumb -O2 -mfloat-abi=soft -w" } */
+/* { dg-skip-if "" { ! { arm_thumb1_ok || arm_thumb2_ok } } } */
+
+struct ST {
+  char *buffer;
+  int used;
+};
+
+struct ST *h;
+
+enum { no_op, duplicate, pop_failure_jump, dummy_failure_jump };
+
+typedef struct {
+  unsigned pointer;
+} byte_fail_stack_elt_t;
+
+typedef struct { unsigned avail; } byte_fail_stack_type;
+
+typedef union {
+  byte_fail_stack_elt_t word;
+  struct {
+    unsigned match_null_string_p : 2;
+    unsigned is_active : 1;
+    unsigned ever_matched_something : 1;
+  } bits;
+} byte_register_info_type;
+
+static int a;
+int b = 0;
+int c, e, f;
+int *d, *g;
+
+int
+byte_re_match_2_internal_size2(const int p2, int p3, const int p4) {
+  int i, p;
+  char *j;
+  char k, l, m, n = h;
+  byte_fail_stack_type o;
+  byte_fail_stack_elt_t *q;
+  unsigned int s = (unsigned int)h;
+  long t, u;
+  char **v, *w, **x, **y, **t1;
+  byte_register_info_type *z, *t2 = __builtin_alloca(s);
+  x = __builtin_alloca(s);
+  y = __builtin_alloca(s);
+  z = __builtin_alloca(sizeof(byte_register_info_type));
+  k = p4 + byte_re_match_2_internal_size2;
+  if (p3)
+    f = p4;
+  for (;;) {
+    if (h == h->used) {
+      g = f;
+      if (o.avail) {
+        b = 1;
+        for (; i < s; i++)
+          t1[i] = w;
+        goto fail;
+      }
+      e = 30 > s;
+      d = p4;
+      d[s] = 1;
+      return;
+    }
+    switch (*h->buffer++) {
+    case no_op:
+      while (m && n ?: *g)
+        ;
+      y[*h->buffer] = z[*h->buffer].bits.match_null_string_p ? w == &a ?: w : w;
+      w = g;
+      if (t) {
+        char r = h;
+        while (r && z[r].bits.is_active)
+          r--;
+        if (r == 0)
+          ;
+        else
+          u = r;
+      }
+      switch (*j++)
+      case dummy_failure_jump:
+      i = j;
+      if (i)
+        if (z[*h->buffer].bits.ever_matched_something) {
+          unsigned r;
+          z[*h->buffer].bits.ever_matched_something = r = *h->buffer;
+          for (; r + *(h->buffer + 1); r++) {
+            v = x[r];
+            w[r] = y[r];
+          }
+        }
+      break;
+    case duplicate: {
+      char *t3 = p2 + p3;
+      if (t3)
+        break;
+    }
+      if ((p3 ?: p4) == k)
+        goto fail;
+    case pop_failure_jump:
+      for (; c; c--)
+        t2[c].word = q[o.avail];
+      char t4;
+      q = t4 = __builtin_allocamemcpy(t4 ?: (p <<= 1));
+    }
+    continue;
+  fail : {
+    unsigned t5;
+    t = q;
+    t5 = u;
+    for (; t5 >= t; t5--)
+      v[t5] = q[--o.avail].pointer;
+    switch (*h->buffer)
+    case pop_failure_jump:
+    goto fail;
+  }
+    m = &l;
+  }
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/pr65729.c
@@ -0,0 +1,10 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_hard_vfp_ok } */
+/* { dg-options "-O2 -march=armv7-a -mfloat-abi=hard -mfpu=vfpv3-d16" } */
+
+int foo (void)
+{
+  double x = 0.0;
+  asm volatile ("" : "+gw" (x));
+  return x;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/pr65924.c
@@ -0,0 +1,9 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_thumb2_ok } */
+/* { dg-options "-O2 -mthumb" } */
+
+int a, b, c;
+int fn1() {
+  if (b + a < 0)
+    c = 0;
+}
--- a/src/gcc/testsuite/gcc.target/arm/simd/simd.exp
+++ b/src/gcc/testsuite/gcc.target/arm/simd/simd.exp
@@ -27,9 +27,22 @@ load_lib gcc-dg.exp
 # Initialize `dg'.
 dg-init
 
+# If the target hardware supports NEON, the default action is "run", otherwise
+# just "compile".
+global dg-do-what-default
+set save-dg-do-what-default ${dg-do-what-default}
+if {![check_effective_target_arm_neon_ok]} then {
+  return
+} elseif {[is-effective-target arm_neon_hw]} then {
+  set dg-do-what-default run
+} else {
+  set dg-do-what-default compile
+}
+
 # Main loop.
 dg-runtest [lsort [glob -nocomplain $srcdir/$subdir/*.\[cCS\]]] \
 	"" ""
 
 # All done.
+set dg-do-what-default ${save-dg-do-what-default}
 dg-finish
--- a/src/gcc/testsuite/gcc.target/arm/simd/vextQf32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vextQf32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vextQf32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O3 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vextQp16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vextQp16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vextQp16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O3 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vextQp64_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vextQp64_1.c
@@ -1,6 +1,5 @@
 /* Test the `vextQp64' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
 /* { dg-require-effective-target arm_crypto_ok } */
 /* { dg-options "-save-temps -O3 -fno-inline" } */
 /* { dg-add-options arm_crypto } */
--- a/src/gcc/testsuite/gcc.target/arm/simd/vextQp8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vextQp8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vextQp8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O3 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vextQs16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vextQs16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vextQs16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O3 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vextQs32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vextQs32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vextQs32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O3 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vextQs64_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vextQs64_1.c
@@ -1,7 +1,5 @@
 /* Test the `vextQs64' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O3 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vextQs8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vextQs8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vextQs8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O3 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vextQu16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vextQu16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vextQu16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O3 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vextQu32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vextQu32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vextQu32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O3 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vextQu64_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vextQu64_1.c
@@ -1,7 +1,5 @@
 /* Test the `vextQu64' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O3 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vextQu8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vextQu8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vextQu8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O3 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vextf32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vextf32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vextf32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O3 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vextp16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vextp16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vextp16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O3 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vextp64_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vextp64_1.c
@@ -1,6 +1,5 @@
 /* Test the `vextp64' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
 /* { dg-require-effective-target arm_crypto_ok } */
 /* { dg-options "-save-temps -O3 -fno-inline" } */
 /* { dg-add-options arm_crypto } */
--- a/src/gcc/testsuite/gcc.target/arm/simd/vextp8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vextp8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vextp8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O3 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vexts16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vexts16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vexts16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O3 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vexts32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vexts32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vexts32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O3 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vexts64_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vexts64_1.c
@@ -1,7 +1,5 @@
 /* Test the `vexts64' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O3 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vexts8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vexts8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vexts8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O3 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vextu16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vextu16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vextu16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O3 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vextu32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vextu32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vextu32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O3 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vextu64_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vextu64_1.c
@@ -1,7 +1,5 @@
 /* Test the `vextu64' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O3 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vextu8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vextu8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vextu8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O3 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev16p8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev16p8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev16p8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev16qp8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev16qp8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev16q_p8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev16qs8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev16qs8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev16q_s8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev16qu8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev16qu8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev16q_u8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev16s8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev16s8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev16s8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev16u8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev16u8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev16u8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev32p16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev32p16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev32p16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev32p8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev32p8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev32p8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev32qp16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev32qp16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev32q_p16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev32qp8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev32qp8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev32q_p8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev32qs16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev32qs16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev32q_s16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev32qs8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev32qs8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev32q_s8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev32qu16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev32qu16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev32q_u16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev32qu8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev32qu8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev32q_u8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev32s16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev32s16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev32s16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev32s8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev32s8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev32s8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev32u16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev32u16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev32u16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev32u8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev32u8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev32u8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev64f32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev64f32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev64f32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev64p16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev64p16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev64p16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev64p8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev64p8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev64p8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev64qf32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev64qf32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev64q_f32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev64qp16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev64qp16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev64q_p16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev64qp8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev64qp8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev64q_p8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev64qs16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev64qs16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev64q_s16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev64qs32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev64qs32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev64q_s32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev64qs8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev64qs8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev64q_s8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev64qu16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev64qu16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev64q_u16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev64qu32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev64qu32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev64q_u32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev64qu8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev64qu8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev64q_u8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev64s16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev64s16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev64s16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev64s32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev64s32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev64s32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev64s8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev64s8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev64s8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev64u16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev64u16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev64u16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev64u32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev64u32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev64u32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev64u8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev64u8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev64u8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vtrnf32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vtrnf32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vtrnf32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vtrnp16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vtrnp16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vtrnp16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vtrnp8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vtrnp8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vtrnp8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vtrnqf32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vtrnqf32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vtrnQf32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vtrnqp16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vtrnqp16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vtrnQp16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vtrnqp8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vtrnqp8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vtrnQp8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vtrnqs16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vtrnqs16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vtrnQs16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vtrnqs32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vtrnqs32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vtrnQs32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vtrnqs8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vtrnqs8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vtrnQs8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vtrnqu16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vtrnqu16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vtrnQu16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vtrnqu32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vtrnqu32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vtrnQu32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vtrnqu8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vtrnqu8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vtrnQu8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vtrns16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vtrns16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vtrns16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vtrns32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vtrns32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vtrns32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vtrns8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vtrns8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vtrns8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vtrnu16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vtrnu16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vtrnu16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vtrnu32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vtrnu32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vtrnu32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vtrnu8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vtrnu8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vtrnu8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vuzpf32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vuzpf32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vuzpf32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vuzpp16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vuzpp16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vuzpp16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vuzpp8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vuzpp8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vuzpp8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vuzpqf32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vuzpqf32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vuzpQf32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vuzpqp16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vuzpqp16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vuzpQp16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vuzpqp8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vuzpqp8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vuzpQp8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vuzpqs16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vuzpqs16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vuzpQs16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vuzpqs32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vuzpqs32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vuzpQs32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vuzpqs8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vuzpqs8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vuzpQs8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vuzpqu16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vuzpqu16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vuzpQu16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vuzpqu32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vuzpqu32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vuzpQu32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vuzpqu8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vuzpqu8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vuzpQu8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vuzps16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vuzps16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vuzps16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vuzps32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vuzps32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vuzps32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vuzps8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vuzps8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vuzps8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vuzpu16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vuzpu16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vuzpu16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vuzpu32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vuzpu32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vuzpu32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vuzpu8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vuzpu8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vuzpu8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vzipf32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vzipf32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vzipf32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vzipp16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vzipp16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vzipp16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vzipp8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vzipp8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vzipp8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vzipqf32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vzipqf32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vzipQf32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vzipqp16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vzipqp16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vzipQp16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vzipqp8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vzipqp8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vzipQp8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vzipqs16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vzipqs16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vzipQs16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vzipqs32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vzipqs32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vzipQs32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vzipqs8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vzipqs8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vzipQs8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vzipqu16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vzipqu16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vzipQu16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vzipqu32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vzipqu32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vzipQu32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vzipqu8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vzipqu8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vzipQu8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vzips16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vzips16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vzips16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vzips32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vzips32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vzips32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vzips8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vzips8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vzips8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vzipu16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vzipu16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vzipu16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vzipu32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vzipu32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vzipu32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vzipu8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vzipu8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vzipu8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/unsigned-float.c
@@ -0,0 +1,20 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_vfp_ok } */
+/* { dg-options "-march=armv7-a -O1 -mfloat-abi=softfp" } */
+/* { dg-skip-if "need fp instructions" { *-*-* } { "-mfloat-abi=soft" } { "" } } */
+
+#include <stdint.h>
+
+double
+f1 (uint16_t x)
+{
+  return (double)(float)x;
+}
+
+float
+f2 (uint16_t x)
+{
+  return (float)(double)x;
+}
+
+/* { dg-final { scan-assembler-not "vcvt.(f32.f64|f64.f32)" } } */
--- a/src/gcc/tree-ssa-loop-ivopts.c
+++ b/src/gcc/tree-ssa-loop-ivopts.c
@@ -226,6 +226,7 @@ struct cost_pair
 struct iv_use
 {
   unsigned id;		/* The id of the use.  */
+  unsigned sub_id;	/* The id of the sub use.  */
   enum use_type type;	/* Type of the use.  */
   struct iv *iv;	/* The induction variable it is based on.  */
   gimple stmt;		/* Statement in that it occurs.  */
@@ -239,6 +240,11 @@ struct iv_use
 
   struct iv_cand *selected;
 			/* The selected candidate.  */
+
+  struct iv_use *next;	/* The next sub use.  */
+  tree addr_base;	/* Base address with const offset stripped.  */
+  unsigned HOST_WIDE_INT addr_offset;
+			/* Const offset stripped from base address.  */
 };
 
 /* The position where the iv is computed.  */
@@ -555,7 +561,11 @@ dump_iv (FILE *file, struct iv *iv)
 void
 dump_use (FILE *file, struct iv_use *use)
 {
-  fprintf (file, "use %d\n", use->id);
+  fprintf (file, "use %d", use->id);
+  if (use->sub_id)
+    fprintf (file, ".%d", use->sub_id);
+
+  fprintf (file, "\n");
 
   switch (use->type)
     {
@@ -604,8 +614,12 @@ dump_uses (FILE *file, struct ivopts_data *data)
   for (i = 0; i < n_iv_uses (data); i++)
     {
       use = iv_use (data, i);
-
-      dump_use (file, use);
+      do
+	{
+	  dump_use (file, use);
+	  use = use->next;
+	}
+      while (use);
       fprintf (file, "\n");
     }
 }
@@ -1326,33 +1340,84 @@ find_induction_variables (struct ivopts_data *data)
   return true;
 }
 
-/* Records a use of type USE_TYPE at *USE_P in STMT whose value is IV.  */
+/* Records a use of type USE_TYPE at *USE_P in STMT whose value is IV.
+   For address type use, ADDR_BASE is the stripped IV base, ADDR_OFFSET
+   is the const offset stripped from IV base.  For uses of other types,
+   ADDR_BASE and ADDR_OFFSET are zero by default.  */
 
 static struct iv_use *
 record_use (struct ivopts_data *data, tree *use_p, struct iv *iv,
-	    gimple stmt, enum use_type use_type)
+	    gimple stmt, enum use_type use_type, tree addr_base = NULL,
+	    unsigned HOST_WIDE_INT addr_offset = 0)
 {
   struct iv_use *use = XCNEW (struct iv_use);
 
   use->id = n_iv_uses (data);
+  use->sub_id = 0;
   use->type = use_type;
   use->iv = iv;
   use->stmt = stmt;
   use->op_p = use_p;
   use->related_cands = BITMAP_ALLOC (NULL);
+  use->next = NULL;
+  use->addr_base = addr_base;
+  use->addr_offset = addr_offset;
 
   /* To avoid showing ssa name in the dumps, if it was not reset by the
      caller.  */
   iv->ssa_name = NULL_TREE;
 
-  if (dump_file && (dump_flags & TDF_DETAILS))
-    dump_use (dump_file, use);
-
   data->iv_uses.safe_push (use);
 
   return use;
 }
 
+/* Records a sub use of type USE_TYPE at *USE_P in STMT whose value is IV.
+   The sub use is recorded under the one whose use id is ID_GROUP.  */
+
+static struct iv_use *
+record_sub_use (struct ivopts_data *data, tree *use_p,
+		    struct iv *iv, gimple stmt, enum use_type use_type,
+		    tree addr_base, unsigned HOST_WIDE_INT addr_offset,
+		    unsigned int id_group)
+{
+  struct iv_use *use = XCNEW (struct iv_use);
+  struct iv_use *group = iv_use (data, id_group);
+
+  use->id = group->id;
+  use->sub_id = 0;
+  use->type = use_type;
+  use->iv = iv;
+  use->stmt = stmt;
+  use->op_p = use_p;
+  use->related_cands = NULL;
+  use->addr_base = addr_base;
+  use->addr_offset = addr_offset;
+
+  /* Sub use list is maintained in offset ascending order.  */
+  if (addr_offset <= group->addr_offset)
+    {
+      use->related_cands = group->related_cands;
+      group->related_cands = NULL;
+      use->next = group;
+      data->iv_uses[id_group] = use;
+    }
+  else
+    {
+      struct iv_use *pre;
+      do
+	{
+	  pre = group;
+	  group = group->next;
+	}
+      while (group && addr_offset > group->addr_offset);
+      use->next = pre->next;
+      pre->next = use;
+    }
+
+  return use;
+}
+
 /* Checks whether OP is a loop-level invariant and if so, records it.
    NONLINEAR_USE is true if the invariant is used in a way we do not
    handle specially.  */
@@ -1837,6 +1902,50 @@ may_be_nonaddressable_p (tree expr)
   return false;
 }
 
+static tree
+strip_offset (tree expr, unsigned HOST_WIDE_INT *offset);
+
+/* Record a use of type USE_TYPE at *USE_P in STMT whose value is IV.
+   If there is an existing use which has same stripped iv base and step,
+   this function records this one as a sub use to that; otherwise records
+   it as a normal one.  */
+
+static struct iv_use *
+record_group_use (struct ivopts_data *data, tree *use_p,
+		  struct iv *iv, gimple stmt, enum use_type use_type)
+{
+  unsigned int i;
+  struct iv_use *use;
+  tree addr_base;
+  unsigned HOST_WIDE_INT addr_offset;
+
+  /* Only support sub use for address type uses, that is, with base
+     object.  */
+  if (!iv->base_object)
+    return record_use (data, use_p, iv, stmt, use_type);
+
+  addr_base = strip_offset (iv->base, &addr_offset);
+  for (i = 0; i < n_iv_uses (data); i++)
+    {
+      use = iv_use (data, i);
+      if (use->type != USE_ADDRESS || !use->iv->base_object)
+	continue;
+
+      /* Check if it has the same stripped base and step.  */
+      if (operand_equal_p (iv->base_object, use->iv->base_object, 0)
+	  && operand_equal_p (iv->step, use->iv->step, 0)
+	  && operand_equal_p (addr_base, use->addr_base, 0))
+	break;
+    }
+
+  if (i == n_iv_uses (data))
+    return record_use (data, use_p, iv, stmt,
+		       use_type, addr_base, addr_offset);
+  else
+    return record_sub_use (data, use_p, iv, stmt,
+			   use_type, addr_base, addr_offset, i);
+}
+
 /* Finds addresses in *OP_P inside STMT.  */
 
 static void
@@ -1947,7 +2056,7 @@ find_interesting_uses_address (struct ivopts_data *data, gimple stmt, tree *op_p
     }
 
   civ = alloc_iv (base, step);
-  record_use (data, op_p, civ, stmt, USE_ADDRESS);
+  record_group_use (data, op_p, civ, stmt, USE_ADDRESS);
   return;
 
 fail:
@@ -2133,6 +2242,172 @@ find_interesting_uses (struct ivopts_data *data)
   free (body);
 }
 
+/* Compute maximum offset of [base + offset] addressing mode
+   for memory reference represented by USE.  */
+
+static HOST_WIDE_INT
+compute_max_addr_offset (struct iv_use *use)
+{
+  int width;
+  rtx reg, addr;
+  HOST_WIDE_INT i, off;
+  unsigned list_index, num;
+  addr_space_t as;
+  machine_mode mem_mode, addr_mode;
+  static vec<HOST_WIDE_INT> max_offset_list;
+
+  as = TYPE_ADDR_SPACE (TREE_TYPE (use->iv->base));
+  mem_mode = TYPE_MODE (TREE_TYPE (*use->op_p));
+
+  num = max_offset_list.length ();
+  list_index = (unsigned) as * MAX_MACHINE_MODE + (unsigned) mem_mode;
+  if (list_index >= num)
+    {
+      max_offset_list.safe_grow (list_index + MAX_MACHINE_MODE);
+      for (; num < max_offset_list.length (); num++)
+	max_offset_list[num] = -1;
+    }
+
+  off = max_offset_list[list_index];
+  if (off != -1)
+    return off;
+
+  addr_mode = targetm.addr_space.address_mode (as);
+  reg = gen_raw_REG (addr_mode, LAST_VIRTUAL_REGISTER + 1);
+  addr = gen_rtx_fmt_ee (PLUS, addr_mode, reg, NULL_RTX);
+
+  width = GET_MODE_BITSIZE (addr_mode) - 1;
+  if (width > (HOST_BITS_PER_WIDE_INT - 1))
+    width = HOST_BITS_PER_WIDE_INT - 1;
+
+  for (i = width; i > 0; i--)
+    {
+      off = ((unsigned HOST_WIDE_INT) 1 << i) - 1;
+      XEXP (addr, 1) = gen_int_mode (off, addr_mode);
+      if (memory_address_addr_space_p (mem_mode, addr, as))
+	break;
+
+      /* For some strict-alignment targets, the offset must be naturally
+	 aligned.  Try an aligned offset if mem_mode is not QImode.  */
+      off = ((unsigned HOST_WIDE_INT) 1 << i);
+      if (off > GET_MODE_SIZE (mem_mode) && mem_mode != QImode)
+	{
+	  off -= GET_MODE_SIZE (mem_mode);
+	  XEXP (addr, 1) = gen_int_mode (off, addr_mode);
+	  if (memory_address_addr_space_p (mem_mode, addr, as))
+	    break;
+	}
+    }
+  if (i == 0)
+    off = 0;
+
+  max_offset_list[list_index] = off;
+  return off;
+}
+
+/* Check if all small groups should be split.  Return true if and
+   only if:
+
+     1) At least one groups contain two uses with different offsets.
+     2) No group contains more than two uses with different offsets.
+
+   Return false otherwise.  We want to split such groups because:
+
+     1) Small groups don't have much benefit and may interfer with
+	general candidate selection.
+     2) Size for problem with only small groups is usually small and
+	general algorithm can handle it well.
+
+   TODO -- Above claim may not hold when auto increment is supported.  */
+
+static bool
+split_all_small_groups (struct ivopts_data *data)
+{
+  bool split_p = false;
+  unsigned int i, n, distinct;
+  struct iv_use *pre, *use;
+
+  n = n_iv_uses (data);
+  for (i = 0; i < n; i++)
+    {
+      use = iv_use (data, i);
+      if (!use->next)
+	continue;
+
+      distinct = 1;
+      gcc_assert (use->type == USE_ADDRESS);
+      for (pre = use, use = use->next; use; pre = use, use = use->next)
+	{
+	  if (pre->addr_offset != use->addr_offset)
+	    distinct++;
+
+	  if (distinct > 2)
+	    return false;
+	}
+      if (distinct == 2)
+	split_p = true;
+    }
+
+  return split_p;
+}
+
+/* For each group of address type uses, this function further groups
+   these uses according to the maximum offset supported by target's
+   [base + offset] addressing mode.  */
+
+static void
+group_address_uses (struct ivopts_data *data)
+{
+  HOST_WIDE_INT max_offset = -1;
+  unsigned int i, n, sub_id;
+  struct iv_use *pre, *use;
+  unsigned HOST_WIDE_INT addr_offset_first;
+
+  /* Reset max offset to split all small groups.  */
+  if (split_all_small_groups (data))
+    max_offset = 0;
+
+  n = n_iv_uses (data);
+  for (i = 0; i < n; i++)
+    {
+      use = iv_use (data, i);
+      if (!use->next)
+	continue;
+
+      gcc_assert (use->type == USE_ADDRESS);
+      if (max_offset != 0)
+	max_offset = compute_max_addr_offset (use);
+
+      while (use)
+	{
+	  sub_id = 0;
+	  addr_offset_first = use->addr_offset;
+	  /* Only uses with offset that can fit in offset part against
+	     the first use can be grouped together.  */
+	  for (pre = use, use = use->next;
+	       use && (use->addr_offset - addr_offset_first
+		       <= (unsigned HOST_WIDE_INT) max_offset);
+	       pre = use, use = use->next)
+	    {
+	      use->id = pre->id;
+	      use->sub_id = ++sub_id;
+	    }
+
+	  /* Break the list and create new group.  */
+	  if (use)
+	    {
+	      pre->next = NULL;
+	      use->id = n_iv_uses (data);
+	      use->related_cands = BITMAP_ALLOC (NULL);
+	      data->iv_uses.safe_push (use);
+	    }
+	}
+    }
+
+  if (dump_file && (dump_flags & TDF_DETAILS))
+    dump_uses (dump_file, data);
+}
+
 /* Strips constant offsets from EXPR and stores them to OFFSET.  If INSIDE_ADDR
    is true, assume we are inside an address.  If TOP_COMPREF is true, assume
    we are at the top-level of the processed address.  */
@@ -2556,6 +2831,8 @@ static void
 add_candidate (struct ivopts_data *data,
 	       tree base, tree step, bool important, struct iv_use *use)
 {
+  gcc_assert (use == NULL || use->sub_id == 0);
+
   if (ip_normal_pos (data->current_loop))
     add_candidate_1 (data, base, step, important, IP_NORMAL, use, NULL);
   if (ip_end_pos (data->current_loop)
@@ -2785,11 +3062,22 @@ new_cost (unsigned runtime, unsigned complexity)
   return cost;
 }
 
+/* Returns true if COST is infinite.  */
+
+static bool
+infinite_cost_p (comp_cost cost)
+{
+  return cost.cost == INFTY;
+}
+
 /* Adds costs COST1 and COST2.  */
 
 static comp_cost
 add_costs (comp_cost cost1, comp_cost cost2)
 {
+  if (infinite_cost_p (cost1) || infinite_cost_p (cost2))
+    return infinite_cost;
+
   cost1.cost += cost2.cost;
   cost1.complexity += cost2.complexity;
 
@@ -2818,14 +3106,6 @@ compare_costs (comp_cost cost1, comp_cost cost2)
   return cost1.cost - cost2.cost;
 }
 
-/* Returns true if COST is infinite.  */
-
-static bool
-infinite_cost_p (comp_cost cost)
-{
-  return cost.cost == INFTY;
-}
-
 /* Sets cost of (USE, CANDIDATE) pair to COST and record that it depends
    on invariants DEPENDS_ON and that the value used in expressing it
    is VALUE, and in case of iv elimination the comparison operator is COMP.  */
@@ -4300,7 +4580,15 @@ get_computation_cost_at (struct ivopts_data *data,
       cost.cost += add_cost (data->speed, TYPE_MODE (ctype));
     }
 
-  if (inv_expr_id)
+  /* Set of invariants depended on by sub use has already been computed
+     for the first use in the group.  */
+  if (use->sub_id)
+    {
+      cost.cost = 0;
+      if (depends_on && *depends_on)
+	bitmap_clear (*depends_on);
+    }
+  else if (inv_expr_id)
     {
       *inv_expr_id =
           get_loop_invariant_expr_id (data, ubase, cbase, ratio, address_p);
@@ -4429,6 +4717,8 @@ determine_use_iv_cost_address (struct ivopts_data *data,
   bitmap depends_on;
   bool can_autoinc;
   int inv_expr_id = -1;
+  struct iv_use *sub_use;
+  comp_cost sub_cost;
   comp_cost cost = get_computation_cost (data, use, cand, true, &depends_on,
 					 &can_autoinc, &inv_expr_id);
 
@@ -4442,6 +4732,15 @@ determine_use_iv_cost_address (struct ivopts_data *data,
       else if (cand->pos == IP_AFTER_USE || cand->pos == IP_BEFORE_USE)
 	cost = infinite_cost;
     }
+  for (sub_use = use->next;
+       sub_use && !infinite_cost_p (cost);
+       sub_use = sub_use->next)
+    {
+       sub_cost = get_computation_cost (data, sub_use, cand, true, &depends_on,
+					&can_autoinc, &inv_expr_id);
+       cost = add_costs (cost, sub_cost);
+    }
+
   set_use_iv_cost (data, use, cand, cost, depends_on, NULL_TREE, ERROR_MARK,
                    inv_expr_id);
 
@@ -6588,8 +6887,8 @@ adjust_iv_update_pos (struct iv_cand *cand, struct iv_use *use)
 /* Rewrites USE (address that is an iv) using candidate CAND.  */
 
 static void
-rewrite_use_address (struct ivopts_data *data,
-		     struct iv_use *use, struct iv_cand *cand)
+rewrite_use_address_1 (struct ivopts_data *data,
+		       struct iv_use *use, struct iv_cand *cand)
 {
   aff_tree aff;
   gimple_stmt_iterator bsi = gsi_for_stmt (use->stmt);
@@ -6624,6 +6923,28 @@ rewrite_use_address (struct ivopts_data *data,
   *use->op_p = ref;
 }
 
+/* Rewrites USE (address that is an iv) using candidate CAND.  If it's the
+   first use of a group, rewrites sub uses in the group too.  */
+
+static void
+rewrite_use_address (struct ivopts_data *data,
+		      struct iv_use *use, struct iv_cand *cand)
+{
+  struct iv_use *next;
+
+  gcc_assert (use->sub_id == 0);
+  rewrite_use_address_1 (data, use, cand);
+  update_stmt (use->stmt);
+
+  for (next = use->next; next != NULL; next = next->next)
+    {
+      rewrite_use_address_1 (data, next, cand);
+      update_stmt (next->stmt);
+    }
+
+  return;
+}
+
 /* Rewrites USE (the condition such that one of the arguments is an iv) using
    candidate CAND.  */
 
@@ -6899,6 +7220,18 @@ free_loop_data (struct ivopts_data *data)
   for (i = 0; i < n_iv_uses (data); i++)
     {
       struct iv_use *use = iv_use (data, i);
+      struct iv_use *pre = use, *sub = use->next;
+
+      while (sub)
+	{
+	  gcc_assert (sub->related_cands == NULL);
+	  gcc_assert (sub->n_map_members == 0 && sub->cost_map == NULL);
+
+	  free (sub->iv);
+	  pre = sub;
+	  sub = sub->next;
+	  free (pre);
+	}
 
       free (use->iv);
       BITMAP_FREE (use->related_cands);
@@ -7025,6 +7358,7 @@ tree_ssa_iv_optimize_loop (struct ivopts_data *data, struct loop *loop)
 
   /* Finds interesting uses (item 1).  */
   find_interesting_uses (data);
+  group_address_uses (data);
   if (n_iv_uses (data) > MAX_CONSIDERED_USES)
     goto finish;
 
--- a/src/gcc/tree-ssa-math-opts.c
+++ b/src/gcc/tree-ssa-math-opts.c
@@ -143,6 +143,7 @@ along with GCC; see the file COPYING3.  If not see
 #include "target.h"
 #include "gimple-pretty-print.h"
 #include "builtins.h"
+#include "params.h"
 
 /* FIXME: RTL headers have to be included here for optabs.  */
 #include "rtl.h"		/* Because optabs.h wants enum rtx_code.  */
@@ -1148,6 +1149,357 @@ build_and_insert_cast (gimple_stmt_iterator *gsi, location_t loc,
   return result;
 }
 
+struct pow_synth_sqrt_info
+{
+  bool *factors;
+  unsigned int deepest;
+  unsigned int num_mults;
+};
+
+/* Return true iff the real value C can be represented as a
+   sum of powers of 0.5 up to N.  That is:
+   C == SUM<i from 1..N> (a[i]*(0.5**i)) where a[i] is either 0 or 1.
+   Record in INFO the various parameters of the synthesis algorithm such
+   as the factors a[i], the maximum 0.5 power and the number of
+   multiplications that will be required.  */
+
+bool
+representable_as_half_series_p (REAL_VALUE_TYPE c, unsigned n,
+				 struct pow_synth_sqrt_info *info)
+{
+  REAL_VALUE_TYPE factor = dconsthalf;
+  REAL_VALUE_TYPE remainder = c;
+
+  info->deepest = 0;
+  info->num_mults = 0;
+  memset (info->factors, 0, n * sizeof (bool));
+
+  for (unsigned i = 0; i < n; i++)
+    {
+      REAL_VALUE_TYPE res;
+
+      /* If something inexact happened bail out now.  */
+      if (REAL_ARITHMETIC (res, MINUS_EXPR, remainder, factor))
+	return false;
+
+      /* We have hit zero.  The number is representable as a sum
+         of powers of 0.5.  */
+      if (REAL_VALUES_EQUAL (res, dconst0))
+	{
+	  info->factors[i] = true;
+	  info->deepest = i + 1;
+	  return true;
+	}
+      else if (!REAL_VALUE_NEGATIVE (res))
+	{
+	  remainder = res;
+	  info->factors[i] = true;
+	  info->num_mults++;
+	}
+      else
+	info->factors[i] = false;
+
+      REAL_ARITHMETIC (factor, MULT_EXPR, factor, dconsthalf);
+    }
+  return false;
+}
+
+/* Return the tree corresponding to FN being applied
+   to ARG N times at GSI and LOC.
+   Look up previous results from CACHE if need be.
+   cache[0] should contain just plain ARG i.e. FN applied to ARG 0 times.  */
+
+static tree
+get_fn_chain (tree arg, unsigned int n, gimple_stmt_iterator *gsi,
+	      tree fn, location_t loc, tree *cache)
+{
+  tree res = cache[n];
+  if (!res)
+    {
+      tree prev = get_fn_chain (arg, n - 1, gsi, fn, loc, cache);
+      res = build_and_insert_call (gsi, loc, fn, prev);
+      cache[n] = res;
+    }
+
+  return res;
+}
+
+/* Print to STREAM the repeated application of function FNAME to ARG
+   N times.  So, for FNAME = "foo", ARG = "x", N = 2 it would print:
+   "foo (foo (x))".  */
+
+static void
+print_nested_fn (FILE* stream, const char *fname, const char* arg,
+		 unsigned int n)
+{
+  if (n == 0)
+    fprintf (stream, "%s", arg);
+  else
+    {
+      fprintf (stream, "%s (", fname);
+      print_nested_fn (stream, fname, arg, n - 1);
+      fprintf (stream, ")");
+    }
+}
+
+/* Print to STREAM the fractional sequence of sqrt chains
+   applied to ARG, described by INFO.  Used for the dump file.  */
+
+static void
+dump_fractional_sqrt_sequence (FILE *stream, const char *arg,
+			        struct pow_synth_sqrt_info *info)
+{
+  for (unsigned int i = 0; i < info->deepest; i++)
+    {
+      bool is_set = info->factors[i];
+      if (is_set)
+	{
+	  print_nested_fn (stream, "sqrt", arg, i + 1);
+	  if (i != info->deepest - 1)
+	    fprintf (stream, " * ");
+	}
+    }
+}
+
+/* Print to STREAM a representation of raising ARG to an integer
+   power N.  Used for the dump file.  */
+
+static void
+dump_integer_part (FILE *stream, const char* arg, HOST_WIDE_INT n)
+{
+  if (n > 1)
+    fprintf (stream, "powi (%s, " HOST_WIDE_INT_PRINT_DEC ")", arg, n);
+  else if (n == 1)
+    fprintf (stream, "%s", arg);
+}
+
+/* Attempt to synthesize a POW[F] (ARG0, ARG1) call using chains of
+   square roots.  Place at GSI and LOC.  Limit the maximum depth
+   of the sqrt chains to MAX_DEPTH.  Return the tree holding the
+   result of the expanded sequence or NULL_TREE if the expansion failed.
+
+   This routine assumes that ARG1 is a real number with a fractional part
+   (the integer exponent case will have been handled earlier in
+   gimple_expand_builtin_pow).
+
+   For ARG1 > 0.0:
+   * For ARG1 composed of a whole part WHOLE_PART and a fractional part
+     FRAC_PART i.e. WHOLE_PART == floor (ARG1) and
+                    FRAC_PART == ARG1 - WHOLE_PART:
+     Produce POWI (ARG0, WHOLE_PART) * POW (ARG0, FRAC_PART) where
+     POW (ARG0, FRAC_PART) is expanded as a product of square root chains
+     if it can be expressed as such, that is if FRAC_PART satisfies:
+     FRAC_PART == <SUM from i = 1 until MAX_DEPTH> (a[i] * (0.5**i))
+     where integer a[i] is either 0 or 1.
+
+     Example:
+     POW (x, 3.625) == POWI (x, 3) * POW (x, 0.625)
+       --> POWI (x, 3) * SQRT (x) * SQRT (SQRT (SQRT (x)))
+
+   For ARG1 < 0.0 there are two approaches:
+   * (A) Expand to 1.0 / POW (ARG0, -ARG1) where POW (ARG0, -ARG1)
+         is calculated as above.
+
+     Example:
+     POW (x, -5.625) == 1.0 / POW (x, 5.625)
+       -->  1.0 / (POWI (x, 5) * SQRT (x) * SQRT (SQRT (SQRT (x))))
+
+   * (B) : WHOLE_PART := - ceil (abs (ARG1))
+           FRAC_PART  := ARG1 - WHOLE_PART
+     and expand to POW (x, FRAC_PART) / POWI (x, WHOLE_PART).
+     Example:
+     POW (x, -5.875) == POW (x, 0.125) / POWI (X, 6)
+       --> SQRT (SQRT (SQRT (x))) / (POWI (x, 6))
+
+   For ARG1 < 0.0 we choose between (A) and (B) depending on
+   how many multiplications we'd have to do.
+   So, for the example in (B): POW (x, -5.875), if we were to
+   follow algorithm (A) we would produce:
+   1.0 / POWI (X, 5) * SQRT (X) * SQRT (SQRT (X)) * SQRT (SQRT (SQRT (X)))
+   which contains more multiplications than approach (B).
+
+   Hopefully, this approach will eliminate potentially expensive POW library
+   calls when unsafe floating point math is enabled and allow the compiler to
+   further optimise the multiplies, square roots and divides produced by this
+   function.  */
+
+static tree
+expand_pow_as_sqrts (gimple_stmt_iterator *gsi, location_t loc,
+		     tree arg0, tree arg1, HOST_WIDE_INT max_depth)
+{
+  tree type = TREE_TYPE (arg0);
+  machine_mode mode = TYPE_MODE (type);
+  tree sqrtfn = mathfn_built_in (type, BUILT_IN_SQRT);
+  bool one_over = true;
+
+  if (!sqrtfn)
+    return NULL_TREE;
+
+  if (TREE_CODE (arg1) != REAL_CST)
+    return NULL_TREE;
+
+  REAL_VALUE_TYPE exp_init = TREE_REAL_CST (arg1);
+
+  gcc_assert (max_depth > 0);
+  tree *cache = XALLOCAVEC (tree, max_depth + 1);
+
+  struct pow_synth_sqrt_info synth_info;
+  synth_info.factors = XALLOCAVEC (bool, max_depth + 1);
+  synth_info.deepest = 0;
+  synth_info.num_mults = 0;
+
+  bool neg_exp = REAL_VALUE_NEGATIVE (exp_init);
+  REAL_VALUE_TYPE exp = real_value_abs (&exp_init);
+
+  /* The whole and fractional parts of exp.  */
+  REAL_VALUE_TYPE whole_part;
+  REAL_VALUE_TYPE frac_part;
+
+  real_floor (&whole_part, mode, &exp);
+  REAL_ARITHMETIC (frac_part, MINUS_EXPR, exp, whole_part);
+
+
+  REAL_VALUE_TYPE ceil_whole = dconst0;
+  REAL_VALUE_TYPE ceil_fract = dconst0;
+
+  if (neg_exp)
+    {
+      real_ceil (&ceil_whole, mode, &exp);
+      REAL_ARITHMETIC (ceil_fract, MINUS_EXPR, ceil_whole, exp);
+    }
+
+  if (!representable_as_half_series_p (frac_part, max_depth, &synth_info))
+    return NULL_TREE;
+
+  /* Check whether it's more profitable to not use 1.0 / ...  */
+  if (neg_exp)
+    {
+      struct pow_synth_sqrt_info alt_synth_info;
+      alt_synth_info.factors = XALLOCAVEC (bool, max_depth + 1);
+      alt_synth_info.deepest = 0;
+      alt_synth_info.num_mults = 0;
+
+      if (representable_as_half_series_p (ceil_fract, max_depth,
+					   &alt_synth_info)
+	  && alt_synth_info.deepest <= synth_info.deepest
+	  && alt_synth_info.num_mults < synth_info.num_mults)
+	{
+	  whole_part = ceil_whole;
+	  frac_part = ceil_fract;
+	  synth_info.deepest = alt_synth_info.deepest;
+	  synth_info.num_mults = alt_synth_info.num_mults;
+	  memcpy (synth_info.factors, alt_synth_info.factors,
+		  (max_depth + 1) * sizeof (bool));
+	  one_over = false;
+	}
+    }
+
+  HOST_WIDE_INT n = real_to_integer (&whole_part);
+  REAL_VALUE_TYPE cint;
+  real_from_integer (&cint, VOIDmode, n, SIGNED);
+
+  if (!real_identical (&whole_part, &cint))
+    return NULL_TREE;
+
+  if (powi_cost (n) + synth_info.num_mults > POWI_MAX_MULTS)
+    return NULL_TREE;
+
+  memset (cache, 0, (max_depth + 1) * sizeof (tree));
+
+  tree integer_res = n == 0 ? build_real (type, dconst1) : arg0;
+
+  /* Calculate the integer part of the exponent.  */
+  if (n > 1)
+    {
+      integer_res = gimple_expand_builtin_powi (gsi, loc, arg0, n);
+      if (!integer_res)
+	return NULL_TREE;
+    }
+
+  if (dump_file)
+    {
+      char string[64];
+
+      real_to_decimal (string, &exp_init, sizeof (string), 0, 1);
+      fprintf (dump_file, "synthesizing pow (x, %s) as:\n", string);
+
+      if (neg_exp)
+	{
+	  if (one_over)
+	    {
+	      fprintf (dump_file, "1.0 / (");
+	      dump_integer_part (dump_file, "x", n);
+	      if (n > 0)
+	        fprintf (dump_file, " * ");
+	      dump_fractional_sqrt_sequence (dump_file, "x", &synth_info);
+	      fprintf (dump_file, ")");
+	    }
+	  else
+	    {
+	      dump_fractional_sqrt_sequence (dump_file, "x", &synth_info);
+	      fprintf (dump_file, " / (");
+	      dump_integer_part (dump_file, "x", n);
+	      fprintf (dump_file, ")");
+	    }
+	}
+      else
+	{
+	  dump_fractional_sqrt_sequence (dump_file, "x", &synth_info);
+	  if (n > 0)
+	    fprintf (dump_file, " * ");
+	  dump_integer_part (dump_file, "x", n);
+	}
+
+      fprintf (dump_file, "\ndeepest sqrt chain: %d\n", synth_info.deepest);
+    }
+
+
+  tree fract_res = NULL_TREE;
+  cache[0] = arg0;
+
+  /* Calculate the fractional part of the exponent.  */
+  for (unsigned i = 0; i < synth_info.deepest; i++)
+    {
+      if (synth_info.factors[i])
+	{
+	  tree sqrt_chain = get_fn_chain (arg0, i + 1, gsi, sqrtfn, loc, cache);
+
+	  if (!fract_res)
+	      fract_res = sqrt_chain;
+
+	  else
+	    fract_res = build_and_insert_binop (gsi, loc, "powroot", MULT_EXPR,
+					   fract_res, sqrt_chain);
+	}
+    }
+
+  tree res = NULL_TREE;
+
+  if (neg_exp)
+    {
+      if (one_over)
+	{
+	  if (n > 0)
+	    res = build_and_insert_binop (gsi, loc, "powroot", MULT_EXPR,
+					   fract_res, integer_res);
+	  else
+	    res = fract_res;
+
+	  res = build_and_insert_binop (gsi, loc, "powrootrecip", RDIV_EXPR,
+					  build_real (type, dconst1), res);
+	}
+      else
+	{
+	  res = build_and_insert_binop (gsi, loc, "powroot", RDIV_EXPR,
+					 fract_res, integer_res);
+	}
+    }
+  else
+    res = build_and_insert_binop (gsi, loc, "powroot", MULT_EXPR,
+				   fract_res, integer_res);
+  return res;
+}
+
 /* ARG0 and ARG1 are the two arguments to a pow builtin call in GSI
    with location info LOC.  If possible, create an equivalent and
    less expensive sequence of statements prior to GSI, and return an
@@ -1157,13 +1509,17 @@ static tree
 gimple_expand_builtin_pow (gimple_stmt_iterator *gsi, location_t loc, 
 			   tree arg0, tree arg1)
 {
-  REAL_VALUE_TYPE c, cint, dconst1_4, dconst3_4, dconst1_3, dconst1_6;
+  REAL_VALUE_TYPE c, cint, dconst1_3, dconst1_4, dconst1_6;
   REAL_VALUE_TYPE c2, dconst3;
   HOST_WIDE_INT n;
-  tree type, sqrtfn, cbrtfn, sqrt_arg0, sqrt_sqrt, result, cbrt_x, powi_cbrt_x;
+  tree type, sqrtfn, cbrtfn, sqrt_arg0, result, cbrt_x, powi_cbrt_x;
   machine_mode mode;
+  bool speed_p = optimize_bb_for_speed_p (gsi_bb (*gsi));
   bool hw_sqrt_exists, c_is_int, c2_is_int;
 
+  dconst1_4 = dconst1;
+  SET_REAL_EXP (&dconst1_4, REAL_EXP (&dconst1_4) - 2);
+
   /* If the exponent isn't a constant, there's nothing of interest
      to be done.  */
   if (TREE_CODE (arg1) != REAL_CST)
@@ -1179,7 +1535,7 @@ gimple_expand_builtin_pow (gimple_stmt_iterator *gsi, location_t loc,
   if (c_is_int
       && ((n >= -1 && n <= 2)
 	  || (flag_unsafe_math_optimizations
-	      && optimize_bb_for_speed_p (gsi_bb (*gsi))
+	      && speed_p
 	      && powi_cost (n) <= POWI_MAX_MULTS)))
     return gimple_expand_builtin_powi (gsi, loc, arg0, n);
 
@@ -1196,49 +1552,8 @@ gimple_expand_builtin_pow (gimple_stmt_iterator *gsi, location_t loc,
       && !HONOR_SIGNED_ZEROS (mode))
     return build_and_insert_call (gsi, loc, sqrtfn, arg0);
 
-  /* Optimize pow(x,0.25) = sqrt(sqrt(x)).  Assume on most machines that
-     a builtin sqrt instruction is smaller than a call to pow with 0.25,
-     so do this optimization even if -Os.  Don't do this optimization
-     if we don't have a hardware sqrt insn.  */
-  dconst1_4 = dconst1;
-  SET_REAL_EXP (&dconst1_4, REAL_EXP (&dconst1_4) - 2);
   hw_sqrt_exists = optab_handler (sqrt_optab, mode) != CODE_FOR_nothing;
 
-  if (flag_unsafe_math_optimizations
-      && sqrtfn
-      && REAL_VALUES_EQUAL (c, dconst1_4)
-      && hw_sqrt_exists)
-    {
-      /* sqrt(x)  */
-      sqrt_arg0 = build_and_insert_call (gsi, loc, sqrtfn, arg0);
-
-      /* sqrt(sqrt(x))  */
-      return build_and_insert_call (gsi, loc, sqrtfn, sqrt_arg0);
-    }
-      
-  /* Optimize pow(x,0.75) = sqrt(x) * sqrt(sqrt(x)) unless we are
-     optimizing for space.  Don't do this optimization if we don't have
-     a hardware sqrt insn.  */
-  real_from_integer (&dconst3_4, VOIDmode, 3, SIGNED);
-  SET_REAL_EXP (&dconst3_4, REAL_EXP (&dconst3_4) - 2);
-
-  if (flag_unsafe_math_optimizations
-      && sqrtfn
-      && optimize_function_for_speed_p (cfun)
-      && REAL_VALUES_EQUAL (c, dconst3_4)
-      && hw_sqrt_exists)
-    {
-      /* sqrt(x)  */
-      sqrt_arg0 = build_and_insert_call (gsi, loc, sqrtfn, arg0);
-
-      /* sqrt(sqrt(x))  */
-      sqrt_sqrt = build_and_insert_call (gsi, loc, sqrtfn, sqrt_arg0);
-
-      /* sqrt(x) * sqrt(sqrt(x))  */
-      return build_and_insert_binop (gsi, loc, "powroot", MULT_EXPR,
-				     sqrt_arg0, sqrt_sqrt);
-    }
-
   /* Optimize pow(x,1./3.) = cbrt(x).  This requires unsafe math
      optimizations since 1./3. is not exactly representable.  If x
      is negative and finite, the correct value of pow(x,1./3.) is
@@ -1263,7 +1578,7 @@ gimple_expand_builtin_pow (gimple_stmt_iterator *gsi, location_t loc,
       && sqrtfn
       && cbrtfn
       && (gimple_val_nonnegative_real_p (arg0) || !HONOR_NANS (mode))
-      && optimize_function_for_speed_p (cfun)
+      && speed_p
       && hw_sqrt_exists
       && REAL_VALUES_EQUAL (c, dconst1_6))
     {
@@ -1274,54 +1589,31 @@ gimple_expand_builtin_pow (gimple_stmt_iterator *gsi, location_t loc,
       return build_and_insert_call (gsi, loc, cbrtfn, sqrt_arg0);
     }
 
-  /* Optimize pow(x,c), where n = 2c for some nonzero integer n
-     and c not an integer, into
-
-       sqrt(x) * powi(x, n/2),                n > 0;
-       1.0 / (sqrt(x) * powi(x, abs(n/2))),   n < 0.
-
-     Do not calculate the powi factor when n/2 = 0.  */
-  real_arithmetic (&c2, MULT_EXPR, &c, &dconst2);
-  n = real_to_integer (&c2);
-  real_from_integer (&cint, VOIDmode, n, SIGNED);
-  c2_is_int = real_identical (&c2, &cint);
 
+  /* Attempt to expand the POW as a product of square root chains.
+     Expand the 0.25 case even when otpimising for size.  */
   if (flag_unsafe_math_optimizations
       && sqrtfn
-      && c2_is_int
-      && !c_is_int
-      && optimize_function_for_speed_p (cfun))
+      && hw_sqrt_exists
+      && (speed_p || REAL_VALUES_EQUAL (c, dconst1_4))
+      && !HONOR_SIGNED_ZEROS (mode))
     {
-      tree powi_x_ndiv2 = NULL_TREE;
-
-      /* Attempt to fold powi(arg0, abs(n/2)) into multiplies.  If not
-         possible or profitable, give up.  Skip the degenerate case when
-         n is 1 or -1, where the result is always 1.  */
-      if (absu_hwi (n) != 1)
-	{
-	  powi_x_ndiv2 = gimple_expand_builtin_powi (gsi, loc, arg0,
-						     abs_hwi (n / 2));
-	  if (!powi_x_ndiv2)
-	    return NULL_TREE;
-	}
+      unsigned int max_depth = speed_p
+				? PARAM_VALUE (PARAM_MAX_POW_SQRT_DEPTH)
+				: 2;
 
-      /* Calculate sqrt(x).  When n is not 1 or -1, multiply it by the
-	 result of the optimal multiply sequence just calculated.  */
-      sqrt_arg0 = build_and_insert_call (gsi, loc, sqrtfn, arg0);
+      tree expand_with_sqrts
+	= expand_pow_as_sqrts (gsi, loc, arg0, arg1, max_depth);
 
-      if (absu_hwi (n) == 1)
-	result = sqrt_arg0;
-      else
-	result = build_and_insert_binop (gsi, loc, "powroot", MULT_EXPR,
-					 sqrt_arg0, powi_x_ndiv2);
-
-      /* If n is negative, reciprocate the result.  */
-      if (n < 0)
-	result = build_and_insert_binop (gsi, loc, "powroot", RDIV_EXPR,
-					 build_real (type, dconst1), result);
-      return result;
+      if (expand_with_sqrts)
+	return expand_with_sqrts;
     }
 
+  real_arithmetic (&c2, MULT_EXPR, &c, &dconst2);
+  n = real_to_integer (&c2);
+  real_from_integer (&cint, VOIDmode, n, SIGNED);
+  c2_is_int = real_identical (&c2, &cint);
+
   /* Optimize pow(x,c), where 3c = n for some nonzero integer n, into
 
      powi(x, n/3) * powi(cbrt(x), n%3),                    n > 0;
--- a/src/libgcc/config.host
+++ b/src/libgcc/config.host
@@ -377,14 +377,15 @@ arm*-*-netbsdelf*)
 	tmake_file="$tmake_file arm/t-arm arm/t-netbsd t-slibgcc-gld-nover"
 	;;
 arm*-*-linux*)			# ARM GNU/Linux with ELF
-	tmake_file="${tmake_file} arm/t-arm t-fixedpoint-gnu-prefix"
+	tmake_file="${tmake_file} arm/t-arm t-fixedpoint-gnu-prefix t-crtfm"
 	tmake_file="${tmake_file} arm/t-elf arm/t-bpabi arm/t-linux-eabi t-slibgcc-libgcc"
 	tm_file="$tm_file arm/bpabi-lib.h"
 	unwind_header=config/arm/unwind-arm.h
 	tmake_file="$tmake_file t-softfp-sfdf t-softfp-excl arm/t-softfp t-softfp"
+	extra_parts="$extra_parts crtfastmath.o"
 	;;
 arm*-*-uclinux*)		# ARM ucLinux
-	tmake_file="${tmake_file} t-fixedpoint-gnu-prefix"
+	tmake_file="${tmake_file} t-fixedpoint-gnu-prefix t-crtfm"
 	tmake_file="$tmake_file arm/t-arm arm/t-elf t-softfp-sfdf t-softfp-excl arm/t-softfp t-softfp"
 	tmake_file="${tmake_file} arm/t-bpabi"
 	tm_file="$tm_file arm/bpabi-lib.h"
@@ -396,7 +397,7 @@ arm*-*-eabi* | arm*-*-symbianelf* | arm*-*-rtems*)
 	tm_file="$tm_file arm/bpabi-lib.h"
 	case ${host} in
 	arm*-*-eabi* | arm*-*-rtems*)
-	  tmake_file="${tmake_file} arm/t-bpabi"
+	  tmake_file="${tmake_file} arm/t-bpabi t-crtfm"
 	  extra_parts="crtbegin.o crtend.o crti.o crtn.o"
 	  ;;
 	arm*-*-symbianelf*)
--- a/src//dev/null
+++ b/src/libgcc/config/arm/crtfastmath.c
@@ -0,0 +1,40 @@
+/*
+ * Copyright (C) 2014 Free Software Foundation, Inc.
+ *
+ * This file is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License as published by the
+ * Free Software Foundation; either version 3, or (at your option) any
+ * later version.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * Under Section 7 of GPL version 3, you are granted additional
+ * permissions described in the GCC Runtime Library Exception, version
+ * 3.1, as published by the Free Software Foundation.
+ *
+ * You should have received a copy of the GNU General Public License and
+ * a copy of the GCC Runtime Library Exception along with this program;
+ * see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
+ * <http://www.gnu.org/licenses/>.
+ */
+
+/* Enable flush-to-zero support for -ffast-math on VFP targets.  */
+#ifndef __SOFTFP__
+
+#define FPSCR_FZ		(1 << 24)
+
+static void __attribute__((constructor))
+__arm_set_fast_math (void)
+{
+  unsigned int fpscr_save;
+
+  /* Set the FZ (flush-to-zero) bit in FPSCR.  */
+  __asm__("vmrs %0, fpscr" : "=r" (fpscr_save));
+  fpscr_save |= FPSCR_FZ;
+  __asm__("vmsr fpscr, %0" : : "r" (fpscr_save));
+}
+
+#endif /* __SOFTFP__  */
--- a/src/libgcc/config/arm/ieee754-df.S
+++ b/src/libgcc/config/arm/ieee754-df.S
@@ -33,8 +33,12 @@
  * Only the default rounding mode is intended for best performances.
  * Exceptions aren't supported yet, but that can be added quite easily
  * if necessary without impacting performances.
+ *
+ * In the CFI related comments, 'previousOffset' refers to the previous offset
+ * from sp used to compute the CFA.
  */
 
+	.cfi_sections .debug_frame
 
 #ifndef __ARMEB__
 #define xl r0
@@ -53,11 +57,13 @@
 
 ARM_FUNC_START negdf2
 ARM_FUNC_ALIAS aeabi_dneg negdf2
+	CFI_START_FUNCTION
 
 	@ flip sign bit
 	eor	xh, xh, #0x80000000
 	RET
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_dneg
 	FUNC_END negdf2
 
@@ -66,6 +72,7 @@ ARM_FUNC_ALIAS aeabi_dneg negdf2
 #ifdef L_arm_addsubdf3
 
 ARM_FUNC_START aeabi_drsub
+	CFI_START_FUNCTION
 
 	eor	xh, xh, #0x80000000	@ flip sign bit of first arg
 	b	1f	
@@ -81,7 +88,11 @@ ARM_FUNC_ALIAS aeabi_dsub subdf3
 ARM_FUNC_START adddf3
 ARM_FUNC_ALIAS aeabi_dadd adddf3
 
-1:	do_push	{r4, r5, lr}
+1:  do_push {r4, r5, lr}        @ sp -= 12
+	.cfi_adjust_cfa_offset 12   @ CFA is now sp + previousOffset + 12
+	.cfi_rel_offset r4, 0       @ Registers are saved from sp to sp + 8
+	.cfi_rel_offset r5, 4
+	.cfi_rel_offset lr, 8
 
 	@ Look for zeroes, equal values, INF, or NAN.
 	shift1	lsl, r4, xh, #1
@@ -148,6 +159,11 @@ ARM_FUNC_ALIAS aeabi_dadd adddf3
 	@ Since this is not common case, rescale them off line.
 	teq	r4, r5
 	beq	LSYM(Lad_d)
+
+@ CFI note: we're lucky that the branches to Lad_* that appear after this function
+@ have a CFI state that's exactly the same as the one we're in at this
+@ point. Otherwise the CFI would change to a different state after the branch,
+@ which would be disastrous for backtracing.
 LSYM(Lad_x):
 
 	@ Compensate for the exponent overlapping the mantissa MSB added later
@@ -413,6 +429,7 @@ LSYM(Lad_i):
 	orrne	xh, xh, #0x00080000	@ quiet NAN
 	RETLDM	"r4, r5"
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_dsub
 	FUNC_END subdf3
 	FUNC_END aeabi_dadd
@@ -420,12 +437,19 @@ LSYM(Lad_i):
 
 ARM_FUNC_START floatunsidf
 ARM_FUNC_ALIAS aeabi_ui2d floatunsidf
+	CFI_START_FUNCTION
 
 	teq	r0, #0
 	do_it	eq, t
 	moveq	r1, #0
 	RETc(eq)
-	do_push	{r4, r5, lr}
+
+	do_push {r4, r5, lr}        @ sp -= 12
+	.cfi_adjust_cfa_offset 12   @ CFA is now sp + previousOffset + 12
+	.cfi_rel_offset r4, 0       @ Registers are saved from sp + 0 to sp + 8.
+	.cfi_rel_offset r5, 4
+	.cfi_rel_offset lr, 8
+
 	mov	r4, #0x400		@ initial exponent
 	add	r4, r4, #(52-1 - 1)
 	mov	r5, #0			@ sign bit is 0
@@ -435,17 +459,25 @@ ARM_FUNC_ALIAS aeabi_ui2d floatunsidf
 	mov	xh, #0
 	b	LSYM(Lad_l)
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_ui2d
 	FUNC_END floatunsidf
 
 ARM_FUNC_START floatsidf
 ARM_FUNC_ALIAS aeabi_i2d floatsidf
+	CFI_START_FUNCTION
 
 	teq	r0, #0
 	do_it	eq, t
 	moveq	r1, #0
 	RETc(eq)
-	do_push	{r4, r5, lr}
+
+	do_push {r4, r5, lr}        @ sp -= 12
+	.cfi_adjust_cfa_offset 12   @ CFA is now sp + previousOffset + 12
+	.cfi_rel_offset r4, 0       @ Registers are saved from sp + 0 to sp + 8.
+	.cfi_rel_offset r5, 4
+	.cfi_rel_offset lr, 8
+
 	mov	r4, #0x400		@ initial exponent
 	add	r4, r4, #(52-1 - 1)
 	ands	r5, r0, #0x80000000	@ sign bit in r5
@@ -457,11 +489,13 @@ ARM_FUNC_ALIAS aeabi_i2d floatsidf
 	mov	xh, #0
 	b	LSYM(Lad_l)
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_i2d
 	FUNC_END floatsidf
 
 ARM_FUNC_START extendsfdf2
 ARM_FUNC_ALIAS aeabi_f2d extendsfdf2
+	CFI_START_FUNCTION
 
 	movs	r2, r0, lsl #1		@ toss sign bit
 	mov	xh, r2, asr #3		@ stretch exponent
@@ -480,34 +514,54 @@ ARM_FUNC_ALIAS aeabi_f2d extendsfdf2
 
 	@ value was denormalized.  We can normalize it now.
 	do_push	{r4, r5, lr}
+	.cfi_adjust_cfa_offset 12   @ CFA is now sp + previousOffset + 12
+	.cfi_rel_offset r4, 0       @ Registers are saved from sp + 0 to sp + 8.
+	.cfi_rel_offset r5, 4
+	.cfi_rel_offset lr, 8
+
 	mov	r4, #0x380		@ setup corresponding exponent
 	and	r5, xh, #0x80000000	@ move sign bit in r5
 	bic	xh, xh, #0x80000000
 	b	LSYM(Lad_l)
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_f2d
 	FUNC_END extendsfdf2
 
 ARM_FUNC_START floatundidf
 ARM_FUNC_ALIAS aeabi_ul2d floatundidf
+	CFI_START_FUNCTION
+	.cfi_remember_state        @ Save the current CFA state.
 
 	orrs	r2, r0, r1
 	do_it	eq
 	RETc(eq)
 
-	do_push	{r4, r5, lr}
+	do_push {r4, r5, lr}       @ sp -= 12
+	.cfi_adjust_cfa_offset 12  @ CFA is now sp + previousOffset + 12
+	.cfi_rel_offset r4, 0      @ Registers are saved from sp + 0 to sp + 8
+	.cfi_rel_offset r5, 4
+	.cfi_rel_offset lr, 8
 
 	mov	r5, #0
 	b	2f
 
 ARM_FUNC_START floatdidf
 ARM_FUNC_ALIAS aeabi_l2d floatdidf
+	.cfi_restore_state
+	@ Restore the CFI state we saved above. If we didn't do this then the
+	@ following instructions would have the CFI state that was set by the
+	@ offset adjustments made in floatundidf.
 
 	orrs	r2, r0, r1
 	do_it	eq
 	RETc(eq)
 
-	do_push	{r4, r5, lr}
+	do_push {r4, r5, lr}       @ sp -= 12
+	.cfi_adjust_cfa_offset 12  @ CFA is now sp + previousOffset + 12
+	.cfi_rel_offset r4, 0      @ Registers are saved from sp to sp + 8
+	.cfi_rel_offset r5, 4
+	.cfi_rel_offset lr, 8
 
 	ands	r5, ah, #0x80000000	@ sign bit in r5
 	bpl	2f
@@ -550,6 +604,7 @@ ARM_FUNC_ALIAS aeabi_l2d floatdidf
 	add	r4, r4, r2
 	b	LSYM(Lad_p)
 
+	CFI_END_FUNCTION
 	FUNC_END floatdidf
 	FUNC_END aeabi_l2d
 	FUNC_END floatundidf
@@ -561,7 +616,14 @@ ARM_FUNC_ALIAS aeabi_l2d floatdidf
 
 ARM_FUNC_START muldf3
 ARM_FUNC_ALIAS aeabi_dmul muldf3
-	do_push	{r4, r5, r6, lr}
+	CFI_START_FUNCTION
+
+	do_push {r4, r5, r6, lr}    @ sp -= 16
+	.cfi_adjust_cfa_offset 16   @ CFA is now sp + previousOffset + 16
+	.cfi_rel_offset r4, 0       @ Registers are saved from sp to sp + 12.
+	.cfi_rel_offset r5, 4
+	.cfi_rel_offset r6, 8
+	.cfi_rel_offset lr, 12
 
 	@ Mask out exponents, trap any zero/denormal/INF/NAN.
 	mov	ip, #0xff
@@ -596,7 +658,16 @@ ARM_FUNC_ALIAS aeabi_dmul muldf3
 	and   r6, r6, #0x80000000
 
 	@ Well, no way to make it shorter without the umull instruction.
-	stmfd	sp!, {r6, r7, r8, r9, sl, fp}
+	stmfd   sp!, {r6, r7, r8, r9, sl, fp}   @ sp -= 24
+	.cfi_remember_state         @ Save the current CFI state.
+	.cfi_adjust_cfa_offset 24   @ CFA is now sp + previousOffset + 24.
+	.cfi_rel_offset r6, 0       @ Registers are saved from sp to sp + 20.
+	.cfi_rel_offset r7, 4
+	.cfi_rel_offset r8, 8
+	.cfi_rel_offset r9, 12
+	.cfi_rel_offset sl, 16
+	.cfi_rel_offset fp, 20
+
 	mov	r7, xl, lsr #16
 	mov	r8, yl, lsr #16
 	mov	r9, xh, lsr #16
@@ -648,8 +719,8 @@ ARM_FUNC_ALIAS aeabi_dmul muldf3
 	mul	fp, xh, yh
 	adcs	r5, r5, fp
 	adc	r6, r6, #0
-	ldmfd	sp!, {yl, r7, r8, r9, sl, fp}
-
+	ldmfd   sp!, {yl, r7, r8, r9, sl, fp}   @ sp += 24
+	.cfi_restore_state   @ Restore the previous CFI state.
 #else
 
 	@ Here is the actual multiplication.
@@ -715,7 +786,6 @@ LSYM(Lml_1):
 	orr	xh, xh, #0x00100000
 	mov	lr, #0
 	subs	r4, r4, #1
-
 LSYM(Lml_u):
 	@ Overflow?
 	bgt	LSYM(Lml_o)
@@ -863,13 +933,20 @@ LSYM(Lml_n):
 	orr	xh, xh, #0x00f80000
 	RETLDM	"r4, r5, r6"
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_dmul
 	FUNC_END muldf3
 
 ARM_FUNC_START divdf3
 ARM_FUNC_ALIAS aeabi_ddiv divdf3
+	CFI_START_FUNCTION
 	
 	do_push	{r4, r5, r6, lr}
+	.cfi_adjust_cfa_offset 16
+	.cfi_rel_offset r4, 0
+	.cfi_rel_offset r5, 4
+	.cfi_rel_offset r6, 8
+	.cfi_rel_offset lr, 12
 
 	@ Mask out exponents, trap any zero/denormal/INF/NAN.
 	mov	ip, #0xff
@@ -1052,6 +1129,7 @@ LSYM(Ldv_s):
 	bne	LSYM(Lml_z)		@ 0 / <non_zero> -> 0
 	b	LSYM(Lml_n)		@ 0 / 0 -> NAN
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_ddiv
 	FUNC_END divdf3
 
@@ -1063,6 +1141,7 @@ LSYM(Ldv_s):
 
 ARM_FUNC_START gtdf2
 ARM_FUNC_ALIAS gedf2 gtdf2
+	CFI_START_FUNCTION
 	mov	ip, #-1
 	b	1f
 
@@ -1077,6 +1156,10 @@ ARM_FUNC_ALIAS eqdf2 cmpdf2
 	mov	ip, #1			@ how should we specify unordered here?
 
 1:	str	ip, [sp, #-4]!
+	.cfi_adjust_cfa_offset 4        @ CFA is now sp + previousOffset + 4.
+	@ We're not adding CFI for ip as it's pushed into the stack
+	@ only because @ it may be popped off later as a return value
+	@ (i.e. we're not preserving @ it anyways).
 
 	@ Trap any INF/NAN first.
 	mov	ip, xh, lsl #1
@@ -1085,10 +1168,18 @@ ARM_FUNC_ALIAS eqdf2 cmpdf2
 	do_it	ne
 	COND(mvn,s,ne)	ip, ip, asr #21
 	beq	3f
-
-	@ Test for equality.
-	@ Note that 0.0 is equal to -0.0.
+	.cfi_remember_state
+	@ Save the current CFI state. This is done because the branch
+	@ is conditional, @ and if we don't take it we'll issue a
+	@ .cfi_adjust_cfa_offset and return.  @ If we do take it,
+	@ however, the .cfi_adjust_cfa_offset from the non-branch @ code
+	@ will affect the branch code as well. To avoid this we'll
+	@ restore @ the current state before executing the branch code.
+
+	@ Test for equality.  @ Note that 0.0 is equal to -0.0.
 2:	add	sp, sp, #4
+	.cfi_adjust_cfa_offset -4       @ CFA is now sp + previousOffset.
+
 	orrs	ip, xl, xh, lsl #1	@ if x == 0.0 or -0.0
 	do_it	eq, e
 	COND(orr,s,eq)	ip, yl, yh, lsl #1	@ and y == 0.0 or -0.0
@@ -1117,8 +1208,13 @@ ARM_FUNC_ALIAS eqdf2 cmpdf2
 	orr	r0, r0, #1
 	RET
 
-	@ Look for a NAN.
-3:	mov	ip, xh, lsl #1
+3:  @ Look for a NAN.
+
+	@ Restore the previous CFI state (i.e. keep the CFI state as it was
+	@ before the branch).
+	.cfi_restore_state
+
+	mov ip, xh, lsl #1
 	mvns	ip, ip, asr #21
 	bne	4f
 	orrs	ip, xl, xh, lsl #12
@@ -1128,9 +1224,13 @@ ARM_FUNC_ALIAS eqdf2 cmpdf2
 	bne	2b
 	orrs	ip, yl, yh, lsl #12
 	beq	2b			@ y is not NAN
+
 5:	ldr	r0, [sp], #4		@ unordered return code
+	.cfi_adjust_cfa_offset -4       @ CFA is now sp + previousOffset.
+
 	RET
 
+	CFI_END_FUNCTION
 	FUNC_END gedf2
 	FUNC_END gtdf2
 	FUNC_END ledf2
@@ -1140,6 +1240,7 @@ ARM_FUNC_ALIAS eqdf2 cmpdf2
 	FUNC_END cmpdf2
 
 ARM_FUNC_START aeabi_cdrcmple
+	CFI_START_FUNCTION
 
 	mov	ip, r0
 	mov	r0, r2
@@ -1148,13 +1249,17 @@ ARM_FUNC_START aeabi_cdrcmple
 	mov	r1, r3
 	mov	r3, ip
 	b	6f
-	
+
 ARM_FUNC_START aeabi_cdcmpeq
 ARM_FUNC_ALIAS aeabi_cdcmple aeabi_cdcmpeq
 
 	@ The status-returning routines are required to preserve all
 	@ registers except ip, lr, and cpsr.
 6:	do_push	{r0, lr}
+	.cfi_adjust_cfa_offset 8  @ CFA is now sp + previousOffset + 8.
+	.cfi_rel_offset r0, 0     @ Previous r0 is saved at sp.
+	.cfi_rel_offset lr, 4     @ Previous lr is saved at sp + 4.
+
 	ARM_CALL cmpdf2
 	@ Set the Z flag correctly, and the C flag unconditionally.
 	cmp	r0, #0
@@ -1162,59 +1267,86 @@ ARM_FUNC_ALIAS aeabi_cdcmple aeabi_cdcmpeq
 	@ that the first operand was smaller than the second.
 	do_it	mi
 	cmnmi	r0, #0
+
 	RETLDM	"r0"
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_cdcmple
 	FUNC_END aeabi_cdcmpeq
 	FUNC_END aeabi_cdrcmple
 	
 ARM_FUNC_START	aeabi_dcmpeq
+	CFI_START_FUNCTION
+
+	str lr, [sp, #-8]!        @ sp -= 8
+	.cfi_adjust_cfa_offset 8  @ CFA is now sp + previousOffset + 8
+	.cfi_rel_offset lr, 0     @ lr is at sp
 
-	str	lr, [sp, #-8]!
 	ARM_CALL aeabi_cdcmple
 	do_it	eq, e
 	moveq	r0, #1	@ Equal to.
 	movne	r0, #0	@ Less than, greater than, or unordered.
+
 	RETLDM
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_dcmpeq
 
 ARM_FUNC_START	aeabi_dcmplt
+	CFI_START_FUNCTION
+
+	str lr, [sp, #-8]!        @ sp -= 8
+	.cfi_adjust_cfa_offset 8  @ CFA is now sp + previousOffset + 8
+	.cfi_rel_offset lr, 0     @ lr is at sp
 
-	str	lr, [sp, #-8]!
 	ARM_CALL aeabi_cdcmple
 	do_it	cc, e
 	movcc	r0, #1	@ Less than.
 	movcs	r0, #0	@ Equal to, greater than, or unordered.
 	RETLDM
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_dcmplt
 
 ARM_FUNC_START	aeabi_dcmple
+	CFI_START_FUNCTION
+
+	str lr, [sp, #-8]!        @ sp -= 8
+	.cfi_adjust_cfa_offset 8  @ CFA is now sp + previousOffset + 8
+	.cfi_rel_offset lr, 0     @ lr is at sp
 
-	str	lr, [sp, #-8]!
 	ARM_CALL aeabi_cdcmple
 	do_it	ls, e
 	movls	r0, #1  @ Less than or equal to.
 	movhi	r0, #0	@ Greater than or unordered.
 	RETLDM
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_dcmple
 
 ARM_FUNC_START	aeabi_dcmpge
+	CFI_START_FUNCTION
+
+	str lr, [sp, #-8]!        @ sp -= 8
+	.cfi_adjust_cfa_offset 8  @ CFA is now sp + previousOffset + 8
+	.cfi_rel_offset lr, 0     @ lr is at sp
 
-	str	lr, [sp, #-8]!
 	ARM_CALL aeabi_cdrcmple
 	do_it	ls, e
 	movls	r0, #1	@ Operand 2 is less than or equal to operand 1.
 	movhi	r0, #0	@ Operand 2 greater than operand 1, or unordered.
 	RETLDM
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_dcmpge
 
 ARM_FUNC_START	aeabi_dcmpgt
+	CFI_START_FUNCTION
+
+	str lr, [sp, #-8]!        @ sp -= 8
+	.cfi_adjust_cfa_offset 8  @ CFA is now sp + previousOffset + 8
+	.cfi_rel_offset lr, 0     @ lr is at sp
 
-	str	lr, [sp, #-8]!
 	ARM_CALL aeabi_cdrcmple
 	do_it	cc, e
 	movcc	r0, #1	@ Operand 2 is less than operand 1.
@@ -1222,6 +1354,7 @@ ARM_FUNC_START	aeabi_dcmpgt
 			@ or they are unordered.
 	RETLDM
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_dcmpgt
 
 #endif /* L_cmpdf2 */
@@ -1230,6 +1363,7 @@ ARM_FUNC_START	aeabi_dcmpgt
 
 ARM_FUNC_START unorddf2
 ARM_FUNC_ALIAS aeabi_dcmpun unorddf2
+	.cfi_startproc
 
 	mov	ip, xh, lsl #1
 	mvns	ip, ip, asr #21
@@ -1247,6 +1381,7 @@ ARM_FUNC_ALIAS aeabi_dcmpun unorddf2
 3:	mov	r0, #1			@ arguments are unordered.
 	RET
 
+	.cfi_endproc
 	FUNC_END aeabi_dcmpun
 	FUNC_END unorddf2
 
@@ -1256,6 +1391,7 @@ ARM_FUNC_ALIAS aeabi_dcmpun unorddf2
 
 ARM_FUNC_START fixdfsi
 ARM_FUNC_ALIAS aeabi_d2iz fixdfsi
+	CFI_START_FUNCTION
 
 	@ check exponent range.
 	mov	r2, xh, lsl #1
@@ -1289,6 +1425,7 @@ ARM_FUNC_ALIAS aeabi_d2iz fixdfsi
 4:	mov	r0, #0			@ How should we convert NAN?
 	RET
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_d2iz
 	FUNC_END fixdfsi
 
@@ -1298,6 +1435,7 @@ ARM_FUNC_ALIAS aeabi_d2iz fixdfsi
 
 ARM_FUNC_START fixunsdfsi
 ARM_FUNC_ALIAS aeabi_d2uiz fixunsdfsi
+	CFI_START_FUNCTION
 
 	@ check exponent range.
 	movs	r2, xh, lsl #1
@@ -1327,6 +1465,7 @@ ARM_FUNC_ALIAS aeabi_d2uiz fixunsdfsi
 4:	mov	r0, #0			@ How should we convert NAN?
 	RET
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_d2uiz
 	FUNC_END fixunsdfsi
 
@@ -1336,6 +1475,7 @@ ARM_FUNC_ALIAS aeabi_d2uiz fixunsdfsi
 
 ARM_FUNC_START truncdfsf2
 ARM_FUNC_ALIAS aeabi_d2f truncdfsf2
+	CFI_START_FUNCTION
 
 	@ check exponent range.
 	mov	r2, xh, lsl #1
@@ -1400,6 +1540,7 @@ ARM_FUNC_ALIAS aeabi_d2f truncdfsf2
 	orr	r0, r0, #0x00800000
 	RET
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_d2f
 	FUNC_END truncdfsf2
 
--- a/src/libgcc/config/arm/ieee754-sf.S
+++ b/src/libgcc/config/arm/ieee754-sf.S
@@ -31,16 +31,21 @@
  * Only the default rounding mode is intended for best performances.
  * Exceptions aren't supported yet, but that can be added quite easily
  * if necessary without impacting performances.
+ *
+ * In the CFI related comments, 'previousOffset' refers to the previous offset
+ * from sp used to compute the CFA.
  */
 
 #ifdef L_arm_negsf2
 	
 ARM_FUNC_START negsf2
 ARM_FUNC_ALIAS aeabi_fneg negsf2
+	CFI_START_FUNCTION
 
 	eor	r0, r0, #0x80000000	@ flip sign bit
 	RET
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_fneg
 	FUNC_END negsf2
 
@@ -49,6 +54,7 @@ ARM_FUNC_ALIAS aeabi_fneg negsf2
 #ifdef L_arm_addsubsf3
 
 ARM_FUNC_START aeabi_frsub
+	CFI_START_FUNCTION
 
 	eor	r0, r0, #0x80000000	@ flip sign bit of first arg
 	b	1f
@@ -284,6 +290,7 @@ LSYM(Lad_i):
 	orrne	r0, r0, #0x00400000	@ quiet NAN
 	RET
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_frsub
 	FUNC_END aeabi_fadd
 	FUNC_END addsf3
@@ -292,6 +299,7 @@ LSYM(Lad_i):
 
 ARM_FUNC_START floatunsisf
 ARM_FUNC_ALIAS aeabi_ui2f floatunsisf
+	CFI_START_FUNCTION
 		
 	mov	r3, #0
 	b	1f
@@ -316,6 +324,7 @@ ARM_FUNC_ALIAS aeabi_i2f floatsisf
 	mov	al, #0
 	b	2f
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_i2f
 	FUNC_END floatsisf
 	FUNC_END aeabi_ui2f
@@ -323,6 +332,7 @@ ARM_FUNC_ALIAS aeabi_i2f floatsisf
 
 ARM_FUNC_START floatundisf
 ARM_FUNC_ALIAS aeabi_ul2f floatundisf
+	CFI_START_FUNCTION
 
 	orrs	r2, r0, r1
 	do_it	eq
@@ -409,6 +419,7 @@ ARM_FUNC_ALIAS aeabi_l2f floatdisf
 	biceq	r0, r0, ip, lsr #31
 	RET
 
+	CFI_END_FUNCTION
 	FUNC_END floatdisf
 	FUNC_END aeabi_l2f
 	FUNC_END floatundisf
@@ -420,6 +431,7 @@ ARM_FUNC_ALIAS aeabi_l2f floatdisf
 
 ARM_FUNC_START mulsf3
 ARM_FUNC_ALIAS aeabi_fmul mulsf3
+	CFI_START_FUNCTION
 
 	@ Mask out exponents, trap any zero/denormal/INF/NAN.
 	mov	ip, #0xff
@@ -454,7 +466,13 @@ LSYM(Lml_x):
 	and	r3, ip, #0x80000000
 
 	@ Well, no way to make it shorter without the umull instruction.
-	do_push	{r3, r4, r5}
+	do_push	{r3, r4, r5}       @ sp -= 12
+	.cfi_remember_state        @ Save the current CFI state
+	.cfi_adjust_cfa_offset 12  @ CFA is now sp + previousOffset + 12
+	.cfi_rel_offset r3, 0      @ Registers are saved from sp to sp + 8
+	.cfi_rel_offset r4, 4
+	.cfi_rel_offset r5, 8
+
 	mov	r4, r0, lsr #16
 	mov	r5, r1, lsr #16
 	bic	r0, r0, r4, lsl #16
@@ -465,7 +483,8 @@ LSYM(Lml_x):
 	mla	r0, r4, r1, r0
 	adds	r3, r3, r0, lsl #16
 	adc	r1, ip, r0, lsr #16
-	do_pop	{r0, r4, r5}
+	do_pop	{r0, r4, r5}       @ sp += 12
+	.cfi_restore_state         @ Restore the previous CFI state
 
 #else
 
@@ -618,11 +637,13 @@ LSYM(Lml_n):
 	orr	r0, r0, #0x00c00000
 	RET
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_fmul
 	FUNC_END mulsf3
 
 ARM_FUNC_START divsf3
 ARM_FUNC_ALIAS aeabi_fdiv divsf3
+	CFI_START_FUNCTION
 
 	@ Mask out exponents, trap any zero/denormal/INF/NAN.
 	mov	ip, #0xff
@@ -758,6 +779,7 @@ LSYM(Ldv_s):
 	bne	LSYM(Lml_z)		@ 0 / <non_zero> -> 0
 	b	LSYM(Lml_n)		@ 0 / 0 -> NAN
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_fdiv
 	FUNC_END divsf3
 
@@ -782,6 +804,7 @@ LSYM(Ldv_s):
 
 ARM_FUNC_START gtsf2
 ARM_FUNC_ALIAS gesf2 gtsf2
+	CFI_START_FUNCTION
 	mov	ip, #-1
 	b	1f
 
@@ -796,6 +819,10 @@ ARM_FUNC_ALIAS eqsf2 cmpsf2
 	mov	ip, #1			@ how should we specify unordered here?
 
 1:	str	ip, [sp, #-4]!
+	.cfi_adjust_cfa_offset 4  @ CFA is now sp + previousOffset + 4.
+	@ We're not adding CFI for ip as it's pushed into the stack only because
+	@ it may be popped off later as a return value (i.e. we're not preserving
+	@ it anyways).
 
 	@ Trap any INF/NAN first.
 	mov	r2, r0, lsl #1
@@ -804,10 +831,18 @@ ARM_FUNC_ALIAS eqsf2 cmpsf2
 	do_it	ne
 	COND(mvn,s,ne)	ip, r3, asr #24
 	beq	3f
+	.cfi_remember_state
+	@ Save the current CFI state. This is done because the branch is conditional,
+	@ and if we don't take it we'll issue a .cfi_adjust_cfa_offset and return.
+	@ If we do take it, however, the .cfi_adjust_cfa_offset from the non-branch
+	@ code will affect the branch code as well. To avoid this we'll restore
+	@ the current state before executing the branch code.
 
 	@ Compare values.
 	@ Note that 0.0 is equal to -0.0.
 2:	add	sp, sp, #4
+	.cfi_adjust_cfa_offset -4       @ CFA is now sp + previousOffset.
+
 	orrs	ip, r2, r3, lsr #1	@ test if both are 0, clear C flag
 	do_it	ne
 	teqne	r0, r1			@ if not 0 compare sign
@@ -823,8 +858,13 @@ ARM_FUNC_ALIAS eqsf2 cmpsf2
 	orrne	r0, r0, #1
 	RET
 
-	@ Look for a NAN. 
-3:	mvns	ip, r2, asr #24
+3:	@ Look for a NAN.
+
+	@ Restore the previous CFI state (i.e. keep the CFI state as it was
+	@ before the branch).
+	.cfi_restore_state
+
+	mvns	ip, r2, asr #24
 	bne	4f
 	movs	ip, r0, lsl #9
 	bne	5f			@ r0 is NAN
@@ -832,9 +872,12 @@ ARM_FUNC_ALIAS eqsf2 cmpsf2
 	bne	2b
 	movs	ip, r1, lsl #9
 	beq	2b			@ r1 is not NAN
+
 5:	ldr	r0, [sp], #4		@ return unordered code.
+	.cfi_adjust_cfa_offset -4       @ CFA is now sp + previousOffset.
 	RET
 
+	CFI_END_FUNCTION
 	FUNC_END gesf2
 	FUNC_END gtsf2
 	FUNC_END lesf2
@@ -844,6 +887,7 @@ ARM_FUNC_ALIAS eqsf2 cmpsf2
 	FUNC_END cmpsf2
 
 ARM_FUNC_START aeabi_cfrcmple
+	CFI_START_FUNCTION
 
 	mov	ip, r0
 	mov	r0, r1
@@ -856,6 +900,13 @@ ARM_FUNC_ALIAS aeabi_cfcmple aeabi_cfcmpeq
 	@ The status-returning routines are required to preserve all
 	@ registers except ip, lr, and cpsr.
 6:	do_push	{r0, r1, r2, r3, lr}
+	.cfi_adjust_cfa_offset 20  @ CFA is at sp + previousOffset + 20
+	.cfi_rel_offset r0, 0      @ Registers are saved from sp to sp + 16
+	.cfi_rel_offset r1, 4
+	.cfi_rel_offset r2, 8
+	.cfi_rel_offset r3, 12
+	.cfi_rel_offset lr, 16
+
 	ARM_CALL cmpsf2
 	@ Set the Z flag correctly, and the C flag unconditionally.
 	cmp	r0, #0
@@ -865,57 +916,82 @@ ARM_FUNC_ALIAS aeabi_cfcmple aeabi_cfcmpeq
 	cmnmi	r0, #0
 	RETLDM	"r0, r1, r2, r3"
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_cfcmple
 	FUNC_END aeabi_cfcmpeq
 	FUNC_END aeabi_cfrcmple
 
 ARM_FUNC_START	aeabi_fcmpeq
+	CFI_START_FUNCTION
+
+	str	lr, [sp, #-8]!    @ sp -= 8
+	.cfi_adjust_cfa_offset 8  @ CFA is now sp + previousOffset + 8
+	.cfi_rel_offset lr, 0     @ lr is at sp
 
-	str	lr, [sp, #-8]!
 	ARM_CALL aeabi_cfcmple
 	do_it	eq, e
 	moveq	r0, #1	@ Equal to.
 	movne	r0, #0	@ Less than, greater than, or unordered.
 	RETLDM
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_fcmpeq
 
 ARM_FUNC_START	aeabi_fcmplt
+	CFI_START_FUNCTION
+
+	str	lr, [sp, #-8]!    @ sp -= 8
+	.cfi_adjust_cfa_offset 8  @ CFA is now sp + previousOffset + 8
+	.cfi_rel_offset lr, 0     @ lr is at sp
 
-	str	lr, [sp, #-8]!
 	ARM_CALL aeabi_cfcmple
 	do_it	cc, e
 	movcc	r0, #1	@ Less than.
 	movcs	r0, #0	@ Equal to, greater than, or unordered.
 	RETLDM
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_fcmplt
 
 ARM_FUNC_START	aeabi_fcmple
+	CFI_START_FUNCTION
+
+	str	lr, [sp, #-8]!    @ sp -= 8
+	.cfi_adjust_cfa_offset 8  @ CFA is now sp + previousOffset + 8
+	.cfi_rel_offset lr, 0     @ lr is at sp
 
-	str	lr, [sp, #-8]!
 	ARM_CALL aeabi_cfcmple
 	do_it	ls, e
 	movls	r0, #1  @ Less than or equal to.
 	movhi	r0, #0	@ Greater than or unordered.
 	RETLDM
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_fcmple
 
 ARM_FUNC_START	aeabi_fcmpge
+	CFI_START_FUNCTION
+
+	str	lr, [sp, #-8]!    @ sp -= 8
+	.cfi_adjust_cfa_offset 8  @ CFA is now sp + previousOffset + 8
+	.cfi_rel_offset lr, 0     @ lr is at sp
 
-	str	lr, [sp, #-8]!
 	ARM_CALL aeabi_cfrcmple
 	do_it	ls, e
 	movls	r0, #1	@ Operand 2 is less than or equal to operand 1.
 	movhi	r0, #0	@ Operand 2 greater than operand 1, or unordered.
 	RETLDM
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_fcmpge
 
 ARM_FUNC_START	aeabi_fcmpgt
+	CFI_START_FUNCTION
+
+	str	lr, [sp, #-8]!    @ sp -= 8
+	.cfi_adjust_cfa_offset 8  @ CFA is now sp + previousOffset + 8
+	.cfi_rel_offset lr, 0     @ lr is at sp
 
-	str	lr, [sp, #-8]!
 	ARM_CALL aeabi_cfrcmple
 	do_it	cc, e
 	movcc	r0, #1	@ Operand 2 is less than operand 1.
@@ -923,6 +999,7 @@ ARM_FUNC_START	aeabi_fcmpgt
 			@ or they are unordered.
 	RETLDM
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_fcmpgt
 
 #endif /* L_cmpsf2 */
@@ -931,6 +1008,7 @@ ARM_FUNC_START	aeabi_fcmpgt
 
 ARM_FUNC_START unordsf2
 ARM_FUNC_ALIAS aeabi_fcmpun unordsf2
+	CFI_START_FUNCTION
 
 	mov	r2, r0, lsl #1
 	mov	r3, r1, lsl #1
@@ -947,6 +1025,7 @@ ARM_FUNC_ALIAS aeabi_fcmpun unordsf2
 3:	mov	r0, #1			@ arguments are unordered.
 	RET
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_fcmpun
 	FUNC_END unordsf2
 
@@ -956,6 +1035,7 @@ ARM_FUNC_ALIAS aeabi_fcmpun unordsf2
 
 ARM_FUNC_START fixsfsi
 ARM_FUNC_ALIAS aeabi_f2iz fixsfsi
+	CFI_START_FUNCTION
 
 	@ check exponent range.
 	mov	r2, r0, lsl #1
@@ -989,6 +1069,7 @@ ARM_FUNC_ALIAS aeabi_f2iz fixsfsi
 4:	mov	r0, #0			@ What should we convert NAN to?
 	RET
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_f2iz
 	FUNC_END fixsfsi
 
@@ -998,6 +1079,7 @@ ARM_FUNC_ALIAS aeabi_f2iz fixsfsi
 
 ARM_FUNC_START fixunssfsi
 ARM_FUNC_ALIAS aeabi_f2uiz fixunssfsi
+	CFI_START_FUNCTION
 
 	@ check exponent range.
 	movs	r2, r0, lsl #1
@@ -1027,6 +1109,7 @@ ARM_FUNC_ALIAS aeabi_f2uiz fixunssfsi
 4:	mov	r0, #0			@ What should we convert NAN to?
 	RET
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_f2uiz
 	FUNC_END fixunssfsi
 
--- a/src/libgcc/config/arm/lib1funcs.S
+++ b/src/libgcc/config/arm/lib1funcs.S
@@ -1965,6 +1965,16 @@ LSYM(Lchange_\register):
 
 #endif /* Arch supports thumb.  */
 
+.macro CFI_START_FUNCTION
+	.cfi_startproc
+	.cfi_remember_state
+.endm
+
+.macro CFI_END_FUNCTION
+	.cfi_restore_state
+	.cfi_endproc
+.endm
+
 #ifndef __symbian__
 #ifndef __ARM_ARCH_6M__
 #include "ieee754-df.S"
--- a/src/libgcc/unwind-dw2-fde-dip.c
+++ b/src/libgcc/unwind-dw2-fde-dip.c
@@ -59,6 +59,12 @@
 
 #if !defined(inhibit_libc) && defined(HAVE_LD_EH_FRAME_HDR) \
     && defined(TARGET_DL_ITERATE_PHDR) \
+    && defined(__linux__)
+# define USE_PT_GNU_EH_FRAME
+#endif
+
+#if !defined(inhibit_libc) && defined(HAVE_LD_EH_FRAME_HDR) \
+    && defined(TARGET_DL_ITERATE_PHDR) \
     && (defined(__DragonFly__) || defined(__FreeBSD__))
 # define ElfW __ElfN
 # define USE_PT_GNU_EH_FRAME
--- a/src/libgfortran/acinclude.m4
+++ b/src/libgfortran/acinclude.m4
@@ -100,7 +100,7 @@ void foo (void);
 	      [Define to 1 if the target supports #pragma weak])
   fi
   case "$host" in
-    *-*-darwin* | *-*-hpux* | *-*-cygwin* | *-*-mingw* )
+    *-*-darwin* | *-*-hpux* | *-*-cygwin* | *-*-mingw* | *-*-musl* )
       AC_DEFINE(GTHREAD_USE_WEAK, 0,
 		[Define to 0 if the target shouldn't use #pragma weak])
       ;;
--- a/src/libgfortran/configure
+++ b/src/libgfortran/configure
@@ -26456,7 +26456,7 @@ $as_echo "#define SUPPORTS_WEAK 1" >>confdefs.h
 
   fi
   case "$host" in
-    *-*-darwin* | *-*-hpux* | *-*-cygwin* | *-*-mingw* )
+    *-*-darwin* | *-*-hpux* | *-*-cygwin* | *-*-mingw* | *-*-musl* )
 
 $as_echo "#define GTHREAD_USE_WEAK 0" >>confdefs.h
 
--- a/src/libitm/config/arm/hwcap.cc
+++ b/src/libitm/config/arm/hwcap.cc
@@ -40,7 +40,7 @@ int GTM_hwcap HIDDEN = 0
 
 #ifdef __linux__
 #include <unistd.h>
-#include <sys/fcntl.h>
+#include <fcntl.h>
 #include <elf.h>
 
 static void __attribute__((constructor))
--- a/src/libitm/config/linux/x86/tls.h
+++ b/src/libitm/config/linux/x86/tls.h
@@ -25,16 +25,19 @@
 #ifndef LIBITM_X86_TLS_H
 #define LIBITM_X86_TLS_H 1
 
-#if defined(__GLIBC_PREREQ) && __GLIBC_PREREQ(2, 10)
+#if defined(__GLIBC_PREREQ)
+#if __GLIBC_PREREQ(2, 10)
 /* Use slots in the TCB head rather than __thread lookups.
    GLIBC has reserved words 10 through 13 for TM.  */
 #define HAVE_ARCH_GTM_THREAD 1
 #define HAVE_ARCH_GTM_THREAD_DISP 1
 #endif
+#endif
 
 #include "config/generic/tls.h"
 
-#if defined(__GLIBC_PREREQ) && __GLIBC_PREREQ(2, 10)
+#if defined(__GLIBC_PREREQ)
+#if __GLIBC_PREREQ(2, 10)
 namespace GTM HIDDEN {
 
 #ifdef __x86_64__
@@ -101,5 +104,6 @@ static inline void set_abi_disp(struct abi_dispatch *x)
 
 } // namespace GTM
 #endif /* >= GLIBC 2.10 */
+#endif
 
 #endif // LIBITM_X86_TLS_H
--- a/src//dev/null
+++ b/src/libstdc++-v3/config/cpu/arm/cpu_defines.h
@@ -0,0 +1,40 @@
+// Specific definitions for generic platforms  -*- C++ -*-
+
+// Copyright (C) 2015 Free Software Foundation, Inc.
+//
+// This file is part of the GNU ISO C++ Library.  This library is free
+// software; you can redistribute it and/or modify it under the
+// terms of the GNU General Public License as published by the
+// Free Software Foundation; either version 3, or (at your option)
+// any later version.
+
+// This library is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+// GNU General Public License for more details.
+
+// Under Section 7 of GPL version 3, you are granted additional
+// permissions described in the GCC Runtime Library Exception, version
+// 3.1, as published by the Free Software Foundation.
+
+// You should have received a copy of the GNU General Public License and
+// a copy of the GCC Runtime Library Exception along with this program;
+// see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
+// <http://www.gnu.org/licenses/>.
+
+/** @file bits/cpu_defines.h
+ *  This is an internal header file, included by other library headers.
+ *  Do not attempt to use it directly. @headername{iosfwd}
+ */
+
+#ifndef _GLIBCXX_CPU_DEFINES
+#define _GLIBCXX_CPU_DEFINES 1
+
+// Integer divide instructions don't trap on ARM.
+#ifdef __ARM_ARCH_EXT_IDIV__
+#define __glibcxx_integral_traps false
+#else
+#define __glibcxx_integral_traps true
+#endif
+
+#endif
--- a/src/libstdc++-v3/config/os/generic/os_defines.h
+++ b/src/libstdc++-v3/config/os/generic/os_defines.h
@@ -33,4 +33,9 @@
 // System-specific #define, typedefs, corrections, etc, go here.  This
 // file will come before all others.
 
+// Disable the weak reference logic in gthr.h for os/generic because it
+// is broken on every platform unless there is implementation specific
+// workaround in gthr-posix.h and at link-time for static linking.
+#define _GLIBCXX_GTHREAD_USE_WEAK 0
+
 #endif
--- a/src/libstdc++-v3/configure.host
+++ b/src/libstdc++-v3/configure.host
@@ -143,6 +143,9 @@ cpu_include_dir=cpu/${try_cpu}
 # Set specific CPU overrides for cpu_defines_dir. Most can just use generic.
 # THIS TABLE IS SORTED.  KEEP IT THAT WAY.
 case "${host_cpu}" in
+  arm*)
+    cpu_defines_dir=cpu/arm
+    ;;
   powerpc* | rs6000)
     cpu_defines_dir=cpu/powerpc
     ;;
@@ -273,6 +276,9 @@ case "${host_os}" in
   freebsd*)
     os_include_dir="os/bsd/freebsd"
     ;;
+  linux-musl*)
+    os_include_dir="os/generic"
+    ;;
   gnu* | linux* | kfreebsd*-gnu | knetbsd*-gnu)
     if [ "$uclibc" = "yes" ]; then
       os_include_dir="os/uclibc"

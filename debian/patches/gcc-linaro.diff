# DP: Changes for the Linaro 5-2015.10 release.

LANG=C git diff 472e2599b141820b2a1565209528750de18731f8..5db159c220ec010ab6ae331802cddc242f83bb38 \
 | egrep -v '^(diff|index) ' \
 | filterdiff --strip=1 --addoldprefix=a/src/  --addnewprefix=b/src/

--- a/src/fixincludes/mkfixinc.sh
+++ b/src/fixincludes/mkfixinc.sh
@@ -19,7 +19,8 @@ case $machine in
     powerpc-*-eabi*    | \
     powerpc-*-rtems*   | \
     powerpcle-*-eabisim* | \
-    powerpcle-*-eabi* )
+    powerpcle-*-eabi* | \
+    *-musl* )
 	#  IF there is no include fixing,
 	#  THEN create a no-op fixer and exit
 	(echo "#! /bin/sh" ; echo "exit 0" ) > ${target}
--- a/src//dev/null
+++ b/src/gcc/LINARO-VERSION
@@ -0,0 +1 @@
+Snapshot 5.2-2015.10
--- a/src/gcc/Makefile.in
+++ b/src/gcc/Makefile.in
@@ -527,10 +527,6 @@ xm_include_list=@xm_include_list@
 xm_defines=@xm_defines@
 lang_checks=
 lang_checks_parallelized=
-# Upper limit to which it is useful to parallelize this lang target.
-# It doesn't make sense to try e.g. 128 goals for small testsuites
-# like objc or go.
-check_gcc_parallelize=10000
 lang_opt_files=@lang_opt_files@ $(srcdir)/c-family/c.opt $(srcdir)/common.opt
 lang_specs_files=@lang_specs_files@
 lang_tree_files=@lang_tree_files@
@@ -814,10 +810,12 @@ BASEVER     := $(srcdir)/BASE-VER  # 4.x.y
 DEVPHASE    := $(srcdir)/DEV-PHASE # experimental, prerelease, ""
 DATESTAMP   := $(srcdir)/DATESTAMP # YYYYMMDD or empty
 REVISION    := $(srcdir)/REVISION  # [BRANCH revision XXXXXX]
+LINAROVER   := $(srcdir)/LINARO-VERSION # M.x-YYYY.MM[-S][~dev]
 
 BASEVER_c   := $(shell cat $(BASEVER))
 DEVPHASE_c  := $(shell cat $(DEVPHASE))
 DATESTAMP_c := $(shell cat $(DATESTAMP))
+LINAROVER_c := $(shell cat $(LINAROVER))
 
 ifeq (,$(wildcard $(REVISION)))
 REVISION_c  :=
@@ -844,6 +842,7 @@ DATESTAMP_s := \
   "\"$(if $(DEVPHASE_c)$(filter-out 0,$(PATCHLEVEL_c)), $(DATESTAMP_c))\""
 PKGVERSION_s:= "\"@PKGVERSION@\""
 BUGURL_s    := "\"@REPORT_BUGS_TO@\""
+LINAROVER_s := "\"$(LINAROVER_c)\""
 
 PKGVERSION  := @PKGVERSION@
 BUGURL_TEXI := @REPORT_BUGS_TEXI@
@@ -2623,8 +2622,9 @@ PREPROCESSOR_DEFINES = \
   -DSTANDARD_EXEC_PREFIX=\"$(libdir)/gcc/\" \
   @TARGET_SYSTEM_ROOT_DEFINE@
 
-CFLAGS-cppbuiltin.o += $(PREPROCESSOR_DEFINES) -DBASEVER=$(BASEVER_s)
-cppbuiltin.o: $(BASEVER)
+CFLAGS-cppbuiltin.o += $(PREPROCESSOR_DEFINES) -DBASEVER=$(BASEVER_s) \
+	-DLINAROVER=$(LINAROVER_s)
+cppbuiltin.o: $(BASEVER) $(LINAROVER)
 
 CFLAGS-cppdefault.o += $(PREPROCESSOR_DEFINES)
 
@@ -3736,7 +3736,9 @@ check_p_subdirs=$(wordlist 1,$(check_p_count),$(wordlist 1, \
 #
 # To parallelize some language check, add the corresponding check-$lang
 # to lang_checks_parallelized variable and define check_$lang_parallelize
-# variable (see above check_gcc_parallelize description).
+# variable.  This is the upper limit to which it is useful to parallelize the
+# check-$lang target.  It doesn't make sense to try e.g. 128 goals for small
+# testsuites like objc or go.
 $(lang_checks_parallelized): check-% : site.exp
 	-rm -rf $(TESTSUITEDIR)/$*-parallel
 	@if [ "$(filter -j, $(MFLAGS))" = "-j" ]; then \
--- a/src/gcc/ada/gcc-interface/Make-lang.in
+++ b/src/gcc/ada/gcc-interface/Make-lang.in
@@ -811,6 +811,7 @@ ada.mostlyclean:
 	-$(RM) ada/*$(coverageexts)
 	-$(RM) ada/sdefault.adb ada/stamp-sdefault ada/stamp-snames
 	-$(RMDIR) ada/tools
+	-$(RM) gnatbind$(exeext) gnat1$(exeext)
 ada.clean:
 ada.distclean:
 	-$(RM) ada/Makefile
--- a/src/gcc/builtins.c
+++ b/src/gcc/builtins.c
@@ -5477,7 +5477,8 @@ expand_builtin_atomic_compare_exchange (machine_mode mode, tree exp,
      the normal case where EXPECT is totally private, i.e. a register.  At
      which point the store can be unconditional.  */
   label = gen_label_rtx ();
-  emit_cmp_and_jump_insns (target, const0_rtx, NE, NULL, VOIDmode, 1, label);
+  emit_cmp_and_jump_insns (target, const0_rtx, NE, NULL,
+			   GET_MODE (target), 1, label);
   emit_move_insn (expect, oldval);
   emit_label (label);
 
--- a/src/gcc/c-family/c-common.h
+++ b/src/gcc/c-family/c-common.h
@@ -1084,6 +1084,8 @@ extern const unsigned char executable_checksum[16];
 /* In c-cppbuiltin.c  */
 extern void builtin_define_std (const char *macro);
 extern void builtin_define_with_value (const char *, const char *, int);
+extern void builtin_define_with_int_value (const char *, HOST_WIDE_INT);
+extern void builtin_define_type_sizeof (const char *, tree);
 extern void c_stddef_cpp_builtins (void);
 extern void fe_file_change (const struct line_map *);
 extern void c_parse_error (const char *, enum cpp_ttype, tree, unsigned char);
--- a/src/gcc/c-family/c-cppbuiltin.c
+++ b/src/gcc/c-family/c-cppbuiltin.c
@@ -58,8 +58,6 @@ along with GCC; see the file COPYING3.  If not see
 #endif
 
 /* Non-static as some targets don't use it.  */
-void builtin_define_std (const char *) ATTRIBUTE_UNUSED;
-static void builtin_define_with_int_value (const char *, HOST_WIDE_INT);
 static void builtin_define_with_hex_fp_value (const char *, tree,
 					      int, const char *,
 					      const char *,
@@ -68,7 +66,6 @@ static void builtin_define_stdint_macros (void);
 static void builtin_define_constants (const char *, tree);
 static void builtin_define_type_max (const char *, tree);
 static void builtin_define_type_minmax (const char *, const char *, tree);
-static void builtin_define_type_sizeof (const char *, tree);
 static void builtin_define_float_constants (const char *,
 					    const char *,
 					    const char *,
@@ -113,7 +110,7 @@ mode_has_fma (machine_mode mode)
 }
 
 /* Define NAME with value TYPE size_unit.  */
-static void
+void
 builtin_define_type_sizeof (const char *name, tree type)
 {
   builtin_define_with_int_value (name,
@@ -1372,7 +1369,7 @@ builtin_define_with_value (const char *macro, const char *expansion, int is_str)
 
 
 /* Pass an object-like macro and an integer value to define it to.  */
-static void
+void
 builtin_define_with_int_value (const char *macro, HOST_WIDE_INT value)
 {
   char *buf;
--- a/src/gcc/c/Make-lang.in
+++ b/src/gcc/c/Make-lang.in
@@ -95,6 +95,8 @@ c.srcman:
 # List of targets that can use the generic check- rule and its // variant.
 lang_checks += check-gcc
 lang_checks_parallelized += check-gcc
+# For description see the check_$lang_parallelize comment in gcc/Makefile.in.
+check_gcc_parallelize=10000
 
 # 'make check' in gcc/ looks for check-c.  Redirect it to check-gcc.
 check-c : check-gcc
--- a/src/gcc/c/c-decl.c
+++ b/src/gcc/c/c-decl.c
@@ -2632,6 +2632,12 @@ merge_decls (tree newdecl, tree olddecl, tree newtype, tree oldtype)
   else if (DECL_PRESERVE_P (newdecl))
     DECL_PRESERVE_P (olddecl) = 1;
 
+  /* Merge DECL_COMMON */
+  if (VAR_P (olddecl) && VAR_P (newdecl)
+      && !lookup_attribute ("common", DECL_ATTRIBUTES (newdecl))
+      && !lookup_attribute ("nocommon", DECL_ATTRIBUTES (newdecl)))
+    DECL_COMMON (newdecl) = DECL_COMMON (newdecl) && DECL_COMMON (olddecl);
+
   /* Copy most of the decl-specific fields of NEWDECL into OLDDECL.
      But preserve OLDDECL's DECL_UID, DECL_CONTEXT and
      DECL_ARGUMENTS (if appropriate).  */
@@ -7524,12 +7530,23 @@ detect_field_duplicates (tree fieldlist)
 /* Finish up struct info used by -Wc++-compat.  */
 
 static void
-warn_cxx_compat_finish_struct (tree fieldlist)
+warn_cxx_compat_finish_struct (tree fieldlist, enum tree_code code,
+			       location_t record_loc)
 {
   unsigned int ix;
   tree x;
   struct c_binding *b;
 
+  if (fieldlist == NULL_TREE)
+    {
+      if (code == RECORD_TYPE)
+	warning_at (record_loc, OPT_Wc___compat,
+		    "empty struct has size 0 in C, size 1 in C++");
+      else
+	warning_at (record_loc, OPT_Wc___compat,
+		    "empty union has size 0 in C, size 1 in C++");
+    }
+
   /* Set the C_TYPE_DEFINED_IN_STRUCT flag for each type defined in
      the current struct.  We do this now at the end of the struct
      because the flag is used to issue visibility warnings, and we
@@ -7862,7 +7879,7 @@ finish_struct (location_t loc, tree t, tree fieldlist, tree attributes,
 			  DECL_EXPR, build_decl (loc, TYPE_DECL, NULL, t)));
 
   if (warn_cxx_compat)
-    warn_cxx_compat_finish_struct (fieldlist);
+    warn_cxx_compat_finish_struct (fieldlist, TREE_CODE (t), loc);
 
   struct_parse_info->struct_types.release ();
   struct_parse_info->fields.release ();
--- a/src/gcc/cfgexpand.c
+++ b/src/gcc/cfgexpand.c
@@ -1382,7 +1382,16 @@ expand_one_var (tree var, bool toplevel, bool really_expand)
   else
     {
       if (really_expand)
-        expand_one_stack_var (origvar);
+        {
+          if (lookup_attribute ("naked",
+                                DECL_ATTRIBUTES (current_function_decl)))
+            error ("cannot allocate stack for variable %q+D, naked function.",
+                   var);
+
+          expand_one_stack_var (origvar);
+        }
+
+
       return tree_to_uhwi (DECL_SIZE_UNIT (var));
     }
   return 0;
--- a/src/gcc/cgraphunit.c
+++ b/src/gcc/cgraphunit.c
@@ -2505,6 +2505,7 @@ cgraph_node::create_wrapper (cgraph_node *target)
   memset (&thunk, 0, sizeof (cgraph_thunk_info));
   thunk.thunk_p = true;
   create_edge (target, NULL, count, CGRAPH_FREQ_BASE);
+  callees->can_throw_external = !TREE_NOTHROW (target->decl);
 
   tree arguments = DECL_ARGUMENTS (decl);
 
--- a/src/gcc/combine.c
+++ b/src/gcc/combine.c
@@ -1650,6 +1650,73 @@ setup_incoming_promotions (rtx_insn *first)
     }
 }
 
+#ifdef SHORT_IMMEDIATES_SIGN_EXTEND
+/* If MODE has a precision lower than PREC and SRC is a non-negative constant
+   that would appear negative in MODE, sign-extend SRC for use in nonzero_bits
+   because some machines (maybe most) will actually do the sign-extension and
+   this is the conservative approach.
+
+   ??? For 2.5, try to tighten up the MD files in this regard instead of this
+   kludge.  */
+
+static rtx
+sign_extend_short_imm (rtx src, machine_mode mode, unsigned int prec)
+{
+  if (GET_MODE_PRECISION (mode) < prec
+      && CONST_INT_P (src)
+      && INTVAL (src) > 0
+      && val_signbit_known_set_p (mode, INTVAL (src)))
+    src = GEN_INT (INTVAL (src) | ~GET_MODE_MASK (mode));
+
+  return src;
+}
+#endif
+
+/* Update RSP for pseudo-register X from INSN's REG_EQUAL note (if one exists)
+   and SET.  */
+
+static void
+update_rsp_from_reg_equal (reg_stat_type *rsp, rtx_insn *insn, const_rtx set,
+			   rtx x)
+{
+  rtx reg_equal_note = insn ? find_reg_equal_equiv_note (insn) : NULL_RTX;
+  unsigned HOST_WIDE_INT bits = 0;
+  rtx reg_equal = NULL, src = SET_SRC (set);
+  unsigned int num = 0;
+
+  if (reg_equal_note)
+    reg_equal = XEXP (reg_equal_note, 0);
+
+#ifdef SHORT_IMMEDIATES_SIGN_EXTEND
+  src = sign_extend_short_imm (src, GET_MODE (x), BITS_PER_WORD);
+  if (reg_equal)
+    reg_equal = sign_extend_short_imm (reg_equal, GET_MODE (x), BITS_PER_WORD);
+#endif
+
+  /* Don't call nonzero_bits if it cannot change anything.  */
+  if (rsp->nonzero_bits != ~(unsigned HOST_WIDE_INT) 0)
+    {
+      bits = nonzero_bits (src, nonzero_bits_mode);
+      if (reg_equal && bits)
+	bits &= nonzero_bits (reg_equal, nonzero_bits_mode);
+      rsp->nonzero_bits |= bits;
+    }
+
+  /* Don't call num_sign_bit_copies if it cannot change anything.  */
+  if (rsp->sign_bit_copies != 1)
+    {
+      num = num_sign_bit_copies (SET_SRC (set), GET_MODE (x));
+      if (reg_equal && num != GET_MODE_PRECISION (GET_MODE (x)))
+	{
+	  unsigned int numeq = num_sign_bit_copies (reg_equal, GET_MODE (x));
+	  if (num == 0 || numeq > num)
+	    num = numeq;
+	}
+      if (rsp->sign_bit_copies == 0 || num < rsp->sign_bit_copies)
+	rsp->sign_bit_copies = num;
+    }
+}
+
 /* Called via note_stores.  If X is a pseudo that is narrower than
    HOST_BITS_PER_WIDE_INT and is being set, record what bits are known zero.
 
@@ -1665,7 +1732,6 @@ static void
 set_nonzero_bits_and_sign_copies (rtx x, const_rtx set, void *data)
 {
   rtx_insn *insn = (rtx_insn *) data;
-  unsigned int num;
 
   if (REG_P (x)
       && REGNO (x) >= FIRST_PSEUDO_REGISTER
@@ -1725,34 +1791,7 @@ set_nonzero_bits_and_sign_copies (rtx x, const_rtx set, void *data)
       if (SET_DEST (set) == x
 	  || (paradoxical_subreg_p (SET_DEST (set))
 	      && SUBREG_REG (SET_DEST (set)) == x))
-	{
-	  rtx src = SET_SRC (set);
-
-#ifdef SHORT_IMMEDIATES_SIGN_EXTEND
-	  /* If X is narrower than a word and SRC is a non-negative
-	     constant that would appear negative in the mode of X,
-	     sign-extend it for use in reg_stat[].nonzero_bits because some
-	     machines (maybe most) will actually do the sign-extension
-	     and this is the conservative approach.
-
-	     ??? For 2.5, try to tighten up the MD files in this regard
-	     instead of this kludge.  */
-
-	  if (GET_MODE_PRECISION (GET_MODE (x)) < BITS_PER_WORD
-	      && CONST_INT_P (src)
-	      && INTVAL (src) > 0
-	      && val_signbit_known_set_p (GET_MODE (x), INTVAL (src)))
-	    src = GEN_INT (INTVAL (src) | ~GET_MODE_MASK (GET_MODE (x)));
-#endif
-
-	  /* Don't call nonzero_bits if it cannot change anything.  */
-	  if (rsp->nonzero_bits != ~(unsigned HOST_WIDE_INT) 0)
-	    rsp->nonzero_bits |= nonzero_bits (src, nonzero_bits_mode);
-	  num = num_sign_bit_copies (SET_SRC (set), GET_MODE (x));
-	  if (rsp->sign_bit_copies == 0
-	      || rsp->sign_bit_copies > num)
-	    rsp->sign_bit_copies = num;
-	}
+	update_rsp_from_reg_equal (rsp, insn, set, x);
       else
 	{
 	  rsp->nonzero_bits = GET_MODE_MASK (GET_MODE (x));
@@ -1914,6 +1953,15 @@ can_combine_p (rtx_insn *insn, rtx_insn *i3, rtx_insn *pred ATTRIBUTE_UNUSED,
   set = expand_field_assignment (set);
   src = SET_SRC (set), dest = SET_DEST (set);
 
+  /* Do not eliminate user-specified register if it is in an
+     asm input because we may break the register asm usage defined
+     in GCC manual if allow to do so.
+     Be aware that this may cover more cases than we expect but this
+     should be harmless.  */
+  if (REG_P (dest) && REG_USERVAR_P (dest) && HARD_REGISTER_P (dest)
+      && extract_asm_operands (PATTERN (i3)))
+    return 0;
+
   /* Don't eliminate a store in the stack pointer.  */
   if (dest == stack_pointer_rtx
       /* Don't combine with an insn that sets a register to itself if it has
@@ -5463,6 +5511,51 @@ combine_simplify_rtx (rtx x, machine_mode op0_mode, int in_dest,
       SUBST (XEXP (x, 1), temp);
     }
 
+  /* Try to fold this expression in case we have constants that weren't
+     present before.  */
+  temp = 0;
+  switch (GET_RTX_CLASS (code))
+    {
+    case RTX_UNARY:
+      if (op0_mode == VOIDmode)
+	op0_mode = GET_MODE (XEXP (x, 0));
+      temp = simplify_unary_operation (code, mode, XEXP (x, 0), op0_mode);
+      break;
+    case RTX_COMPARE:
+    case RTX_COMM_COMPARE:
+      {
+	machine_mode cmp_mode = GET_MODE (XEXP (x, 0));
+	if (cmp_mode == VOIDmode)
+	  {
+	    cmp_mode = GET_MODE (XEXP (x, 1));
+	    if (cmp_mode == VOIDmode)
+	      cmp_mode = op0_mode;
+	  }
+	temp = simplify_relational_operation (code, mode, cmp_mode,
+					      XEXP (x, 0), XEXP (x, 1));
+      }
+      break;
+    case RTX_COMM_ARITH:
+    case RTX_BIN_ARITH:
+      temp = simplify_binary_operation (code, mode, XEXP (x, 0), XEXP (x, 1));
+      break;
+    case RTX_BITFIELD_OPS:
+    case RTX_TERNARY:
+      temp = simplify_ternary_operation (code, mode, op0_mode, XEXP (x, 0),
+					 XEXP (x, 1), XEXP (x, 2));
+      break;
+    default:
+      break;
+    }
+
+  if (temp)
+    {
+      x = temp;
+      code = GET_CODE (temp);
+      op0_mode = VOIDmode;
+      mode = GET_MODE (temp);
+    }
+
   /* If this is a simple operation applied to an IF_THEN_ELSE, try
      applying it to the arms of the IF_THEN_ELSE.  This often simplifies
      things.  Check for cases where both arms are testing the same
@@ -5562,51 +5655,6 @@ combine_simplify_rtx (rtx x, machine_mode op0_mode, int in_dest,
 	}
     }
 
-  /* Try to fold this expression in case we have constants that weren't
-     present before.  */
-  temp = 0;
-  switch (GET_RTX_CLASS (code))
-    {
-    case RTX_UNARY:
-      if (op0_mode == VOIDmode)
-	op0_mode = GET_MODE (XEXP (x, 0));
-      temp = simplify_unary_operation (code, mode, XEXP (x, 0), op0_mode);
-      break;
-    case RTX_COMPARE:
-    case RTX_COMM_COMPARE:
-      {
-	machine_mode cmp_mode = GET_MODE (XEXP (x, 0));
-	if (cmp_mode == VOIDmode)
-	  {
-	    cmp_mode = GET_MODE (XEXP (x, 1));
-	    if (cmp_mode == VOIDmode)
-	      cmp_mode = op0_mode;
-	  }
-	temp = simplify_relational_operation (code, mode, cmp_mode,
-					      XEXP (x, 0), XEXP (x, 1));
-      }
-      break;
-    case RTX_COMM_ARITH:
-    case RTX_BIN_ARITH:
-      temp = simplify_binary_operation (code, mode, XEXP (x, 0), XEXP (x, 1));
-      break;
-    case RTX_BITFIELD_OPS:
-    case RTX_TERNARY:
-      temp = simplify_ternary_operation (code, mode, op0_mode, XEXP (x, 0),
-					 XEXP (x, 1), XEXP (x, 2));
-      break;
-    default:
-      break;
-    }
-
-  if (temp)
-    {
-      x = temp;
-      code = GET_CODE (temp);
-      op0_mode = VOIDmode;
-      mode = GET_MODE (temp);
-    }
-
   /* First see if we can apply the inverse distributive law.  */
   if (code == PLUS || code == MINUS
       || code == AND || code == IOR || code == XOR)
@@ -7723,9 +7771,8 @@ extract_left_shift (rtx x, int count)
    We try, as much as possible, to re-use rtl expressions to save memory.
 
    IN_CODE says what kind of expression we are processing.  Normally, it is
-   SET.  In a memory address (inside a MEM, PLUS or minus, the latter two
-   being kludges), it is MEM.  When processing the arguments of a comparison
-   or a COMPARE against zero, it is COMPARE.  */
+   SET.  In a memory address it is MEM.  When processing the arguments of
+   a comparison or a COMPARE against zero, it is COMPARE.  */
 
 rtx
 make_compound_operation (rtx x, enum rtx_code in_code)
@@ -7745,8 +7792,6 @@ make_compound_operation (rtx x, enum rtx_code in_code)
      but once inside, go back to our default of SET.  */
 
   next_code = (code == MEM ? MEM
-	       : ((code == PLUS || code == MINUS)
-		  && SCALAR_INT_MODE_P (mode)) ? MEM
 	       : ((code == COMPARE || COMPARISON_P (x))
 		  && XEXP (x, 1) == const0_rtx) ? COMPARE
 	       : in_code == COMPARE ? SET : in_code);
@@ -9797,20 +9842,8 @@ reg_nonzero_bits_for_combine (const_rtx x, machine_mode mode,
   if (tem)
     {
 #ifdef SHORT_IMMEDIATES_SIGN_EXTEND
-      /* If X is narrower than MODE and TEM is a non-negative
-	 constant that would appear negative in the mode of X,
-	 sign-extend it for use in reg_nonzero_bits because some
-	 machines (maybe most) will actually do the sign-extension
-	 and this is the conservative approach.
-
-	 ??? For 2.5, try to tighten up the MD files in this regard
-	 instead of this kludge.  */
-
-      if (GET_MODE_PRECISION (GET_MODE (x)) < GET_MODE_PRECISION (mode)
-	  && CONST_INT_P (tem)
-	  && INTVAL (tem) > 0
-	  && val_signbit_known_set_p (GET_MODE (x), INTVAL (tem)))
-	tem = GEN_INT (INTVAL (tem) | ~GET_MODE_MASK (GET_MODE (x)));
+      tem = sign_extend_short_imm (tem, GET_MODE (x),
+				   GET_MODE_PRECISION (mode));
 #endif
       return tem;
     }
--- a/src/gcc/config.gcc
+++ b/src/gcc/config.gcc
@@ -302,7 +302,7 @@ m32c*-*-*)
 aarch64*-*-*)
 	cpu_type=aarch64
 	extra_headers="arm_neon.h arm_acle.h"
-	extra_objs="aarch64-builtins.o aarch-common.o"
+	extra_objs="aarch64-builtins.o aarch-common.o cortex-a57-fma-steering.o"
 	target_gtfiles="\$(srcdir)/config/aarch64/aarch64-builtins.c"
 	target_has_targetm_common=yes
 	;;
@@ -575,7 +575,7 @@ case ${target} in
 esac
 
 # Common C libraries.
-tm_defines="$tm_defines LIBC_GLIBC=1 LIBC_UCLIBC=2 LIBC_BIONIC=3"
+tm_defines="$tm_defines LIBC_GLIBC=1 LIBC_UCLIBC=2 LIBC_BIONIC=3 LIBC_MUSL=4"
 
 # 32-bit x86 processors supported by --with-arch=.  Each processor
 # MUST be separated by exactly one space.
@@ -720,6 +720,9 @@ case ${target} in
     *-*-*uclibc*)
       tm_defines="$tm_defines DEFAULT_LIBC=LIBC_UCLIBC"
       ;;
+    *-*-*musl*)
+      tm_defines="$tm_defines DEFAULT_LIBC=LIBC_MUSL"
+      ;;
     *)
       tm_defines="$tm_defines DEFAULT_LIBC=LIBC_GLIBC"
       ;;
@@ -3484,7 +3487,7 @@ case "${target}" in
 
 			eval "val=\$with_$which"
 			base_val=`echo $val | sed -e 's/\+.*//'`
-			ext_val=`echo $val | sed -e 's/[a-z0-9\-]\+//'`
+			ext_val=`echo $val | sed -e 's/[a-z0-9.-]\+//'`
 
 			if [ $which = arch ]; then
 			  def=aarch64-arches.def
--- a/src/gcc/config.host
+++ b/src/gcc/config.host
@@ -99,6 +99,14 @@ case ${host} in
 esac
 
 case ${host} in
+  aarch64*-*-linux*)
+    case ${target} in
+      aarch64*-*-*)
+	host_extra_gcc_objs="driver-aarch64.o"
+	host_xmake_file="${host_xmake_file} aarch64/x-aarch64"
+	;;
+    esac
+    ;;
   arm*-*-freebsd* | arm*-*-linux*)
     case ${target} in
       arm*-*-*)
--- a/src/gcc/config/aarch64/aarch64-arches.def
+++ b/src/gcc/config/aarch64/aarch64-arches.def
@@ -27,3 +27,4 @@
    the flags implied by the architecture.  */
 
 AARCH64_ARCH("armv8-a",	      generic,	     8,  AARCH64_FL_FOR_ARCH8)
+AARCH64_ARCH("armv8.1-a",     generic,	     8,  AARCH64_FL_FOR_ARCH8_1)
--- a/src/gcc/config/aarch64/aarch64-builtins.c
+++ b/src/gcc/config/aarch64/aarch64-builtins.c
@@ -133,7 +133,9 @@ enum aarch64_type_qualifiers
   /* Polynomial types.  */
   qualifier_poly = 0x100,
   /* Lane indices - must be in range, and flipped for bigendian.  */
-  qualifier_lane_index = 0x200
+  qualifier_lane_index = 0x200,
+  /* Lane indices for single lane structure loads and stores.  */
+  qualifier_struct_load_store_lane_index = 0x400
 };
 
 typedef struct
@@ -235,7 +237,7 @@ aarch64_types_load1_qualifiers[SIMD_MAX_BUILTIN_ARGS]
 static enum aarch64_type_qualifiers
 aarch64_types_loadstruct_lane_qualifiers[SIMD_MAX_BUILTIN_ARGS]
   = { qualifier_none, qualifier_const_pointer_map_mode,
-      qualifier_none, qualifier_none };
+      qualifier_none, qualifier_struct_load_store_lane_index };
 #define TYPES_LOADSTRUCT_LANE (aarch64_types_loadstruct_lane_qualifiers)
 
 static enum aarch64_type_qualifiers
@@ -267,7 +269,7 @@ aarch64_types_store1_qualifiers[SIMD_MAX_BUILTIN_ARGS]
 static enum aarch64_type_qualifiers
 aarch64_types_storestruct_lane_qualifiers[SIMD_MAX_BUILTIN_ARGS]
   = { qualifier_void, qualifier_pointer_map_mode,
-      qualifier_none, qualifier_none };
+      qualifier_none, qualifier_struct_load_store_lane_index };
 #define TYPES_STORESTRUCT_LANE (aarch64_types_storestruct_lane_qualifiers)
 
 #define CF0(N, X) CODE_FOR_aarch64_##N##X
@@ -883,12 +885,14 @@ typedef enum
   SIMD_ARG_COPY_TO_REG,
   SIMD_ARG_CONSTANT,
   SIMD_ARG_LANE_INDEX,
+  SIMD_ARG_STRUCT_LOAD_STORE_LANE_INDEX,
   SIMD_ARG_STOP
 } builtin_simd_arg;
 
 static rtx
 aarch64_simd_expand_args (rtx target, int icode, int have_retval,
-			  tree exp, builtin_simd_arg *args)
+			  tree exp, builtin_simd_arg *args,
+			  enum machine_mode builtin_mode)
 {
   rtx pat;
   rtx op[SIMD_MAX_BUILTIN_ARGS + 1]; /* First element for result operand.  */
@@ -927,6 +931,19 @@ aarch64_simd_expand_args (rtx target, int icode, int have_retval,
 		op[opc] = copy_to_mode_reg (mode, op[opc]);
 	      break;
 
+	    case SIMD_ARG_STRUCT_LOAD_STORE_LANE_INDEX:
+	      gcc_assert (opc > 1);
+	      if (CONST_INT_P (op[opc]))
+		{
+		  aarch64_simd_lane_bounds (op[opc], 0,
+					    GET_MODE_NUNITS (builtin_mode),
+					    exp);
+		  /* Keep to GCC-vector-extension lane indices in the RTL.  */
+		  op[opc] =
+		    GEN_INT (ENDIAN_LANE_N (builtin_mode, INTVAL (op[opc])));
+		}
+	      goto constant_arg;
+
 	    case SIMD_ARG_LANE_INDEX:
 	      /* Must be a previous operand into which this is an index.  */
 	      gcc_assert (opc > 0);
@@ -941,6 +958,7 @@ aarch64_simd_expand_args (rtx target, int icode, int have_retval,
 	      /* Fall through - if the lane index isn't a constant then
 		 the next case will error.  */
 	    case SIMD_ARG_CONSTANT:
+constant_arg:
 	      if (!(*insn_data[icode].operand[opc].predicate)
 		  (op[opc], mode))
 	      {
@@ -1049,6 +1067,8 @@ aarch64_simd_expand_builtin (int fcode, tree exp, rtx target)
 
       if (d->qualifiers[qualifiers_k] & qualifier_lane_index)
 	args[k] = SIMD_ARG_LANE_INDEX;
+      else if (d->qualifiers[qualifiers_k] & qualifier_struct_load_store_lane_index)
+	args[k] = SIMD_ARG_STRUCT_LOAD_STORE_LANE_INDEX;
       else if (d->qualifiers[qualifiers_k] & qualifier_immediate)
 	args[k] = SIMD_ARG_CONSTANT;
       else if (d->qualifiers[qualifiers_k] & qualifier_maybe_immediate)
@@ -1072,7 +1092,7 @@ aarch64_simd_expand_builtin (int fcode, tree exp, rtx target)
   /* The interface to aarch64_simd_expand_args expects a 0 if
      the function is void, and a 1 if it is not.  */
   return aarch64_simd_expand_args
-	  (target, icode, !is_void, exp, &args[1]);
+	  (target, icode, !is_void, exp, &args[1], d->mode);
 }
 
 rtx
--- a/src/gcc/config/aarch64/aarch64-cores.def
+++ b/src/gcc/config/aarch64/aarch64-cores.def
@@ -21,7 +21,7 @@
 
    Before using #include to read this file, define a macro:
 
-      AARCH64_CORE(CORE_NAME, CORE_IDENT, SCHEDULER_IDENT, ARCH, FLAGS, COSTS)
+      AARCH64_CORE(CORE_NAME, CORE_IDENT, SCHEDULER_IDENT, ARCH, FLAGS, COSTS, IMP, PART)
 
    The CORE_NAME is the name of the core, represented as a string constant.
    The CORE_IDENT is the name of the core, represented as an identifier.
@@ -30,18 +30,23 @@
    ARCH is the architecture revision implemented by the chip.
    FLAGS are the bitwise-or of the traits that apply to that core.
    This need not include flags implied by the architecture.
-   COSTS is the name of the rtx_costs routine to use.  */
+   COSTS is the name of the rtx_costs routine to use.
+   IMP is the implementer ID of the CPU vendor.  On a GNU/Linux system it can
+   be found in /proc/cpuinfo.
+   PART is the part number of the CPU.  On a GNU/Linux system it can be found
+   in /proc/cpuinfo.  For big.LITTLE systems this should have the form at of
+   "<big core part number>.<LITTLE core part number>".  */
 
 /* V8 Architecture Processors.  */
 
-AARCH64_CORE("cortex-a53",  cortexa53, cortexa53, 8,  AARCH64_FL_FOR_ARCH8 | AARCH64_FL_CRC, cortexa53)
-AARCH64_CORE("cortex-a57",  cortexa57, cortexa57, 8,  AARCH64_FL_FOR_ARCH8 | AARCH64_FL_CRC, cortexa57)
-AARCH64_CORE("cortex-a72",  cortexa72, cortexa57, 8,  AARCH64_FL_FOR_ARCH8 | AARCH64_FL_CRC, cortexa57)
-AARCH64_CORE("exynos-m1",   exynosm1,  cortexa57, 8,  AARCH64_FL_FOR_ARCH8 | AARCH64_FL_CRC | AARCH64_FL_CRYPTO, cortexa57)
-AARCH64_CORE("thunderx",    thunderx,  thunderx, 8,  AARCH64_FL_FOR_ARCH8 | AARCH64_FL_CRC | AARCH64_FL_CRYPTO, thunderx)
-AARCH64_CORE("xgene1",      xgene1,    xgene1,    8,  AARCH64_FL_FOR_ARCH8, xgene1)
+AARCH64_CORE("cortex-a53",  cortexa53, cortexa53, 8,  AARCH64_FL_FOR_ARCH8 | AARCH64_FL_CRC, cortexa53, "0x41", "0xd03")
+AARCH64_CORE("cortex-a57",  cortexa57, cortexa57, 8,  AARCH64_FL_FOR_ARCH8 | AARCH64_FL_CRC, cortexa57, "0x41", "0xd07")
+AARCH64_CORE("cortex-a72",  cortexa72, cortexa57, 8,  AARCH64_FL_FOR_ARCH8 | AARCH64_FL_CRC, cortexa72, "0x41", "0xd08")
+AARCH64_CORE("exynos-m1",   exynosm1,  cortexa57, 8,  AARCH64_FL_FOR_ARCH8 | AARCH64_FL_CRC | AARCH64_FL_CRYPTO, cortexa72, "0x53", "0x001")
+AARCH64_CORE("thunderx",    thunderx,  thunderx,  8,  AARCH64_FL_FOR_ARCH8 | AARCH64_FL_CRC | AARCH64_FL_CRYPTO, thunderx,  "0x43", "0x0a1")
+AARCH64_CORE("xgene1",      xgene1,    xgene1,    8,  AARCH64_FL_FOR_ARCH8, xgene1, "0x50", "0x000")
 
 /* V8 big.LITTLE implementations.  */
 
-AARCH64_CORE("cortex-a57.cortex-a53",  cortexa57cortexa53, cortexa53, 8,  AARCH64_FL_FOR_ARCH8 | AARCH64_FL_CRC, cortexa57)
-AARCH64_CORE("cortex-a72.cortex-a53",  cortexa72cortexa53, cortexa53, 8,  AARCH64_FL_FOR_ARCH8 | AARCH64_FL_CRC, cortexa57)
+AARCH64_CORE("cortex-a57.cortex-a53",  cortexa57cortexa53, cortexa53, 8,  AARCH64_FL_FOR_ARCH8 | AARCH64_FL_CRC, cortexa57, "0x41", "0xd07.0xd03")
+AARCH64_CORE("cortex-a72.cortex-a53",  cortexa72cortexa53, cortexa53, 8,  AARCH64_FL_FOR_ARCH8 | AARCH64_FL_CRC, cortexa72, "0x41", "0xd08.0xd03")
--- a/src/gcc/config/aarch64/aarch64-cost-tables.h
+++ b/src/gcc/config/aarch64/aarch64-cost-tables.h
@@ -83,7 +83,9 @@ const struct cpu_cost_table thunderx_extra_costs =
     0,			/* N/A: Stm_regs_per_insn_subsequent.  */
     0,			/* Storef.  */
     0,			/* Stored.  */
-    COSTS_N_INSNS (1)  /* Store_unaligned.  */
+    COSTS_N_INSNS (1),	/* Store_unaligned.  */
+    COSTS_N_INSNS (1),	/* Loadv.  */
+    COSTS_N_INSNS (1)	/* Storev.  */
   },
   {
     /* FP SFmode */
--- a/src/gcc/config/aarch64/aarch64-elf.h
+++ b/src/gcc/config/aarch64/aarch64-elf.h
@@ -132,7 +132,8 @@
 #undef DRIVER_SELF_SPECS
 #define DRIVER_SELF_SPECS \
   " %{!mbig-endian:%{!mlittle-endian:" ENDIAN_SPEC "}}" \
-  " %{!mabi=*:" ABI_SPEC "}"
+  " %{!mabi=*:" ABI_SPEC "}" \
+  MCPU_MTUNE_NATIVE_SPECS
 
 #ifdef HAVE_AS_MABI_OPTION
 #define ASM_MABI_SPEC	"%{mabi=*:-mabi=%*}"
--- a/src//dev/null
+++ b/src/gcc/config/aarch64/aarch64-fusion-pairs.def
@@ -0,0 +1,38 @@
+/* Copyright (C) 2015 Free Software Foundation, Inc.
+   Contributed by ARM Ltd.
+
+   This file is part of GCC.
+
+   GCC is free software; you can redistribute it and/or modify it
+   under the terms of the GNU General Public License as published
+   by the Free Software Foundation; either version 3, or (at your
+   option) any later version.
+
+   GCC is distributed in the hope that it will be useful, but WITHOUT
+   ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
+   or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public
+   License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with GCC; see the file COPYING3.  If not see
+   <http://www.gnu.org/licenses/>.  */
+
+/* Pairs of instructions which can be fused. before including this file,
+   define a macro:
+
+     AARCH64_FUSION_PAIR (name, internal_name, index_bit)
+
+   Where:
+
+     NAME is a string giving a friendly name for the instructions to fuse.
+     INTERNAL_NAME gives the internal name suitable for appending to
+     AARCH64_FUSE_ to give an enum name.
+     INDEX_BIT is the bit to set in the bitmask of supported fusion
+     operations.  */
+
+AARCH64_FUSION_PAIR ("mov+movk", MOV_MOVK, 0)
+AARCH64_FUSION_PAIR ("adrp+add", ADRP_ADD, 1)
+AARCH64_FUSION_PAIR ("movk+movk", MOVK_MOVK, 2)
+AARCH64_FUSION_PAIR ("adrp+ldr", ADRP_LDR, 3)
+AARCH64_FUSION_PAIR ("cmp+branch", CMP_BRANCH, 4)
+
--- a/src/gcc/config/aarch64/aarch64-linux.h
+++ b/src/gcc/config/aarch64/aarch64-linux.h
@@ -23,6 +23,9 @@
 
 #define GLIBC_DYNAMIC_LINKER "/lib/ld-linux-aarch64%{mbig-endian:_be}%{mabi=ilp32:_ilp32}.so.1"
 
+#undef MUSL_DYNAMIC_LINKER
+#define MUSL_DYNAMIC_LINKER "/lib/ld-musl-aarch64%{mbig-endian:_be}%{mabi=ilp32:_ilp32}.so.1"
+
 #undef  ASAN_CC1_SPEC
 #define ASAN_CC1_SPEC "%{%:sanitize(address):-funwind-tables}"
 
--- a/src/gcc/config/aarch64/aarch64-option-extensions.def
+++ b/src/gcc/config/aarch64/aarch64-option-extensions.def
@@ -21,18 +21,29 @@
 
    Before using #include to read this file, define a macro:
 
-      AARCH64_OPT_EXTENSION(EXT_NAME, FLAGS_ON, FLAGS_OFF)
+      AARCH64_OPT_EXTENSION(EXT_NAME, FLAGS_ON, FLAGS_OFF, FEATURE_STRING)
 
    EXT_NAME is the name of the extension, represented as a string constant.
    FLAGS_ON are the bitwise-or of the features that the extension adds.
-   FLAGS_OFF are the bitwise-or of the features that the extension removes.  */
+   FLAGS_OFF are the bitwise-or of the features that the extension removes.
+   FEAT_STRING is a string containing the entries in the 'Features' field of
+   /proc/cpuinfo on a GNU/Linux system that correspond to this architecture
+   extension being available.  Sometimes multiple entries are needed to enable
+   the extension (for example, the 'crypto' extension depends on four
+   entries: aes, pmull, sha1, sha2 being present).  In that case this field
+   should contain a whitespace-separated list of the strings in 'Features'
+   that are required.  Their order is not important.  */
 
 /* V8 Architecture Extensions.
    This list currently contains example extensions for CPUs that implement
    AArch64, and therefore serves as a template for adding more CPUs in the
    future.  */
 
-AARCH64_OPT_EXTENSION("fp",	AARCH64_FL_FP,	AARCH64_FL_FPSIMD | AARCH64_FL_CRYPTO)
-AARCH64_OPT_EXTENSION("simd",	AARCH64_FL_FPSIMD,	AARCH64_FL_SIMD | AARCH64_FL_CRYPTO)
-AARCH64_OPT_EXTENSION("crypto",	AARCH64_FL_CRYPTO | AARCH64_FL_FPSIMD,	AARCH64_FL_CRYPTO)
-AARCH64_OPT_EXTENSION("crc",	AARCH64_FL_CRC,	AARCH64_FL_CRC)
+AARCH64_OPT_EXTENSION("fp",	AARCH64_FL_FP,                          AARCH64_FL_FPSIMD | AARCH64_FL_CRYPTO | AARCH64_FL_RDMA, "fp")
+AARCH64_OPT_EXTENSION("simd",	AARCH64_FL_FPSIMD,                      AARCH64_FL_SIMD | AARCH64_FL_CRYPTO | AARCH64_FL_RDMA,   "asimd")
+AARCH64_OPT_EXTENSION("crypto",	AARCH64_FL_CRYPTO | AARCH64_FL_FPSIMD,  AARCH64_FL_CRYPTO,   "aes pmull sha1 sha2")
+AARCH64_OPT_EXTENSION("crc",	AARCH64_FL_CRC,                         AARCH64_FL_CRC,                        "crc32")
+AARCH64_OPT_EXTENSION("lse",	AARCH64_FL_LSE,                         AARCH64_FL_LSE,                        "lse")
+AARCH64_OPT_EXTENSION("pan",	AARCH64_FL_PAN,		AARCH64_FL_PAN,		"pan")
+AARCH64_OPT_EXTENSION("lor",	AARCH64_FL_LOR,		AARCH64_FL_LOR,		"lor")
+AARCH64_OPT_EXTENSION("rdma",	AARCH64_FL_RDMA | AARCH64_FL_FPSIMD,	AARCH64_FL_RDMA,	"rdma")
--- a/src/gcc/config/aarch64/aarch64-opts.h
+++ b/src/gcc/config/aarch64/aarch64-opts.h
@@ -25,7 +25,7 @@
 /* The various cores that implement AArch64.  */
 enum aarch64_processor
 {
-#define AARCH64_CORE(NAME, INTERNAL_IDENT, SCHED, ARCH, FLAGS, COSTS) \
+#define AARCH64_CORE(NAME, INTERNAL_IDENT, SCHED, ARCH, FLAGS, COSTS, IMP, PART) \
   INTERNAL_IDENT,
 #include "aarch64-cores.def"
 #undef AARCH64_CORE
--- a/src/gcc/config/aarch64/aarch64-protos.h
+++ b/src/gcc/config/aarch64/aarch64-protos.h
@@ -162,26 +162,78 @@ struct cpu_vector_cost
   const int cond_not_taken_branch_cost;  /* Cost of not taken branch.  */
 };
 
+/* Branch costs.  */
+struct cpu_branch_cost
+{
+  const int predictable;    /* Predictable branch or optimizing for size.  */
+  const int unpredictable;  /* Unpredictable branch or optimizing for speed.  */
+};
+
 struct tune_params
 {
-  const struct cpu_cost_table *const insn_extra_cost;
-  const struct cpu_addrcost_table *const addr_cost;
-  const struct cpu_regmove_cost *const regmove_cost;
-  const struct cpu_vector_cost *const vec_costs;
-  const int memmov_cost;
-  const int issue_rate;
-  const unsigned int fuseable_ops;
-  const int function_align;
-  const int jump_align;
-  const int loop_align;
-  const int int_reassoc_width;
-  const int fp_reassoc_width;
-  const int vec_reassoc_width;
+  const struct cpu_cost_table *insn_extra_cost;
+  const struct cpu_addrcost_table *addr_cost;
+  const struct cpu_regmove_cost *regmove_cost;
+  const struct cpu_vector_cost *vec_costs;
+  const struct cpu_branch_cost *branch_costs;
+  int memmov_cost;
+  int issue_rate;
+  unsigned int fusible_ops;
+  int function_align;
+  int jump_align;
+  int loop_align;
+  int int_reassoc_width;
+  int fp_reassoc_width;
+  int vec_reassoc_width;
+  int min_div_recip_mul_sf;
+  int min_div_recip_mul_df;
+  unsigned int extra_tuning_flags;
+};
+
+#define AARCH64_FUSION_PAIR(x, name, index) \
+  AARCH64_FUSE_##name = (1 << index),
+/* Supported fusion operations.  */
+enum aarch64_fusion_pairs
+{
+  AARCH64_FUSE_NOTHING = 0,
+#include "aarch64-fusion-pairs.def"
+
+/* Hacky macro to build AARCH64_FUSE_ALL.  The sequence below expands
+   to:
+   AARCH64_FUSE_ALL = 0 | AARCH64_FUSE_index1 | AARCH64_FUSE_index2 ...  */
+#undef AARCH64_FUSION_PAIR
+#define AARCH64_FUSION_PAIR(x, name, y) \
+  | AARCH64_FUSE_##name
+
+  AARCH64_FUSE_ALL = 0
+#include "aarch64-fusion-pairs.def"
 };
+#undef AARCH64_FUSION_PAIR
+
+#define AARCH64_EXTRA_TUNING_OPTION(x, name, index) \
+  AARCH64_EXTRA_TUNE_##name = (1 << index),
+/* Supported tuning flags.  */
+enum aarch64_extra_tuning_flags
+{
+  AARCH64_EXTRA_TUNE_NONE = 0,
+#include "aarch64-tuning-flags.def"
+
+/* Hacky macro to build the "all" flag mask.
+   Expands to 0 | AARCH64_TUNE_index0 | AARCH64_TUNE_index1 , etc.  */
+#undef AARCH64_EXTRA_TUNING_OPTION
+#define AARCH64_EXTRA_TUNING_OPTION(x, name, y) \
+  | AARCH64_EXTRA_TUNE_##name
+  AARCH64_EXTRA_TUNE_ALL = 0
+#include "aarch64-tuning-flags.def"
+};
+#undef AARCH64_EXTRA_TUNING_OPTION
+
+extern struct tune_params aarch64_tune_params;
 
 HOST_WIDE_INT aarch64_initial_elimination_offset (unsigned, unsigned);
 int aarch64_get_condition_code (rtx);
 bool aarch64_bitmask_imm (HOST_WIDE_INT val, machine_mode);
+int aarch64_branch_cost (bool, bool);
 enum aarch64_symbol_type
 aarch64_classify_symbolic_expression (rtx, enum aarch64_symbol_context);
 bool aarch64_const_vec_all_same_int_p (rtx, HOST_WIDE_INT);
@@ -248,6 +300,7 @@ unsigned aarch64_dbx_register_number (unsigned);
 unsigned aarch64_trampoline_size (void);
 void aarch64_asm_output_labelref (FILE *, const char *);
 void aarch64_elf_asm_named_section (const char *, unsigned, tree);
+void aarch64_err_no_fpadvsimd (machine_mode, const char *);
 void aarch64_expand_epilogue (bool);
 void aarch64_expand_mov_immediate (rtx, rtx);
 void aarch64_expand_prologue (void);
@@ -264,12 +317,6 @@ void init_aarch64_simd_builtins (void);
 
 void aarch64_simd_emit_reg_reg_move (rtx *, enum machine_mode, unsigned int);
 
-/* Emit code to place a AdvSIMD pair result in memory locations (with equal
-   registers).  */
-void aarch64_simd_emit_pair_result_insn (machine_mode,
-					 rtx (*intfn) (rtx, rtx, rtx), rtx,
-					 rtx);
-
 /* Expand builtins for SIMD intrinsics.  */
 rtx aarch64_simd_expand_builtin (int, tree, rtx);
 
@@ -295,6 +342,10 @@ rtx aarch64_load_tp (rtx);
 
 void aarch64_expand_compare_and_swap (rtx op[]);
 void aarch64_split_compare_and_swap (rtx op[]);
+void aarch64_gen_atomic_cas (rtx, rtx, rtx, rtx, rtx);
+
+bool aarch64_atomic_ldop_supported_p (enum rtx_code);
+void aarch64_gen_atomic_ldop (enum rtx_code, rtx, rtx, rtx, rtx, rtx);
 void aarch64_split_atomic_op (enum rtx_code, rtx, rtx, rtx, rtx, rtx, rtx);
 
 bool aarch64_gen_adjusted_ldpstp (rtx *, bool, enum machine_mode, RTX_CODE);
--- a/src/gcc/config/aarch64/aarch64-simd-builtins.def
+++ b/src/gcc/config/aarch64/aarch64-simd-builtins.def
@@ -88,9 +88,9 @@
   BUILTIN_VALLDIF (LOADSTRUCT, ld3r, 0)
   BUILTIN_VALLDIF (LOADSTRUCT, ld4r, 0)
   /* Implemented by aarch64_ld<VSTRUCT:nregs>_lane<VQ:mode>.  */
-  BUILTIN_VQ (LOADSTRUCT_LANE, ld2_lane, 0)
-  BUILTIN_VQ (LOADSTRUCT_LANE, ld3_lane, 0)
-  BUILTIN_VQ (LOADSTRUCT_LANE, ld4_lane, 0)
+  BUILTIN_VALLDIF (LOADSTRUCT_LANE, ld2_lane, 0)
+  BUILTIN_VALLDIF (LOADSTRUCT_LANE, ld3_lane, 0)
+  BUILTIN_VALLDIF (LOADSTRUCT_LANE, ld4_lane, 0)
   /* Implemented by aarch64_st<VSTRUCT:nregs><VDC:mode>.  */
   BUILTIN_VDC (STORESTRUCT, st2, 0)
   BUILTIN_VDC (STORESTRUCT, st3, 0)
@@ -100,9 +100,9 @@
   BUILTIN_VQ (STORESTRUCT, st3, 0)
   BUILTIN_VQ (STORESTRUCT, st4, 0)
 
-  BUILTIN_VQ (STORESTRUCT_LANE, st2_lane, 0)
-  BUILTIN_VQ (STORESTRUCT_LANE, st3_lane, 0)
-  BUILTIN_VQ (STORESTRUCT_LANE, st4_lane, 0)
+  BUILTIN_VALLDIF (STORESTRUCT_LANE, st2_lane, 0)
+  BUILTIN_VALLDIF (STORESTRUCT_LANE, st3_lane, 0)
+  BUILTIN_VALLDIF (STORESTRUCT_LANE, st4_lane, 0)
 
   BUILTIN_VQW (BINOP, saddl2, 0)
   BUILTIN_VQW (BINOP, uaddl2, 0)
--- a/src/gcc/config/aarch64/aarch64-simd.md
+++ b/src/gcc/config/aarch64/aarch64-simd.md
@@ -2057,13 +2057,13 @@
 })
 
 (define_expand "aarch64_vcond_internal<mode><mode>"
-  [(set (match_operand:VDQ_I 0 "register_operand")
-	(if_then_else:VDQ_I
+  [(set (match_operand:VSDQ_I_DI 0 "register_operand")
+	(if_then_else:VSDQ_I_DI
 	  (match_operator 3 "comparison_operator"
-	    [(match_operand:VDQ_I 4 "register_operand")
-	     (match_operand:VDQ_I 5 "nonmemory_operand")])
-	  (match_operand:VDQ_I 1 "nonmemory_operand")
-	  (match_operand:VDQ_I 2 "nonmemory_operand")))]
+	    [(match_operand:VSDQ_I_DI 4 "register_operand")
+	     (match_operand:VSDQ_I_DI 5 "nonmemory_operand")])
+	  (match_operand:VSDQ_I_DI 1 "nonmemory_operand")
+	  (match_operand:VSDQ_I_DI 2 "nonmemory_operand")))]
   "TARGET_SIMD"
 {
   rtx op1 = operands[1];
@@ -2365,13 +2365,13 @@
 })
 
 (define_expand "vcond<mode><mode>"
-  [(set (match_operand:VALL 0 "register_operand")
-	(if_then_else:VALL
+  [(set (match_operand:VALLDI 0 "register_operand")
+	(if_then_else:VALLDI
 	  (match_operator 3 "comparison_operator"
-	    [(match_operand:VALL 4 "register_operand")
-	     (match_operand:VALL 5 "nonmemory_operand")])
-	  (match_operand:VALL 1 "nonmemory_operand")
-	  (match_operand:VALL 2 "nonmemory_operand")))]
+	    [(match_operand:VALLDI 4 "register_operand")
+	     (match_operand:VALLDI 5 "nonmemory_operand")])
+	  (match_operand:VALLDI 1 "nonmemory_operand")
+	  (match_operand:VALLDI 2 "nonmemory_operand")))]
   "TARGET_SIMD"
 {
   emit_insn (gen_aarch64_vcond_internal<mode><mode> (operands[0], operands[1],
@@ -2398,13 +2398,13 @@
 })
 
 (define_expand "vcondu<mode><mode>"
-  [(set (match_operand:VDQ_I 0 "register_operand")
-	(if_then_else:VDQ_I
+  [(set (match_operand:VSDQ_I_DI 0 "register_operand")
+	(if_then_else:VSDQ_I_DI
 	  (match_operator 3 "comparison_operator"
-	    [(match_operand:VDQ_I 4 "register_operand")
-	     (match_operand:VDQ_I 5 "nonmemory_operand")])
-	  (match_operand:VDQ_I 1 "nonmemory_operand")
-	  (match_operand:VDQ_I 2 "nonmemory_operand")))]
+	    [(match_operand:VSDQ_I_DI 4 "register_operand")
+	     (match_operand:VSDQ_I_DI 5 "nonmemory_operand")])
+	  (match_operand:VSDQ_I_DI 1 "nonmemory_operand")
+	  (match_operand:VSDQ_I_DI 2 "nonmemory_operand")))]
   "TARGET_SIMD"
 {
   emit_insn (gen_aarch64_vcond_internal<mode><mode> (operands[0], operands[1],
@@ -3919,10 +3919,13 @@
 	(unspec:OI [(match_operand:<V_TWO_ELEM> 1 "aarch64_simd_struct_operand" "Utv")
 		    (match_operand:OI 2 "register_operand" "0")
 		    (match_operand:SI 3 "immediate_operand" "i")
-		    (unspec:VQ [(const_int 0)] UNSPEC_VSTRUCTDUMMY) ]
+		    (unspec:VALLDIF [(const_int 0)] UNSPEC_VSTRUCTDUMMY) ]
 		   UNSPEC_LD2_LANE))]
   "TARGET_SIMD"
-  "ld2\\t{%S0.<Vetype> - %T0.<Vetype>}[%3], %1"
+  {
+    operands[3] = GEN_INT (ENDIAN_LANE_N (<MODE>mode, INTVAL (operands[3])));
+    return "ld2\\t{%S0.<Vetype> - %T0.<Vetype>}[%3], %1";
+  }
   [(set_attr "type" "neon_load2_one_lane")]
 )
 
@@ -3955,15 +3958,19 @@
   [(set_attr "type" "neon_store2_2reg<q>")]
 )
 
+;; RTL uses GCC vector extension indices, so flip only for assembly.
 (define_insn "vec_store_lanesoi_lane<mode>"
   [(set (match_operand:<V_TWO_ELEM> 0 "aarch64_simd_struct_operand" "=Utv")
 	(unspec:<V_TWO_ELEM> [(match_operand:OI 1 "register_operand" "w")
-                    (unspec:VQ [(const_int 0)] UNSPEC_VSTRUCTDUMMY)
+		    (unspec:VALLDIF [(const_int 0)] UNSPEC_VSTRUCTDUMMY)
 		    (match_operand:SI 2 "immediate_operand" "i")]
-                   UNSPEC_ST2_LANE))]
+		   UNSPEC_ST2_LANE))]
   "TARGET_SIMD"
-  "st2\\t{%S1.<Vetype> - %T1.<Vetype>}[%2], %0"
-  [(set_attr "type" "neon_store3_one_lane<q>")]
+  {
+    operands[2] = GEN_INT (ENDIAN_LANE_N (<MODE>mode, INTVAL (operands[2])));
+    return "st2\\t{%S1.<Vetype> - %T1.<Vetype>}[%2], %0";
+  }
+  [(set_attr "type" "neon_store2_one_lane<q>")]
 )
 
 (define_expand "vec_store_lanesoi<mode>"
@@ -4010,10 +4017,13 @@
 	(unspec:CI [(match_operand:<V_THREE_ELEM> 1 "aarch64_simd_struct_operand" "Utv")
 		    (match_operand:CI 2 "register_operand" "0")
 		    (match_operand:SI 3 "immediate_operand" "i")
-		    (unspec:VQ [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]
+		    (unspec:VALLDIF [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]
 		   UNSPEC_LD3_LANE))]
   "TARGET_SIMD"
-  "ld3\\t{%S0.<Vetype> - %U0.<Vetype>}[%3], %1"
+{
+    operands[3] = GEN_INT (ENDIAN_LANE_N (<MODE>mode, INTVAL (operands[3])));
+    return "ld3\\t{%S0.<Vetype> - %U0.<Vetype>}[%3], %1";
+}
   [(set_attr "type" "neon_load3_one_lane")]
 )
 
@@ -4046,14 +4056,18 @@
   [(set_attr "type" "neon_store3_3reg<q>")]
 )
 
+;; RTL uses GCC vector extension indices, so flip only for assembly.
 (define_insn "vec_store_lanesci_lane<mode>"
   [(set (match_operand:<V_THREE_ELEM> 0 "aarch64_simd_struct_operand" "=Utv")
 	(unspec:<V_THREE_ELEM> [(match_operand:CI 1 "register_operand" "w")
-                    (unspec:VQ [(const_int 0)] UNSPEC_VSTRUCTDUMMY)
+		    (unspec:VALLDIF [(const_int 0)] UNSPEC_VSTRUCTDUMMY)
 		    (match_operand:SI 2 "immediate_operand" "i")]
-                   UNSPEC_ST3_LANE))]
+		   UNSPEC_ST3_LANE))]
   "TARGET_SIMD"
-  "st3\\t{%S1.<Vetype> - %U1.<Vetype>}[%2], %0"
+  {
+    operands[2] = GEN_INT (ENDIAN_LANE_N (<MODE>mode, INTVAL (operands[2])));
+    return "st3\\t{%S1.<Vetype> - %U1.<Vetype>}[%2], %0";
+  }
   [(set_attr "type" "neon_store3_one_lane<q>")]
 )
 
@@ -4101,10 +4115,13 @@
 	(unspec:XI [(match_operand:<V_FOUR_ELEM> 1 "aarch64_simd_struct_operand" "Utv")
 		    (match_operand:XI 2 "register_operand" "0")
 		    (match_operand:SI 3 "immediate_operand" "i")
-		    (unspec:VQ [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]
+		    (unspec:VALLDIF [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]
 		   UNSPEC_LD4_LANE))]
   "TARGET_SIMD"
-  "ld4\\t{%S0.<Vetype> - %V0.<Vetype>}[%3], %1"
+{
+    operands[3] = GEN_INT (ENDIAN_LANE_N (<MODE>mode, INTVAL (operands[3])));
+    return "ld4\\t{%S0.<Vetype> - %V0.<Vetype>}[%3], %1";
+}
   [(set_attr "type" "neon_load4_one_lane")]
 )
 
@@ -4137,14 +4154,18 @@
   [(set_attr "type" "neon_store4_4reg<q>")]
 )
 
+;; RTL uses GCC vector extension indices, so flip only for assembly.
 (define_insn "vec_store_lanesxi_lane<mode>"
   [(set (match_operand:<V_FOUR_ELEM> 0 "aarch64_simd_struct_operand" "=Utv")
 	(unspec:<V_FOUR_ELEM> [(match_operand:XI 1 "register_operand" "w")
-                    (unspec:VQ [(const_int 0)] UNSPEC_VSTRUCTDUMMY)
+		    (unspec:VALLDIF [(const_int 0)] UNSPEC_VSTRUCTDUMMY)
 		    (match_operand:SI 2 "immediate_operand" "i")]
-                   UNSPEC_ST4_LANE))]
+		   UNSPEC_ST4_LANE))]
   "TARGET_SIMD"
-  "st4\\t{%S1.<Vetype> - %V1.<Vetype>}[%2], %0"
+  {
+    operands[2] = GEN_INT (ENDIAN_LANE_N (<MODE>mode, INTVAL (operands[2])));
+    return "st4\\t{%S1.<Vetype> - %V1.<Vetype>}[%2], %0";
+  }
   [(set_attr "type" "neon_store4_one_lane<q>")]
 )
 
@@ -4554,14 +4575,12 @@
 	(match_operand:DI 1 "register_operand" "w")
 	(match_operand:OI 2 "register_operand" "0")
 	(match_operand:SI 3 "immediate_operand" "i")
-	(unspec:VQ [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]
+	(unspec:VALLDIF [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]
   "TARGET_SIMD"
 {
   machine_mode mode = <V_TWO_ELEM>mode;
   rtx mem = gen_rtx_MEM (mode, operands[1]);
 
-  aarch64_simd_lane_bounds (operands[3], 0, GET_MODE_NUNITS (<VCONQ>mode),
-			    NULL);
   emit_insn (gen_aarch64_vec_load_lanesoi_lane<mode> (operands[0],
 						      mem,
 						      operands[2],
@@ -4574,14 +4593,12 @@
 	(match_operand:DI 1 "register_operand" "w")
 	(match_operand:CI 2 "register_operand" "0")
 	(match_operand:SI 3 "immediate_operand" "i")
-	(unspec:VQ [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]
+	(unspec:VALLDIF [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]
   "TARGET_SIMD"
 {
   machine_mode mode = <V_THREE_ELEM>mode;
   rtx mem = gen_rtx_MEM (mode, operands[1]);
 
-  aarch64_simd_lane_bounds (operands[3], 0, GET_MODE_NUNITS (<VCONQ>mode),
-			    NULL);
   emit_insn (gen_aarch64_vec_load_lanesci_lane<mode> (operands[0],
 						      mem,
 						      operands[2],
@@ -4594,14 +4611,12 @@
 	(match_operand:DI 1 "register_operand" "w")
 	(match_operand:XI 2 "register_operand" "0")
 	(match_operand:SI 3 "immediate_operand" "i")
-	(unspec:VQ [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]
+	(unspec:VALLDIF [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]
   "TARGET_SIMD"
 {
   machine_mode mode = <V_FOUR_ELEM>mode;
   rtx mem = gen_rtx_MEM (mode, operands[1]);
 
-  aarch64_simd_lane_bounds (operands[3], 0, GET_MODE_NUNITS (<VCONQ>mode),
-			    NULL);
   emit_insn (gen_aarch64_vec_load_lanesxi_lane<mode> (operands[0],
 						      mem,
 						      operands[2],
@@ -4838,54 +4853,45 @@
   DONE;
 })
 
-(define_expand "aarch64_st2_lane<VQ:mode>"
+(define_expand "aarch64_st2_lane<mode>"
  [(match_operand:DI 0 "register_operand" "r")
   (match_operand:OI 1 "register_operand" "w")
-  (unspec:VQ [(const_int 0)] UNSPEC_VSTRUCTDUMMY)
+  (unspec:VALLDIF [(const_int 0)] UNSPEC_VSTRUCTDUMMY)
   (match_operand:SI 2 "immediate_operand")]
   "TARGET_SIMD"
 {
   machine_mode mode = <V_TWO_ELEM>mode;
   rtx mem = gen_rtx_MEM (mode, operands[0]);
-  operands[2] = GEN_INT (ENDIAN_LANE_N (<MODE>mode, INTVAL (operands[2])));
 
-  emit_insn (gen_vec_store_lanesoi_lane<VQ:mode> (mem,
-						  operands[1],
-						  operands[2]));
+  emit_insn (gen_vec_store_lanesoi_lane<mode> (mem, operands[1], operands[2]));
   DONE;
 })
 
-(define_expand "aarch64_st3_lane<VQ:mode>"
+(define_expand "aarch64_st3_lane<mode>"
  [(match_operand:DI 0 "register_operand" "r")
   (match_operand:CI 1 "register_operand" "w")
-  (unspec:VQ [(const_int 0)] UNSPEC_VSTRUCTDUMMY)
+  (unspec:VALLDIF [(const_int 0)] UNSPEC_VSTRUCTDUMMY)
   (match_operand:SI 2 "immediate_operand")]
   "TARGET_SIMD"
 {
   machine_mode mode = <V_THREE_ELEM>mode;
   rtx mem = gen_rtx_MEM (mode, operands[0]);
-  operands[2] = GEN_INT (ENDIAN_LANE_N (<MODE>mode, INTVAL (operands[2])));
 
-  emit_insn (gen_vec_store_lanesci_lane<VQ:mode> (mem,
-						  operands[1],
-						  operands[2]));
+  emit_insn (gen_vec_store_lanesci_lane<mode> (mem, operands[1], operands[2]));
   DONE;
 })
 
-(define_expand "aarch64_st4_lane<VQ:mode>"
+(define_expand "aarch64_st4_lane<mode>"
  [(match_operand:DI 0 "register_operand" "r")
   (match_operand:XI 1 "register_operand" "w")
-  (unspec:VQ [(const_int 0)] UNSPEC_VSTRUCTDUMMY)
+  (unspec:VALLDIF [(const_int 0)] UNSPEC_VSTRUCTDUMMY)
   (match_operand:SI 2 "immediate_operand")]
   "TARGET_SIMD"
 {
   machine_mode mode = <V_FOUR_ELEM>mode;
   rtx mem = gen_rtx_MEM (mode, operands[0]);
-  operands[2] = GEN_INT (ENDIAN_LANE_N (<MODE>mode, INTVAL (operands[2])));
 
-  emit_insn (gen_vec_store_lanesxi_lane<VQ:mode> (mem,
-						  operands[1],
-						  operands[2]));
+  emit_insn (gen_vec_store_lanesxi_lane<mode> (mem, operands[1], operands[2]));
   DONE;
 })
 
--- a/src//dev/null
+++ b/src/gcc/config/aarch64/aarch64-tuning-flags.def
@@ -0,0 +1,34 @@
+/* Copyright (C) 2015 Free Software Foundation, Inc.
+   Contributed by ARM Ltd.
+
+   This file is part of GCC.
+
+   GCC is free software; you can redistribute it and/or modify it
+   under the terms of the GNU General Public License as published
+   by the Free Software Foundation; either version 3, or (at your
+   option) any later version.
+
+   GCC is distributed in the hope that it will be useful, but WITHOUT
+   ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
+   or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public
+   License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with GCC; see the file COPYING3.  If not see
+   <http://www.gnu.org/licenses/>.  */
+
+/* Additional control over certain tuning parameters.  Before including
+   this file, define a macro:
+
+     AARCH64_EXTRA_TUNING_OPTION (name, internal_name, index_bit)
+
+   Where:
+
+     NAME is a string giving a friendly name for the tuning flag.
+     INTERNAL_NAME gives the internal name suitable for appending to
+     AARCH64_TUNE_ to give an enum name.
+     INDEX_BIT is the bit to set in the bitmask of supported tuning
+     flags.  */
+
+AARCH64_EXTRA_TUNING_OPTION ("rename_fma_regs", RENAME_FMA_REGS, 0)
+
--- a/src/gcc/config/aarch64/aarch64.c
+++ b/src/gcc/config/aarch64/aarch64.c
@@ -95,6 +95,7 @@
 #include "rtl-iter.h"
 #include "tm-constrs.h"
 #include "sched-int.h"
+#include "cortex-a57-fma-steering.h"
 
 /* Defined for convenience.  */
 #define POINTER_BYTES (POINTER_SIZE / BITS_PER_UNIT)
@@ -177,15 +178,42 @@ unsigned aarch64_architecture_version;
 /* The processor for which instructions should be scheduled.  */
 enum aarch64_processor aarch64_tune = cortexa53;
 
-/* The current tuning set.  */
-const struct tune_params *aarch64_tune_params;
-
 /* Mask to specify which instructions we are allowed to generate.  */
 unsigned long aarch64_isa_flags = 0;
 
 /* Mask to specify which instruction scheduling options should be used.  */
 unsigned long aarch64_tune_flags = 0;
 
+/* Support for command line parsing of boolean flags in the tuning
+   structures.  */
+struct aarch64_flag_desc
+{
+  const char* name;
+  unsigned int flag;
+};
+
+#define AARCH64_FUSION_PAIR(name, internal_name, y) \
+  { name, AARCH64_FUSE_##internal_name },
+static const struct aarch64_flag_desc aarch64_fusible_pairs[] =
+{
+  { "none", AARCH64_FUSE_NOTHING },
+#include "aarch64-fusion-pairs.def"
+  { "all", AARCH64_FUSE_ALL },
+  { NULL, AARCH64_FUSE_NOTHING }
+};
+#undef AARCH64_FUION_PAIR
+
+#define AARCH64_EXTRA_TUNING_OPTION(name, internal_name, y) \
+  { name, AARCH64_EXTRA_TUNE_##internal_name },
+static const struct aarch64_flag_desc aarch64_tuning_flags[] =
+{
+  { "none", AARCH64_EXTRA_TUNE_NONE },
+#include "aarch64-tuning-flags.def"
+  { "all", AARCH64_EXTRA_TUNE_ALL },
+  { NULL, AARCH64_EXTRA_TUNE_NONE }
+};
+#undef AARCH64_EXTRA_TUNING_OPTION
+
 /* Tuning parameters.  */
 
 static const struct cpu_addrcost_table generic_addrcost_table =
@@ -332,12 +360,12 @@ static const struct cpu_vector_cost xgene1_vector_cost =
   1 /* cond_not_taken_branch_cost  */
 };
 
-#define AARCH64_FUSE_NOTHING	(0)
-#define AARCH64_FUSE_MOV_MOVK	(1 << 0)
-#define AARCH64_FUSE_ADRP_ADD	(1 << 1)
-#define AARCH64_FUSE_MOVK_MOVK	(1 << 2)
-#define AARCH64_FUSE_ADRP_LDR	(1 << 3)
-#define AARCH64_FUSE_CMP_BRANCH	(1 << 4)
+/* Generic costs for branch instructions.  */
+static const struct cpu_branch_cost generic_branch_cost =
+{
+  2,  /* Predictable.  */
+  2   /* Unpredictable.  */
+};
 
 static const struct tune_params generic_tunings =
 {
@@ -345,15 +373,19 @@ static const struct tune_params generic_tunings =
   &generic_addrcost_table,
   &generic_regmove_cost,
   &generic_vector_cost,
+  &generic_branch_cost,
   4, /* memmov_cost  */
   2, /* issue_rate  */
-  AARCH64_FUSE_NOTHING, /* fuseable_ops  */
+  AARCH64_FUSE_NOTHING, /* fusible_ops  */
   8,	/* function_align.  */
   8,	/* jump_align.  */
   4,	/* loop_align.  */
   2,	/* int_reassoc_width.  */
   4,	/* fp_reassoc_width.  */
-  1	/* vec_reassoc_width.  */
+  1,	/* vec_reassoc_width.  */
+  2,	/* min_div_recip_mul_sf.  */
+  2,	/* min_div_recip_mul_df.  */
+  (AARCH64_EXTRA_TUNE_NONE)	/* tune_flags.  */
 };
 
 static const struct tune_params cortexa53_tunings =
@@ -362,16 +394,20 @@ static const struct tune_params cortexa53_tunings =
   &generic_addrcost_table,
   &cortexa53_regmove_cost,
   &generic_vector_cost,
+  &generic_branch_cost,
   4, /* memmov_cost  */
   2, /* issue_rate  */
   (AARCH64_FUSE_MOV_MOVK | AARCH64_FUSE_ADRP_ADD
-   | AARCH64_FUSE_MOVK_MOVK | AARCH64_FUSE_ADRP_LDR), /* fuseable_ops  */
+   | AARCH64_FUSE_MOVK_MOVK | AARCH64_FUSE_ADRP_LDR), /* fusible_ops  */
   8,	/* function_align.  */
   8,	/* jump_align.  */
   4,	/* loop_align.  */
   2,	/* int_reassoc_width.  */
   4,	/* fp_reassoc_width.  */
-  1	/* vec_reassoc_width.  */
+  1,	/* vec_reassoc_width.  */
+  2,	/* min_div_recip_mul_sf.  */
+  2,	/* min_div_recip_mul_df.  */
+  (AARCH64_EXTRA_TUNE_NONE)	/* tune_flags.  */
 };
 
 static const struct tune_params cortexa57_tunings =
@@ -380,16 +416,42 @@ static const struct tune_params cortexa57_tunings =
   &cortexa57_addrcost_table,
   &cortexa57_regmove_cost,
   &cortexa57_vector_cost,
+  &generic_branch_cost,
+  4, /* memmov_cost  */
+  3, /* issue_rate  */
+  (AARCH64_FUSE_MOV_MOVK | AARCH64_FUSE_ADRP_ADD
+   | AARCH64_FUSE_MOVK_MOVK), /* fusible_ops  */
+  16,	/* function_align.  */
+  8,	/* jump_align.  */
+  4,	/* loop_align.  */
+  2,	/* int_reassoc_width.  */
+  4,	/* fp_reassoc_width.  */
+  1,	/* vec_reassoc_width.  */
+  2,	/* min_div_recip_mul_sf.  */
+  2,	/* min_div_recip_mul_df.  */
+  (AARCH64_EXTRA_TUNE_RENAME_FMA_REGS)	/* tune_flags.  */
+};
+
+static const struct tune_params cortexa72_tunings =
+{
+  &cortexa57_extra_costs,
+  &cortexa57_addrcost_table,
+  &cortexa57_regmove_cost,
+  &cortexa57_vector_cost,
+  &generic_branch_cost,
   4, /* memmov_cost  */
   3, /* issue_rate  */
   (AARCH64_FUSE_MOV_MOVK | AARCH64_FUSE_ADRP_ADD
-   | AARCH64_FUSE_MOVK_MOVK), /* fuseable_ops  */
+   | AARCH64_FUSE_MOVK_MOVK), /* fusible_ops  */
   16,	/* function_align.  */
   8,	/* jump_align.  */
   4,	/* loop_align.  */
   2,	/* int_reassoc_width.  */
   4,	/* fp_reassoc_width.  */
-  1	/* vec_reassoc_width.  */
+  1,	/* vec_reassoc_width.  */
+  2,	/* min_div_recip_mul_sf.  */
+  2,	/* min_div_recip_mul_df.  */
+  (AARCH64_EXTRA_TUNE_NONE)	/* tune_flags.  */
 };
 
 static const struct tune_params thunderx_tunings =
@@ -398,15 +460,19 @@ static const struct tune_params thunderx_tunings =
   &generic_addrcost_table,
   &thunderx_regmove_cost,
   &generic_vector_cost,
+  &generic_branch_cost,
   6, /* memmov_cost  */
   2, /* issue_rate  */
-  AARCH64_FUSE_CMP_BRANCH, /* fuseable_ops  */
+  AARCH64_FUSE_CMP_BRANCH, /* fusible_ops  */
   8,	/* function_align.  */
   8,	/* jump_align.  */
   8,	/* loop_align.  */
   2,	/* int_reassoc_width.  */
   4,	/* fp_reassoc_width.  */
-  1	/* vec_reassoc_width.  */
+  1,	/* vec_reassoc_width.  */
+  2,	/* min_div_recip_mul_sf.  */
+  2,	/* min_div_recip_mul_df.  */
+  (AARCH64_EXTRA_TUNE_NONE)	/* tune_flags.  */
 };
 
 static const struct tune_params xgene1_tunings =
@@ -415,15 +481,37 @@ static const struct tune_params xgene1_tunings =
   &xgene1_addrcost_table,
   &xgene1_regmove_cost,
   &xgene1_vector_cost,
+  &generic_branch_cost,
   6, /* memmov_cost  */
   4, /* issue_rate  */
-  AARCH64_FUSE_NOTHING, /* fuseable_ops  */
+  AARCH64_FUSE_NOTHING, /* fusible_ops  */
   16,	/* function_align.  */
   8,	/* jump_align.  */
   16,	/* loop_align.  */
   2,	/* int_reassoc_width.  */
   4,	/* fp_reassoc_width.  */
-  1	/* vec_reassoc_width.  */
+  1,	/* vec_reassoc_width.  */
+  2,	/* min_div_recip_mul_sf.  */
+  2,	/* min_div_recip_mul_df.  */
+  (AARCH64_EXTRA_TUNE_NONE)	/* tune_flags.  */
+};
+
+/* Support for fine-grained override of the tuning structures.  */
+struct aarch64_tuning_override_function
+{
+  const char* name;
+  void (*parse_override)(const char*, struct tune_params*);
+};
+
+static void aarch64_parse_fuse_string (const char*, struct tune_params*);
+static void aarch64_parse_tune_string (const char*, struct tune_params*);
+
+static const struct aarch64_tuning_override_function
+aarch64_tuning_override_functions[] =
+{
+  { "fuse", aarch64_parse_fuse_string },
+  { "tune", aarch64_parse_tune_string },
+  { NULL, NULL }
 };
 
 /* A processor implementing AArch64.  */
@@ -440,7 +528,7 @@ struct processor
 /* Processor cores implementing AArch64.  */
 static const struct processor all_cores[] =
 {
-#define AARCH64_CORE(NAME, IDENT, SCHED, ARCH, FLAGS, COSTS) \
+#define AARCH64_CORE(NAME, IDENT, SCHED, ARCH, FLAGS, COSTS, IMP, PART) \
   {NAME, SCHED, #ARCH, ARCH, FLAGS, &COSTS##_tunings},
 #include "aarch64-cores.def"
 #undef AARCH64_CORE
@@ -464,6 +552,9 @@ static const struct processor *selected_arch;
 static const struct processor *selected_cpu;
 static const struct processor *selected_tune;
 
+/* The current tuning set.  */
+struct tune_params aarch64_tune_params = generic_tunings;
+
 #define AARCH64_CPU_DEFAULT_FLAGS ((selected_cpu) ? selected_cpu->flags : 0)
 
 /* An ISA extension in the co-processor and main instruction set space.  */
@@ -477,7 +568,7 @@ struct aarch64_option_extension
 /* ISA extensions in AArch64.  */
 static const struct aarch64_option_extension all_extensions[] =
 {
-#define AARCH64_OPT_EXTENSION(NAME, FLAGS_ON, FLAGS_OFF) \
+#define AARCH64_OPT_EXTENSION(NAME, FLAGS_ON, FLAGS_OFF, FEATURE_STRING) \
   {NAME, FLAGS_ON, FLAGS_OFF},
 #include "aarch64-option-extensions.def"
 #undef AARCH64_OPT_EXTENSION
@@ -511,10 +602,22 @@ static const char * const aarch64_condition_codes[] =
   "hi", "ls", "ge", "lt", "gt", "le", "al", "nv"
 };
 
+void
+aarch64_err_no_fpadvsimd (machine_mode mode, const char *msg)
+{
+  const char *mc = FLOAT_MODE_P (mode) ? "floating-point" : "vector";
+  if (TARGET_GENERAL_REGS_ONLY)
+    error ("%qs is incompatible with %s %s", "-mgeneral-regs-only", mc, msg);
+  else
+    error ("%qs feature modifier is incompatible with %s %s", "+nofp", mc, msg);
+}
+
 static unsigned int
-aarch64_min_divisions_for_recip_mul (enum machine_mode mode ATTRIBUTE_UNUSED)
+aarch64_min_divisions_for_recip_mul (enum machine_mode mode)
 {
-  return 2;
+  if (GET_MODE_UNIT_SIZE (mode) == 4)
+    return aarch64_tune_params.min_div_recip_mul_sf;
+  return aarch64_tune_params.min_div_recip_mul_df;
 }
 
 static int
@@ -522,11 +625,11 @@ aarch64_reassociation_width (unsigned opc ATTRIBUTE_UNUSED,
 			     enum machine_mode mode)
 {
   if (VECTOR_MODE_P (mode))
-    return aarch64_tune_params->vec_reassoc_width;
+    return aarch64_tune_params.vec_reassoc_width;
   if (INTEGRAL_MODE_P (mode))
-    return aarch64_tune_params->int_reassoc_width;
+    return aarch64_tune_params.int_reassoc_width;
   if (FLOAT_MODE_P (mode))
-    return aarch64_tune_params->fp_reassoc_width;
+    return aarch64_tune_params.fp_reassoc_width;
   return 1;
 }
 
@@ -1763,6 +1866,9 @@ aarch64_layout_arg (cumulative_args_t pcum_v, machine_mode mode,
      and homogenous short-vector aggregates (HVA).  */
   if (allocate_nvrn)
     {
+      if (!TARGET_FLOAT)
+	aarch64_err_no_fpadvsimd (mode, "argument");
+
       if (nvrn + nregs <= NUM_FP_ARG_REGS)
 	{
 	  pcum->aapcs_nextnvrn = nvrn + nregs;
@@ -1889,6 +1995,17 @@ aarch64_init_cumulative_args (CUMULATIVE_ARGS *pcum,
   pcum->aapcs_stack_words = 0;
   pcum->aapcs_stack_size = 0;
 
+  if (!TARGET_FLOAT
+      && fndecl && TREE_PUBLIC (fndecl)
+      && fntype && fntype != error_mark_node)
+    {
+      const_tree type = TREE_TYPE (fntype);
+      machine_mode mode ATTRIBUTE_UNUSED; /* To pass pointer as argument.  */
+      int nregs ATTRIBUTE_UNUSED; /* Likewise.  */
+      if (aarch64_vfp_is_call_or_return_candidate (TYPE_MODE (type), type,
+						   &mode, &nregs, NULL))
+	aarch64_err_no_fpadvsimd (TYPE_MODE (type), "return type");
+    }
   return;
 }
 
@@ -4901,8 +5018,9 @@ aarch64_class_max_nregs (reg_class_t regclass, machine_mode mode)
     case FP_REGS:
     case FP_LO_REGS:
       return
-	aarch64_vector_mode_p (mode) ? (GET_MODE_SIZE (mode) + 15) / 16 :
-				       (GET_MODE_SIZE (mode) + 7) / 8;
+	aarch64_vector_mode_p (mode)
+	  ? (GET_MODE_SIZE (mode) + UNITS_PER_VREG - 1) / UNITS_PER_VREG
+	  : (GET_MODE_SIZE (mode) + UNITS_PER_WORD - 1) / UNITS_PER_WORD;
     case STACK_REG:
       return 1;
 
@@ -5157,9 +5275,18 @@ aarch64_strip_extend (rtx x)
   return x;
 }
 
+/* Return true iff CODE is a shift supported in combination
+   with arithmetic instructions.  */
+
+static bool
+aarch64_shift_p (enum rtx_code code)
+{
+  return code == ASHIFT || code == ASHIFTRT || code == LSHIFTRT;
+}
+
 /* Helper function for rtx cost calculation.  Calculate the cost of
-   a MULT, which may be part of a multiply-accumulate rtx.  Return
-   the calculated cost of the expression, recursing manually in to
+   a MULT or ASHIFT, which may be part of a compound PLUS/MINUS rtx.
+   Return the calculated cost of the expression, recursing manually in to
    operands where needed.  */
 
 static int
@@ -5167,9 +5294,9 @@ aarch64_rtx_mult_cost (rtx x, int code, int outer, bool speed)
 {
   rtx op0, op1;
   const struct cpu_cost_table *extra_cost
-    = aarch64_tune_params->insn_extra_cost;
+    = aarch64_tune_params.insn_extra_cost;
   int cost = 0;
-  bool maybe_fma = (outer == PLUS || outer == MINUS);
+  bool compound_p = (outer == PLUS || outer == MINUS);
   machine_mode mode = GET_MODE (x);
 
   gcc_checking_assert (code == MULT);
@@ -5184,24 +5311,50 @@ aarch64_rtx_mult_cost (rtx x, int code, int outer, bool speed)
   if (GET_MODE_CLASS (mode) == MODE_INT)
     {
       /* The multiply will be canonicalized as a shift, cost it as such.  */
-      if (CONST_INT_P (op1)
-	  && exact_log2 (INTVAL (op1)) > 0)
+      if (aarch64_shift_p (GET_CODE (x))
+	  || (CONST_INT_P (op1)
+	      && exact_log2 (INTVAL (op1)) > 0))
 	{
+	  bool is_extend = GET_CODE (op0) == ZERO_EXTEND
+	                   || GET_CODE (op0) == SIGN_EXTEND;
 	  if (speed)
 	    {
-	      if (maybe_fma)
-		/* ADD (shifted register).  */
-		cost += extra_cost->alu.arith_shift;
+	      if (compound_p)
+	        {
+	          if (REG_P (op1))
+		    /* ARITH + shift-by-register.  */
+		    cost += extra_cost->alu.arith_shift_reg;
+		  else if (is_extend)
+		    /* ARITH + extended register.  We don't have a cost field
+		       for ARITH+EXTEND+SHIFT, so use extend_arith here.  */
+		    cost += extra_cost->alu.extend_arith;
+		  else
+		    /* ARITH + shift-by-immediate.  */
+		    cost += extra_cost->alu.arith_shift;
+		}
 	      else
 		/* LSL (immediate).  */
-		cost += extra_cost->alu.shift;
+	        cost += extra_cost->alu.shift;
+
 	    }
+	  /* Strip extends as we will have costed them in the case above.  */
+	  if (is_extend)
+	    op0 = aarch64_strip_extend (op0);
 
 	  cost += rtx_cost (op0, GET_CODE (op0), 0, speed);
 
 	  return cost;
 	}
 
+      /* MNEG or [US]MNEGL.  Extract the NEG operand and indicate that it's a
+	 compound and let the below cases handle it.  After all, MNEG is a
+	 special-case alias of MSUB.  */
+      if (GET_CODE (op0) == NEG)
+	{
+	  op0 = XEXP (op0, 0);
+	  compound_p = true;
+	}
+
       /* Integer multiplies or FMAs have zero/sign extending variants.  */
       if ((GET_CODE (op0) == ZERO_EXTEND
 	   && GET_CODE (op1) == ZERO_EXTEND)
@@ -5213,8 +5366,8 @@ aarch64_rtx_mult_cost (rtx x, int code, int outer, bool speed)
 
 	  if (speed)
 	    {
-	      if (maybe_fma)
-		/* MADD/SMADDL/UMADDL.  */
+	      if (compound_p)
+		/* SMADDL/UMADDL/UMSUBL/SMSUBL.  */
 		cost += extra_cost->mult[0].extend_add;
 	      else
 		/* MUL/SMULL/UMULL.  */
@@ -5224,15 +5377,15 @@ aarch64_rtx_mult_cost (rtx x, int code, int outer, bool speed)
 	  return cost;
 	}
 
-      /* This is either an integer multiply or an FMA.  In both cases
+      /* This is either an integer multiply or a MADD.  In both cases
 	 we want to recurse and cost the operands.  */
       cost += rtx_cost (op0, MULT, 0, speed)
 	      + rtx_cost (op1, MULT, 1, speed);
 
       if (speed)
 	{
-	  if (maybe_fma)
-	    /* MADD.  */
+	  if (compound_p)
+	    /* MADD/MSUB.  */
 	    cost += extra_cost->mult[mode == DImode].add;
 	  else
 	    /* MUL.  */
@@ -5250,7 +5403,7 @@ aarch64_rtx_mult_cost (rtx x, int code, int outer, bool speed)
 	     which case FNMUL is different than FMUL with operand negation.  */
 	  bool neg0 = GET_CODE (op0) == NEG;
 	  bool neg1 = GET_CODE (op1) == NEG;
-	  if (maybe_fma || !flag_rounding_math || (neg0 && neg1))
+	  if (compound_p || !flag_rounding_math || (neg0 && neg1))
 	    {
 	      if (neg0)
 		op0 = XEXP (op0, 0);
@@ -5258,7 +5411,7 @@ aarch64_rtx_mult_cost (rtx x, int code, int outer, bool speed)
 		op1 = XEXP (op1, 0);
 	    }
 
-	  if (maybe_fma)
+	  if (compound_p)
 	    /* FMADD/FNMADD/FNMSUB/FMSUB.  */
 	    cost += extra_cost->fp[mode == DFmode].fma;
 	  else
@@ -5279,7 +5432,7 @@ aarch64_address_cost (rtx x,
 		      bool speed)
 {
   enum rtx_code c = GET_CODE (x);
-  const struct cpu_addrcost_table *addr_cost = aarch64_tune_params->addr_cost;
+  const struct cpu_addrcost_table *addr_cost = aarch64_tune_params.addr_cost;
   struct aarch64_address_info info;
   int cost = 0;
   info.shift = 0;
@@ -5367,6 +5520,23 @@ aarch64_address_cost (rtx x,
   return cost;
 }
 
+/* Return the cost of a branch.  If SPEED_P is true then the compiler is
+   optimizing for speed.  If PREDICTABLE_P is true then the branch is predicted
+   to be taken.  */
+
+int
+aarch64_branch_cost (bool speed_p, bool predictable_p)
+{
+  /* When optimizing for speed, use the cost of unpredictable branches.  */
+  const struct cpu_branch_cost *branch_costs =
+    aarch64_tune_params.branch_costs;
+
+  if (!speed_p || predictable_p)
+    return branch_costs->predictable;
+  else
+    return branch_costs->unpredictable;
+}
+
 /* Return true if the RTX X in mode MODE is a zero or sign extract
    usable in an ADD or SUB (extended register) instruction.  */
 static bool
@@ -5415,6 +5585,51 @@ aarch64_frint_unspec_p (unsigned int u)
     }
 }
 
+/* Return true iff X is an rtx that will match an extr instruction
+   i.e. as described in the *extr<mode>5_insn family of patterns.
+   OP0 and OP1 will be set to the operands of the shifts involved
+   on success and will be NULL_RTX otherwise.  */
+
+static bool
+aarch64_extr_rtx_p (rtx x, rtx *res_op0, rtx *res_op1)
+{
+  rtx op0, op1;
+  machine_mode mode = GET_MODE (x);
+
+  *res_op0 = NULL_RTX;
+  *res_op1 = NULL_RTX;
+
+  if (GET_CODE (x) != IOR)
+    return false;
+
+  op0 = XEXP (x, 0);
+  op1 = XEXP (x, 1);
+
+  if ((GET_CODE (op0) == ASHIFT && GET_CODE (op1) == LSHIFTRT)
+      || (GET_CODE (op1) == ASHIFT && GET_CODE (op0) == LSHIFTRT))
+    {
+     /* Canonicalise locally to ashift in op0, lshiftrt in op1.  */
+      if (GET_CODE (op1) == ASHIFT)
+        std::swap (op0, op1);
+
+      if (!CONST_INT_P (XEXP (op0, 1)) || !CONST_INT_P (XEXP (op1, 1)))
+        return false;
+
+      unsigned HOST_WIDE_INT shft_amnt_0 = UINTVAL (XEXP (op0, 1));
+      unsigned HOST_WIDE_INT shft_amnt_1 = UINTVAL (XEXP (op1, 1));
+
+      if (shft_amnt_0 < GET_MODE_BITSIZE (mode)
+          && shft_amnt_0 + shft_amnt_1 == GET_MODE_BITSIZE (mode))
+        {
+          *res_op0 = XEXP (op0, 0);
+          *res_op1 = XEXP (op1, 0);
+          return true;
+        }
+    }
+
+  return false;
+}
+
 /* Calculate the cost of calculating (if_then_else (OP0) (OP1) (OP2)),
    storing it in *COST.  Result is true if the total cost of the operation
    has now been calculated.  */
@@ -5497,7 +5712,7 @@ aarch64_rtx_costs (rtx x, int code, int outer ATTRIBUTE_UNUSED,
 {
   rtx op0, op1, op2;
   const struct cpu_cost_table *extra_cost
-    = aarch64_tune_params->insn_extra_cost;
+    = aarch64_tune_params.insn_extra_cost;
   machine_mode mode = GET_MODE (x);
 
   /* By default, assume that everything has equivalent cost to the
@@ -5505,16 +5720,6 @@ aarch64_rtx_costs (rtx x, int code, int outer ATTRIBUTE_UNUSED,
      above this default.  */
   *cost = COSTS_N_INSNS (1);
 
-  /* TODO: The cost infrastructure currently does not handle
-     vector operations.  Assume that all vector operations
-     are equally expensive.  */
-  if (VECTOR_MODE_P (mode))
-    {
-      if (speed)
-	*cost += extra_cost->vect.alu;
-      return true;
-    }
-
   switch (code)
     {
     case SET:
@@ -5529,7 +5734,9 @@ aarch64_rtx_costs (rtx x, int code, int outer ATTRIBUTE_UNUSED,
 	  if (speed)
 	    {
 	      rtx address = XEXP (op0, 0);
-	      if (GET_MODE_CLASS (mode) == MODE_INT)
+	      if (VECTOR_MODE_P (mode))
+		*cost += extra_cost->ldst.storev;
+	      else if (GET_MODE_CLASS (mode) == MODE_INT)
 		*cost += extra_cost->ldst.store;
 	      else if (mode == SFmode)
 		*cost += extra_cost->ldst.storef;
@@ -5550,15 +5757,22 @@ aarch64_rtx_costs (rtx x, int code, int outer ATTRIBUTE_UNUSED,
 
 	  /* Fall through.  */
 	case REG:
+	  /* The cost is one per vector-register copied.  */
+	  if (VECTOR_MODE_P (GET_MODE (op0)) && REG_P (op1))
+	    {
+	      int n_minus_1 = (GET_MODE_SIZE (GET_MODE (op0)) - 1)
+			      / GET_MODE_SIZE (V4SImode);
+	      *cost = COSTS_N_INSNS (n_minus_1 + 1);
+	    }
 	  /* const0_rtx is in general free, but we will use an
 	     instruction to set a register to 0.  */
-          if (REG_P (op1) || op1 == const0_rtx)
-            {
-              /* The cost is 1 per register copied.  */
-              int n_minus_1 = (GET_MODE_SIZE (GET_MODE (op0)) - 1)
+	  else if (REG_P (op1) || op1 == const0_rtx)
+	    {
+	      /* The cost is 1 per register copied.  */
+	      int n_minus_1 = (GET_MODE_SIZE (GET_MODE (op0)) - 1)
 			      / UNITS_PER_WORD;
-              *cost = COSTS_N_INSNS (n_minus_1 + 1);
-            }
+	      *cost = COSTS_N_INSNS (n_minus_1 + 1);
+	    }
           else
 	    /* Cost is just the cost of the RHS of the set.  */
 	    *cost += rtx_cost (op1, SET, 1, speed);
@@ -5656,7 +5870,9 @@ aarch64_rtx_costs (rtx x, int code, int outer ATTRIBUTE_UNUSED,
 	     approximation for the additional cost of the addressing
 	     mode.  */
 	  rtx address = XEXP (x, 0);
-	  if (GET_MODE_CLASS (mode) == MODE_INT)
+	  if (VECTOR_MODE_P (mode))
+	    *cost += extra_cost->ldst.loadv;
+	  else if (GET_MODE_CLASS (mode) == MODE_INT)
 	    *cost += extra_cost->ldst.load;
 	  else if (mode == SFmode)
 	    *cost += extra_cost->ldst.loadf;
@@ -5673,6 +5889,16 @@ aarch64_rtx_costs (rtx x, int code, int outer ATTRIBUTE_UNUSED,
     case NEG:
       op0 = XEXP (x, 0);
 
+      if (VECTOR_MODE_P (mode))
+	{
+	  if (speed)
+	    {
+	      /* FNEG.  */
+	      *cost += extra_cost->vect.alu;
+	    }
+	  return false;
+	}
+
       if (GET_MODE_CLASS (GET_MODE (x)) == MODE_INT)
        {
           if (GET_RTX_CLASS (GET_CODE (op0)) == RTX_COMPARE
@@ -5717,7 +5943,12 @@ aarch64_rtx_costs (rtx x, int code, int outer ATTRIBUTE_UNUSED,
     case CLRSB:
     case CLZ:
       if (speed)
-        *cost += extra_cost->alu.clz;
+	{
+	  if (VECTOR_MODE_P (mode))
+	    *cost += extra_cost->vect.alu;
+	  else
+	    *cost += extra_cost->alu.clz;
+	}
 
       return false;
 
@@ -5796,12 +6027,27 @@ aarch64_rtx_costs (rtx x, int code, int outer ATTRIBUTE_UNUSED,
 
           if (CONST_DOUBLE_P (op1) && aarch64_float_const_zero_rtx_p (op1))
             {
+              *cost += rtx_cost (op0, COMPARE, 0, speed);
               /* FCMP supports constant 0.0 for no extra cost. */
               return true;
             }
           return false;
         }
 
+      if (VECTOR_MODE_P (mode))
+	{
+	  /* Vector compare.  */
+	  if (speed)
+	    *cost += extra_cost->vect.alu;
+
+	  if (aarch64_float_const_zero_rtx_p (op1))
+	    {
+	      /* Vector cm (eq|ge|gt|lt|le) supports constant 0.0 for no extra
+		 cost.  */
+	      return true;
+	    }
+	  return false;
+	}
       return false;
 
     case MINUS:
@@ -5810,6 +6056,8 @@ aarch64_rtx_costs (rtx x, int code, int outer ATTRIBUTE_UNUSED,
 	op1 = XEXP (x, 1);
 
 cost_minus:
+	*cost += rtx_cost (op0, MINUS, 0, speed);
+
 	/* Detect valid immediates.  */
 	if ((GET_MODE_CLASS (mode) == MODE_INT
 	     || (GET_MODE_CLASS (mode) == MODE_CC
@@ -5817,20 +6065,17 @@ cost_minus:
 	    && CONST_INT_P (op1)
 	    && aarch64_uimm12_shift (INTVAL (op1)))
 	  {
-	    *cost += rtx_cost (op0, MINUS, 0, speed);
-
 	    if (speed)
 	      /* SUB(S) (immediate).  */
 	      *cost += extra_cost->alu.arith;
 	    return true;
-
 	  }
 
 	/* Look for SUB (extended register).  */
         if (aarch64_rtx_arith_op_extract_p (op1, mode))
 	  {
 	    if (speed)
-	      *cost += extra_cost->alu.arith_shift;
+	      *cost += extra_cost->alu.extend_arith;
 
 	    *cost += rtx_cost (XEXP (XEXP (op1, 0), 0),
 			       (enum rtx_code) GET_CODE (op1),
@@ -5842,13 +6087,12 @@ cost_minus:
 
 	/* Cost this as an FMA-alike operation.  */
 	if ((GET_CODE (new_op1) == MULT
-	     || GET_CODE (new_op1) == ASHIFT)
+	     || aarch64_shift_p (GET_CODE (new_op1)))
 	    && code != COMPARE)
 	  {
 	    *cost += aarch64_rtx_mult_cost (new_op1, MULT,
 					    (enum rtx_code) code,
 					    speed);
-	    *cost += rtx_cost (op0, MINUS, 0, speed);
 	    return true;
 	  }
 
@@ -5856,12 +6100,21 @@ cost_minus:
 
 	if (speed)
 	  {
-	    if (GET_MODE_CLASS (mode) == MODE_INT)
-	      /* SUB(S).  */
-	      *cost += extra_cost->alu.arith;
+	    if (VECTOR_MODE_P (mode))
+	      {
+		/* Vector SUB.  */
+		*cost += extra_cost->vect.alu;
+	      }
+	    else if (GET_MODE_CLASS (mode) == MODE_INT)
+	      {
+		/* SUB(S).  */
+		*cost += extra_cost->alu.arith;
+	      }
 	    else if (GET_MODE_CLASS (mode) == MODE_FLOAT)
-	      /* FSUB.  */
-	      *cost += extra_cost->fp[mode == DFmode].addsub;
+	      {
+		/* FSUB.  */
+		*cost += extra_cost->fp[mode == DFmode].addsub;
+	      }
 	  }
 	return true;
       }
@@ -5895,11 +6148,13 @@ cost_plus:
 	    return true;
 	  }
 
+	*cost += rtx_cost (op1, PLUS, 1, speed);
+
 	/* Look for ADD (extended register).  */
         if (aarch64_rtx_arith_op_extract_p (op0, mode))
 	  {
 	    if (speed)
-	      *cost += extra_cost->alu.arith_shift;
+	      *cost += extra_cost->alu.extend_arith;
 
 	    *cost += rtx_cost (XEXP (XEXP (op0, 0), 0),
 			       (enum rtx_code) GET_CODE (op0),
@@ -5912,25 +6167,32 @@ cost_plus:
 	new_op0 = aarch64_strip_extend (op0);
 
 	if (GET_CODE (new_op0) == MULT
-	    || GET_CODE (new_op0) == ASHIFT)
+	    || aarch64_shift_p (GET_CODE (new_op0)))
 	  {
 	    *cost += aarch64_rtx_mult_cost (new_op0, MULT, PLUS,
 					    speed);
-	    *cost += rtx_cost (op1, PLUS, 1, speed);
 	    return true;
 	  }
 
-	*cost += (rtx_cost (new_op0, PLUS, 0, speed)
-		  + rtx_cost (op1, PLUS, 1, speed));
+	*cost += rtx_cost (new_op0, PLUS, 0, speed);
 
 	if (speed)
 	  {
-	    if (GET_MODE_CLASS (mode) == MODE_INT)
-	      /* ADD.  */
-	      *cost += extra_cost->alu.arith;
+	    if (VECTOR_MODE_P (mode))
+	      {
+		/* Vector ADD.  */
+		*cost += extra_cost->vect.alu;
+	      }
+	    else if (GET_MODE_CLASS (mode) == MODE_INT)
+	      {
+		/* ADD.  */
+		*cost += extra_cost->alu.arith;
+	      }
 	    else if (GET_MODE_CLASS (mode) == MODE_FLOAT)
-	      /* FADD.  */
-	      *cost += extra_cost->fp[mode == DFmode].addsub;
+	      {
+		/* FADD.  */
+		*cost += extra_cost->fp[mode == DFmode].addsub;
+	      }
 	  }
 	return true;
       }
@@ -5939,8 +6201,12 @@ cost_plus:
       *cost = COSTS_N_INSNS (1);
 
       if (speed)
-        *cost += extra_cost->alu.rev;
-
+	{
+	  if (VECTOR_MODE_P (mode))
+	    *cost += extra_cost->vect.alu;
+	  else
+	    *cost += extra_cost->alu.rev;
+	}
       return false;
 
     case IOR:
@@ -5948,8 +6214,22 @@ cost_plus:
         {
           *cost = COSTS_N_INSNS (1);
 
+	  if (speed)
+	    {
+	      if (VECTOR_MODE_P (mode))
+		*cost += extra_cost->vect.alu;
+	      else
+		*cost += extra_cost->alu.rev;
+	    }
+	  return true;
+        }
+
+      if (aarch64_extr_rtx_p (x, &op0, &op1))
+        {
+          *cost += rtx_cost (op0, IOR, 0, speed)
+                   + rtx_cost (op1, IOR, 1, speed);
           if (speed)
-            *cost += extra_cost->alu.rev;
+            *cost += extra_cost->alu.shift;
 
           return true;
         }
@@ -5960,6 +6240,13 @@ cost_plus:
       op0 = XEXP (x, 0);
       op1 = XEXP (x, 1);
 
+      if (VECTOR_MODE_P (mode))
+	{
+	  if (speed)
+	    *cost += extra_cost->vect.alu;
+	  return true;
+	}
+
       if (code == AND
           && GET_CODE (op0) == MULT
           && CONST_INT_P (XEXP (op0, 1))
@@ -6025,13 +6312,52 @@ cost_plus:
       return false;
 
     case NOT:
+      x = XEXP (x, 0);
+      op0 = aarch64_strip_shift (x);
+
+      if (VECTOR_MODE_P (mode))
+	{
+	  /* Vector NOT.  */
+	  *cost += extra_cost->vect.alu;
+	  return false;
+	}
+
+      /* MVN-shifted-reg.  */
+      if (op0 != x)
+        {
+          *cost += rtx_cost (op0, (enum rtx_code) code, 0, speed);
+
+          if (speed)
+            *cost += extra_cost->alu.log_shift;
+
+          return true;
+        }
+      /* EON can have two forms: (xor (not a) b) but also (not (xor a b)).
+         Handle the second form here taking care that 'a' in the above can
+         be a shift.  */
+      else if (GET_CODE (op0) == XOR)
+        {
+          rtx newop0 = XEXP (op0, 0);
+          rtx newop1 = XEXP (op0, 1);
+          rtx op0_stripped = aarch64_strip_shift (newop0);
+
+          *cost += rtx_cost (newop1, (enum rtx_code) code, 1, speed)
+                   + rtx_cost (op0_stripped, XOR, 0, speed);
+
+          if (speed)
+            {
+              if (op0_stripped != newop0)
+                *cost += extra_cost->alu.log_shift;
+              else
+                *cost += extra_cost->alu.logical;
+            }
+
+          return true;
+        }
       /* MVN.  */
       if (speed)
 	*cost += extra_cost->alu.logical;
 
-      /* The logical instruction could have the shifted register form,
-         but the cost is the same if the shift is processed as a separate
-         instruction, so we don't bother with it here.  */
       return false;
 
     case ZERO_EXTEND:
@@ -6067,10 +6393,19 @@ cost_plus:
 	  return true;
 	}
 
-      /* UXTB/UXTH.  */
       if (speed)
-	*cost += extra_cost->alu.extend;
-
+	{
+	  if (VECTOR_MODE_P (mode))
+	    {
+	      /* UMOV.  */
+	      *cost += extra_cost->vect.alu;
+	    }
+	  else
+	    {
+	      /* UXTB/UXTH.  */
+	      *cost += extra_cost->alu.extend;
+	    }
+	}
       return false;
 
     case SIGN_EXTEND:
@@ -6090,7 +6425,12 @@ cost_plus:
 	}
 
       if (speed)
-	*cost += extra_cost->alu.extend;
+	{
+	  if (VECTOR_MODE_P (mode))
+	    *cost += extra_cost->vect.alu;
+	  else
+	    *cost += extra_cost->alu.extend;
+	}
       return false;
 
     case ASHIFT:
@@ -6099,10 +6439,20 @@ cost_plus:
 
       if (CONST_INT_P (op1))
         {
-	  /* LSL (immediate), UBMF, UBFIZ and friends.  These are all
-	     aliases.  */
 	  if (speed)
-	    *cost += extra_cost->alu.shift;
+	    {
+	      if (VECTOR_MODE_P (mode))
+		{
+		  /* Vector shift (immediate).  */
+		  *cost += extra_cost->vect.alu;
+		}
+	      else
+		{
+		  /* LSL (immediate), UBMF, UBFIZ and friends.  These are all
+		     aliases.  */
+		  *cost += extra_cost->alu.shift;
+		}
+	    }
 
           /* We can incorporate zero/sign extend for free.  */
           if (GET_CODE (op0) == ZERO_EXTEND
@@ -6114,10 +6464,19 @@ cost_plus:
         }
       else
         {
-	  /* LSLV.  */
 	  if (speed)
-	    *cost += extra_cost->alu.shift_reg;
-
+	    {
+	      if (VECTOR_MODE_P (mode))
+		{
+		  /* Vector shift (register).  */
+		  *cost += extra_cost->vect.alu;
+		}
+	      else
+		{
+		  /* LSLV.  */
+		  *cost += extra_cost->alu.shift_reg;
+		}
+	    }
 	  return false;  /* All arguments need to be in registers.  */
         }
 
@@ -6132,7 +6491,12 @@ cost_plus:
 	{
 	  /* ASR (immediate) and friends.  */
 	  if (speed)
-	    *cost += extra_cost->alu.shift;
+	    {
+	      if (VECTOR_MODE_P (mode))
+		*cost += extra_cost->vect.alu;
+	      else
+		*cost += extra_cost->alu.shift;
+	    }
 
 	  *cost += rtx_cost (op0, (enum rtx_code) code, 0, speed);
 	  return true;
@@ -6142,8 +6506,12 @@ cost_plus:
 
 	  /* ASR (register) and friends.  */
 	  if (speed)
-	    *cost += extra_cost->alu.shift_reg;
-
+	    {
+	      if (VECTOR_MODE_P (mode))
+		*cost += extra_cost->vect.alu;
+	      else
+		*cost += extra_cost->alu.shift_reg;
+	    }
 	  return false;  /* All arguments need to be in registers.  */
 	}
 
@@ -6191,7 +6559,12 @@ cost_plus:
     case SIGN_EXTRACT:
       /* UBFX/SBFX.  */
       if (speed)
-	*cost += extra_cost->alu.bfx;
+	{
+	  if (VECTOR_MODE_P (mode))
+	    *cost += extra_cost->vect.alu;
+	  else
+	    *cost += extra_cost->alu.bfx;
+	}
 
       /* We can trust that the immediates used will be correct (there
 	 are no by-register forms), so we need only cost op0.  */
@@ -6208,7 +6581,9 @@ cost_plus:
     case UMOD:
       if (speed)
 	{
-	  if (GET_MODE_CLASS (GET_MODE (x)) == MODE_INT)
+	  if (VECTOR_MODE_P (mode))
+	    *cost += extra_cost->vect.alu;
+	  else if (GET_MODE_CLASS (GET_MODE (x)) == MODE_INT)
 	    *cost += (extra_cost->mult[GET_MODE (x) == DImode].add
 		      + extra_cost->mult[GET_MODE (x) == DImode].idiv);
 	  else if (GET_MODE (x) == DFmode)
@@ -6225,7 +6600,9 @@ cost_plus:
     case SQRT:
       if (speed)
 	{
-	  if (GET_MODE_CLASS (mode) == MODE_INT)
+	  if (VECTOR_MODE_P (mode))
+	    *cost += extra_cost->vect.alu;
+	  else if (GET_MODE_CLASS (mode) == MODE_INT)
 	    /* There is no integer SQRT, so only DIV and UDIV can get
 	       here.  */
 	    *cost += extra_cost->mult[mode == DImode].idiv;
@@ -6257,7 +6634,12 @@ cost_plus:
       op2 = XEXP (x, 2);
 
       if (speed)
-	*cost += extra_cost->fp[mode == DFmode].fma;
+	{
+	  if (VECTOR_MODE_P (mode))
+	    *cost += extra_cost->vect.alu;
+	  else
+	    *cost += extra_cost->fp[mode == DFmode].fma;
+	}
 
       /* FMSUB, FNMADD, and FNMSUB are free.  */
       if (GET_CODE (op0) == NEG)
@@ -6295,14 +6677,36 @@ cost_plus:
       *cost += rtx_cost (op2, FMA, 2, speed);
       return true;
 
+    case FLOAT:
+    case UNSIGNED_FLOAT:
+      if (speed)
+	*cost += extra_cost->fp[mode == DFmode].fromint;
+      return false;
+
     case FLOAT_EXTEND:
       if (speed)
-	*cost += extra_cost->fp[mode == DFmode].widen;
+	{
+	  if (VECTOR_MODE_P (mode))
+	    {
+	      /*Vector truncate.  */
+	      *cost += extra_cost->vect.alu;
+	    }
+	  else
+	    *cost += extra_cost->fp[mode == DFmode].widen;
+	}
       return false;
 
     case FLOAT_TRUNCATE:
       if (speed)
-	*cost += extra_cost->fp[mode == DFmode].narrow;
+	{
+	  if (VECTOR_MODE_P (mode))
+	    {
+	      /*Vector conversion.  */
+	      *cost += extra_cost->vect.alu;
+	    }
+	  else
+	    *cost += extra_cost->fp[mode == DFmode].narrow;
+	}
       return false;
 
     case FIX:
@@ -6323,15 +6727,37 @@ cost_plus:
         }
 
       if (speed)
-        *cost += extra_cost->fp[GET_MODE (x) == DFmode].toint;
-
+	{
+	  if (VECTOR_MODE_P (mode))
+	    *cost += extra_cost->vect.alu;
+	  else
+	    *cost += extra_cost->fp[GET_MODE (x) == DFmode].toint;
+	}
       *cost += rtx_cost (x, (enum rtx_code) code, 0, speed);
       return true;
 
     case ABS:
-      if (GET_MODE_CLASS (mode) == MODE_FLOAT)
+      if (VECTOR_MODE_P (mode))
+	{
+	  /* ABS (vector).  */
+	  if (speed)
+	    *cost += extra_cost->vect.alu;
+	}
+      else if (GET_MODE_CLASS (mode) == MODE_FLOAT)
 	{
-	  /* FABS and FNEG are analogous.  */
+	  op0 = XEXP (x, 0);
+
+	  /* FABD, which is analogous to FADD.  */
+	  if (GET_CODE (op0) == MINUS)
+	    {
+	      *cost += rtx_cost (XEXP (op0, 0), MINUS, 0, speed);
+			+ rtx_cost (XEXP (op0, 1), MINUS, 1, speed);
+	      if (speed)
+		*cost += extra_cost->fp[mode == DFmode].addsub;
+
+	      return true;
+	    }
+	  /* Simple FABS is analogous to FNEG.  */
 	  if (speed)
 	    *cost += extra_cost->fp[mode == DFmode].neg;
 	}
@@ -6350,10 +6776,15 @@ cost_plus:
     case SMIN:
       if (speed)
 	{
-	  /* FMAXNM/FMINNM/FMAX/FMIN.
-	     TODO: This may not be accurate for all implementations, but
-	     we do not model this in the cost tables.  */
-	  *cost += extra_cost->fp[mode == DFmode].addsub;
+	  if (VECTOR_MODE_P (mode))
+	    *cost += extra_cost->vect.alu;
+	  else
+	    {
+	      /* FMAXNM/FMINNM/FMAX/FMIN.
+	         TODO: This may not be accurate for all implementations, but
+	         we do not model this in the cost tables.  */
+	      *cost += extra_cost->fp[mode == DFmode].addsub;
+	    }
 	}
       return false;
 
@@ -6447,7 +6878,7 @@ aarch64_register_move_cost (machine_mode mode,
   enum reg_class from = (enum reg_class) from_i;
   enum reg_class to = (enum reg_class) to_i;
   const struct cpu_regmove_cost *regmove_cost
-    = aarch64_tune_params->regmove_cost;
+    = aarch64_tune_params.regmove_cost;
 
   /* Caller save and pointer regs are equivalent to GENERAL_REGS.  */
   if (to == CALLER_SAVE_REGS || to == POINTER_REGS)
@@ -6502,14 +6933,14 @@ aarch64_memory_move_cost (machine_mode mode ATTRIBUTE_UNUSED,
 			  reg_class_t rclass ATTRIBUTE_UNUSED,
 			  bool in ATTRIBUTE_UNUSED)
 {
-  return aarch64_tune_params->memmov_cost;
+  return aarch64_tune_params.memmov_cost;
 }
 
 /* Return the number of instructions that can be issued per cycle.  */
 static int
 aarch64_sched_issue_rate (void)
 {
-  return aarch64_tune_params->issue_rate;
+  return aarch64_tune_params.issue_rate;
 }
 
 static int
@@ -6533,44 +6964,44 @@ aarch64_builtin_vectorization_cost (enum vect_cost_for_stmt type_of_cost,
   switch (type_of_cost)
     {
       case scalar_stmt:
-	return aarch64_tune_params->vec_costs->scalar_stmt_cost;
+	return aarch64_tune_params.vec_costs->scalar_stmt_cost;
 
       case scalar_load:
-	return aarch64_tune_params->vec_costs->scalar_load_cost;
+	return aarch64_tune_params.vec_costs->scalar_load_cost;
 
       case scalar_store:
-	return aarch64_tune_params->vec_costs->scalar_store_cost;
+	return aarch64_tune_params.vec_costs->scalar_store_cost;
 
       case vector_stmt:
-	return aarch64_tune_params->vec_costs->vec_stmt_cost;
+	return aarch64_tune_params.vec_costs->vec_stmt_cost;
 
       case vector_load:
-	return aarch64_tune_params->vec_costs->vec_align_load_cost;
+	return aarch64_tune_params.vec_costs->vec_align_load_cost;
 
       case vector_store:
-	return aarch64_tune_params->vec_costs->vec_store_cost;
+	return aarch64_tune_params.vec_costs->vec_store_cost;
 
       case vec_to_scalar:
-	return aarch64_tune_params->vec_costs->vec_to_scalar_cost;
+	return aarch64_tune_params.vec_costs->vec_to_scalar_cost;
 
       case scalar_to_vec:
-	return aarch64_tune_params->vec_costs->scalar_to_vec_cost;
+	return aarch64_tune_params.vec_costs->scalar_to_vec_cost;
 
       case unaligned_load:
-	return aarch64_tune_params->vec_costs->vec_unalign_load_cost;
+	return aarch64_tune_params.vec_costs->vec_unalign_load_cost;
 
       case unaligned_store:
-	return aarch64_tune_params->vec_costs->vec_unalign_store_cost;
+	return aarch64_tune_params.vec_costs->vec_unalign_store_cost;
 
       case cond_branch_taken:
-	return aarch64_tune_params->vec_costs->cond_taken_branch_cost;
+	return aarch64_tune_params.vec_costs->cond_taken_branch_cost;
 
       case cond_branch_not_taken:
-	return aarch64_tune_params->vec_costs->cond_not_taken_branch_cost;
+	return aarch64_tune_params.vec_costs->cond_not_taken_branch_cost;
 
       case vec_perm:
       case vec_promote_demote:
-	return aarch64_tune_params->vec_costs->vec_stmt_cost;
+	return aarch64_tune_params.vec_costs->vec_stmt_cost;
 
       case vec_construct:
         elements = TYPE_VECTOR_SUBPARTS (vectype);
@@ -6813,9 +7244,181 @@ aarch64_parse_tune (void)
   return;
 }
 
+/* Parse TOKEN, which has length LENGTH to see if it is an option
+   described in FLAG.  If it is, return the index bit for that fusion type.
+   If not, error (printing OPTION_NAME) and return zero.  */
+
+static unsigned int
+aarch64_parse_one_option_token (const char *token,
+				size_t length,
+				const struct aarch64_flag_desc *flag,
+				const char *option_name)
+{
+  for (; flag->name != NULL; flag++)
+    {
+      if (length == strlen (flag->name)
+	  && !strncmp (flag->name, token, length))
+	return flag->flag;
+    }
+
+  error ("unknown flag passed in -moverride=%s (%s)", option_name, token);
+  return 0;
+}
+
+/* Parse OPTION which is a comma-separated list of flags to enable.
+   FLAGS gives the list of flags we understand, INITIAL_STATE gives any
+   default state we inherit from the CPU tuning structures.  OPTION_NAME
+   gives the top-level option we are parsing in the -moverride string,
+   for use in error messages.  */
+
+static unsigned int
+aarch64_parse_boolean_options (const char *option,
+			       const struct aarch64_flag_desc *flags,
+			       unsigned int initial_state,
+			       const char *option_name)
+{
+  const char separator = '.';
+  const char* specs = option;
+  const char* ntoken = option;
+  unsigned int found_flags = initial_state;
+
+  while ((ntoken = strchr (specs, separator)))
+    {
+      size_t token_length = ntoken - specs;
+      unsigned token_ops = aarch64_parse_one_option_token (specs,
+							   token_length,
+							   flags,
+							   option_name);
+      /* If we find "none" (or, for simplicity's sake, an error) anywhere
+	 in the token stream, reset the supported operations.  So:
+
+	   adrp+add.cmp+branch.none.adrp+add
+
+	   would have the result of turning on only adrp+add fusion.  */
+      if (!token_ops)
+	found_flags = 0;
+
+      found_flags |= token_ops;
+      specs = ++ntoken;
+    }
+
+  /* We ended with a comma, print something.  */
+  if (!(*specs))
+    {
+      error ("%s string ill-formed\n", option_name);
+      return 0;
+    }
+
+  /* We still have one more token to parse.  */
+  size_t token_length = strlen (specs);
+  unsigned token_ops = aarch64_parse_one_option_token (specs,
+						       token_length,
+						       flags,
+						       option_name);
+   if (!token_ops)
+     found_flags = 0;
+
+  found_flags |= token_ops;
+  return found_flags;
+}
+
+/* Support for overriding instruction fusion.  */
+
+static void
+aarch64_parse_fuse_string (const char *fuse_string,
+			    struct tune_params *tune)
+{
+  tune->fusible_ops = aarch64_parse_boolean_options (fuse_string,
+						     aarch64_fusible_pairs,
+						     tune->fusible_ops,
+						     "fuse=");
+}
+
+/* Support for overriding other tuning flags.  */
+
+static void
+aarch64_parse_tune_string (const char *tune_string,
+			    struct tune_params *tune)
+{
+  tune->extra_tuning_flags
+    = aarch64_parse_boolean_options (tune_string,
+				     aarch64_tuning_flags,
+				     tune->extra_tuning_flags,
+				     "tune=");
+}
+
+/* Parse TOKEN, which has length LENGTH to see if it is a tuning option
+   we understand.  If it is, extract the option string and handoff to
+   the appropriate function.  */
+
+void
+aarch64_parse_one_override_token (const char* token,
+				  size_t length,
+				  struct tune_params *tune)
+{
+  const struct aarch64_tuning_override_function *fn
+    = aarch64_tuning_override_functions;
+
+  const char *option_part = strchr (token, '=');
+  if (!option_part)
+    {
+      error ("tuning string missing in option (%s)", token);
+      return;
+    }
+
+  /* Get the length of the option name.  */
+  length = option_part - token;
+  /* Skip the '=' to get to the option string.  */
+  option_part++;
+
+  for (; fn->name != NULL; fn++)
+    {
+      if (!strncmp (fn->name, token, length))
+	{
+	  fn->parse_override (option_part, tune);
+	  return;
+	}
+    }
+
+  error ("unknown tuning option (%s)",token);
+  return;
+}
+
+/* Parse STRING looking for options in the format:
+     string	:: option:string
+     option	:: name=substring
+     name	:: {a-z}
+     substring	:: defined by option.  */
+
+static void
+aarch64_parse_override_string (const char* input_string,
+			       struct tune_params* tune)
+{
+  const char separator = ':';
+  size_t string_length = strlen (input_string) + 1;
+  char *string_root = (char *) xmalloc (sizeof (*string_root) * string_length);
+  char *string = string_root;
+  strncpy (string, input_string, string_length);
+  string[string_length - 1] = '\0';
+
+  char* ntoken = string;
+
+  while ((ntoken = strchr (string, separator)))
+    {
+      size_t token_length = ntoken - string;
+      /* Make this substring look like a string.  */
+      *ntoken = '\0';
+      aarch64_parse_one_override_token (string, token_length, tune);
+      string = ++ntoken;
+    }
+
+  /* One last option to parse.  */
+  aarch64_parse_one_override_token (string, strlen (string), tune);
+  free (string_root);
+}
+
+/* Implement TARGET_OPTION_OVERRIDE.  */
 
-/* Implement TARGET_OPTION_OVERRIDE.  */
-
 static void
 aarch64_override_options (void)
 {
@@ -6872,9 +7475,15 @@ aarch64_override_options (void)
 
   aarch64_tune_flags = selected_tune->flags;
   aarch64_tune = selected_tune->core;
-  aarch64_tune_params = selected_tune->tune;
+  /* Make a copy of the tuning parameters attached to the core, which
+     we may later overwrite.  */
+  aarch64_tune_params = *(selected_tune->tune);
   aarch64_architecture_version = selected_cpu->architecture_version;
 
+  if (aarch64_override_tune_string)
+    aarch64_parse_override_string (aarch64_override_tune_string,
+				   &aarch64_tune_params);
+
   if (aarch64_fix_a53_err835769 == 2)
     {
 #ifdef TARGET_FIX_ERR_A53_835769_DEFAULT
@@ -6884,6 +7493,8 @@ aarch64_override_options (void)
 #endif
     }
 
+  aarch64_register_fma_steering ();
+
   aarch64_override_options_after_change ();
 }
 
@@ -6902,11 +7513,11 @@ aarch64_override_options_after_change (void)
   if (!optimize_size)
     {
       if (align_loops <= 0)
-	align_loops = aarch64_tune_params->loop_align;
+	align_loops = aarch64_tune_params.loop_align;
       if (align_jumps <= 0)
-	align_jumps = aarch64_tune_params->jump_align;
+	align_jumps = aarch64_tune_params.jump_align;
       if (align_functions <= 0)
-	align_functions = aarch64_tune_params->function_align;
+	align_functions = aarch64_tune_params.function_align;
     }
 }
 
@@ -7104,16 +7715,13 @@ aarch64_valid_floating_const (machine_mode mode, rtx x)
   if (!CONST_DOUBLE_P (x))
     return false;
 
-  /* TODO: We could handle moving 0.0 to a TFmode register,
-     but first we would like to refactor the movtf_aarch64
-     to be more amicable to split moves properly and
-     correctly gate on TARGET_SIMD.  For now - reject all
-     constants which are not to SFmode or DFmode registers.  */
+  if (aarch64_float_const_zero_rtx_p (x))
+    return true;
+
+  /* We only handle moving 0.0 to a TFmode register.  */
   if (!(mode == SFmode || mode == DFmode))
     return false;
 
-  if (aarch64_float_const_zero_rtx_p (x))
-    return true;
   return aarch64_float_const_representable_p (x);
 }
 
@@ -7247,9 +7855,7 @@ aarch64_expand_builtin_va_start (tree valist, rtx nextarg ATTRIBUTE_UNUSED)
 
   if (!TARGET_FLOAT)
     {
-      if (cum->aapcs_nvrn > 0)
-	sorry ("%qs and floating point or vector arguments",
-	       "-mgeneral-regs-only");
+      gcc_assert (cum->aapcs_nvrn == 0);
       vr_save_area_size = 0;
     }
 
@@ -7356,8 +7962,7 @@ aarch64_gimplify_va_arg_expr (tree valist, tree type, gimple_seq *pre_p,
     {
       /* TYPE passed in fp/simd registers.  */
       if (!TARGET_FLOAT)
-	sorry ("%qs and floating point or vector arguments",
-	       "-mgeneral-regs-only");
+	aarch64_err_no_fpadvsimd (mode, "varargs");
 
       f_top = build3 (COMPONENT_REF, TREE_TYPE (f_vrtop),
 		      unshare_expr (valist), f_vrtop, NULL_TREE);
@@ -7594,9 +8199,7 @@ aarch64_setup_incoming_varargs (cumulative_args_t cum_v, machine_mode mode,
 
   if (!TARGET_FLOAT)
     {
-      if (local_cum.aapcs_nvrn > 0)
-	sorry ("%qs and floating point or vector arguments",
-	       "-mgeneral-regs-only");
+      gcc_assert (local_cum.aapcs_nvrn == 0);
       vr_saved = 0;
     }
 
@@ -7830,6 +8433,26 @@ aapcs_vfp_sub_candidate (const_tree type, machine_mode *modep)
   return -1;
 }
 
+/* Return TRUE if the type, as described by TYPE and MODE, is a short vector
+   type as described in AAPCS64 \S 4.1.2.
+
+   See the comment above aarch64_composite_type_p for the notes on MODE.  */
+
+static bool
+aarch64_short_vector_p (const_tree type,
+			machine_mode mode)
+{
+  HOST_WIDE_INT size = -1;
+
+  if (type && TREE_CODE (type) == VECTOR_TYPE)
+    size = int_size_in_bytes (type);
+  else if (GET_MODE_CLASS (mode) == MODE_VECTOR_INT
+	    || GET_MODE_CLASS (mode) == MODE_VECTOR_FLOAT)
+    size = GET_MODE_SIZE (mode);
+
+  return (size == 8 || size == 16);
+}
+
 /* Return TRUE if the type, as described by TYPE and MODE, is a composite
    type as described in AAPCS64 \S 4.3.  This includes aggregate, union and
    array types.  The C99 floating-point complex types are also considered
@@ -7851,6 +8474,9 @@ static bool
 aarch64_composite_type_p (const_tree type,
 			  machine_mode mode)
 {
+  if (aarch64_short_vector_p (type, mode))
+    return false;
+
   if (type && (AGGREGATE_TYPE_P (type) || TREE_CODE (type) == COMPLEX_TYPE))
     return true;
 
@@ -7862,27 +8488,6 @@ aarch64_composite_type_p (const_tree type,
   return false;
 }
 
-/* Return TRUE if the type, as described by TYPE and MODE, is a short vector
-   type as described in AAPCS64 \S 4.1.2.
-
-   See the comment above aarch64_composite_type_p for the notes on MODE.  */
-
-static bool
-aarch64_short_vector_p (const_tree type,
-			machine_mode mode)
-{
-  HOST_WIDE_INT size = -1;
-
-  if (type && TREE_CODE (type) == VECTOR_TYPE)
-    size = int_size_in_bytes (type);
-  else if (!aarch64_composite_type_p (type, mode)
-	   && (GET_MODE_CLASS (mode) == MODE_VECTOR_INT
-	       || GET_MODE_CLASS (mode) == MODE_VECTOR_FLOAT))
-    size = GET_MODE_SIZE (mode);
-
-  return (size == 8 || size == 16) ? true : false;
-}
-
 /* Return TRUE if an argument, whose type is described by TYPE and MODE,
    shall be passed or returned in simd/fp register(s) (providing these
    parameter passing registers are available).
@@ -8575,30 +9180,12 @@ aarch64_simd_lane_bounds (rtx operand, HOST_WIDE_INT low, HOST_WIDE_INT high,
   if (lane < low || lane >= high)
   {
     if (exp)
-      error ("%Klane %ld out of range %ld - %ld", exp, lane, low, high - 1);
+      error ("%Klane %wd out of range %wd - %wd", exp, lane, low, high - 1);
     else
-      error ("lane %ld out of range %ld - %ld", lane, low, high - 1);
+      error ("lane %wd out of range %wd - %wd", lane, low, high - 1);
   }
 }
 
-/* Emit code to place a AdvSIMD pair result in memory locations (with equal
-   registers).  */
-void
-aarch64_simd_emit_pair_result_insn (machine_mode mode,
-			    rtx (*intfn) (rtx, rtx, rtx), rtx destaddr,
-                            rtx op1)
-{
-  rtx mem = gen_rtx_MEM (mode, destaddr);
-  rtx tmp1 = gen_reg_rtx (mode);
-  rtx tmp2 = gen_reg_rtx (mode);
-
-  emit_insn (intfn (tmp1, op1, tmp2));
-
-  emit_move_insn (mem, tmp1);
-  mem = adjust_address (mem, mode, GET_MODE_SIZE (mode));
-  emit_move_insn (mem, tmp2);
-}
-
 /* Return TRUE if OP is a valid vector addressing mode.  */
 bool
 aarch64_simd_mem_operand_p (rtx op)
@@ -8781,22 +9368,19 @@ aarch64_expand_vector_init (rtx target, rtx vals)
   machine_mode mode = GET_MODE (target);
   machine_mode inner_mode = GET_MODE_INNER (mode);
   int n_elts = GET_MODE_NUNITS (mode);
-  int n_var = 0, one_var = -1;
+  int n_var = 0;
+  rtx any_const = NULL_RTX;
   bool all_same = true;
-  rtx x, mem;
-  int i;
 
-  x = XVECEXP (vals, 0, 0);
-  if (!CONST_INT_P (x) && !CONST_DOUBLE_P (x))
-    n_var = 1, one_var = 0;
-  
-  for (i = 1; i < n_elts; ++i)
+  for (int i = 0; i < n_elts; ++i)
     {
-      x = XVECEXP (vals, 0, i);
+      rtx x = XVECEXP (vals, 0, i);
       if (!CONST_INT_P (x) && !CONST_DOUBLE_P (x))
-	++n_var, one_var = i;
+	++n_var;
+      else
+	any_const = x;
 
-      if (!rtx_equal_p (x, XVECEXP (vals, 0, 0)))
+      if (i > 0 && !rtx_equal_p (x, XVECEXP (vals, 0, 0)))
 	all_same = false;
     }
 
@@ -8813,36 +9397,60 @@ aarch64_expand_vector_init (rtx target, rtx vals)
   /* Splat a single non-constant element if we can.  */
   if (all_same)
     {
-      x = copy_to_mode_reg (inner_mode, XVECEXP (vals, 0, 0));
+      rtx x = copy_to_mode_reg (inner_mode, XVECEXP (vals, 0, 0));
       aarch64_emit_move (target, gen_rtx_VEC_DUPLICATE (mode, x));
       return;
     }
 
-  /* One field is non-constant.  Load constant then overwrite varying
-     field.  This is more efficient than using the stack.  */
-  if (n_var == 1)
+  /* Half the fields (or less) are non-constant.  Load constant then overwrite
+     varying fields.  Hope that this is more efficient than using the stack.  */
+  if (n_var <= n_elts/2)
     {
       rtx copy = copy_rtx (vals);
-      rtx index = GEN_INT (one_var);
-      enum insn_code icode;
 
-      /* Load constant part of vector, substitute neighboring value for
-	 varying element.  */
-      XVECEXP (copy, 0, one_var) = XVECEXP (vals, 0, one_var ^ 1);
+      /* Load constant part of vector.  We really don't care what goes into the
+	 parts we will overwrite, but we're more likely to be able to load the
+	 constant efficiently if it has fewer, larger, repeating parts
+	 (see aarch64_simd_valid_immediate).  */
+      for (int i = 0; i < n_elts; i++)
+	{
+	  rtx x = XVECEXP (vals, 0, i);
+	  if (CONST_INT_P (x) || CONST_DOUBLE_P (x))
+	    continue;
+	  rtx subst = any_const;
+	  for (int bit = n_elts / 2; bit > 0; bit /= 2)
+	    {
+	      /* Look in the copied vector, as more elements are const.  */
+	      rtx test = XVECEXP (copy, 0, i ^ bit);
+	      if (CONST_INT_P (test) || CONST_DOUBLE_P (test))
+		{
+		  subst = test;
+		  break;
+		}
+	    }
+	  XVECEXP (copy, 0, i) = subst;
+	}
       aarch64_expand_vector_init (target, copy);
 
-      /* Insert variable.  */
-      x = copy_to_mode_reg (inner_mode, XVECEXP (vals, 0, one_var));
-      icode = optab_handler (vec_set_optab, mode);
+      /* Insert variables.  */
+      enum insn_code icode = optab_handler (vec_set_optab, mode);
       gcc_assert (icode != CODE_FOR_nothing);
-      emit_insn (GEN_FCN (icode) (target, x, index));
+
+      for (int i = 0; i < n_elts; i++)
+	{
+	  rtx x = XVECEXP (vals, 0, i);
+	  if (CONST_INT_P (x) || CONST_DOUBLE_P (x))
+	    continue;
+	  x = copy_to_mode_reg (inner_mode, x);
+	  emit_insn (GEN_FCN (icode) (target, x, GEN_INT (i)));
+	}
       return;
     }
 
   /* Construct the vector in memory one field at a time
      and load the whole vector.  */
-  mem = assign_stack_temp (mode, GET_MODE_SIZE (mode));
-  for (i = 0; i < n_elts; i++)
+  rtx mem = assign_stack_temp (mode, GET_MODE_SIZE (mode));
+  for (int i = 0; i < n_elts; i++)
     emit_move_insn (adjust_address_nv (mem, inner_mode,
 				    i * GET_MODE_SIZE (inner_mode)),
 		    XVECEXP (vals, 0, i));
@@ -9015,7 +9623,23 @@ aarch64_expand_compare_and_swap (rtx operands[])
 {
   rtx bval, rval, mem, oldval, newval, is_weak, mod_s, mod_f, x;
   machine_mode mode, cmp_mode;
-  rtx (*gen) (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
+  typedef rtx (*gen_cas_fn) (rtx, rtx, rtx, rtx, rtx, rtx, rtx);
+  int idx;
+  gen_cas_fn gen;
+  const gen_cas_fn split_cas[] =
+  {
+    gen_aarch64_compare_and_swapqi,
+    gen_aarch64_compare_and_swaphi,
+    gen_aarch64_compare_and_swapsi,
+    gen_aarch64_compare_and_swapdi
+  };
+  const gen_cas_fn atomic_cas[] =
+  {
+    gen_aarch64_compare_and_swapqi_lse,
+    gen_aarch64_compare_and_swaphi_lse,
+    gen_aarch64_compare_and_swapsi_lse,
+    gen_aarch64_compare_and_swapdi_lse
+  };
 
   bval = operands[0];
   rval = operands[1];
@@ -9060,13 +9684,17 @@ aarch64_expand_compare_and_swap (rtx operands[])
 
   switch (mode)
     {
-    case QImode: gen = gen_atomic_compare_and_swapqi_1; break;
-    case HImode: gen = gen_atomic_compare_and_swaphi_1; break;
-    case SImode: gen = gen_atomic_compare_and_swapsi_1; break;
-    case DImode: gen = gen_atomic_compare_and_swapdi_1; break;
+    case QImode: idx = 0; break;
+    case HImode: idx = 1; break;
+    case SImode: idx = 2; break;
+    case DImode: idx = 3; break;
     default:
       gcc_unreachable ();
     }
+  if (TARGET_LSE)
+    gen = atomic_cas[idx];
+  else
+    gen = split_cas[idx];
 
   emit_insn (gen (rval, mem, oldval, newval, is_weak, mod_s, mod_f));
 
@@ -9078,6 +9706,32 @@ aarch64_expand_compare_and_swap (rtx operands[])
   emit_insn (gen_rtx_SET (VOIDmode, bval, x));
 }
 
+/* Test whether the target supports using a atomic load-operate instruction.
+   CODE is the operation and AFTER is TRUE if the data in memory after the
+   operation should be returned and FALSE if the data before the operation
+   should be returned.  Returns FALSE if the operation isn't supported by the
+   architecture.  */
+
+bool
+aarch64_atomic_ldop_supported_p (enum rtx_code code)
+{
+  if (!TARGET_LSE)
+    return false;
+
+  switch (code)
+    {
+    case SET:
+    case AND:
+    case IOR:
+    case XOR:
+    case MINUS:
+    case PLUS:
+      return true;
+    default:
+      return false;
+    }
+}
+
 /* Emit a barrier, that is appropriate for memory model MODEL, at the end of a
    sequence implementing an atomic operation.  */
 
@@ -9095,6 +9749,42 @@ aarch64_emit_post_barrier (enum memmodel model)
     }
 }
 
+/* Emit an atomic compare-and-swap operation.  RVAL is the destination register
+   for the data in memory.  EXPECTED is the value expected to be in memory.
+   DESIRED is the value to store to memory.  MEM is the memory location.  MODEL
+   is the memory ordering to use.  */
+
+void
+aarch64_gen_atomic_cas (rtx rval, rtx mem,
+			rtx expected, rtx desired,
+			rtx model)
+{
+  rtx (*gen) (rtx, rtx, rtx, rtx);
+  machine_mode mode;
+
+  mode = GET_MODE (mem);
+
+  switch (mode)
+    {
+    case QImode: gen = gen_aarch64_atomic_casqi; break;
+    case HImode: gen = gen_aarch64_atomic_cashi; break;
+    case SImode: gen = gen_aarch64_atomic_cassi; break;
+    case DImode: gen = gen_aarch64_atomic_casdi; break;
+    default:
+      gcc_unreachable ();
+    }
+
+  /* Move the expected value into the CAS destination register.  */
+  emit_insn (gen_rtx_SET (VOIDmode, rval, expected));
+
+  /* Emit the CAS.  */
+  emit_insn (gen (rval, mem, desired, model));
+
+  /* Compare the expected value with the value loaded by the CAS, to establish
+     whether the swap was made.  */
+  aarch64_gen_compare_reg (EQ, rval, expected);
+}
+
 /* Split a compare and swap pattern.  */
 
 void
@@ -9163,11 +9853,257 @@ aarch64_split_compare_and_swap (rtx operands[])
     aarch64_emit_post_barrier (model);
 }
 
+/* Emit a BIC instruction.  */
+
+static void
+aarch64_emit_bic (machine_mode mode, rtx dst, rtx s1, rtx s2, int shift)
+{
+  rtx shift_rtx = GEN_INT (shift);
+  rtx (*gen) (rtx, rtx, rtx, rtx);
+
+  switch (mode)
+    {
+    case SImode: gen = gen_and_one_cmpl_lshrsi3; break;
+    case DImode: gen = gen_and_one_cmpl_lshrdi3; break;
+    default:
+      gcc_unreachable ();
+    }
+
+  emit_insn (gen (dst, s2, shift_rtx, s1));
+}
+
+/* Emit an atomic swap.  */
+
+static void
+aarch64_emit_atomic_swap (machine_mode mode, rtx dst, rtx value,
+			  rtx mem, rtx model)
+{
+  rtx (*gen) (rtx, rtx, rtx, rtx);
+
+  switch (mode)
+    {
+    case QImode: gen = gen_aarch64_atomic_swpqi; break;
+    case HImode: gen = gen_aarch64_atomic_swphi; break;
+    case SImode: gen = gen_aarch64_atomic_swpsi; break;
+    case DImode: gen = gen_aarch64_atomic_swpdi; break;
+    default:
+      gcc_unreachable ();
+    }
+
+  emit_insn (gen (dst, mem, value, model));
+}
+
+/* Operations supported by aarch64_emit_atomic_load_op.  */
+
+enum aarch64_atomic_load_op_code
+{
+  AARCH64_LDOP_PLUS,	/* A + B  */
+  AARCH64_LDOP_XOR,	/* A ^ B  */
+  AARCH64_LDOP_OR,	/* A | B  */
+  AARCH64_LDOP_BIC	/* A & ~B  */
+};
+
+/* Emit an atomic load-operate.  */
+
+static void
+aarch64_emit_atomic_load_op (enum aarch64_atomic_load_op_code code,
+			     machine_mode mode, rtx dst, rtx src,
+			     rtx mem, rtx model)
+{
+  typedef rtx (*aarch64_atomic_load_op_fn) (rtx, rtx, rtx, rtx);
+  const aarch64_atomic_load_op_fn plus[] =
+  {
+    gen_aarch64_atomic_loadaddqi,
+    gen_aarch64_atomic_loadaddhi,
+    gen_aarch64_atomic_loadaddsi,
+    gen_aarch64_atomic_loadadddi
+  };
+  const aarch64_atomic_load_op_fn eor[] =
+  {
+    gen_aarch64_atomic_loadeorqi,
+    gen_aarch64_atomic_loadeorhi,
+    gen_aarch64_atomic_loadeorsi,
+    gen_aarch64_atomic_loadeordi
+  };
+  const aarch64_atomic_load_op_fn ior[] =
+  {
+    gen_aarch64_atomic_loadsetqi,
+    gen_aarch64_atomic_loadsethi,
+    gen_aarch64_atomic_loadsetsi,
+    gen_aarch64_atomic_loadsetdi
+  };
+  const aarch64_atomic_load_op_fn bic[] =
+  {
+    gen_aarch64_atomic_loadclrqi,
+    gen_aarch64_atomic_loadclrhi,
+    gen_aarch64_atomic_loadclrsi,
+    gen_aarch64_atomic_loadclrdi
+  };
+  aarch64_atomic_load_op_fn gen;
+  int idx = 0;
+
+  switch (mode)
+    {
+    case QImode: idx = 0; break;
+    case HImode: idx = 1; break;
+    case SImode: idx = 2; break;
+    case DImode: idx = 3; break;
+    default:
+      gcc_unreachable ();
+    }
+
+  switch (code)
+    {
+    case AARCH64_LDOP_PLUS: gen = plus[idx]; break;
+    case AARCH64_LDOP_XOR: gen = eor[idx]; break;
+    case AARCH64_LDOP_OR: gen = ior[idx]; break;
+    case AARCH64_LDOP_BIC: gen = bic[idx]; break;
+    default:
+      gcc_unreachable ();
+    }
+
+  emit_insn (gen (dst, mem, src, model));
+}
+
+/* Emit an atomic load+operate.  CODE is the operation.  OUT_DATA is the
+   location to store the data read from memory.  OUT_RESULT is the location to
+   store the result of the operation.  MEM is the memory location to read and
+   modify.  MODEL_RTX is the memory ordering to use.  VALUE is the second
+   operand for the operation.  Either OUT_DATA or OUT_RESULT, but not both, can
+   be NULL.  */
+
+void
+aarch64_gen_atomic_ldop (enum rtx_code code, rtx out_data, rtx out_result,
+			 rtx mem, rtx value, rtx model_rtx)
+{
+  machine_mode mode = GET_MODE (mem);
+  machine_mode wmode = (mode == DImode ? DImode : SImode);
+  const bool short_mode = (mode < SImode);
+  aarch64_atomic_load_op_code ldop_code;
+  rtx src;
+  rtx x;
+
+  if (out_data)
+    out_data = gen_lowpart (mode, out_data);
+
+  if (out_result)
+    out_result = gen_lowpart (mode, out_result);
+
+  /* Make sure the value is in a register, putting it into a destination
+     register if it needs to be manipulated.  */
+  if (!register_operand (value, mode)
+      || code == AND || code == MINUS)
+    {
+      src = out_result ? out_result : out_data;
+      emit_move_insn (src, gen_lowpart (mode, value));
+    }
+  else
+    src = value;
+  gcc_assert (register_operand (src, mode));
+
+  /* Preprocess the data for the operation as necessary.  If the operation is
+     a SET then emit a swap instruction and finish.  */
+  switch (code)
+    {
+    case SET:
+      aarch64_emit_atomic_swap (mode, out_data, src, mem, model_rtx);
+      return;
+
+    case MINUS:
+      /* Negate the value and treat it as a PLUS.  */
+      {
+	rtx neg_src;
+
+	/* Resize the value if necessary.  */
+	if (short_mode)
+	  src = gen_lowpart (wmode, src);
+
+	neg_src = gen_rtx_NEG (wmode, src);
+	emit_insn (gen_rtx_SET (VOIDmode, src, neg_src));
+
+	if (short_mode)
+	  src = gen_lowpart (mode, src);
+      }
+      /* Fall-through.  */
+    case PLUS:
+      ldop_code = AARCH64_LDOP_PLUS;
+      break;
+
+    case IOR:
+      ldop_code = AARCH64_LDOP_OR;
+      break;
+
+    case XOR:
+      ldop_code = AARCH64_LDOP_XOR;
+      break;
+
+    case AND:
+      {
+	rtx not_src;
+
+	/* Resize the value if necessary.  */
+	if (short_mode)
+	  src = gen_lowpart (wmode, src);
+
+	not_src = gen_rtx_NOT (wmode, src);
+	emit_insn (gen_rtx_SET (VOIDmode, src, not_src));
+
+	if (short_mode)
+	  src = gen_lowpart (mode, src);
+      }
+      ldop_code = AARCH64_LDOP_BIC;
+      break;
+
+    default:
+      /* The operation can't be done with atomic instructions.  */
+      gcc_unreachable ();
+    }
+
+  aarch64_emit_atomic_load_op (ldop_code, mode, out_data, src, mem, model_rtx);
+
+  /* If necessary, calculate the data in memory after the update by redoing the
+     operation from values in registers.  */
+  if (!out_result)
+    return;
+
+  if (short_mode)
+    {
+      src = gen_lowpart (wmode, src);
+      out_data = gen_lowpart (wmode, out_data);
+      out_result = gen_lowpart (wmode, out_result);
+    }
+
+  x = NULL_RTX;
+
+  switch (code)
+    {
+    case MINUS:
+    case PLUS:
+      x = gen_rtx_PLUS (wmode, out_data, src);
+      break;
+    case IOR:
+      x = gen_rtx_IOR (wmode, out_data, src);
+      break;
+    case XOR:
+      x = gen_rtx_XOR (wmode, out_data, src);
+      break;
+    case AND:
+      aarch64_emit_bic (wmode, out_result, out_data, src, 0);
+      return;
+    default:
+      gcc_unreachable ();
+    }
+
+  emit_set_insn (out_result, x);
+
+  return;
+}
+
 /* Split an atomic operation.  */
 
 void
 aarch64_split_atomic_op (enum rtx_code code, rtx old_out, rtx new_out, rtx mem,
-		     rtx value, rtx model_rtx, rtx cond)
+			 rtx value, rtx model_rtx, rtx cond)
 {
   machine_mode mode = GET_MODE (mem);
   machine_mode wmode = (mode == DImode ? DImode : SImode);
@@ -9176,6 +10112,7 @@ aarch64_split_atomic_op (enum rtx_code code, rtx old_out, rtx new_out, rtx mem,
   rtx_code_label *label;
   rtx x;
 
+  /* Split the atomic operation into a sequence.  */
   label = gen_label_rtx ();
   emit_label (label);
 
@@ -10604,7 +11541,7 @@ aarch64_gen_ccmp_next (rtx *prep_seq, rtx *gen_seq, rtx prev, int cmp_code,
 static bool
 aarch64_macro_fusion_p (void)
 {
-  return aarch64_tune_params->fuseable_ops != AARCH64_FUSE_NOTHING;
+  return aarch64_tune_params.fusible_ops != AARCH64_FUSE_NOTHING;
 }
 
 
@@ -10624,7 +11561,7 @@ aarch_macro_fusion_pair_p (rtx_insn *prev, rtx_insn *curr)
     return false;
 
   if (simple_sets_p
-      && (aarch64_tune_params->fuseable_ops & AARCH64_FUSE_MOV_MOVK))
+      && (aarch64_tune_params.fusible_ops & AARCH64_FUSE_MOV_MOVK))
     {
       /* We are trying to match:
          prev (mov)  == (set (reg r0) (const_int imm16))
@@ -10649,7 +11586,7 @@ aarch_macro_fusion_pair_p (rtx_insn *prev, rtx_insn *curr)
     }
 
   if (simple_sets_p
-      && (aarch64_tune_params->fuseable_ops & AARCH64_FUSE_ADRP_ADD))
+      && (aarch64_tune_params.fusible_ops & AARCH64_FUSE_ADRP_ADD))
     {
 
       /*  We're trying to match:
@@ -10675,7 +11612,7 @@ aarch_macro_fusion_pair_p (rtx_insn *prev, rtx_insn *curr)
     }
 
   if (simple_sets_p
-      && (aarch64_tune_params->fuseable_ops & AARCH64_FUSE_MOVK_MOVK))
+      && (aarch64_tune_params.fusible_ops & AARCH64_FUSE_MOVK_MOVK))
     {
 
       /* We're trying to match:
@@ -10704,7 +11641,7 @@ aarch_macro_fusion_pair_p (rtx_insn *prev, rtx_insn *curr)
 
     }
   if (simple_sets_p
-      && (aarch64_tune_params->fuseable_ops & AARCH64_FUSE_ADRP_LDR))
+      && (aarch64_tune_params.fusible_ops & AARCH64_FUSE_ADRP_LDR))
     {
       /* We're trying to match:
           prev (adrp) == (set (reg r0)
@@ -10735,7 +11672,7 @@ aarch_macro_fusion_pair_p (rtx_insn *prev, rtx_insn *curr)
         }
     }
 
-  if ((aarch64_tune_params->fuseable_ops & AARCH64_FUSE_CMP_BRANCH)
+  if ((aarch64_tune_params.fusible_ops & AARCH64_FUSE_CMP_BRANCH)
       && any_condjump_p (curr))
     {
       enum attr_type prev_type = get_attr_type (prev);
--- a/src/gcc/config/aarch64/aarch64.h
+++ b/src/gcc/config/aarch64/aarch64.h
@@ -29,6 +29,10 @@
       builtin_define ("__aarch64__");                   \
       builtin_define ("__ARM_64BIT_STATE");             \
       builtin_define_with_int_value                     \
+        ("__ARM_ALIGN_MAX_PWR", 28);                    \
+      builtin_define_with_int_value                     \
+        ("__ARM_ALIGN_MAX_STACK_PWR", 16);              \
+      builtin_define_with_int_value                     \
         ("__ARM_ARCH", aarch64_architecture_version);   \
       cpp_define_formatted                                              \
         (parse_in, "__ARM_ARCH_%dA", aarch64_architecture_version);     \
@@ -198,8 +202,12 @@ extern unsigned aarch64_architecture_version;
 #define AARCH64_FL_SIMD       (1 << 0)	/* Has SIMD instructions.  */
 #define AARCH64_FL_FP         (1 << 1)	/* Has FP.  */
 #define AARCH64_FL_CRYPTO     (1 << 2)	/* Has crypto.  */
-#define AARCH64_FL_SLOWMUL    (1 << 3)	/* A slow multiply core.  */
-#define AARCH64_FL_CRC        (1 << 4)	/* Has CRC.  */
+#define AARCH64_FL_CRC        (1 << 3)	/* Has CRC.  */
+/* ARMv8.1 architecture extensions.  */
+#define AARCH64_FL_LSE	      (1 << 4)  /* Has Large System Extensions.  */
+#define AARCH64_FL_PAN	      (1 << 5)  /* Has Privileged Access Never.  */
+#define AARCH64_FL_LOR	      (1 << 6)  /* Has Limited Ordering regions.  */
+#define AARCH64_FL_RDMA	      (1 << 7)  /* Has ARMv8.1 Adv.SIMD.  */
 
 /* Has FP and SIMD.  */
 #define AARCH64_FL_FPSIMD     (AARCH64_FL_FP | AARCH64_FL_SIMD)
@@ -209,6 +217,9 @@ extern unsigned aarch64_architecture_version;
 
 /* Architecture flags that effect instruction selection.  */
 #define AARCH64_FL_FOR_ARCH8       (AARCH64_FL_FPSIMD)
+#define AARCH64_FL_FOR_ARCH8_1			       \
+  (AARCH64_FL_FOR_ARCH8 | AARCH64_FL_LSE | AARCH64_FL_PAN \
+   | AARCH64_FL_LOR | AARCH64_FL_RDMA)
 
 /* Macros to test ISA flags.  */
 extern unsigned long aarch64_isa_flags;
@@ -216,10 +227,7 @@ extern unsigned long aarch64_isa_flags;
 #define AARCH64_ISA_CRYPTO         (aarch64_isa_flags & AARCH64_FL_CRYPTO)
 #define AARCH64_ISA_FP             (aarch64_isa_flags & AARCH64_FL_FP)
 #define AARCH64_ISA_SIMD           (aarch64_isa_flags & AARCH64_FL_SIMD)
-
-/* Macros to test tuning flags.  */
-extern unsigned long aarch64_tune_flags;
-#define AARCH64_TUNE_SLOWMUL       (aarch64_tune_flags & AARCH64_FL_SLOWMUL)
+#define AARCH64_ISA_LSE		   (aarch64_isa_flags & AARCH64_FL_LSE)
 
 /* Crypto is an optional extension to AdvSIMD.  */
 #define TARGET_CRYPTO (TARGET_SIMD && AARCH64_ISA_CRYPTO)
@@ -227,6 +235,9 @@ extern unsigned long aarch64_tune_flags;
 /* CRC instructions that can be enabled through +crc arch extension.  */
 #define TARGET_CRC32 (AARCH64_ISA_CRC)
 
+/* Atomic instructions that can be enabled through the +lse extension.  */
+#define TARGET_LSE (AARCH64_ISA_LSE)
+
 /* Standard register usage.  */
 
 /* 31 64-bit general purpose registers R0-R30:
@@ -506,7 +517,7 @@ enum reg_class
 
 enum target_cpus
 {
-#define AARCH64_CORE(NAME, INTERNAL_IDENT, SCHED, ARCH, FLAGS, COSTS) \
+#define AARCH64_CORE(NAME, INTERNAL_IDENT, SCHED, ARCH, FLAGS, COSTS, IMP, PART) \
   TARGET_CPU_##INTERNAL_IDENT,
 #include "aarch64-cores.def"
 #undef AARCH64_CORE
@@ -823,7 +834,8 @@ do {									     \
 #define TRAMPOLINE_SECTION text_section
 
 /* To start with.  */
-#define BRANCH_COST(SPEED_P, PREDICTABLE_P) 2
+#define BRANCH_COST(SPEED_P, PREDICTABLE_P) \
+  (aarch64_branch_cost (SPEED_P, PREDICTABLE_P))
 
 
 /* Assembly output.  */
@@ -929,11 +941,24 @@ extern const char *aarch64_rewrite_mcpu (int argc, const char **argv);
 #define BIG_LITTLE_CPU_SPEC_FUNCTIONS \
   { "rewrite_mcpu", aarch64_rewrite_mcpu },
 
+#if defined(__aarch64__)
+extern const char *host_detect_local_cpu (int argc, const char **argv);
+# define EXTRA_SPEC_FUNCTIONS						\
+  { "local_cpu_detect", host_detect_local_cpu },			\
+  BIG_LITTLE_CPU_SPEC_FUNCTIONS
+
+# define MCPU_MTUNE_NATIVE_SPECS					\
+   " %{march=native:%<march=native %:local_cpu_detect(arch)}"		\
+   " %{mcpu=native:%<mcpu=native %:local_cpu_detect(cpu)}"		\
+   " %{mtune=native:%<mtune=native %:local_cpu_detect(tune)}"
+#else
+# define MCPU_MTUNE_NATIVE_SPECS ""
+# define EXTRA_SPEC_FUNCTIONS BIG_LITTLE_CPU_SPEC_FUNCTIONS
+#endif
+
 #define ASM_CPU_SPEC \
    BIG_LITTLE_SPEC
 
-#define EXTRA_SPEC_FUNCTIONS BIG_LITTLE_CPU_SPEC_FUNCTIONS
-
 #define EXTRA_SPECS						\
   { "asm_cpu_spec",		ASM_CPU_SPEC }
 
--- a/src/gcc/config/aarch64/aarch64.md
+++ b/src/gcc/config/aarch64/aarch64.md
@@ -827,7 +827,7 @@
        gcc_unreachable ();
      }
 }
-  [(set_attr "type" "mov_reg,mov_imm,mov_imm,load1,load1,store1,store1,\
+  [(set_attr "type" "mov_reg,mov_imm,neon_move,load1,load1,store1,store1,\
                      neon_to_gp<q>,neon_from_gp<q>,neon_dup")
    (set_attr "simd" "*,*,yes,*,*,*,*,yes,yes,yes")]
 )
@@ -912,7 +912,7 @@
        DONE;
     }"
   [(set_attr "type" "mov_reg,mov_reg,mov_reg,mov_imm,mov_imm,load1,load1,store1,store1,\
-                     adr,adr,f_mcr,f_mrc,fmov,fmov")
+                     adr,adr,f_mcr,f_mrc,fmov,neon_move")
    (set_attr "fp" "*,*,*,*,*,*,yes,*,yes,*,*,yes,yes,yes,*")
    (set_attr "simd" "*,*,*,*,*,*,*,*,*,*,*,*,*,*,yes")]
 )
@@ -979,23 +979,25 @@
   [(set (match_operand:GPF 0 "nonimmediate_operand" "")
 	(match_operand:GPF 1 "general_operand" ""))]
   ""
-  "
+  {
     if (!TARGET_FLOAT)
-     {
-	sorry (\"%qs and floating point code\", \"-mgeneral-regs-only\");
+      {
+	aarch64_err_no_fpadvsimd (<MODE>mode, "code");
 	FAIL;
-     }
+      }
 
-    if (GET_CODE (operands[0]) == MEM)
+    if (GET_CODE (operands[0]) == MEM
+        && ! (GET_CODE (operands[1]) == CONST_DOUBLE
+	      && aarch64_float_const_zero_rtx_p (operands[1])))
       operands[1] = force_reg (<MODE>mode, operands[1]);
-  "
+  }
 )
 
 (define_insn "*movsf_aarch64"
   [(set (match_operand:SF 0 "nonimmediate_operand" "=w, ?r,w,w  ,w,m,r,m ,r")
 	(match_operand:SF 1 "general_operand"      "?rY, w,w,Ufc,m,w,m,rY,r"))]
   "TARGET_FLOAT && (register_operand (operands[0], SFmode)
-    || register_operand (operands[1], SFmode))"
+    || aarch64_reg_or_fp_zero (operands[1], SFmode))"
   "@
    fmov\\t%s0, %w1
    fmov\\t%w0, %s1
@@ -1007,14 +1009,14 @@
    str\\t%w1, %0
    mov\\t%w0, %w1"
   [(set_attr "type" "f_mcr,f_mrc,fmov,fconsts,\
-                     f_loads,f_stores,f_loads,f_stores,mov_reg")]
+                     f_loads,f_stores,load1,store1,mov_reg")]
 )
 
 (define_insn "*movdf_aarch64"
   [(set (match_operand:DF 0 "nonimmediate_operand" "=w, ?r,w,w  ,w,m,r,m ,r")
 	(match_operand:DF 1 "general_operand"      "?rY, w,w,Ufc,m,w,m,rY,r"))]
   "TARGET_FLOAT && (register_operand (operands[0], DFmode)
-    || register_operand (operands[1], DFmode))"
+    || aarch64_reg_or_fp_zero (operands[1], DFmode))"
   "@
    fmov\\t%d0, %x1
    fmov\\t%x0, %d1
@@ -1026,32 +1028,34 @@
    str\\t%x1, %0
    mov\\t%x0, %x1"
   [(set_attr "type" "f_mcr,f_mrc,fmov,fconstd,\
-                     f_loadd,f_stored,f_loadd,f_stored,mov_reg")]
+                     f_loadd,f_stored,load1,store1,mov_reg")]
 )
 
 (define_expand "movtf"
   [(set (match_operand:TF 0 "nonimmediate_operand" "")
 	(match_operand:TF 1 "general_operand" ""))]
   ""
-  "
+  {
     if (!TARGET_FLOAT)
-     {
-	sorry (\"%qs and floating point code\", \"-mgeneral-regs-only\");
+      {
+	aarch64_err_no_fpadvsimd (TFmode, "code");
 	FAIL;
-     }
+      }
 
-    if (GET_CODE (operands[0]) == MEM)
+    if (GET_CODE (operands[0]) == MEM
+        && ! (GET_CODE (operands[1]) == CONST_DOUBLE
+	      && aarch64_float_const_zero_rtx_p (operands[1])))
       operands[1] = force_reg (TFmode, operands[1]);
-  "
+  }
 )
 
 (define_insn "*movtf_aarch64"
   [(set (match_operand:TF 0
-	 "nonimmediate_operand" "=w,?&r,w ,?r,w,?w,w,m,?r ,Ump")
+	 "nonimmediate_operand" "=w,?&r,w ,?r,w,?w,w,m,?r ,Ump,Ump")
 	(match_operand:TF 1
-	 "general_operand"      " w,?r, ?r,w ,Y,Y ,m,w,Ump,?rY"))]
+	 "general_operand"      " w,?r, ?r,w ,Y,Y ,m,w,Ump,?r ,Y"))]
   "TARGET_FLOAT && (register_operand (operands[0], TFmode)
-    || register_operand (operands[1], TFmode))"
+    || aarch64_reg_or_fp_zero (operands[1], TFmode))"
   "@
    orr\\t%0.16b, %1.16b, %1.16b
    #
@@ -1062,12 +1066,13 @@
    ldr\\t%q0, %1
    str\\t%q1, %0
    ldp\\t%0, %H0, %1
-   stp\\t%1, %H1, %0"
-  [(set_attr "type" "logic_reg,multiple,f_mcr,f_mrc,fconstd,fconstd,\
-                     f_loadd,f_stored,neon_load1_2reg,neon_store1_2reg")
-   (set_attr "length" "4,8,8,8,4,4,4,4,4,4")
-   (set_attr "fp" "*,*,yes,yes,*,yes,yes,yes,*,*")
-   (set_attr "simd" "yes,*,*,*,yes,*,*,*,*,*")]
+   stp\\t%1, %H1, %0
+   stp\\txzr, xzr, %0"
+  [(set_attr "type" "logic_reg,multiple,f_mcr,f_mrc,neon_move_q,fconstd,\
+                     f_loadd,f_stored,load2,store2,store2")
+   (set_attr "length" "4,8,8,8,4,4,4,4,4,4,4")
+   (set_attr "fp" "*,*,yes,yes,*,yes,yes,yes,*,*,*")
+   (set_attr "simd" "yes,*,*,*,yes,*,*,*,*,*,*")]
 )
 
 (define_split
@@ -1414,18 +1419,28 @@
   "
   if (! aarch64_plus_operand (operands[2], VOIDmode))
     {
-      rtx subtarget = ((optimize && can_create_pseudo_p ())
-		       ? gen_reg_rtx (<MODE>mode) : operands[0]);
       HOST_WIDE_INT imm = INTVAL (operands[2]);
 
-      if (imm < 0)
-	imm = -(-imm & ~0xfff);
+      if (aarch64_move_imm (imm, <MODE>mode) && can_create_pseudo_p ())
+        {
+	  rtx tmp = gen_reg_rtx (<MODE>mode);
+	  emit_move_insn (tmp, operands[2]);
+	  operands[2] = tmp;
+        }
       else
-        imm &= ~0xfff;
-
-      emit_insn (gen_add<mode>3 (subtarget, operands[1], GEN_INT (imm)));
-      operands[1] = subtarget;
-      operands[2] = GEN_INT (INTVAL (operands[2]) - imm);
+        {
+	  rtx subtarget = ((optimize && can_create_pseudo_p ())
+			   ? gen_reg_rtx (<MODE>mode) : operands[0]);
+
+	  if (imm < 0)
+	    imm = -(-imm & ~0xfff);
+	  else
+	    imm &= ~0xfff;
+
+	  emit_insn (gen_add<mode>3 (subtarget, operands[1], GEN_INT (imm)));
+	  operands[1] = subtarget;
+	  operands[2] = GEN_INT (INTVAL (operands[2]) - imm);
+        }
     }
   "
 )
@@ -1529,6 +1544,38 @@
   [(set_attr "type" "alus_sreg,alus_imm,alus_imm")]
 )
 
+(define_insn "*adds_shift_imm_<mode>"
+  [(set (reg:CC_NZ CC_REGNUM)
+	(compare:CC_NZ
+	 (plus:GPI (ASHIFT:GPI 
+		    (match_operand:GPI 1 "register_operand" "r")
+		    (match_operand:QI 2 "aarch64_shift_imm_<mode>" "n"))
+		   (match_operand:GPI 3 "register_operand" "r"))
+	 (const_int 0)))
+   (set (match_operand:GPI 0 "register_operand" "=r")
+	(plus:GPI (ASHIFT:GPI (match_dup 1) (match_dup 2))
+		  (match_dup 3)))]
+  ""
+  "adds\\t%<w>0, %<w>3, %<w>1, <shift> %2"
+  [(set_attr "type" "alus_shift_imm")]
+)
+
+(define_insn "*subs_shift_imm_<mode>"
+  [(set (reg:CC_NZ CC_REGNUM)
+	(compare:CC_NZ
+	 (minus:GPI (match_operand:GPI 1 "register_operand" "r")
+		    (ASHIFT:GPI
+		     (match_operand:GPI 2 "register_operand" "r")
+		     (match_operand:QI 3 "aarch64_shift_imm_<mode>" "n")))
+	 (const_int 0)))
+   (set (match_operand:GPI 0 "register_operand" "=r")
+	(minus:GPI (match_dup 1)
+		   (ASHIFT:GPI (match_dup 2) (match_dup 3))))]
+  ""
+  "subs\\t%<w>0, %<w>1, %<w>2, <shift> %3"
+  [(set_attr "type" "alus_shift_imm")]
+)
+
 (define_insn "*adds_mul_imm_<mode>"
   [(set (reg:CC_NZ CC_REGNUM)
 	(compare:CC_NZ
@@ -1589,6 +1636,42 @@
   [(set_attr "type" "alus_ext")]
 )
 
+(define_insn "*adds_<optab><ALLX:mode>_shift_<GPI:mode>"
+  [(set (reg:CC_NZ CC_REGNUM)
+	(compare:CC_NZ
+	 (plus:GPI (ashift:GPI 
+		    (ANY_EXTEND:GPI 
+		     (match_operand:ALLX 1 "register_operand" "r"))
+		    (match_operand 2 "aarch64_imm3" "Ui3"))
+		   (match_operand:GPI 3 "register_operand" "r"))
+	 (const_int 0)))
+   (set (match_operand:GPI 0 "register_operand" "=rk")
+	(plus:GPI (ashift:GPI (ANY_EXTEND:GPI (match_dup 1))
+			      (match_dup 2))
+		  (match_dup 3)))]
+  ""
+  "adds\\t%<GPI:w>0, %<GPI:w>3, %<GPI:w>1, <su>xt<ALLX:size> %2"
+  [(set_attr "type" "alus_ext")]
+)
+
+(define_insn "*subs_<optab><ALLX:mode>_shift_<GPI:mode>"
+  [(set (reg:CC_NZ CC_REGNUM)
+	(compare:CC_NZ
+	 (minus:GPI (match_operand:GPI 1 "register_operand" "r")
+		    (ashift:GPI 
+		     (ANY_EXTEND:GPI
+		      (match_operand:ALLX 2 "register_operand" "r"))
+		     (match_operand 3 "aarch64_imm3" "Ui3")))
+	 (const_int 0)))
+   (set (match_operand:GPI 0 "register_operand" "=rk")
+	(minus:GPI (match_dup 1)
+		   (ashift:GPI (ANY_EXTEND:GPI (match_dup 2))
+			       (match_dup 3))))]
+  ""
+  "subs\\t%<GPI:w>0, %<GPI:w>1, %<GPI:w>2, <su>xt<ALLX:size> %3"
+  [(set_attr "type" "alus_ext")]
+)
+
 (define_insn "*adds_<optab><mode>_multp2"
   [(set (reg:CC_NZ CC_REGNUM)
 	(compare:CC_NZ
@@ -1884,6 +1967,38 @@
   [(set_attr "type" "adc_reg")]
 )
 
+(define_insn "*add_uxt<mode>_shift2"
+  [(set (match_operand:GPI 0 "register_operand" "=rk")
+	(plus:GPI (and:GPI
+		   (ashift:GPI (match_operand:GPI 1 "register_operand" "r")
+			       (match_operand 2 "aarch64_imm3" "Ui3"))
+		   (match_operand 3 "const_int_operand" "n"))
+		  (match_operand:GPI 4 "register_operand" "r")))]
+  "aarch64_uxt_size (INTVAL (operands[2]), INTVAL (operands[3])) != 0"
+  "*
+  operands[3] = GEN_INT (aarch64_uxt_size (INTVAL(operands[2]),
+					   INTVAL (operands[3])));
+  return \"add\t%<w>0, %<w>4, %<w>1, uxt%e3 %2\";"
+  [(set_attr "type" "alu_ext")]
+)
+
+;; zero_extend version of above
+(define_insn "*add_uxtsi_shift2_uxtw"
+  [(set (match_operand:DI 0 "register_operand" "=rk")
+	(zero_extend:DI
+	 (plus:SI (and:SI
+		   (ashift:SI (match_operand:SI 1 "register_operand" "r")
+			      (match_operand 2 "aarch64_imm3" "Ui3"))
+		   (match_operand 3 "const_int_operand" "n"))
+		  (match_operand:SI 4 "register_operand" "r"))))]
+  "aarch64_uxt_size (INTVAL (operands[2]), INTVAL (operands[3])) != 0"
+  "*
+  operands[3] = GEN_INT (aarch64_uxt_size (INTVAL (operands[2]),
+					   INTVAL (operands[3])));
+  return \"add\t%w0, %w4, %w1, uxt%e3 %2\";"
+  [(set_attr "type" "alu_ext")]
+)
+
 (define_insn "*add_uxt<mode>_multp2"
   [(set (match_operand:GPI 0 "register_operand" "=rk")
 	(plus:GPI (and:GPI
@@ -2140,6 +2255,38 @@
   [(set_attr "type" "adc_reg")]
 )
 
+(define_insn "*sub_uxt<mode>_shift2"
+  [(set (match_operand:GPI 0 "register_operand" "=rk")
+	(minus:GPI (match_operand:GPI 4 "register_operand" "rk")
+		   (and:GPI
+		    (ashift:GPI (match_operand:GPI 1 "register_operand" "r")
+				(match_operand 2 "aarch64_imm3" "Ui3"))
+		    (match_operand 3 "const_int_operand" "n"))))]
+  "aarch64_uxt_size (INTVAL (operands[2]),INTVAL (operands[3])) != 0"
+  "*
+  operands[3] = GEN_INT (aarch64_uxt_size (INTVAL (operands[2]),
+					   INTVAL (operands[3])));
+  return \"sub\t%<w>0, %<w>4, %<w>1, uxt%e3 %2\";"
+  [(set_attr "type" "alu_ext")]
+)
+
+;; zero_extend version of above
+(define_insn "*sub_uxtsi_shift2_uxtw"
+  [(set (match_operand:DI 0 "register_operand" "=rk")
+	(zero_extend:DI
+	 (minus:SI (match_operand:SI 4 "register_operand" "rk")
+		   (and:SI
+		    (ashift:SI (match_operand:SI 1 "register_operand" "r")
+			       (match_operand 2 "aarch64_imm3" "Ui3"))
+		    (match_operand 3 "const_int_operand" "n")))))]
+  "aarch64_uxt_size (INTVAL (operands[2]),INTVAL (operands[3])) != 0"
+  "*
+  operands[3] = GEN_INT (aarch64_uxt_size (INTVAL (operands[2]),
+					   INTVAL (operands[3])));
+  return \"sub\t%w0, %w4, %w1, uxt%e3 %2\";"
+  [(set_attr "type" "alu_ext")]
+)
+
 (define_insn "*sub_uxt<mode>_multp2"
   [(set (match_operand:GPI 0 "register_operand" "=rk")
 	(minus:GPI (match_operand:GPI 4 "register_operand" "rk")
@@ -2172,35 +2319,16 @@
   [(set_attr "type" "alu_ext")]
 )
 
-(define_insn_and_split "absdi2"
-  [(set (match_operand:DI 0 "register_operand" "=&r,w")
-	(abs:DI (match_operand:DI 1 "register_operand" "r,w")))]
+(define_expand "abs<mode>2"
+  [(match_operand:GPI 0 "register_operand" "")
+   (match_operand:GPI 1 "register_operand" "")]
   ""
-  "@
-   #
-   abs\\t%d0, %d1"
-  "reload_completed
-   && GP_REGNUM_P (REGNO (operands[0]))
-   && GP_REGNUM_P (REGNO (operands[1]))"
-  [(const_int 0)]
   {
-    emit_insn (gen_rtx_SET (VOIDmode, operands[0],
-			    gen_rtx_XOR (DImode,
-					 gen_rtx_ASHIFTRT (DImode,
-							   operands[1],
-							   GEN_INT (63)),
-					 operands[1])));
-    emit_insn (gen_rtx_SET (VOIDmode,
-			    operands[0],
-			    gen_rtx_MINUS (DImode,
-					   operands[0],
-					   gen_rtx_ASHIFTRT (DImode,
-							     operands[1],
-							     GEN_INT (63)))));
+    rtx ccreg = aarch64_gen_compare_reg (LT, operands[1], const0_rtx);
+    rtx x = gen_rtx_LT (VOIDmode, ccreg, const0_rtx);
+    emit_insn (gen_csneg3<mode>_insn (operands[0], x, operands[1], operands[1]));
     DONE;
   }
-  [(set_attr "type" "alu_sreg")
-   (set_attr "simd" "no,yes")]
 )
 
 (define_insn "neg<mode>2"
@@ -2852,7 +2980,7 @@
         (plus:GPI (match_operand 2 "aarch64_comparison_operation" "")
                   (match_operand:GPI 1 "register_operand" "r")))]
   ""
-  "csinc\\t%<w>0, %<w>1, %<w>1, %M2"
+  "cinc\\t%<w>0, %<w>1, %m2"
   [(set_attr "type" "csel")]
 )
 
@@ -2879,7 +3007,7 @@
   [(set_attr "type" "csel")]
 )
 
-(define_insn "*csneg3<mode>_insn"
+(define_insn "csneg3<mode>_insn"
   [(set (match_operand:GPI 0 "register_operand" "=r")
         (if_then_else:GPI
 	  (match_operand 1 "aarch64_comparison_operation" "")
@@ -3058,6 +3186,26 @@
    (set_attr "simd" "*,yes")]
 )
 
+(define_insn "*<NLOGICAL:optab>_one_cmplsidi3_ze"
+  [(set (match_operand:DI 0 "register_operand" "=r")
+	(zero_extend:DI
+	  (NLOGICAL:SI (not:SI (match_operand:SI 1 "register_operand" "r"))
+	               (match_operand:SI 2 "register_operand" "r"))))]
+  ""
+  "<NLOGICAL:nlogical>\\t%w0, %w2, %w1"
+  [(set_attr "type" "logic_reg")]
+)
+
+(define_insn "*xor_one_cmplsidi3_ze"
+  [(set (match_operand:DI 0 "register_operand" "=r")
+        (zero_extend:DI
+          (not:SI (xor:SI (match_operand:SI 1 "register_operand" "r")
+                          (match_operand:SI 2 "register_operand" "r")))))]
+  ""
+  "eon\\t%w0, %w1, %w2"
+  [(set_attr "type" "logic_reg")]
+)
+
 ;; (xor (not a) b) is simplify_rtx-ed down to (not (xor a b)).
 ;; eon does not operate on SIMD registers so the vector variant must be split.
 (define_insn_and_split "*xor_one_cmpl<mode>3"
@@ -3119,7 +3267,7 @@
   [(set_attr "type" "logics_reg")]
 )
 
-(define_insn "*<LOGICAL:optab>_one_cmpl_<SHIFT:optab><mode>3"
+(define_insn "<LOGICAL:optab>_one_cmpl_<SHIFT:optab><mode>3"
   [(set (match_operand:GPI 0 "register_operand" "=r")
 	(LOGICAL:GPI (not:GPI
 		      (SHIFT:GPI
@@ -3128,7 +3276,33 @@
 		     (match_operand:GPI 3 "register_operand" "r")))]
   ""
   "<LOGICAL:nlogical>\\t%<w>0, %<w>3, %<w>1, <SHIFT:shift> %2"
-  [(set_attr "type" "logics_shift_imm")]
+  [(set_attr "type" "logic_shift_imm")]
+)
+
+(define_insn "*eor_one_cmpl_<SHIFT:optab><mode>3_alt"
+  [(set (match_operand:GPI 0 "register_operand" "=r")
+	(not:GPI (xor:GPI
+		      (SHIFT:GPI
+		       (match_operand:GPI 1 "register_operand" "r")
+		       (match_operand:QI 2 "aarch64_shift_imm_<mode>" "n"))
+		     (match_operand:GPI 3 "register_operand" "r"))))]
+  ""
+  "eon\\t%<w>0, %<w>3, %<w>1, <SHIFT:shift> %2"
+  [(set_attr "type" "logic_shift_imm")]
+)
+
+;; Zero-extend version of the above.
+(define_insn "*eor_one_cmpl_<SHIFT:optab>sidi3_alt_ze"
+  [(set (match_operand:DI 0 "register_operand" "=r")
+	(zero_extend:DI
+	  (not:SI (xor:SI
+		    (SHIFT:SI
+		      (match_operand:SI 1 "register_operand" "r")
+		      (match_operand:QI 2 "aarch64_shift_imm_si" "n"))
+		    (match_operand:SI 3 "register_operand" "r")))))]
+  ""
+  "eon\\t%w0, %w3, %w1, <SHIFT:shift> %2"
+  [(set_attr "type" "logic_shift_imm")]
 )
 
 (define_insn "*and_one_cmpl_<SHIFT:optab><mode>3_compare0"
@@ -3347,32 +3521,33 @@
 
 ;; Logical left shift using SISD or Integer instruction
 (define_insn "*aarch64_ashl_sisd_or_int_<mode>3"
-  [(set (match_operand:GPI 0 "register_operand" "=w,w,r")
+  [(set (match_operand:GPI 0 "register_operand" "=r,w,w")
         (ashift:GPI
-          (match_operand:GPI 1 "register_operand" "w,w,r")
-          (match_operand:QI 2 "aarch64_reg_or_shift_imm_<mode>" "Us<cmode>,w,rUs<cmode>")))]
+          (match_operand:GPI 1 "register_operand" "r,w,w")
+          (match_operand:QI 2 "aarch64_reg_or_shift_imm_<mode>" "rUs<cmode>,Us<cmode>,w")))]
   ""
   "@
+   lsl\t%<w>0, %<w>1, %<w>2
    shl\t%<rtn>0<vas>, %<rtn>1<vas>, %2
-   ushl\t%<rtn>0<vas>, %<rtn>1<vas>, %<rtn>2<vas>
-   lsl\t%<w>0, %<w>1, %<w>2"
-  [(set_attr "simd" "yes,yes,no")
-   (set_attr "type" "neon_shift_imm<q>, neon_shift_reg<q>,shift_reg")]
+   ushl\t%<rtn>0<vas>, %<rtn>1<vas>, %<rtn>2<vas>"
+  [(set_attr "simd" "no,yes,yes")
+   (set_attr "type" "shift_reg,neon_shift_imm<q>, neon_shift_reg<q>")]
 )
 
 ;; Logical right shift using SISD or Integer instruction
 (define_insn "*aarch64_lshr_sisd_or_int_<mode>3"
-  [(set (match_operand:GPI 0 "register_operand" "=w,&w,r")
+  [(set (match_operand:GPI 0 "register_operand" "=r,w,&w,&w")
         (lshiftrt:GPI
-          (match_operand:GPI 1 "register_operand" "w,w,r")
-          (match_operand:QI 2 "aarch64_reg_or_shift_imm_<mode>" "Us<cmode>,w,rUs<cmode>")))]
+          (match_operand:GPI 1 "register_operand" "r,w,w,w")
+          (match_operand:QI 2 "aarch64_reg_or_shift_imm_<mode>" "rUs<cmode>,Us<cmode>,w,0")))]
   ""
   "@
+   lsr\t%<w>0, %<w>1, %<w>2
    ushr\t%<rtn>0<vas>, %<rtn>1<vas>, %2
    #
-   lsr\t%<w>0, %<w>1, %<w>2"
-  [(set_attr "simd" "yes,yes,no")
-   (set_attr "type" "neon_shift_imm<q>,neon_shift_reg<q>,shift_reg")]
+   #"
+  [(set_attr "simd" "no,yes,yes,yes")
+   (set_attr "type" "shift_reg,neon_shift_imm<q>,neon_shift_reg<q>,neon_shift_reg<q>")]
 )
 
 (define_split
@@ -3407,18 +3582,18 @@
 
 ;; Arithmetic right shift using SISD or Integer instruction
 (define_insn "*aarch64_ashr_sisd_or_int_<mode>3"
-  [(set (match_operand:GPI 0 "register_operand" "=w,&w,&w,r")
+  [(set (match_operand:GPI 0 "register_operand" "=r,w,&w,&w")
         (ashiftrt:GPI
-          (match_operand:GPI 1 "register_operand" "w,w,w,r")
-          (match_operand:QI 2 "aarch64_reg_or_shift_imm_di" "Us<cmode>,w,0,rUs<cmode>")))]
+          (match_operand:GPI 1 "register_operand" "r,w,w,w")
+          (match_operand:QI 2 "aarch64_reg_or_shift_imm_di" "rUs<cmode>,Us<cmode>,w,0")))]
   ""
   "@
+   asr\t%<w>0, %<w>1, %<w>2
    sshr\t%<rtn>0<vas>, %<rtn>1<vas>, %2
    #
-   #
-   asr\t%<w>0, %<w>1, %<w>2"
-  [(set_attr "simd" "yes,yes,yes,no")
-   (set_attr "type" "neon_shift_imm<q>,neon_shift_reg<q>,neon_shift_reg<q>,shift_reg")]
+   #"
+  [(set_attr "simd" "no,yes,yes,yes")
+   (set_attr "type" "shift_reg,neon_shift_imm<q>,neon_shift_reg<q>,neon_shift_reg<q>")]
 )
 
 (define_split
@@ -3551,6 +3726,21 @@
   [(set_attr "type" "shift_imm")]
 )
 
+;; There are no canonicalisation rules for ashift and lshiftrt inside an ior
+;; so we have to match both orderings.
+(define_insn "*extr<mode>5_insn_alt"
+  [(set (match_operand:GPI 0 "register_operand" "=r")
+	(ior:GPI  (lshiftrt:GPI (match_operand:GPI 2 "register_operand" "r")
+			        (match_operand 4 "const_int_operand" "n"))
+		  (ashift:GPI (match_operand:GPI 1 "register_operand" "r")
+			      (match_operand 3 "const_int_operand" "n"))))]
+  "UINTVAL (operands[3]) < GET_MODE_BITSIZE (<MODE>mode)
+   && (UINTVAL (operands[3]) + UINTVAL (operands[4])
+       == GET_MODE_BITSIZE (<MODE>mode))"
+  "extr\\t%<w>0, %<w>1, %<w>2, %4"
+  [(set_attr "type" "shift_imm")]
+)
+
 ;; zero_extend version of the above
 (define_insn "*extrsi5_insn_uxtw"
   [(set (match_operand:DI 0 "register_operand" "=r")
@@ -3565,6 +3755,19 @@
   [(set_attr "type" "shift_imm")]
 )
 
+(define_insn "*extrsi5_insn_uxtw_alt"
+  [(set (match_operand:DI 0 "register_operand" "=r")
+	(zero_extend:DI
+	 (ior:SI (lshiftrt:SI (match_operand:SI 2 "register_operand" "r")
+			       (match_operand 4 "const_int_operand" "n"))
+		 (ashift:SI (match_operand:SI 1 "register_operand" "r")
+			    (match_operand 3 "const_int_operand" "n")))))]
+  "UINTVAL (operands[3]) < 32 &&
+   (UINTVAL (operands[3]) + UINTVAL (operands[4]) == 32)"
+  "extr\\t%w0, %w1, %w2, %4"
+  [(set_attr "type" "shift_imm")]
+)
+
 (define_insn "*ror<mode>3_insn"
   [(set (match_operand:GPI 0 "register_operand" "=r")
 	(rotate:GPI (match_operand:GPI 1 "register_operand" "r")
@@ -3915,7 +4118,7 @@
 (define_insn "<optab><fcvt_target><GPF:mode>2"
   [(set (match_operand:GPF 0 "register_operand" "=w,w")
         (FLOATUORS:GPF (match_operand:<FCVT_TARGET> 1 "register_operand" "w,r")))]
-  ""
+  "TARGET_FLOAT"
   "@
    <su_optab>cvtf\t%<GPF:s>0, %<s>1
    <su_optab>cvtf\t%<GPF:s>0, %<w1>1"
--- a/src/gcc/config/aarch64/aarch64.opt
+++ b/src/gcc/config/aarch64/aarch64.opt
@@ -111,6 +111,10 @@ mabi=
 Target RejectNegative Joined Enum(aarch64_abi) Var(aarch64_abi) Init(AARCH64_ABI_DEFAULT)
 -mabi=ABI	Generate code that conforms to the specified ABI
 
+moverride=
+Target RejectNegative ToLower Joined Var(aarch64_override_tune_string)
+-moverride=STRING	Power users only! Override CPU optimization parameters
+
 Enum
 Name(aarch64_abi) Type(int)
 Known AArch64 ABIs (for use with the -mabi= option):
--- a/src/gcc/config/aarch64/arm_neon.h
+++ b/src/gcc/config/aarch64/arm_neon.h
@@ -5665,8 +5665,6 @@ vaddlvq_u32 (uint32x4_t a)
 
 /* vcvt_high_f32_f16 not supported */
 
-static float32x2_t vdup_n_f32 (float32_t);
-
 #define vcvt_n_f32_s32(a, b)                                            \
   __extension__                                                         \
     ({                                                                  \
@@ -9824,272 +9822,6 @@ vrsqrtss_f32 (float32_t a, float32_t b)
        result;                                                          \
      })
 
-#define vst1_lane_f32(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       float32x2_t b_ = (b);                                            \
-       float32_t * a_ = (a);                                            \
-       __asm__ ("st1 {%1.s}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
-
-#define vst1_lane_f64(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       float64x1_t b_ = (b);                                            \
-       float64_t * a_ = (a);                                            \
-       __asm__ ("st1 {%1.d}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
-
-#define vst1_lane_p8(a, b, c)                                           \
-  __extension__                                                         \
-    ({                                                                  \
-       poly8x8_t b_ = (b);                                              \
-       poly8_t * a_ = (a);                                              \
-       __asm__ ("st1 {%1.b}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
-
-#define vst1_lane_p16(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       poly16x4_t b_ = (b);                                             \
-       poly16_t * a_ = (a);                                             \
-       __asm__ ("st1 {%1.h}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
-
-#define vst1_lane_s8(a, b, c)                                           \
-  __extension__                                                         \
-    ({                                                                  \
-       int8x8_t b_ = (b);                                               \
-       int8_t * a_ = (a);                                               \
-       __asm__ ("st1 {%1.b}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
-
-#define vst1_lane_s16(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       int16x4_t b_ = (b);                                              \
-       int16_t * a_ = (a);                                              \
-       __asm__ ("st1 {%1.h}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
-
-#define vst1_lane_s32(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       int32x2_t b_ = (b);                                              \
-       int32_t * a_ = (a);                                              \
-       __asm__ ("st1 {%1.s}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
-
-#define vst1_lane_s64(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       int64x1_t b_ = (b);                                              \
-       int64_t * a_ = (a);                                              \
-       __asm__ ("st1 {%1.d}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
-
-#define vst1_lane_u8(a, b, c)                                           \
-  __extension__                                                         \
-    ({                                                                  \
-       uint8x8_t b_ = (b);                                              \
-       uint8_t * a_ = (a);                                              \
-       __asm__ ("st1 {%1.b}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
-
-#define vst1_lane_u16(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       uint16x4_t b_ = (b);                                             \
-       uint16_t * a_ = (a);                                             \
-       __asm__ ("st1 {%1.h}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
-
-#define vst1_lane_u32(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       uint32x2_t b_ = (b);                                             \
-       uint32_t * a_ = (a);                                             \
-       __asm__ ("st1 {%1.s}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
-
-#define vst1_lane_u64(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       uint64x1_t b_ = (b);                                             \
-       uint64_t * a_ = (a);                                             \
-       __asm__ ("st1 {%1.d}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
-
-
-#define vst1q_lane_f32(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       float32x4_t b_ = (b);                                            \
-       float32_t * a_ = (a);                                            \
-       __asm__ ("st1 {%1.s}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
-
-#define vst1q_lane_f64(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       float64x2_t b_ = (b);                                            \
-       float64_t * a_ = (a);                                            \
-       __asm__ ("st1 {%1.d}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
-
-#define vst1q_lane_p8(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       poly8x16_t b_ = (b);                                             \
-       poly8_t * a_ = (a);                                              \
-       __asm__ ("st1 {%1.b}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
-
-#define vst1q_lane_p16(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       poly16x8_t b_ = (b);                                             \
-       poly16_t * a_ = (a);                                             \
-       __asm__ ("st1 {%1.h}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
-
-#define vst1q_lane_s8(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       int8x16_t b_ = (b);                                              \
-       int8_t * a_ = (a);                                               \
-       __asm__ ("st1 {%1.b}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
-
-#define vst1q_lane_s16(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       int16x8_t b_ = (b);                                              \
-       int16_t * a_ = (a);                                              \
-       __asm__ ("st1 {%1.h}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
-
-#define vst1q_lane_s32(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       int32x4_t b_ = (b);                                              \
-       int32_t * a_ = (a);                                              \
-       __asm__ ("st1 {%1.s}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
-
-#define vst1q_lane_s64(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       int64x2_t b_ = (b);                                              \
-       int64_t * a_ = (a);                                              \
-       __asm__ ("st1 {%1.d}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
-
-#define vst1q_lane_u8(a, b, c)                                          \
-  __extension__                                                         \
-    ({                                                                  \
-       uint8x16_t b_ = (b);                                             \
-       uint8_t * a_ = (a);                                              \
-       __asm__ ("st1 {%1.b}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
-
-#define vst1q_lane_u16(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       uint16x8_t b_ = (b);                                             \
-       uint16_t * a_ = (a);                                             \
-       __asm__ ("st1 {%1.h}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
-
-#define vst1q_lane_u32(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       uint32x4_t b_ = (b);                                             \
-       uint32_t * a_ = (a);                                             \
-       __asm__ ("st1 {%1.s}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
-
-#define vst1q_lane_u64(a, b, c)                                         \
-  __extension__                                                         \
-    ({                                                                  \
-       uint64x2_t b_ = (b);                                             \
-       uint64_t * a_ = (a);                                             \
-       __asm__ ("st1 {%1.d}[%2],[%0]"                                   \
-                :                                                       \
-                : "r"(a_), "w"(b_), "i"(c)                              \
-                : "memory");                                            \
-     })
-
-
 __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
 vtst_p8 (poly8x8_t a, poly8x8_t b)
 {
@@ -10218,8 +9950,8 @@ __STRUCTN (float, 64, 4)
 #undef __STRUCTN
 
 
-#define __ST2_LANE_FUNC(intype, largetype, ptrtype,			     \
-			mode, ptr_mode, funcsuffix, signedtype)		     \
+#define __ST2_LANE_FUNC(intype, largetype, ptrtype, mode,		     \
+			qmode, ptr_mode, funcsuffix, signedtype)	     \
 __extension__ static __inline void					     \
 __attribute__ ((__always_inline__))					     \
 vst2_lane_ ## funcsuffix (ptrtype *__ptr,				     \
@@ -10233,31 +9965,37 @@ vst2_lane_ ## funcsuffix (ptrtype *__ptr,				     \
   __temp.val[1]								     \
     = vcombine_##funcsuffix (__b.val[1],				     \
 			     vcreate_##funcsuffix (__AARCH64_UINT64_C (0))); \
-  __o = __builtin_aarch64_set_qregoi##mode (__o,			     \
-					    (signedtype) __temp.val[0], 0);  \
-  __o = __builtin_aarch64_set_qregoi##mode (__o,			     \
-					    (signedtype) __temp.val[1], 1);  \
+  __o = __builtin_aarch64_set_qregoi##qmode (__o,			     \
+					     (signedtype) __temp.val[0], 0); \
+  __o = __builtin_aarch64_set_qregoi##qmode (__o,			     \
+					     (signedtype) __temp.val[1], 1); \
   __builtin_aarch64_st2_lane##mode ((__builtin_aarch64_simd_ ## ptr_mode *)  \
 				     __ptr, __o, __c);			     \
 }
 
-__ST2_LANE_FUNC (float32x2x2_t, float32x4x2_t, float32_t, v4sf, sf, f32,
+__ST2_LANE_FUNC (float32x2x2_t, float32x4x2_t, float32_t, v2sf, v4sf, sf, f32,
 		 float32x4_t)
-__ST2_LANE_FUNC (float64x1x2_t, float64x2x2_t, float64_t, v2df, df, f64,
+__ST2_LANE_FUNC (float64x1x2_t, float64x2x2_t, float64_t, df, v2df, df, f64,
 		 float64x2_t)
-__ST2_LANE_FUNC (poly8x8x2_t, poly8x16x2_t, poly8_t, v16qi, qi, p8, int8x16_t)
-__ST2_LANE_FUNC (poly16x4x2_t, poly16x8x2_t, poly16_t, v8hi, hi, p16,
+__ST2_LANE_FUNC (poly8x8x2_t, poly8x16x2_t, poly8_t, v8qi, v16qi, qi, p8,
+		 int8x16_t)
+__ST2_LANE_FUNC (poly16x4x2_t, poly16x8x2_t, poly16_t, v4hi, v8hi, hi, p16,
 		 int16x8_t)
-__ST2_LANE_FUNC (int8x8x2_t, int8x16x2_t, int8_t, v16qi, qi, s8, int8x16_t)
-__ST2_LANE_FUNC (int16x4x2_t, int16x8x2_t, int16_t, v8hi, hi, s16, int16x8_t)
-__ST2_LANE_FUNC (int32x2x2_t, int32x4x2_t, int32_t, v4si, si, s32, int32x4_t)
-__ST2_LANE_FUNC (int64x1x2_t, int64x2x2_t, int64_t, v2di, di, s64, int64x2_t)
-__ST2_LANE_FUNC (uint8x8x2_t, uint8x16x2_t, uint8_t, v16qi, qi, u8, int8x16_t)
-__ST2_LANE_FUNC (uint16x4x2_t, uint16x8x2_t, uint16_t, v8hi, hi, u16,
+__ST2_LANE_FUNC (int8x8x2_t, int8x16x2_t, int8_t, v8qi, v16qi, qi, s8,
+		 int8x16_t)
+__ST2_LANE_FUNC (int16x4x2_t, int16x8x2_t, int16_t, v4hi, v8hi, hi, s16,
+		 int16x8_t)
+__ST2_LANE_FUNC (int32x2x2_t, int32x4x2_t, int32_t, v2si, v4si, si, s32,
+		 int32x4_t)
+__ST2_LANE_FUNC (int64x1x2_t, int64x2x2_t, int64_t, di, v2di, di, s64,
+		 int64x2_t)
+__ST2_LANE_FUNC (uint8x8x2_t, uint8x16x2_t, uint8_t, v8qi, v16qi, qi, u8,
+		 int8x16_t)
+__ST2_LANE_FUNC (uint16x4x2_t, uint16x8x2_t, uint16_t, v4hi, v8hi, hi, u16,
 		 int16x8_t)
-__ST2_LANE_FUNC (uint32x2x2_t, uint32x4x2_t, uint32_t, v4si, si, u32,
+__ST2_LANE_FUNC (uint32x2x2_t, uint32x4x2_t, uint32_t, v2si, v4si, si, u32,
 		 int32x4_t)
-__ST2_LANE_FUNC (uint64x1x2_t, uint64x2x2_t, uint64_t, v2di, di, u64,
+__ST2_LANE_FUNC (uint64x1x2_t, uint64x2x2_t, uint64_t, di, v2di, di, u64,
 		 int64x2_t)
 
 #undef __ST2_LANE_FUNC
@@ -10286,8 +10024,8 @@ __ST2_LANE_FUNC (uint16x8x2_t, uint16_t, v8hi, hi, u16)
 __ST2_LANE_FUNC (uint32x4x2_t, uint32_t, v4si, si, u32)
 __ST2_LANE_FUNC (uint64x2x2_t, uint64_t, v2di, di, u64)
 
-#define __ST3_LANE_FUNC(intype, largetype, ptrtype,			     \
-			mode, ptr_mode, funcsuffix, signedtype)		     \
+#define __ST3_LANE_FUNC(intype, largetype, ptrtype, mode,		     \
+			qmode, ptr_mode, funcsuffix, signedtype)	     \
 __extension__ static __inline void					     \
 __attribute__ ((__always_inline__))					     \
 vst3_lane_ ## funcsuffix (ptrtype *__ptr,				     \
@@ -10304,33 +10042,39 @@ vst3_lane_ ## funcsuffix (ptrtype *__ptr,				     \
   __temp.val[2]								     \
     = vcombine_##funcsuffix (__b.val[2],				     \
 			     vcreate_##funcsuffix (__AARCH64_UINT64_C (0))); \
-  __o = __builtin_aarch64_set_qregci##mode (__o,			     \
-					    (signedtype) __temp.val[0], 0);  \
-  __o = __builtin_aarch64_set_qregci##mode (__o,			     \
-					    (signedtype) __temp.val[1], 1);  \
-  __o = __builtin_aarch64_set_qregci##mode (__o,			     \
-					    (signedtype) __temp.val[2], 2);  \
+  __o = __builtin_aarch64_set_qregci##qmode (__o,			     \
+					     (signedtype) __temp.val[0], 0); \
+  __o = __builtin_aarch64_set_qregci##qmode (__o,			     \
+					     (signedtype) __temp.val[1], 1); \
+  __o = __builtin_aarch64_set_qregci##qmode (__o,			     \
+					     (signedtype) __temp.val[2], 2); \
   __builtin_aarch64_st3_lane##mode ((__builtin_aarch64_simd_ ## ptr_mode *)  \
 				     __ptr, __o, __c);			     \
 }
 
-__ST3_LANE_FUNC (float32x2x3_t, float32x4x3_t, float32_t, v4sf, sf, f32,
+__ST3_LANE_FUNC (float32x2x3_t, float32x4x3_t, float32_t, v2sf, v4sf, sf, f32,
 		 float32x4_t)
-__ST3_LANE_FUNC (float64x1x3_t, float64x2x3_t, float64_t, v2df, df, f64,
+__ST3_LANE_FUNC (float64x1x3_t, float64x2x3_t, float64_t, df, v2df, df, f64,
 		 float64x2_t)
-__ST3_LANE_FUNC (poly8x8x3_t, poly8x16x3_t, poly8_t, v16qi, qi, p8, int8x16_t)
-__ST3_LANE_FUNC (poly16x4x3_t, poly16x8x3_t, poly16_t, v8hi, hi, p16,
+__ST3_LANE_FUNC (poly8x8x3_t, poly8x16x3_t, poly8_t, v8qi, v16qi, qi, p8,
+		 int8x16_t)
+__ST3_LANE_FUNC (poly16x4x3_t, poly16x8x3_t, poly16_t, v4hi, v8hi, hi, p16,
+		 int16x8_t)
+__ST3_LANE_FUNC (int8x8x3_t, int8x16x3_t, int8_t, v8qi, v16qi, qi, s8,
+		 int8x16_t)
+__ST3_LANE_FUNC (int16x4x3_t, int16x8x3_t, int16_t, v4hi, v8hi, hi, s16,
 		 int16x8_t)
-__ST3_LANE_FUNC (int8x8x3_t, int8x16x3_t, int8_t, v16qi, qi, s8, int8x16_t)
-__ST3_LANE_FUNC (int16x4x3_t, int16x8x3_t, int16_t, v8hi, hi, s16, int16x8_t)
-__ST3_LANE_FUNC (int32x2x3_t, int32x4x3_t, int32_t, v4si, si, s32, int32x4_t)
-__ST3_LANE_FUNC (int64x1x3_t, int64x2x3_t, int64_t, v2di, di, s64, int64x2_t)
-__ST3_LANE_FUNC (uint8x8x3_t, uint8x16x3_t, uint8_t, v16qi, qi, u8, int8x16_t)
-__ST3_LANE_FUNC (uint16x4x3_t, uint16x8x3_t, uint16_t, v8hi, hi, u16,
+__ST3_LANE_FUNC (int32x2x3_t, int32x4x3_t, int32_t, v2si, v4si, si, s32,
+		 int32x4_t)
+__ST3_LANE_FUNC (int64x1x3_t, int64x2x3_t, int64_t, di, v2di, di, s64,
+		 int64x2_t)
+__ST3_LANE_FUNC (uint8x8x3_t, uint8x16x3_t, uint8_t, v8qi, v16qi, qi, u8,
+		 int8x16_t)
+__ST3_LANE_FUNC (uint16x4x3_t, uint16x8x3_t, uint16_t, v4hi, v8hi, hi, u16,
 		 int16x8_t)
-__ST3_LANE_FUNC (uint32x2x3_t, uint32x4x3_t, uint32_t, v4si, si, u32,
+__ST3_LANE_FUNC (uint32x2x3_t, uint32x4x3_t, uint32_t, v2si, v4si, si, u32,
 		 int32x4_t)
-__ST3_LANE_FUNC (uint64x1x3_t, uint64x2x3_t, uint64_t, v2di, di, u64,
+__ST3_LANE_FUNC (uint64x1x3_t, uint64x2x3_t, uint64_t, di, v2di, di, u64,
 		 int64x2_t)
 
 #undef __ST3_LANE_FUNC
@@ -10359,8 +10103,8 @@ __ST3_LANE_FUNC (uint16x8x3_t, uint16_t, v8hi, hi, u16)
 __ST3_LANE_FUNC (uint32x4x3_t, uint32_t, v4si, si, u32)
 __ST3_LANE_FUNC (uint64x2x3_t, uint64_t, v2di, di, u64)
 
-#define __ST4_LANE_FUNC(intype, largetype, ptrtype,			     \
-			mode, ptr_mode, funcsuffix, signedtype)		     \
+#define __ST4_LANE_FUNC(intype, largetype, ptrtype, mode,		     \
+			qmode, ptr_mode, funcsuffix, signedtype)	     \
 __extension__ static __inline void					     \
 __attribute__ ((__always_inline__))					     \
 vst4_lane_ ## funcsuffix (ptrtype *__ptr,				     \
@@ -10380,35 +10124,41 @@ vst4_lane_ ## funcsuffix (ptrtype *__ptr,				     \
   __temp.val[3]								     \
     = vcombine_##funcsuffix (__b.val[3],				     \
 			     vcreate_##funcsuffix (__AARCH64_UINT64_C (0))); \
-  __o = __builtin_aarch64_set_qregxi##mode (__o,			     \
-					    (signedtype) __temp.val[0], 0);  \
-  __o = __builtin_aarch64_set_qregxi##mode (__o,			     \
-					    (signedtype) __temp.val[1], 1);  \
-  __o = __builtin_aarch64_set_qregxi##mode (__o,			     \
-					    (signedtype) __temp.val[2], 2);  \
-  __o = __builtin_aarch64_set_qregxi##mode (__o,			     \
-					    (signedtype) __temp.val[3], 3);  \
+  __o = __builtin_aarch64_set_qregxi##qmode (__o,			     \
+					     (signedtype) __temp.val[0], 0); \
+  __o = __builtin_aarch64_set_qregxi##qmode (__o,			     \
+					     (signedtype) __temp.val[1], 1); \
+  __o = __builtin_aarch64_set_qregxi##qmode (__o,			     \
+					     (signedtype) __temp.val[2], 2); \
+  __o = __builtin_aarch64_set_qregxi##qmode (__o,			     \
+					     (signedtype) __temp.val[3], 3); \
   __builtin_aarch64_st4_lane##mode ((__builtin_aarch64_simd_ ## ptr_mode *)  \
 				     __ptr, __o, __c);			     \
 }
 
-__ST4_LANE_FUNC (float32x2x4_t, float32x4x4_t, float32_t, v4sf, sf, f32,
+__ST4_LANE_FUNC (float32x2x4_t, float32x4x4_t, float32_t, v2sf, v4sf, sf, f32,
 		 float32x4_t)
-__ST4_LANE_FUNC (float64x1x4_t, float64x2x4_t, float64_t, v2df, df, f64,
+__ST4_LANE_FUNC (float64x1x4_t, float64x2x4_t, float64_t, df, v2df, df, f64,
 		 float64x2_t)
-__ST4_LANE_FUNC (poly8x8x4_t, poly8x16x4_t, poly8_t, v16qi, qi, p8, int8x16_t)
-__ST4_LANE_FUNC (poly16x4x4_t, poly16x8x4_t, poly16_t, v8hi, hi, p16,
+__ST4_LANE_FUNC (poly8x8x4_t, poly8x16x4_t, poly8_t, v8qi, v16qi, qi, p8,
+		 int8x16_t)
+__ST4_LANE_FUNC (poly16x4x4_t, poly16x8x4_t, poly16_t, v4hi, v8hi, hi, p16,
 		 int16x8_t)
-__ST4_LANE_FUNC (int8x8x4_t, int8x16x4_t, int8_t, v16qi, qi, s8, int8x16_t)
-__ST4_LANE_FUNC (int16x4x4_t, int16x8x4_t, int16_t, v8hi, hi, s16, int16x8_t)
-__ST4_LANE_FUNC (int32x2x4_t, int32x4x4_t, int32_t, v4si, si, s32, int32x4_t)
-__ST4_LANE_FUNC (int64x1x4_t, int64x2x4_t, int64_t, v2di, di, s64, int64x2_t)
-__ST4_LANE_FUNC (uint8x8x4_t, uint8x16x4_t, uint8_t, v16qi, qi, u8, int8x16_t)
-__ST4_LANE_FUNC (uint16x4x4_t, uint16x8x4_t, uint16_t, v8hi, hi, u16,
+__ST4_LANE_FUNC (int8x8x4_t, int8x16x4_t, int8_t, v8qi, v16qi, qi, s8,
+		 int8x16_t)
+__ST4_LANE_FUNC (int16x4x4_t, int16x8x4_t, int16_t, v4hi, v8hi, hi, s16,
 		 int16x8_t)
-__ST4_LANE_FUNC (uint32x2x4_t, uint32x4x4_t, uint32_t, v4si, si, u32,
+__ST4_LANE_FUNC (int32x2x4_t, int32x4x4_t, int32_t, v2si, v4si, si, s32,
 		 int32x4_t)
-__ST4_LANE_FUNC (uint64x1x4_t, uint64x2x4_t, uint64_t, v2di, di, u64,
+__ST4_LANE_FUNC (int64x1x4_t, int64x2x4_t, int64_t, di, v2di, di, s64,
+		 int64x2_t)
+__ST4_LANE_FUNC (uint8x8x4_t, uint8x16x4_t, uint8_t, v8qi, v16qi, qi, u8,
+		 int8x16_t)
+__ST4_LANE_FUNC (uint16x4x4_t, uint16x8x4_t, uint16_t, v4hi, v8hi, hi, u16,
+		 int16x8_t)
+__ST4_LANE_FUNC (uint32x2x4_t, uint32x4x4_t, uint32_t, v2si, v4si, si, u32,
+		 int32x4_t)
+__ST4_LANE_FUNC (uint64x1x4_t, uint64x2x4_t, uint64_t, di, v2di, di, u64,
 		 int64x2_t)
 
 #undef __ST4_LANE_FUNC
@@ -11668,25 +11418,25 @@ vbslq_u64 (uint64x2_t __a, uint64x2_t __b, uint64x2_t __c)
 
 /* vaes  */
 
-static __inline uint8x16_t
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
 vaeseq_u8 (uint8x16_t data, uint8x16_t key)
 {
   return __builtin_aarch64_crypto_aesev16qi_uuu (data, key);
 }
 
-static __inline uint8x16_t
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
 vaesdq_u8 (uint8x16_t data, uint8x16_t key)
 {
   return __builtin_aarch64_crypto_aesdv16qi_uuu (data, key);
 }
 
-static __inline uint8x16_t
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
 vaesmcq_u8 (uint8x16_t data)
 {
   return __builtin_aarch64_crypto_aesmcv16qi_uu (data);
 }
 
-static __inline uint8x16_t
+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
 vaesimcq_u8 (uint8x16_t data)
 {
   return __builtin_aarch64_crypto_aesimcv16qi_uu (data);
@@ -11887,7 +11637,7 @@ vceq_s32 (int32x2_t __a, int32x2_t __b)
 __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
 vceq_s64 (int64x1_t __a, int64x1_t __b)
 {
-  return (uint64x1_t) {__a[0] == __b[0] ? -1ll : 0ll};
+  return (uint64x1_t) (__a == __b);
 }
 
 __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
@@ -11911,7 +11661,7 @@ vceq_u32 (uint32x2_t __a, uint32x2_t __b)
 __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
 vceq_u64 (uint64x1_t __a, uint64x1_t __b)
 {
-  return (uint64x1_t) {__a[0] == __b[0] ? -1ll : 0ll};
+  return (__a == __b);
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
@@ -12047,7 +11797,7 @@ vceqz_s32 (int32x2_t __a)
 __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
 vceqz_s64 (int64x1_t __a)
 {
-  return (uint64x1_t) {__a[0] == 0ll ? -1ll : 0ll};
+  return (uint64x1_t) (__a == __AARCH64_INT64_C (0));
 }
 
 __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
@@ -12071,7 +11821,7 @@ vceqz_u32 (uint32x2_t __a)
 __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
 vceqz_u64 (uint64x1_t __a)
 {
-  return (uint64x1_t) {__a[0] == 0ll ? -1ll : 0ll};
+  return (__a == __AARCH64_UINT64_C (0));
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
@@ -12201,7 +11951,7 @@ vcge_s32 (int32x2_t __a, int32x2_t __b)
 __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
 vcge_s64 (int64x1_t __a, int64x1_t __b)
 {
-  return (uint64x1_t) {__a[0] >= __b[0] ? -1ll : 0ll};
+  return (uint64x1_t) (__a >= __b);
 }
 
 __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
@@ -12225,7 +11975,7 @@ vcge_u32 (uint32x2_t __a, uint32x2_t __b)
 __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
 vcge_u64 (uint64x1_t __a, uint64x1_t __b)
 {
-  return (uint64x1_t) {__a[0] >= __b[0] ? -1ll : 0ll};
+  return (__a >= __b);
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
@@ -12349,7 +12099,7 @@ vcgez_s32 (int32x2_t __a)
 __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
 vcgez_s64 (int64x1_t __a)
 {
-  return (uint64x1_t) {__a[0] >= 0ll ? -1ll : 0ll};
+  return (uint64x1_t) (__a >= __AARCH64_INT64_C (0));
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
@@ -12443,7 +12193,7 @@ vcgt_s32 (int32x2_t __a, int32x2_t __b)
 __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
 vcgt_s64 (int64x1_t __a, int64x1_t __b)
 {
-  return (uint64x1_t) (__a[0] > __b[0] ? -1ll : 0ll);
+  return (uint64x1_t) (__a > __b);
 }
 
 __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
@@ -12467,7 +12217,7 @@ vcgt_u32 (uint32x2_t __a, uint32x2_t __b)
 __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
 vcgt_u64 (uint64x1_t __a, uint64x1_t __b)
 {
-  return (uint64x1_t) (__a[0] > __b[0] ? -1ll : 0ll);
+  return (__a > __b);
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
@@ -12591,7 +12341,7 @@ vcgtz_s32 (int32x2_t __a)
 __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
 vcgtz_s64 (int64x1_t __a)
 {
-  return (uint64x1_t) {__a[0] > 0ll ? -1ll : 0ll};
+  return (uint64x1_t) (__a > __AARCH64_INT64_C (0));
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
@@ -12685,7 +12435,7 @@ vcle_s32 (int32x2_t __a, int32x2_t __b)
 __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
 vcle_s64 (int64x1_t __a, int64x1_t __b)
 {
-  return (uint64x1_t) {__a[0] <= __b[0] ? -1ll : 0ll};
+  return (uint64x1_t) (__a <= __b);
 }
 
 __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
@@ -12709,7 +12459,7 @@ vcle_u32 (uint32x2_t __a, uint32x2_t __b)
 __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
 vcle_u64 (uint64x1_t __a, uint64x1_t __b)
 {
-  return (uint64x1_t) {__a[0] <= __b[0] ? -1ll : 0ll};
+  return (__a <= __b);
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
@@ -12833,7 +12583,7 @@ vclez_s32 (int32x2_t __a)
 __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
 vclez_s64 (int64x1_t __a)
 {
-  return (uint64x1_t) {__a[0] <= 0ll ? -1ll : 0ll};
+  return (uint64x1_t) (__a <= __AARCH64_INT64_C (0));
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
@@ -12927,7 +12677,7 @@ vclt_s32 (int32x2_t __a, int32x2_t __b)
 __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
 vclt_s64 (int64x1_t __a, int64x1_t __b)
 {
-  return (uint64x1_t) {__a[0] < __b[0] ? -1ll : 0ll};
+  return (uint64x1_t) (__a < __b);
 }
 
 __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
@@ -12951,7 +12701,7 @@ vclt_u32 (uint32x2_t __a, uint32x2_t __b)
 __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
 vclt_u64 (uint64x1_t __a, uint64x1_t __b)
 {
-  return (uint64x1_t) {__a[0] < __b[0] ? -1ll : 0ll};
+  return (__a < __b);
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
@@ -13075,7 +12825,7 @@ vcltz_s32 (int32x2_t __a)
 __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
 vcltz_s64 (int64x1_t __a)
 {
-  return (uint64x1_t) {__a[0] < 0ll ? -1ll : 0ll};
+  return (uint64x1_t) (__a < __AARCH64_INT64_C (0));
 }
 
 __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
@@ -17067,8 +16817,8 @@ vld4q_dup_f64 (const float64_t * __a)
 
 /* vld2_lane */
 
-#define __LD2_LANE_FUNC(intype, vectype, largetype, ptrtype,		   \
-			 mode, ptrmode, funcsuffix, signedtype)		   \
+#define __LD2_LANE_FUNC(intype, vectype, largetype, ptrtype, mode,	   \
+			 qmode, ptrmode, funcsuffix, signedtype)	   \
 __extension__ static __inline intype __attribute__ ((__always_inline__))   \
 vld2_lane_##funcsuffix (const ptrtype * __ptr, intype __b, const int __c)  \
 {									   \
@@ -17078,12 +16828,12 @@ vld2_lane_##funcsuffix (const ptrtype * __ptr, intype __b, const int __c)  \
     vcombine_##funcsuffix (__b.val[0], vcreate_##funcsuffix (0));	   \
   __temp.val[1] =							   \
     vcombine_##funcsuffix (__b.val[1], vcreate_##funcsuffix (0));	   \
-  __o = __builtin_aarch64_set_qregoi##mode (__o,			   \
-					   (signedtype) __temp.val[0],	   \
-					   0);				   \
-  __o = __builtin_aarch64_set_qregoi##mode (__o,			   \
-					   (signedtype) __temp.val[1],	   \
-					   1);				   \
+  __o = __builtin_aarch64_set_qregoi##qmode (__o,			   \
+					    (signedtype) __temp.val[0],	   \
+					    0);				   \
+  __o = __builtin_aarch64_set_qregoi##qmode (__o,			   \
+					    (signedtype) __temp.val[1],	   \
+					    1);				   \
   __o =	__builtin_aarch64_ld2_lane##mode (				   \
 	  (__builtin_aarch64_simd_##ptrmode *) __ptr, __o, __c);	   \
   __b.val[0] = (vectype) __builtin_aarch64_get_dregoidi (__o, 0);	   \
@@ -17091,29 +16841,29 @@ vld2_lane_##funcsuffix (const ptrtype * __ptr, intype __b, const int __c)  \
   return __b;								   \
 }
 
-__LD2_LANE_FUNC (float32x2x2_t, float32x2_t, float32x4x2_t, float32_t, v4sf,
+__LD2_LANE_FUNC (float32x2x2_t, float32x2_t, float32x4x2_t, float32_t, v2sf, v4sf,
 		 sf, f32, float32x4_t)
-__LD2_LANE_FUNC (float64x1x2_t, float64x1_t, float64x2x2_t, float64_t, v2df,
+__LD2_LANE_FUNC (float64x1x2_t, float64x1_t, float64x2x2_t, float64_t, df, v2df,
 		 df, f64, float64x2_t)
-__LD2_LANE_FUNC (poly8x8x2_t, poly8x8_t, poly8x16x2_t, poly8_t, v16qi, qi, p8,
+__LD2_LANE_FUNC (poly8x8x2_t, poly8x8_t, poly8x16x2_t, poly8_t, v8qi, v16qi, qi, p8,
 		 int8x16_t)
-__LD2_LANE_FUNC (poly16x4x2_t, poly16x4_t, poly16x8x2_t, poly16_t, v8hi, hi,
+__LD2_LANE_FUNC (poly16x4x2_t, poly16x4_t, poly16x8x2_t, poly16_t, v4hi, v8hi, hi,
 		 p16, int16x8_t)
-__LD2_LANE_FUNC (int8x8x2_t, int8x8_t, int8x16x2_t, int8_t, v16qi, qi, s8,
+__LD2_LANE_FUNC (int8x8x2_t, int8x8_t, int8x16x2_t, int8_t, v8qi, v16qi, qi, s8,
 		 int8x16_t)
-__LD2_LANE_FUNC (int16x4x2_t, int16x4_t, int16x8x2_t, int16_t, v8hi, hi, s16,
+__LD2_LANE_FUNC (int16x4x2_t, int16x4_t, int16x8x2_t, int16_t, v4hi, v8hi, hi, s16,
 		 int16x8_t)
-__LD2_LANE_FUNC (int32x2x2_t, int32x2_t, int32x4x2_t, int32_t, v4si, si, s32,
+__LD2_LANE_FUNC (int32x2x2_t, int32x2_t, int32x4x2_t, int32_t, v2si, v4si, si, s32,
 		 int32x4_t)
-__LD2_LANE_FUNC (int64x1x2_t, int64x1_t, int64x2x2_t, int64_t, v2di, di, s64,
+__LD2_LANE_FUNC (int64x1x2_t, int64x1_t, int64x2x2_t, int64_t, di, v2di, di, s64,
 		 int64x2_t)
-__LD2_LANE_FUNC (uint8x8x2_t, uint8x8_t, uint8x16x2_t, uint8_t, v16qi, qi, u8,
+__LD2_LANE_FUNC (uint8x8x2_t, uint8x8_t, uint8x16x2_t, uint8_t, v8qi, v16qi, qi, u8,
 		 int8x16_t)
-__LD2_LANE_FUNC (uint16x4x2_t, uint16x4_t, uint16x8x2_t, uint16_t, v8hi, hi,
+__LD2_LANE_FUNC (uint16x4x2_t, uint16x4_t, uint16x8x2_t, uint16_t, v4hi, v8hi, hi,
 		 u16, int16x8_t)
-__LD2_LANE_FUNC (uint32x2x2_t, uint32x2_t, uint32x4x2_t, uint32_t, v4si, si,
+__LD2_LANE_FUNC (uint32x2x2_t, uint32x2_t, uint32x4x2_t, uint32_t, v2si, v4si, si,
 		 u32, int32x4_t)
-__LD2_LANE_FUNC (uint64x1x2_t, uint64x1_t, uint64x2x2_t, uint64_t, v2di, di,
+__LD2_LANE_FUNC (uint64x1x2_t, uint64x1_t, uint64x2x2_t, uint64_t, di, v2di, di,
 		 u64, int64x2_t)
 
 #undef __LD2_LANE_FUNC
@@ -17152,8 +16902,8 @@ __LD2_LANE_FUNC (uint64x2x2_t, uint64x2_t, uint64_t, v2di, di, u64)
 
 /* vld3_lane */
 
-#define __LD3_LANE_FUNC(intype, vectype, largetype, ptrtype,		   \
-			 mode, ptrmode, funcsuffix, signedtype)		   \
+#define __LD3_LANE_FUNC(intype, vectype, largetype, ptrtype, mode,	   \
+			 qmode, ptrmode, funcsuffix, signedtype)	   \
 __extension__ static __inline intype __attribute__ ((__always_inline__))   \
 vld3_lane_##funcsuffix (const ptrtype * __ptr, intype __b, const int __c)  \
 {									   \
@@ -17165,15 +16915,15 @@ vld3_lane_##funcsuffix (const ptrtype * __ptr, intype __b, const int __c)  \
     vcombine_##funcsuffix (__b.val[1], vcreate_##funcsuffix (0));	   \
   __temp.val[2] =							   \
     vcombine_##funcsuffix (__b.val[2], vcreate_##funcsuffix (0));	   \
-  __o = __builtin_aarch64_set_qregci##mode (__o,			   \
-					   (signedtype) __temp.val[0],	   \
-					   0);				   \
-  __o = __builtin_aarch64_set_qregci##mode (__o,			   \
-					   (signedtype) __temp.val[1],	   \
-					   1);				   \
-  __o = __builtin_aarch64_set_qregci##mode (__o,			   \
-					   (signedtype) __temp.val[2],	   \
-					   2);				   \
+  __o = __builtin_aarch64_set_qregci##qmode (__o,			   \
+					    (signedtype) __temp.val[0],	   \
+					    0);				   \
+  __o = __builtin_aarch64_set_qregci##qmode (__o,			   \
+					    (signedtype) __temp.val[1],	   \
+					    1);				   \
+  __o = __builtin_aarch64_set_qregci##qmode (__o,			   \
+					    (signedtype) __temp.val[2],	   \
+					    2);				   \
   __o =	__builtin_aarch64_ld3_lane##mode (				   \
 	  (__builtin_aarch64_simd_##ptrmode *) __ptr, __o, __c);	   \
   __b.val[0] = (vectype) __builtin_aarch64_get_dregcidi (__o, 0);	   \
@@ -17182,29 +16932,29 @@ vld3_lane_##funcsuffix (const ptrtype * __ptr, intype __b, const int __c)  \
   return __b;								   \
 }
 
-__LD3_LANE_FUNC (float32x2x3_t, float32x2_t, float32x4x3_t, float32_t, v4sf,
+__LD3_LANE_FUNC (float32x2x3_t, float32x2_t, float32x4x3_t, float32_t, v2sf, v4sf,
 		 sf, f32, float32x4_t)
-__LD3_LANE_FUNC (float64x1x3_t, float64x1_t, float64x2x3_t, float64_t, v2df,
+__LD3_LANE_FUNC (float64x1x3_t, float64x1_t, float64x2x3_t, float64_t, df, v2df,
 		 df, f64, float64x2_t)
-__LD3_LANE_FUNC (poly8x8x3_t, poly8x8_t, poly8x16x3_t, poly8_t, v16qi, qi, p8,
+__LD3_LANE_FUNC (poly8x8x3_t, poly8x8_t, poly8x16x3_t, poly8_t, v8qi, v16qi, qi, p8,
 		 int8x16_t)
-__LD3_LANE_FUNC (poly16x4x3_t, poly16x4_t, poly16x8x3_t, poly16_t, v8hi, hi,
+__LD3_LANE_FUNC (poly16x4x3_t, poly16x4_t, poly16x8x3_t, poly16_t, v4hi, v8hi, hi,
 		 p16, int16x8_t)
-__LD3_LANE_FUNC (int8x8x3_t, int8x8_t, int8x16x3_t, int8_t, v16qi, qi, s8,
+__LD3_LANE_FUNC (int8x8x3_t, int8x8_t, int8x16x3_t, int8_t, v8qi, v16qi, qi, s8,
 		 int8x16_t)
-__LD3_LANE_FUNC (int16x4x3_t, int16x4_t, int16x8x3_t, int16_t, v8hi, hi, s16,
+__LD3_LANE_FUNC (int16x4x3_t, int16x4_t, int16x8x3_t, int16_t, v4hi, v8hi, hi, s16,
 		 int16x8_t)
-__LD3_LANE_FUNC (int32x2x3_t, int32x2_t, int32x4x3_t, int32_t, v4si, si, s32,
+__LD3_LANE_FUNC (int32x2x3_t, int32x2_t, int32x4x3_t, int32_t, v2si, v4si, si, s32,
 		 int32x4_t)
-__LD3_LANE_FUNC (int64x1x3_t, int64x1_t, int64x2x3_t, int64_t, v2di, di, s64,
+__LD3_LANE_FUNC (int64x1x3_t, int64x1_t, int64x2x3_t, int64_t, di, v2di, di, s64,
 		 int64x2_t)
-__LD3_LANE_FUNC (uint8x8x3_t, uint8x8_t, uint8x16x3_t, uint8_t, v16qi, qi, u8,
+__LD3_LANE_FUNC (uint8x8x3_t, uint8x8_t, uint8x16x3_t, uint8_t, v8qi, v16qi, qi, u8,
 		 int8x16_t)
-__LD3_LANE_FUNC (uint16x4x3_t, uint16x4_t, uint16x8x3_t, uint16_t, v8hi, hi,
+__LD3_LANE_FUNC (uint16x4x3_t, uint16x4_t, uint16x8x3_t, uint16_t, v4hi, v8hi, hi,
 		 u16, int16x8_t)
-__LD3_LANE_FUNC (uint32x2x3_t, uint32x2_t, uint32x4x3_t, uint32_t, v4si, si,
+__LD3_LANE_FUNC (uint32x2x3_t, uint32x2_t, uint32x4x3_t, uint32_t, v2si, v4si, si,
 		 u32, int32x4_t)
-__LD3_LANE_FUNC (uint64x1x3_t, uint64x1_t, uint64x2x3_t, uint64_t, v2di, di,
+__LD3_LANE_FUNC (uint64x1x3_t, uint64x1_t, uint64x2x3_t, uint64_t, di, v2di, di,
 		 u64, int64x2_t)
 
 #undef __LD3_LANE_FUNC
@@ -17245,8 +16995,8 @@ __LD3_LANE_FUNC (uint64x2x3_t, uint64x2_t, uint64_t, v2di, di, u64)
 
 /* vld4_lane */
 
-#define __LD4_LANE_FUNC(intype, vectype, largetype, ptrtype,		   \
-			 mode, ptrmode, funcsuffix, signedtype)		   \
+#define __LD4_LANE_FUNC(intype, vectype, largetype, ptrtype, mode,	   \
+			 qmode, ptrmode, funcsuffix, signedtype)	   \
 __extension__ static __inline intype __attribute__ ((__always_inline__))   \
 vld4_lane_##funcsuffix (const ptrtype * __ptr, intype __b, const int __c)  \
 {									   \
@@ -17260,18 +17010,18 @@ vld4_lane_##funcsuffix (const ptrtype * __ptr, intype __b, const int __c)  \
     vcombine_##funcsuffix (__b.val[2], vcreate_##funcsuffix (0));	   \
   __temp.val[3] =							   \
     vcombine_##funcsuffix (__b.val[3], vcreate_##funcsuffix (0));	   \
-  __o = __builtin_aarch64_set_qregxi##mode (__o,			   \
-					   (signedtype) __temp.val[0],	   \
-					   0);				   \
-  __o = __builtin_aarch64_set_qregxi##mode (__o,			   \
-					   (signedtype) __temp.val[1],	   \
-					   1);				   \
-  __o = __builtin_aarch64_set_qregxi##mode (__o,			   \
-					   (signedtype) __temp.val[2],	   \
-					   2);				   \
-  __o = __builtin_aarch64_set_qregxi##mode (__o,			   \
-					   (signedtype) __temp.val[3],	   \
-					   3);				   \
+  __o = __builtin_aarch64_set_qregxi##qmode (__o,			   \
+					    (signedtype) __temp.val[0],	   \
+					    0);				   \
+  __o = __builtin_aarch64_set_qregxi##qmode (__o,			   \
+					    (signedtype) __temp.val[1],	   \
+					    1);				   \
+  __o = __builtin_aarch64_set_qregxi##qmode (__o,			   \
+					    (signedtype) __temp.val[2],	   \
+					    2);				   \
+  __o = __builtin_aarch64_set_qregxi##qmode (__o,			   \
+					    (signedtype) __temp.val[3],	   \
+					    3);				   \
   __o =	__builtin_aarch64_ld4_lane##mode (				   \
 	  (__builtin_aarch64_simd_##ptrmode *) __ptr, __o, __c);	   \
   __b.val[0] = (vectype) __builtin_aarch64_get_dregxidi (__o, 0);	   \
@@ -17283,29 +17033,29 @@ vld4_lane_##funcsuffix (const ptrtype * __ptr, intype __b, const int __c)  \
 
 /* vld4q_lane */
 
-__LD4_LANE_FUNC (float32x2x4_t, float32x2_t, float32x4x4_t, float32_t, v4sf,
+__LD4_LANE_FUNC (float32x2x4_t, float32x2_t, float32x4x4_t, float32_t, v2sf, v4sf,
 		 sf, f32, float32x4_t)
-__LD4_LANE_FUNC (float64x1x4_t, float64x1_t, float64x2x4_t, float64_t, v2df,
+__LD4_LANE_FUNC (float64x1x4_t, float64x1_t, float64x2x4_t, float64_t, df, v2df,
 		 df, f64, float64x2_t)
-__LD4_LANE_FUNC (poly8x8x4_t, poly8x8_t, poly8x16x4_t, poly8_t, v16qi, qi, p8,
+__LD4_LANE_FUNC (poly8x8x4_t, poly8x8_t, poly8x16x4_t, poly8_t, v8qi, v16qi, qi, p8,
 		 int8x16_t)
-__LD4_LANE_FUNC (poly16x4x4_t, poly16x4_t, poly16x8x4_t, poly16_t, v8hi, hi,
+__LD4_LANE_FUNC (poly16x4x4_t, poly16x4_t, poly16x8x4_t, poly16_t, v4hi, v8hi, hi,
 		 p16, int16x8_t)
-__LD4_LANE_FUNC (int8x8x4_t, int8x8_t, int8x16x4_t, int8_t, v16qi, qi, s8,
+__LD4_LANE_FUNC (int8x8x4_t, int8x8_t, int8x16x4_t, int8_t, v8qi, v16qi, qi, s8,
 		 int8x16_t)
-__LD4_LANE_FUNC (int16x4x4_t, int16x4_t, int16x8x4_t, int16_t, v8hi, hi, s16,
+__LD4_LANE_FUNC (int16x4x4_t, int16x4_t, int16x8x4_t, int16_t, v4hi, v8hi, hi, s16,
 		 int16x8_t)
-__LD4_LANE_FUNC (int32x2x4_t, int32x2_t, int32x4x4_t, int32_t, v4si, si, s32,
+__LD4_LANE_FUNC (int32x2x4_t, int32x2_t, int32x4x4_t, int32_t, v2si, v4si, si, s32,
 		 int32x4_t)
-__LD4_LANE_FUNC (int64x1x4_t, int64x1_t, int64x2x4_t, int64_t, v2di, di, s64,
+__LD4_LANE_FUNC (int64x1x4_t, int64x1_t, int64x2x4_t, int64_t, di, v2di, di, s64,
 		 int64x2_t)
-__LD4_LANE_FUNC (uint8x8x4_t, uint8x8_t, uint8x16x4_t, uint8_t, v16qi, qi, u8,
+__LD4_LANE_FUNC (uint8x8x4_t, uint8x8_t, uint8x16x4_t, uint8_t, v8qi, v16qi, qi, u8,
 		 int8x16_t)
-__LD4_LANE_FUNC (uint16x4x4_t, uint16x4_t, uint16x8x4_t, uint16_t, v8hi, hi,
+__LD4_LANE_FUNC (uint16x4x4_t, uint16x4_t, uint16x8x4_t, uint16_t, v4hi, v8hi, hi,
 		 u16, int16x8_t)
-__LD4_LANE_FUNC (uint32x2x4_t, uint32x2_t, uint32x4x4_t, uint32_t, v4si, si,
+__LD4_LANE_FUNC (uint32x2x4_t, uint32x2_t, uint32x4x4_t, uint32_t, v2si, v4si, si,
 		 u32, int32x4_t)
-__LD4_LANE_FUNC (uint64x1x4_t, uint64x1_t, uint64x2x4_t, uint64_t, v2di, di,
+__LD4_LANE_FUNC (uint64x1x4_t, uint64x1_t, uint64x2x4_t, uint64_t, di, v2di, di,
 		 u64, int64x2_t)
 
 #undef __LD4_LANE_FUNC
@@ -21321,72 +21071,74 @@ vrsrad_n_u64 (uint64_t __a, uint64_t __b, const int __c)
 
 /* vsha1  */
 
-static __inline uint32x4_t
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
 vsha1cq_u32 (uint32x4_t hash_abcd, uint32_t hash_e, uint32x4_t wk)
 {
   return __builtin_aarch64_crypto_sha1cv4si_uuuu (hash_abcd, hash_e, wk);
 }
-static __inline uint32x4_t
+
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
 vsha1mq_u32 (uint32x4_t hash_abcd, uint32_t hash_e, uint32x4_t wk)
 {
   return __builtin_aarch64_crypto_sha1mv4si_uuuu (hash_abcd, hash_e, wk);
 }
-static __inline uint32x4_t
+
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
 vsha1pq_u32 (uint32x4_t hash_abcd, uint32_t hash_e, uint32x4_t wk)
 {
   return __builtin_aarch64_crypto_sha1pv4si_uuuu (hash_abcd, hash_e, wk);
 }
 
-static __inline uint32_t
+__extension__ static __inline uint32_t __attribute__ ((__always_inline__))
 vsha1h_u32 (uint32_t hash_e)
 {
   return __builtin_aarch64_crypto_sha1hsi_uu (hash_e);
 }
 
-static __inline uint32x4_t
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
 vsha1su0q_u32 (uint32x4_t w0_3, uint32x4_t w4_7, uint32x4_t w8_11)
 {
   return __builtin_aarch64_crypto_sha1su0v4si_uuuu (w0_3, w4_7, w8_11);
 }
 
-static __inline uint32x4_t
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
 vsha1su1q_u32 (uint32x4_t tw0_3, uint32x4_t w12_15)
 {
   return __builtin_aarch64_crypto_sha1su1v4si_uuu (tw0_3, w12_15);
 }
 
-static __inline uint32x4_t
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
 vsha256hq_u32 (uint32x4_t hash_abcd, uint32x4_t hash_efgh, uint32x4_t wk)
 {
   return __builtin_aarch64_crypto_sha256hv4si_uuuu (hash_abcd, hash_efgh, wk);
 }
 
-static __inline uint32x4_t
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
 vsha256h2q_u32 (uint32x4_t hash_efgh, uint32x4_t hash_abcd, uint32x4_t wk)
 {
   return __builtin_aarch64_crypto_sha256h2v4si_uuuu (hash_efgh, hash_abcd, wk);
 }
 
-static __inline uint32x4_t
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
 vsha256su0q_u32 (uint32x4_t w0_3, uint32x4_t w4_7)
 {
   return __builtin_aarch64_crypto_sha256su0v4si_uuu (w0_3, w4_7);
 }
 
-static __inline uint32x4_t
+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))
 vsha256su1q_u32 (uint32x4_t tw0_3, uint32x4_t w8_11, uint32x4_t w12_15)
 {
   return __builtin_aarch64_crypto_sha256su1v4si_uuuu (tw0_3, w8_11, w12_15);
 }
 
-static __inline poly128_t
+__extension__ static __inline poly128_t __attribute__ ((__always_inline__))
 vmull_p64 (poly64_t a, poly64_t b)
 {
   return
     __builtin_aarch64_crypto_pmulldi_ppp (a, b);
 }
 
-static __inline poly128_t
+__extension__ static __inline poly128_t __attribute__ ((__always_inline__))
 vmull_high_p64 (poly64x2_t a, poly64x2_t b)
 {
   return __builtin_aarch64_crypto_pmullv2di_ppp (a, b);
@@ -22302,6 +22054,8 @@ vst1_u64 (uint64_t *a, uint64x1_t b)
   *a = b[0];
 }
 
+/* vst1q */
+
 __extension__ static __inline void __attribute__ ((__always_inline__))
 vst1q_f32 (float32_t *a, float32x4_t b)
 {
@@ -22314,8 +22068,6 @@ vst1q_f64 (float64_t *a, float64x2_t b)
   __builtin_aarch64_st1v2df ((__builtin_aarch64_simd_df *) a, b);
 }
 
-/* vst1q */
-
 __extension__ static __inline void __attribute__ ((__always_inline__))
 vst1q_p8 (poly8_t *a, poly8x16_t b)
 {
@@ -22382,6 +22134,154 @@ vst1q_u64 (uint64_t *a, uint64x2_t b)
 			     (int64x2_t) b);
 }
 
+/* vst1_lane */
+
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst1_lane_f32 (float32_t *__a, float32x2_t __b, const int __lane)
+{
+  *__a = __aarch64_vget_lane_any (__b, __lane);
+}
+
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst1_lane_f64 (float64_t *__a, float64x1_t __b, const int __lane)
+{
+  *__a = __aarch64_vget_lane_any (__b, __lane);
+}
+
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst1_lane_p8 (poly8_t *__a, poly8x8_t __b, const int __lane)
+{
+  *__a = __aarch64_vget_lane_any (__b, __lane);
+}
+
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst1_lane_p16 (poly16_t *__a, poly16x4_t __b, const int __lane)
+{
+  *__a = __aarch64_vget_lane_any (__b, __lane);
+}
+
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst1_lane_s8 (int8_t *__a, int8x8_t __b, const int __lane)
+{
+  *__a = __aarch64_vget_lane_any (__b, __lane);
+}
+
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst1_lane_s16 (int16_t *__a, int16x4_t __b, const int __lane)
+{
+  *__a = __aarch64_vget_lane_any (__b, __lane);
+}
+
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst1_lane_s32 (int32_t *__a, int32x2_t __b, const int __lane)
+{
+  *__a = __aarch64_vget_lane_any (__b, __lane);
+}
+
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst1_lane_s64 (int64_t *__a, int64x1_t __b, const int __lane)
+{
+  *__a = __aarch64_vget_lane_any (__b, __lane);
+}
+
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst1_lane_u8 (uint8_t *__a, uint8x8_t __b, const int __lane)
+{
+  *__a = __aarch64_vget_lane_any (__b, __lane);
+}
+
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst1_lane_u16 (uint16_t *__a, uint16x4_t __b, const int __lane)
+{
+  *__a = __aarch64_vget_lane_any (__b, __lane);
+}
+
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst1_lane_u32 (uint32_t *__a, uint32x2_t __b, const int __lane)
+{
+  *__a = __aarch64_vget_lane_any (__b, __lane);
+}
+
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst1_lane_u64 (uint64_t *__a, uint64x1_t __b, const int __lane)
+{
+  *__a = __aarch64_vget_lane_any (__b, __lane);
+}
+
+/* vst1q_lane */
+
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst1q_lane_f32 (float32_t *__a, float32x4_t __b, const int __lane)
+{
+  *__a = __aarch64_vget_lane_any (__b, __lane);
+}
+
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst1q_lane_f64 (float64_t *__a, float64x2_t __b, const int __lane)
+{
+  *__a = __aarch64_vget_lane_any (__b, __lane);
+}
+
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst1q_lane_p8 (poly8_t *__a, poly8x16_t __b, const int __lane)
+{
+  *__a = __aarch64_vget_lane_any (__b, __lane);
+}
+
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst1q_lane_p16 (poly16_t *__a, poly16x8_t __b, const int __lane)
+{
+  *__a = __aarch64_vget_lane_any (__b, __lane);
+}
+
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst1q_lane_s8 (int8_t *__a, int8x16_t __b, const int __lane)
+{
+  *__a = __aarch64_vget_lane_any (__b, __lane);
+}
+
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst1q_lane_s16 (int16_t *__a, int16x8_t __b, const int __lane)
+{
+  *__a = __aarch64_vget_lane_any (__b, __lane);
+}
+
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst1q_lane_s32 (int32_t *__a, int32x4_t __b, const int __lane)
+{
+  *__a = __aarch64_vget_lane_any (__b, __lane);
+}
+
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst1q_lane_s64 (int64_t *__a, int64x2_t __b, const int __lane)
+{
+  *__a = __aarch64_vget_lane_any (__b, __lane);
+}
+
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst1q_lane_u8 (uint8_t *__a, uint8x16_t __b, const int __lane)
+{
+  *__a = __aarch64_vget_lane_any (__b, __lane);
+}
+
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst1q_lane_u16 (uint16_t *__a, uint16x8_t __b, const int __lane)
+{
+  *__a = __aarch64_vget_lane_any (__b, __lane);
+}
+
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst1q_lane_u32 (uint32_t *__a, uint32x4_t __b, const int __lane)
+{
+  *__a = __aarch64_vget_lane_any (__b, __lane);
+}
+
+__extension__ static __inline void __attribute__ ((__always_inline__))
+vst1q_lane_u64 (uint64_t *__a, uint64x2_t __b, const int __lane)
+{
+  *__a = __aarch64_vget_lane_any (__b, __lane);
+}
+
 /* vstn */
 
 __extension__ static __inline void
@@ -23887,7 +23787,7 @@ vtst_s32 (int32x2_t __a, int32x2_t __b)
 __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
 vtst_s64 (int64x1_t __a, int64x1_t __b)
 {
-  return (uint64x1_t) {(__a[0] & __b[0]) ? -1ll : 0ll};
+  return (uint64x1_t) ((__a & __b) != __AARCH64_INT64_C (0));
 }
 
 __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))
@@ -23911,7 +23811,7 @@ vtst_u32 (uint32x2_t __a, uint32x2_t __b)
 __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))
 vtst_u64 (uint64x1_t __a, uint64x1_t __b)
 {
-  return (uint64x1_t) {(__a[0] & __b[0]) ? -1ll : 0ll};
+  return ((__a & __b) != __AARCH64_UINT64_C (0));
 }
 
 __extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))
--- a/src/gcc/config/aarch64/atomics.md
+++ b/src/gcc/config/aarch64/atomics.md
@@ -26,9 +26,28 @@
     UNSPECV_STL				; Represent an atomic store or store-release.
     UNSPECV_ATOMIC_CMPSW		; Represent an atomic compare swap.
     UNSPECV_ATOMIC_EXCHG		; Represent an atomic exchange.
+    UNSPECV_ATOMIC_CAS			; Represent an atomic CAS.
+    UNSPECV_ATOMIC_SWP			; Represent an atomic SWP.
     UNSPECV_ATOMIC_OP			; Represent an atomic operation.
+    UNSPECV_ATOMIC_LDOP			; Represent an atomic load-operation
+    UNSPECV_ATOMIC_LDOP_OR		; Represent an atomic load-or
+    UNSPECV_ATOMIC_LDOP_BIC		; Represent an atomic load-bic
+    UNSPECV_ATOMIC_LDOP_XOR		; Represent an atomic load-xor
+    UNSPECV_ATOMIC_LDOP_PLUS		; Represent an atomic load-add
 ])
 
+;; Iterators for load-operate instructions.
+
+(define_int_iterator ATOMIC_LDOP
+ [UNSPECV_ATOMIC_LDOP_OR UNSPECV_ATOMIC_LDOP_BIC
+  UNSPECV_ATOMIC_LDOP_XOR UNSPECV_ATOMIC_LDOP_PLUS])
+
+(define_int_attr atomic_ldop
+ [(UNSPECV_ATOMIC_LDOP_OR "set") (UNSPECV_ATOMIC_LDOP_BIC "clr")
+  (UNSPECV_ATOMIC_LDOP_XOR "eor") (UNSPECV_ATOMIC_LDOP_PLUS "add")])
+
+;; Instruction patterns.
+
 (define_expand "atomic_compare_and_swap<mode>"
   [(match_operand:SI 0 "register_operand" "")			;; bool out
    (match_operand:ALLI 1 "register_operand" "")			;; val out
@@ -45,10 +64,10 @@
   }
 )
 
-(define_insn_and_split "atomic_compare_and_swap<mode>_1"
+(define_insn_and_split "aarch64_compare_and_swap<mode>"
   [(set (reg:CC CC_REGNUM)					;; bool out
     (unspec_volatile:CC [(const_int 0)] UNSPECV_ATOMIC_CMPSW))
-   (set (match_operand:SI 0 "register_operand" "=&r")		;; val out
+   (set (match_operand:SI 0 "register_operand" "=&r")	   ;; val out
     (zero_extend:SI
       (match_operand:SHORT 1 "aarch64_sync_memory_operand" "+Q"))) ;; memory
    (set (match_dup 1)
@@ -57,7 +76,7 @@
        (match_operand:SHORT 3 "register_operand" "r")	;; desired
        (match_operand:SI 4 "const_int_operand")		;; is_weak
        (match_operand:SI 5 "const_int_operand")		;; mod_s
-       (match_operand:SI 6 "const_int_operand")]		;; mod_f
+       (match_operand:SI 6 "const_int_operand")]	;; mod_f
       UNSPECV_ATOMIC_CMPSW))
    (clobber (match_scratch:SI 7 "=&r"))]
   ""
@@ -70,17 +89,17 @@
   }
 )
 
-(define_insn_and_split "atomic_compare_and_swap<mode>_1"
+(define_insn_and_split "aarch64_compare_and_swap<mode>"
   [(set (reg:CC CC_REGNUM)					;; bool out
     (unspec_volatile:CC [(const_int 0)] UNSPECV_ATOMIC_CMPSW))
    (set (match_operand:GPI 0 "register_operand" "=&r")		;; val out
-    (match_operand:GPI 1 "aarch64_sync_memory_operand" "+Q")) ;; memory
+    (match_operand:GPI 1 "aarch64_sync_memory_operand" "+Q"))   ;; memory
    (set (match_dup 1)
     (unspec_volatile:GPI
       [(match_operand:GPI 2 "aarch64_plus_operand" "rI")	;; expect
        (match_operand:GPI 3 "register_operand" "r")		;; desired
-       (match_operand:SI 4 "const_int_operand")		;; is_weak
-       (match_operand:SI 5 "const_int_operand")		;; mod_s
+       (match_operand:SI 4 "const_int_operand")			;; is_weak
+       (match_operand:SI 5 "const_int_operand")			;; mod_s
        (match_operand:SI 6 "const_int_operand")]		;; mod_f
       UNSPECV_ATOMIC_CMPSW))
    (clobber (match_scratch:SI 7 "=&r"))]
@@ -94,7 +113,79 @@
   }
 )
 
-(define_insn_and_split "atomic_exchange<mode>"
+(define_insn_and_split "aarch64_compare_and_swap<mode>_lse"
+  [(set (reg:CC CC_REGNUM)					;; bool out
+    (unspec_volatile:CC [(const_int 0)] UNSPECV_ATOMIC_CMPSW))
+   (set (match_operand:SI 0 "register_operand" "=&r")		;; val out
+    (zero_extend:SI
+      (match_operand:SHORT 1 "aarch64_sync_memory_operand" "+Q"))) ;; memory
+   (set (match_dup 1)
+    (unspec_volatile:SHORT
+      [(match_operand:SI 2 "aarch64_plus_operand" "rI")	;; expected
+       (match_operand:SHORT 3 "register_operand" "r")	;; desired
+       (match_operand:SI 4 "const_int_operand")		;; is_weak
+       (match_operand:SI 5 "const_int_operand")		;; mod_s
+       (match_operand:SI 6 "const_int_operand")]	;; mod_f
+      UNSPECV_ATOMIC_CMPSW))]
+  "TARGET_LSE"
+  "#"
+  "&& reload_completed"
+  [(const_int 0)]
+  {
+    aarch64_gen_atomic_cas (operands[0], operands[1],
+			    operands[2], operands[3],
+			    operands[5]);
+    DONE;
+  }
+)
+
+(define_insn_and_split "aarch64_compare_and_swap<mode>_lse"
+  [(set (reg:CC CC_REGNUM)					;; bool out
+    (unspec_volatile:CC [(const_int 0)] UNSPECV_ATOMIC_CMPSW))
+   (set (match_operand:GPI 0 "register_operand" "=&r")		;; val out
+    (match_operand:GPI 1 "aarch64_sync_memory_operand" "+Q"))   ;; memory
+   (set (match_dup 1)
+    (unspec_volatile:GPI
+      [(match_operand:GPI 2 "aarch64_plus_operand" "rI")	;; expect
+       (match_operand:GPI 3 "register_operand" "r")		;; desired
+       (match_operand:SI 4 "const_int_operand")			;; is_weak
+       (match_operand:SI 5 "const_int_operand")			;; mod_s
+       (match_operand:SI 6 "const_int_operand")]		;; mod_f
+      UNSPECV_ATOMIC_CMPSW))]
+  "TARGET_LSE"
+  "#"
+  "&& reload_completed"
+  [(const_int 0)]
+  {
+    aarch64_gen_atomic_cas (operands[0], operands[1],
+			    operands[2], operands[3],
+			    operands[5]);
+    DONE;
+  }
+)
+
+(define_expand "atomic_exchange<mode>"
+ [(match_operand:ALLI 0 "register_operand" "")
+  (match_operand:ALLI 1 "aarch64_sync_memory_operand" "")
+  (match_operand:ALLI 2 "register_operand" "")
+  (match_operand:SI 3 "const_int_operand" "")]
+  ""
+  {
+    rtx (*gen) (rtx, rtx, rtx, rtx);
+
+    /* Use an atomic SWP when available.  */
+    if (TARGET_LSE)
+      gen = gen_aarch64_atomic_exchange<mode>_lse;
+    else
+      gen = gen_aarch64_atomic_exchange<mode>;
+
+    emit_insn (gen (operands[0], operands[1], operands[2], operands[3]));
+
+    DONE;
+  }
+)
+
+(define_insn_and_split "aarch64_atomic_exchange<mode>"
   [(set (match_operand:ALLI 0 "register_operand" "=&r")		;; output
     (match_operand:ALLI 1 "aarch64_sync_memory_operand" "+Q")) ;; memory
    (set (match_dup 1)
@@ -110,28 +201,87 @@
   [(const_int 0)]
   {
     aarch64_split_atomic_op (SET, operands[0], NULL, operands[1],
-			    operands[2], operands[3], operands[4]);
+			     operands[2], operands[3], operands[4]);
+    DONE;
+  }
+)
+
+(define_insn_and_split "aarch64_atomic_exchange<mode>_lse"
+  [(set (match_operand:ALLI 0 "register_operand" "=&r")
+    (match_operand:ALLI 1 "aarch64_sync_memory_operand" "+Q"))
+   (set (match_dup 1)
+    (unspec_volatile:ALLI
+      [(match_operand:ALLI 2 "register_operand" "r")
+       (match_operand:SI 3 "const_int_operand" "")]
+      UNSPECV_ATOMIC_EXCHG))]
+  "TARGET_LSE"
+  "#"
+  "&& reload_completed"
+  [(const_int 0)]
+  {
+    aarch64_gen_atomic_ldop (SET, operands[0], NULL, operands[1],
+			     operands[2], operands[3]);
+    DONE;
+  }
+)
+
+(define_expand "atomic_<atomic_optab><mode>"
+ [(match_operand:ALLI 0 "aarch64_sync_memory_operand" "")
+  (atomic_op:ALLI
+   (match_operand:ALLI 1 "<atomic_op_operand>" "")
+   (match_operand:SI 2 "const_int_operand"))]
+  ""
+  {
+    rtx (*gen) (rtx, rtx, rtx);
+
+    /* Use an atomic load-operate instruction when possible.  */
+    if (aarch64_atomic_ldop_supported_p (<CODE>))
+      gen = gen_aarch64_atomic_<atomic_optab><mode>_lse;
+    else
+      gen = gen_aarch64_atomic_<atomic_optab><mode>;
+
+    emit_insn (gen (operands[0], operands[1], operands[2]));
+
     DONE;
   }
 )
 
-(define_insn_and_split "atomic_<atomic_optab><mode>"
+(define_insn_and_split "aarch64_atomic_<atomic_optab><mode>"
+ [(set (match_operand:ALLI 0 "aarch64_sync_memory_operand" "+Q")
+   (unspec_volatile:ALLI
+    [(atomic_op:ALLI (match_dup 0)
+      (match_operand:ALLI 1 "<atomic_op_operand>" "r<const_atomic>"))
+     (match_operand:SI 2 "const_int_operand")]
+    UNSPECV_ATOMIC_OP))
+  (clobber (reg:CC CC_REGNUM))
+  (clobber (match_scratch:ALLI 3 "=&r"))
+  (clobber (match_scratch:SI 4 "=&r"))]
+  ""
+  "#"
+  "&& reload_completed"
+  [(const_int 0)]
+  {
+    aarch64_split_atomic_op (<CODE>, NULL, operands[3], operands[0],
+			     operands[1], operands[2], operands[4]);
+    DONE;
+  }
+)
+
+(define_insn_and_split "aarch64_atomic_<atomic_optab><mode>_lse"
   [(set (match_operand:ALLI 0 "aarch64_sync_memory_operand" "+Q")
     (unspec_volatile:ALLI
       [(atomic_op:ALLI (match_dup 0)
 	(match_operand:ALLI 1 "<atomic_op_operand>" "r<const_atomic>"))
-       (match_operand:SI 2 "const_int_operand")]		;; model
+       (match_operand:SI 2 "const_int_operand")]
       UNSPECV_ATOMIC_OP))
-       (clobber (reg:CC CC_REGNUM))
-   (clobber (match_scratch:ALLI 3 "=&r"))
-   (clobber (match_scratch:SI 4 "=&r"))]
-  ""
+   (clobber (match_scratch:ALLI 3 "=&r"))]
+  "TARGET_LSE"
   "#"
   "&& reload_completed"
   [(const_int 0)]
   {
-    aarch64_split_atomic_op (<CODE>, NULL, operands[3], operands[0],
-			    operands[1], operands[2], operands[4]);
+    aarch64_gen_atomic_ldop (<CODE>, operands[3], NULL, operands[0],
+			     operands[1], operands[2]);
     DONE;
   }
 )
@@ -158,7 +308,30 @@
   }
 )
 
-(define_insn_and_split "atomic_fetch_<atomic_optab><mode>"
+;; Load-operate-store, returning the updated memory data.
+
+(define_expand "atomic_fetch_<atomic_optab><mode>"
+ [(match_operand:ALLI 0 "register_operand" "")
+  (match_operand:ALLI 1 "aarch64_sync_memory_operand" "")
+  (atomic_op:ALLI
+   (match_operand:ALLI 2 "<atomic_op_operand>" "")
+   (match_operand:SI 3 "const_int_operand"))]
+ ""
+{
+  rtx (*gen) (rtx, rtx, rtx, rtx);
+
+  /* Use an atomic load-operate instruction when possible.  */
+  if (aarch64_atomic_ldop_supported_p (<CODE>))
+    gen = gen_aarch64_atomic_fetch_<atomic_optab><mode>_lse;
+  else
+    gen = gen_aarch64_atomic_fetch_<atomic_optab><mode>;
+
+  emit_insn (gen (operands[0], operands[1], operands[2], operands[3]));
+
+  DONE;
+})
+
+(define_insn_and_split "aarch64_atomic_fetch_<atomic_optab><mode>"
   [(set (match_operand:ALLI 0 "register_operand" "=&r")
     (match_operand:ALLI 1 "aarch64_sync_memory_operand" "+Q"))
    (set (match_dup 1)
@@ -181,6 +354,26 @@
   }
 )
 
+(define_insn_and_split "aarch64_atomic_fetch_<atomic_optab><mode>_lse"
+  [(set (match_operand:ALLI 0 "register_operand" "=&r")
+    (match_operand:ALLI 1 "aarch64_sync_memory_operand" "+Q"))
+   (set (match_dup 1)
+    (unspec_volatile:ALLI
+      [(atomic_op:ALLI (match_dup 1)
+	(match_operand:ALLI 2 "<atomic_op_operand>" "r<const_atomic>"))
+       (match_operand:SI 3 "const_int_operand")]
+      UNSPECV_ATOMIC_LDOP))]
+  "TARGET_LSE"
+  "#"
+  "&& reload_completed"
+  [(const_int 0)]
+  {
+    aarch64_gen_atomic_ldop (<CODE>, operands[0], NULL, operands[1],
+			     operands[2], operands[3]);
+    DONE;
+  }
+)
+
 (define_insn_and_split "atomic_fetch_nand<mode>"
   [(set (match_operand:ALLI 0 "register_operand" "=&r")
     (match_operand:ALLI 1 "aarch64_sync_memory_operand" "+Q"))
@@ -205,7 +398,31 @@
   }
 )
 
-(define_insn_and_split "atomic_<atomic_optab>_fetch<mode>"
+;; Load-operate-store, returning the original memory data.
+
+(define_expand "atomic_<atomic_optab>_fetch<mode>"
+ [(match_operand:ALLI 0 "register_operand" "")
+  (atomic_op:ALLI
+   (match_operand:ALLI 1 "aarch64_sync_memory_operand" "")
+   (match_operand:ALLI 2 "<atomic_op_operand>" ""))
+  (match_operand:SI 3 "const_int_operand")]
+ ""
+{
+  rtx (*gen) (rtx, rtx, rtx, rtx);
+  rtx value = operands[2];
+
+  /* Use an atomic load-operate instruction when possible.  */
+  if (aarch64_atomic_ldop_supported_p (<CODE>))
+    gen = gen_aarch64_atomic_<atomic_optab>_fetch<mode>_lse;
+  else
+    gen = gen_aarch64_atomic_<atomic_optab>_fetch<mode>;
+
+  emit_insn (gen (operands[0], operands[1], value, operands[3]));
+
+  DONE;
+})
+
+(define_insn_and_split "aarch64_atomic_<atomic_optab>_fetch<mode>"
   [(set (match_operand:ALLI 0 "register_operand" "=&r")
     (atomic_op:ALLI
       (match_operand:ALLI 1 "aarch64_sync_memory_operand" "+Q")
@@ -228,6 +445,29 @@
   }
 )
 
+(define_insn_and_split "aarch64_atomic_<atomic_optab>_fetch<mode>_lse"
+  [(set (match_operand:ALLI 0 "register_operand" "=&r")
+    (atomic_op:ALLI
+     (match_operand:ALLI 1 "aarch64_sync_memory_operand" "+Q")
+     (match_operand:ALLI 2 "<atomic_op_operand>" "r<const_atomic>")))
+   (set (match_dup 1)
+    (unspec_volatile:ALLI
+      [(match_dup 1)
+       (match_dup 2)
+       (match_operand:SI 3 "const_int_operand")]
+      UNSPECV_ATOMIC_LDOP))
+     (clobber (match_scratch:ALLI 4 "=r"))]
+  "TARGET_LSE"
+  "#"
+  "&& reload_completed"
+  [(const_int 0)]
+  {
+    aarch64_gen_atomic_ldop (<CODE>, operands[4], operands[0], operands[1],
+			     operands[2], operands[3]);
+    DONE;
+  }
+)
+
 (define_insn_and_split "atomic_nand_fetch<mode>"
   [(set (match_operand:ALLI 0 "register_operand" "=&r")
     (not:ALLI
@@ -370,3 +610,100 @@
       return "dmb\\tish";
   }
 )
+
+;; ARMv8.1 LSE instructions.
+
+;; Atomic swap with memory.
+(define_insn "aarch64_atomic_swp<mode>"
+ [(set (match_operand:ALLI 0 "register_operand" "+&r")
+   (match_operand:ALLI 1 "aarch64_sync_memory_operand" "+Q"))
+  (set (match_dup 1)
+   (unspec_volatile:ALLI
+    [(match_operand:ALLI 2 "register_operand" "r")
+     (match_operand:SI 3 "const_int_operand" "")]
+    UNSPECV_ATOMIC_SWP))]
+  "TARGET_LSE && reload_completed"
+  {
+    enum memmodel model = memmodel_from_int (INTVAL (operands[3]));
+    if (is_mm_relaxed (model))
+      return "swp<atomic_sfx>\t%<w>2, %<w>0, %1";
+    else if (is_mm_acquire (model) || is_mm_consume (model))
+      return "swpa<atomic_sfx>\t%<w>2, %<w>0, %1";
+    else if (is_mm_release (model))
+      return "swpl<atomic_sfx>\t%<w>2, %<w>0, %1";
+    else
+      return "swpal<atomic_sfx>\t%<w>2, %<w>0, %1";
+  })
+
+;; Atomic compare-and-swap: HI and smaller modes.
+
+(define_insn "aarch64_atomic_cas<mode>"
+ [(set (match_operand:SI 0 "register_operand" "+&r")		  ;; out
+   (zero_extend:SI
+    (match_operand:SHORT 1 "aarch64_sync_memory_operand" "+Q")))  ;; memory.
+  (set (match_dup 1)
+   (unspec_volatile:SHORT
+    [(match_dup 0)
+     (match_operand:SHORT 2 "register_operand" "r")	;; value.
+     (match_operand:SI 3 "const_int_operand" "")]	;; model.
+    UNSPECV_ATOMIC_CAS))]
+ "TARGET_LSE && reload_completed"
+{
+  enum memmodel model = memmodel_from_int (INTVAL (operands[3]));
+  if (is_mm_relaxed (model))
+    return "cas<atomic_sfx>\t%<w>0, %<w>2, %1";
+  else if (is_mm_acquire (model) || is_mm_consume (model))
+    return "casa<atomic_sfx>\t%<w>0, %<w>2, %1";
+  else if (is_mm_release (model))
+    return "casl<atomic_sfx>\t%<w>0, %<w>2, %1";
+  else
+    return "casal<atomic_sfx>\t%<w>0, %<w>2, %1";
+})
+
+;; Atomic compare-and-swap: SI and larger modes.
+
+(define_insn "aarch64_atomic_cas<mode>"
+ [(set (match_operand:GPI 0 "register_operand" "+&r")	      ;; out
+   (match_operand:GPI 1 "aarch64_sync_memory_operand" "+Q"))  ;; memory.
+  (set (match_dup 1)
+   (unspec_volatile:GPI
+    [(match_dup 0)
+     (match_operand:GPI 2 "register_operand" "r")	;; value.
+     (match_operand:SI 3 "const_int_operand" "")]	;; model.
+    UNSPECV_ATOMIC_CAS))]
+  "TARGET_LSE && reload_completed"
+{
+    enum memmodel model = memmodel_from_int (INTVAL (operands[3]));
+    if (is_mm_relaxed (model))
+      return "cas<atomic_sfx>\t%<w>0, %<w>2, %1";
+    else if (is_mm_acquire (model) || is_mm_consume (model))
+      return "casa<atomic_sfx>\t%<w>0, %<w>2, %1";
+    else if (is_mm_release (model))
+      return "casl<atomic_sfx>\t%<w>0, %<w>2, %1";
+    else
+      return "casal<atomic_sfx>\t%<w>0, %<w>2, %1";
+})
+
+;; Atomic load-op: Load data, operate, store result, keep data.
+
+(define_insn "aarch64_atomic_load<atomic_ldop><mode>"
+ [(set (match_operand:ALLI 0 "register_operand" "=r")
+   (match_operand:ALLI 1 "aarch64_sync_memory_operand" "+Q"))
+  (set (match_dup 1)
+   (unspec_volatile:ALLI
+    [(match_dup 1)
+     (match_operand:ALLI 2 "register_operand")
+     (match_operand:SI 3 "const_int_operand")]
+    ATOMIC_LDOP))]
+ "TARGET_LSE && reload_completed"
+ {
+   enum memmodel model = memmodel_from_int (INTVAL (operands[3]));
+   if (is_mm_relaxed (model))
+     return "ld<atomic_ldop><atomic_sfx>\t%<w>2, %<w>0, %1";
+   else if (is_mm_acquire (model) || is_mm_consume (model))
+     return "ld<atomic_ldop>a<atomic_sfx>\t%<w>2, %<w>0, %1";
+   else if (is_mm_release (model))
+     return "ld<atomic_ldop>l<atomic_sfx>\t%<w>2, %<w>0, %1";
+   else
+     return "ld<atomic_ldop>al<atomic_sfx>\t%<w>2, %<w>0, %1";
+ })
--- a/src//dev/null
+++ b/src/gcc/config/aarch64/cortex-a57-fma-steering.c
@@ -0,0 +1,1099 @@
+/* FMA steering optimization pass for Cortex-A57.
+   Copyright (C) 2015 Free Software Foundation, Inc.
+   Contributed by ARM Ltd.
+
+   This file is part of GCC.
+
+   GCC is free software; you can redistribute it and/or modify it
+   under the terms of the GNU General Public License as published by
+   the Free Software Foundation; either version 3, or (at your option)
+   any later version.
+
+   GCC is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with GCC; see the file COPYING3.  If not see
+   <http://www.gnu.org/licenses/>.  */
+
+#include "config.h"
+#include "system.h"
+#include "coretypes.h"
+#include "tm.h"
+#include "regs.h"
+#include "insn-config.h"
+#include "hard-reg-set.h"
+#include "dominance.h"
+#include "cfg.h"
+#include "cfganal.h"
+#include "predict.h"
+#include "basic-block.h"
+#include "insn-attr.h"
+#include "machmode.h"
+#include "recog.h"
+#include "output.h"
+#include "vec.h"
+#include "hash-map.h"
+#include "bitmap.h"
+#include "obstack.h"
+#include "df.h"
+#include "target.h"
+#include "rtl.h"
+#include "context.h"
+#include "tree-pass.h"
+#include "regrename.h"
+#include "cortex-a57-fma-steering.h"
+#include "aarch64-protos.h"
+
+#include <list>
+
+/* For better performance, the destination of FMADD/FMSUB instructions should
+   have the same parity as their accumulator register if the accumulator
+   contains the result of a previous FMUL or FMADD/FMSUB instruction if
+   targetting Cortex-A57 processors.  Performance is also increased by
+   otherwise keeping a good balance in the parity of the destination register
+   of FMUL or FMADD/FMSUB.
+
+   This pass ensure that registers are renamed so that these conditions hold.
+   We reuse the existing register renaming facility from regrename.c to build
+   dependency chains and expose candidate registers for renaming.
+
+
+   The algorithm has three steps:
+
+   First, the functions of the register renaming pass are called.  These
+   analyze the instructions and produce a list of def/use chains of
+   instructions.
+
+   Next, this information is used to build trees of multiply and
+   multiply-accumulate instructions.  The roots of these trees are any
+   multiply, or any multiply-accumulate whose accumulator is not dependent on
+   a multiply or multiply-accumulate instruction.  A child is added to the
+   tree where a dependency chain exists between the result of the parent
+   instruction and the accumulator operand of the child, as in the diagram
+   below:
+
+		 fmul s2, s0, s1
+		/		\
+   fmadd s0, s1, s1, s2   fmadd s4, s1, s1 s2
+	    |
+   fmadd s3, s1, s1, s0
+
+   Trees made of a single instruction are permitted.
+
+   Finally, renaming is performed.  The parity of the destination register at
+   the root of a tree is checked against the current balance of multiply and
+   multiply-accumulate on each pipeline.  If necessary, the root of a tree is
+   renamed, in which case the rest of the tree is then renamed to keep the same
+   parity in the destination registers of all instructions in the tree.  */
+
+
+
+/* Forward declarations.  */
+class fma_node;
+class fma_root_node;
+class func_fma_steering;
+
+/* Dependencies between FMUL or FMADD/FMSUB instructions and subsequent
+   FMADD/FMSUB instructions form a graph.  This is because alternatives can
+   make a register be set by several FMUL or FMADD/FMSUB instructions in
+   different basic blocks and because of loops.  For ease of browsing, the
+   connected components of this graph are broken up into forests of trees.
+   Forests are represented by fma_forest objects, contained in the fma_forests
+   list.  Using a separate object for the forests allows for a better use of
+   memory as there is some information that is global to each forest, such as
+   the number of FMSUB and FMADD/FMSUB instructions currently scheduled on each
+   floating-point execution pipelines.  */
+
+class fma_forest
+{
+public:
+  fma_forest (func_fma_steering *, fma_root_node *, int);
+  ~fma_forest ();
+
+  int get_id ();
+  std::list<fma_root_node *> *get_roots ();
+  func_fma_steering *get_globals ();
+  int get_target_parity ();
+  void fma_node_created (fma_node *);
+  void merge_forest (fma_forest *);
+  void dump_info ();
+  void dispatch ();
+
+private:
+  /* The list of roots that form this forest.  */
+  std::list<fma_root_node *> *m_roots;
+
+  /* Target parity the destination register of all FMUL and FMADD/FMSUB
+     instructions in this forest should have.  */
+  int m_target_parity;
+
+  /* Link to the instance of func_fma_steering holding data related to the
+     FMA steering of the current function (cfun).  */
+  func_fma_steering *m_globals;
+
+  /* Identifier for the forest (used for dumps).  */
+  int m_id;
+
+  /* Total number of nodes in the forest (for statistics).  */
+  int m_nb_nodes;
+};
+
+class fma_node
+{
+public:
+  fma_node (fma_node *parent, du_chain *chain);
+  ~fma_node ();
+
+  bool root_p ();
+  fma_forest *get_forest ();
+  std::list<fma_node *> *get_children ();
+  rtx_insn *get_insn ();
+  void add_child (fma_node *);
+  int get_parity ();
+  void set_head (du_head *);
+  void rename (fma_forest *);
+  void dump_info (fma_forest *);
+
+protected:
+  /* Root node that lead to this node.  */
+  fma_root_node *m_root;
+
+  /* The parent node of this node.  If the node belong to a chain with several
+     parent nodes, the first one encountered in a depth-first search is chosen
+     as canonical parent.  */
+  fma_node *m_parent;
+
+  /* The list of child nodes.  If a chain contains several parent nodes, one is
+     chosen as canonical parent and the others will have no children.  */
+  std::list<fma_node *> *m_children;
+
+  /* The associated DU_HEAD chain that the insn represented by this object
+     is (one of) the root of.  When a chain contains several roots, the non
+     canonical ones have this field set to NULL.  */
+  struct du_head *m_head;
+
+  /* The FMUL or FMADD/FMSUB instruction this object corresponds to.  */
+  rtx_insn *m_insn;
+};
+
+class fma_root_node : public fma_node
+{
+public:
+  fma_root_node (func_fma_steering *, du_chain *, int);
+
+  fma_forest *get_forest ();
+  void set_forest (fma_forest *);
+  void dump_info (fma_forest *);
+
+private:
+  /* The forest this node belonged to when it was created.  */
+  fma_forest *m_forest;
+};
+
+/* Class holding all data and methods relative to the FMA steering of a given
+   function.  The FMA steering pass could then run in parallel for different
+   functions.  */
+
+class func_fma_steering
+{
+public:
+  func_fma_steering ();
+  ~func_fma_steering ();
+
+  int get_fpu_balance ();
+  void remove_forest (fma_forest *);
+  bool put_node (fma_node *);
+  void update_balance (int);
+  fma_node *get_fma_node (rtx_insn *);
+  void analyze_fma_fmul_insn (fma_forest *, du_chain *, du_head_p);
+  void execute_fma_steering ();
+
+private:
+  void dfs (void (*) (fma_forest *), void (*) (fma_forest *, fma_root_node *),
+	    void (*) (fma_forest *, fma_node *), bool);
+  void analyze ();
+  void rename_fma_trees ();
+
+  /* Mapping between FMUL or FMADD/FMSUB instructions and the associated
+     fma_node object.  Used when analyzing an instruction that is a root of
+     a chain to find if such an object was created because this instruction
+     is also a use in another chain.  */
+  hash_map<rtx_insn *, fma_node *> *m_insn_fma_head_map;
+
+  /* A list of all the forests in a given function.  */
+  std::list<fma_forest *> m_fma_forests;
+
+  /* Balance of FMUL and FMADD/FMSUB instructions between the two FPU
+     pipelines:
+     < 0: more instruction dispatched to the first pipeline
+     == 0: perfect balance
+     > 0: more instruction dispatched to the second pipeline.  */
+  int m_fpu_balance;
+
+  /* Identifier for the next forest created.  */
+  int m_next_forest_id;
+};
+
+/* Rename the register HEAD->regno in all the insns in the chain HEAD to any
+   register not in the set UNAVAILABLE.  Adapted from rename_chains in
+   regrename.c.  */
+
+static bool
+rename_single_chain (du_head_p head, HARD_REG_SET *unavailable)
+{
+  int best_new_reg;
+  int n_uses = 0;
+  struct du_chain *tmp;
+  int reg = head->regno;
+  enum reg_class super_class = NO_REGS;
+
+  if (head->cannot_rename)
+    return false;
+
+  if (fixed_regs[reg] || global_regs[reg]
+      || (frame_pointer_needed && reg == HARD_FRAME_POINTER_REGNUM))
+    return false;
+
+  /* Iterate over elements in the chain in order to:
+     1. Count number of uses, and narrow the set of registers we can
+	use for renaming.
+     2. Compute the superunion of register classes in this chain.  */
+  for (tmp = head->first; tmp; tmp = tmp->next_use)
+    {
+      if (DEBUG_INSN_P (tmp->insn))
+	continue;
+      n_uses++;
+      IOR_COMPL_HARD_REG_SET (*unavailable, reg_class_contents[tmp->cl]);
+      super_class = reg_class_superunion[(int) super_class][(int) tmp->cl];
+    }
+
+  if (n_uses < 1)
+    return false;
+
+  best_new_reg = find_rename_reg (head, super_class, unavailable, reg,
+				  false);
+
+  if (dump_file)
+    {
+      fprintf (dump_file, "Register %s in insn %d", reg_names[reg],
+	       INSN_UID (head->first->insn));
+      if (head->need_caller_save_reg)
+	fprintf (dump_file, " crosses a call");
+    }
+
+  if (best_new_reg == reg)
+    {
+      if (dump_file)
+	fprintf (dump_file, "; no available better choice\n");
+      return false;
+    }
+
+  if (regrename_do_replace (head, best_new_reg))
+    {
+      if (dump_file)
+	fprintf (dump_file, ", renamed as %s\n", reg_names[best_new_reg]);
+      df_set_regs_ever_live (best_new_reg, true);
+    }
+  else
+    {
+      if (dump_file)
+	fprintf (dump_file, ", renaming as %s failed\n",
+		 reg_names[best_new_reg]);
+      return false;
+    }
+  return true;
+}
+
+/* Return whether T is the attribute of a FMADD/FMSUB-like instruction.  */
+
+static bool
+is_fmac_op (enum attr_type t)
+{
+  return (t == TYPE_FMACS) || (t == TYPE_FMACD) || (t == TYPE_NEON_FP_MLA_S);
+}
+
+/* Return whether T is the attribute of a FMUL instruction.  */
+
+static bool
+is_fmul_op (enum attr_type t)
+{
+  return (t == TYPE_FMULS) || (t == TYPE_FMULD) || (t == TYPE_NEON_FP_MUL_S);
+}
+
+/* Return whether INSN is an FMUL (if FMUL_OK is true) or FMADD/FMSUB
+   instruction.  */
+
+static bool
+is_fmul_fmac_insn (rtx_insn *insn, bool fmul_ok)
+{
+  enum attr_type t;
+
+  if (!NONDEBUG_INSN_P (insn))
+    return false;
+
+  if (recog_memoized (insn) < 0)
+    return false;
+
+  /* Only consider chain(s) this instruction is a root of if this is an FMUL or
+     FMADD/FMSUB instruction.  This allows to avoid browsing chains of all
+     instructions for FMUL or FMADD/FMSUB in them.  */
+  t = get_attr_type (insn);
+  return is_fmac_op (t) || (fmul_ok && is_fmul_op (t));
+}
+
+
+/*
+ * Class fma_forest method definitions.
+ */
+
+fma_forest::fma_forest (func_fma_steering *fma_steer, fma_root_node *fma_root,
+			int id)
+{
+      memset (this, 0, sizeof (*this));
+      this->m_globals = fma_steer;
+      this->m_roots = new std::list<fma_root_node *>;
+      this->m_roots->push_back (fma_root);
+      this->m_id = id;
+}
+
+fma_forest::~fma_forest ()
+{
+  delete this->m_roots;
+}
+
+int
+fma_forest::get_id ()
+{
+  return this->m_id;
+}
+
+std::list<fma_root_node *> *
+fma_forest::get_roots ()
+{
+  return this->m_roots;
+}
+
+func_fma_steering *
+fma_forest::get_globals ()
+{
+  return this->m_globals;
+}
+
+int
+fma_forest::get_target_parity ()
+{
+  return this->m_target_parity;
+}
+
+/* Act on the creation of NODE by updating statistics in FOREST and adding an
+   entry for it in the func_fma_steering hashmap.  */
+
+void fma_forest::fma_node_created (fma_node *node)
+{
+  bool created = !this->m_globals->put_node (node);
+
+  gcc_assert (created);
+  this->m_nb_nodes++;
+}
+
+/* Merge REF_FOREST and OTHER_FOREST together, making REF_FOREST the canonical
+   fma_forest object to represent both.  */
+
+void
+fma_forest::merge_forest (fma_forest *other_forest)
+{
+  std::list<fma_root_node *> *other_roots;
+  std::list<fma_root_node *>::iterator other_root_iter;
+
+  if (this == other_forest)
+    return;
+
+  other_roots = other_forest->m_roots;
+
+  /* Update root nodes' pointer to forest.  */
+  for (other_root_iter = other_roots->begin ();
+       other_root_iter != other_roots->end (); other_root_iter++)
+    (*other_root_iter)->set_forest (this);
+
+  /* Remove other_forest from the list of forests and move its tree roots in
+     the list of tree roots of ref_forest.  */
+  this->m_globals->remove_forest (other_forest);
+  this->m_roots->splice (this->m_roots->begin (), *other_roots);
+  delete other_forest;
+
+  this->m_nb_nodes += other_forest->m_nb_nodes;
+}
+
+/* Dump information about the forest FOREST.  */
+
+void
+fma_forest::dump_info ()
+{
+  gcc_assert (dump_file);
+
+  fprintf (dump_file, "Forest #%d has %d nodes\n", this->m_id,
+	   this->m_nb_nodes);
+}
+
+/* Wrapper around fma_forest::dump_info for use as parameter of function
+   pointer type in func_fma_steering::dfs.  */
+
+static void
+dump_forest_info (fma_forest *forest)
+{
+  forest->dump_info ();
+}
+
+/* Dispatch forest to the least utilized pipeline.  */
+
+void
+fma_forest::dispatch ()
+{
+  this->m_target_parity = this->m_roots->front ()->get_parity ();
+  int fpu_balance = this->m_globals->get_fpu_balance ();
+  if (fpu_balance != 0)
+    this->m_target_parity = (fpu_balance < 0);
+
+  if (dump_file)
+    fprintf (dump_file, "Target parity for forest #%d: %s\n", this->m_id,
+	     this->m_target_parity ? "odd" : "even");
+}
+
+/* Wrapper around fma_forest::dispatch for use as parameter of function pointer
+   type in func_fma_steering::dfs.  */
+
+static void
+dispatch_forest (fma_forest *forest)
+{
+  forest->dispatch ();
+}
+
+fma_node::fma_node (fma_node *parent, du_chain *chain)
+{
+  memset (this, 0, sizeof (*this));
+  this->m_parent = parent;
+  this->m_children = new std::list<fma_node *>;
+  this->m_insn = chain->insn;
+  /* root_p () cannot be used to check for root before root is set.  */
+  if (this->m_parent == this)
+    this->m_root = static_cast<fma_root_node *> (parent);
+  else
+    {
+      this->m_root = parent->m_root;
+      this->get_forest ()->fma_node_created (this);
+    }
+}
+
+fma_node::~fma_node ()
+{
+  delete this->m_children;
+}
+
+std::list<fma_node *> *
+fma_node::get_children ()
+{
+  return this->m_children;
+}
+
+rtx_insn *
+fma_node::get_insn ()
+{
+  return this->m_insn;
+}
+
+void
+fma_node::set_head (du_head *head)
+{
+  gcc_assert (!this->m_head);
+  this->m_head = head;
+}
+
+/* Add a child to this node in the list of children.  */
+
+void
+fma_node::add_child (fma_node *child)
+{
+  this->m_children->push_back (child);
+}
+
+/* Return the parity of the destination register of the instruction represented
+   by this node.  */
+
+int
+fma_node::get_parity ()
+{
+  return this->m_head->regno % 2;
+}
+
+/* Get the actual forest associated with a non root node as the one the node
+   points to might have been merged into another one.  In that case the pointer
+   in the root nodes are updated so we return the forest pointer of a root node
+   pointed to by the initial forest.  Despite being a oneliner, this method is
+   defined here as it references a method from fma_root_node.  */
+
+fma_forest *
+fma_node::get_forest ()
+{
+  return this->m_root->get_forest ();
+}
+
+/* Return whether a node is a root node.  */
+
+bool
+fma_node::root_p ()
+{
+  return this->m_root == this;
+}
+
+/* Dump information about the children of node FMA_NODE in forest FOREST.  */
+
+void
+fma_node::dump_info (ATTRIBUTE_UNUSED fma_forest *forest)
+{
+  struct du_chain *chain;
+  std::list<fma_node *>::iterator fma_child;
+
+  gcc_assert (dump_file);
+
+  if (this->get_children ()->empty ())
+    return;
+
+  fprintf (dump_file, "Instruction(s)");
+  for (chain = this->m_head->first; chain; chain = chain->next_use)
+    {
+      if (!is_fmul_fmac_insn (chain->insn, true))
+	continue;
+
+      if (chain->loc != &SET_DEST (PATTERN (chain->insn)))
+	continue;
+
+      fprintf (dump_file, " %d", INSN_UID (chain->insn));
+    }
+
+  fprintf (dump_file, " is(are) accumulator dependency of instructions");
+  for (fma_child = this->get_children ()->begin ();
+       fma_child != this->get_children ()->end (); fma_child++)
+    fprintf (dump_file, " %d", INSN_UID ((*fma_child)->m_insn));
+  fprintf (dump_file, "\n");
+}
+
+/* Wrapper around fma_node::dump_info for use as parameter of function pointer
+   type in func_fma_steering::dfs.  */
+
+static void
+dump_tree_node_info (fma_forest *forest, fma_node *node)
+{
+  node->dump_info (forest);
+}
+
+/* Rename the destination register of a single FMUL or FMADD/FMSUB instruction
+   represented by FMA_NODE to a register that respect the target parity for
+   FOREST or with same parity of the instruction represented by its parent node
+   if it has one.  */
+
+void
+fma_node::rename (fma_forest *forest)
+{
+  int cur_parity, target_parity;
+
+  /* This is alternate root of a chain and thus has no children.  It will be
+     renamed when processing the canonical root for that chain.  */
+  if (!this->m_head)
+    return;
+
+  target_parity = forest->get_target_parity ();
+  if (this->m_parent)
+    target_parity = this->m_parent->get_parity ();
+  cur_parity = this->get_parity ();
+
+  /* Rename if parity differs.  */
+  if (cur_parity != target_parity)
+    {
+      rtx_insn *insn = this->m_insn;
+      HARD_REG_SET unavailable;
+      enum machine_mode mode;
+      int reg;
+
+      if (dump_file)
+	{
+	  unsigned cur_dest_reg = this->m_head->regno;
+
+	  fprintf (dump_file, "FMA or FMUL at insn %d but destination "
+		   "register (%s) has different parity from expected to "
+		   "maximize FPU pipeline utilization\n", INSN_UID (insn),
+		   reg_names[cur_dest_reg]);
+	}
+
+      /* Don't clobber traceback for noreturn functions.  */
+      CLEAR_HARD_REG_SET (unavailable);
+      if (frame_pointer_needed)
+	{
+	  add_to_hard_reg_set (&unavailable, Pmode, FRAME_POINTER_REGNUM);
+	  add_to_hard_reg_set (&unavailable, Pmode, HARD_FRAME_POINTER_REGNUM);
+	}
+
+      /* Exclude registers with wrong parity.  */
+      mode = GET_MODE (SET_DEST (PATTERN (insn)));
+      for (reg = cur_parity; reg < FIRST_PSEUDO_REGISTER; reg += 2)
+	add_to_hard_reg_set (&unavailable, mode, reg);
+
+      if (!rename_single_chain (this->m_head, &unavailable))
+	{
+	  if (dump_file)
+	    fprintf (dump_file, "Destination register of insn %d could not be "
+		     "renamed. Dependent FMA insns will use this parity from "
+		     "there on.\n", INSN_UID (insn));
+	}
+      else
+	cur_parity = target_parity;
+    }
+
+  forest->get_globals ()->update_balance (cur_parity);
+}
+
+/* Wrapper around fma_node::dump_info for use as parameter of function pointer
+   type in func_fma_steering::dfs.  */
+
+static void
+rename_fma_node (fma_forest *forest, fma_node *node)
+{
+  node->rename (forest);
+}
+
+fma_root_node::fma_root_node (func_fma_steering *globals, du_chain *chain,
+			      int id) : fma_node (this, chain)
+{
+  this->m_forest = new fma_forest (globals, this, id);
+  this->m_forest->fma_node_created (this);
+}
+
+fma_forest *
+fma_root_node::get_forest ()
+{
+  return this->m_forest;
+}
+
+void
+fma_root_node::set_forest (fma_forest *ref_forest)
+{
+  this->m_forest = ref_forest;
+}
+
+/* Dump information about the roots of forest FOREST.  */
+
+void
+fma_root_node::dump_info (fma_forest *forest)
+{
+  gcc_assert (dump_file);
+
+  if (this == forest->get_roots ()->front ())
+    fprintf (dump_file, "Instruction(s) at root of forest #%d:",
+	     forest->get_id ());
+  fprintf (dump_file, " %d", INSN_UID (this->m_insn));
+  if (this == forest->get_roots ()->back ())
+    fprintf (dump_file, "\n");
+}
+
+/* Wrapper around fma_root_node::dump_info for use as parameter of function
+   pointer type in func_fma_steering::dfs.  */
+
+static void
+dump_tree_root_info (fma_forest *forest, fma_root_node *node)
+{
+  node->dump_info (forest);
+}
+
+func_fma_steering::func_fma_steering () : m_fpu_balance (0)
+{
+  this->m_insn_fma_head_map = new hash_map<rtx_insn *, fma_node *>;
+  this->m_fma_forests.clear ();
+  this->m_next_forest_id = 0;
+}
+
+func_fma_steering::~func_fma_steering ()
+{
+  delete this->m_insn_fma_head_map;
+}
+
+int
+func_fma_steering::get_fpu_balance ()
+{
+  return this->m_fpu_balance;
+}
+
+void
+func_fma_steering::remove_forest (fma_forest *forest)
+{
+  this->m_fma_forests.remove (forest);
+}
+
+/* Memorize the mapping of this instruction to its fma_node object and return
+   whether such a mapping existed.  */
+
+bool
+func_fma_steering::put_node (fma_node *node)
+{
+  return this->m_insn_fma_head_map->put (node->get_insn (), node);
+}
+
+/* Update the current balance considering a node with the given PARITY.  */
+
+void
+func_fma_steering::update_balance (int parity)
+{
+  this->m_fpu_balance = parity ? this->m_fpu_balance + 1
+			       : this->m_fpu_balance - 1;
+}
+
+/* Return whether an fma_node object exists for instruction INSN and, if not,
+   allocate one in *RET.  */
+
+fma_node *
+func_fma_steering::get_fma_node (rtx_insn *insn)
+{
+  fma_node **fma_slot;
+
+  fma_slot = this->m_insn_fma_head_map->get (insn);
+  if (fma_slot)
+    return *fma_slot;
+  return NULL;
+}
+
+/* Allocate and initialize fma_node objects for the FMUL or FMADD/FMSUB
+   instruction in CHAIN->insn and its dependent FMADD/FMSUB instructions, all
+   part of FOREST.  For the children, the associated head is left untouched
+   (and thus null) as this function will be called again when considering the
+   chain where they are def.  For the parent, the chain is given in HEAD.  */
+
+void
+func_fma_steering::analyze_fma_fmul_insn (fma_forest *ref_forest,
+					  du_chain *chain, du_head_p head)
+{
+  fma_forest *forest;
+  fma_node *node = this->get_fma_node (chain->insn);
+
+  /* This is a root node.  */
+  if (!node)
+    {
+      fma_root_node *root_node;
+
+      root_node = new fma_root_node (this, chain, this->m_next_forest_id++);
+      forest = root_node->get_forest ();
+      node = root_node;
+
+      /* Until proved otherwise, assume this root is not part of an existing
+	 forest and thus add its forest to the list of forests.  */
+      this->m_fma_forests.push_back (forest);
+    }
+  else
+    forest = node->get_forest ();
+
+  node->set_head (head);
+
+  /* fma_node is part of a chain with several defs, one of them having already
+     been processed.  The root of that already processed def is the canonical
+     one and the root of fma_node is added to its forest.  No need to process
+     the children nodes as they were already processed when the other def was
+     processed.  */
+  if (ref_forest)
+    {
+      ref_forest->merge_forest (forest);
+      return;
+    }
+
+  for (chain = head->first; chain; chain = chain->next_use)
+    {
+      fma_node *child_fma;
+      rtx fma_rtx, *accum_rtx_p;
+
+      if (!is_fmul_fmac_insn (chain->insn, false))
+	continue;
+
+      /* Get FMA rtx.  */
+      fma_rtx = SET_SRC (PATTERN (chain->insn));
+      /* FMA is negated.  */
+      if (GET_CODE (fma_rtx) == NEG)
+	fma_rtx = XEXP (fma_rtx, 0);
+      /* Get accumulator rtx.  */
+      accum_rtx_p = &XEXP (fma_rtx, 2);
+      /* Accumulator is negated.  */
+      if (!REG_P (*accum_rtx_p))
+	accum_rtx_p = &XEXP (*accum_rtx_p, 0);
+
+      /* This du_chain structure is not for the accumulator register.  */
+      if (accum_rtx_p != chain->loc)
+	continue;
+
+      /* If object already created, this is a loop carried dependency.  We
+	 don't include this object in the children as we want trees for
+	 rename_fma_trees to not be an infinite loop.  */
+      if (this->get_fma_node (chain->insn))
+	continue;
+
+      child_fma = new fma_node (node, chain);
+
+      /* Memorize the mapping of this instruction to its fma_node object
+	 as it will be processed for the chain starting at its destination
+	 register later.  */
+
+      /* Link to siblings.  */
+      node->add_child (child_fma);
+    }
+}
+
+/* Perform a depth-first search of the forests of fma_node in
+   THIS->m_fma_forests, calling PROCESS_FOREST () on each fma_forest object in
+   THIS->m_fma_forests list, PROCESS_ROOT () on each tree root and
+   PROCESS_NODE () on each node.  If FREE is true, free all std::list in the
+   same dfs.  */
+
+void
+func_fma_steering::dfs (void (*process_forest) (fma_forest *),
+			void (*process_root) (fma_forest *, fma_root_node *),
+			void (*process_node) (fma_forest *, fma_node *),
+			bool free)
+{
+  vec<fma_node *> to_process;
+  std::list<fma_forest *>::iterator forest_iter;
+
+  to_process.create (0);
+
+  /* For each forest.  */
+  for (forest_iter = this->m_fma_forests.begin ();
+       forest_iter != this->m_fma_forests.end (); forest_iter++)
+    {
+      std::list<fma_root_node *>::iterator root_iter;
+
+      if (process_forest)
+	process_forest (*forest_iter);
+
+      /* For each tree root in this forest.  */
+      for (root_iter = (*forest_iter)->get_roots ()->begin ();
+	   root_iter != (*forest_iter)->get_roots ()->end (); root_iter++)
+	{
+	  if (process_root)
+	    process_root (*forest_iter, *root_iter);
+	  to_process.safe_push (*root_iter);
+	}
+
+      /* For each tree node in this forest.  */
+      while (!to_process.is_empty ())
+	{
+	  fma_node *node;
+	  std::list<fma_node *>::iterator child_iter;
+
+	  node = to_process.pop ();
+
+	  if (process_node)
+	    process_node (*forest_iter, node);
+
+	  /* Absence of children might indicate an alternate root of a *chain*.
+	     It's ok to skip it here as the chain will be renamed when
+	     processing the canonical root for that chain.  */
+	  if (node->get_children ()->empty ())
+	    continue;
+
+	  for (child_iter = node->get_children ()->begin ();
+	       child_iter != node->get_children ()->end (); child_iter++)
+	    to_process.safe_push (*child_iter);
+	  if (free)
+	    {
+	      if (node->root_p ())
+		delete static_cast<fma_root_node *> (node);
+	      else
+		delete node;
+	    }
+	}
+      if (free)
+	delete *forest_iter;
+    }
+
+  to_process.release ();
+}
+
+/* Build the dependency trees of FMUL and FMADD/FMSUB instructions.  */
+
+void
+func_fma_steering::analyze ()
+{
+  int i, n_blocks, *bb_dfs_preorder;
+  basic_block bb;
+  rtx_insn *insn;
+
+  bb_dfs_preorder = XNEWVEC (int, last_basic_block_for_fn (cfun));
+  n_blocks = pre_and_rev_post_order_compute (bb_dfs_preorder, NULL, false);
+
+  /* Browse the graph of basic blocks looking for FMUL or FMADD/FMSUB
+     instructions.  */
+  for (i = 0; i < n_blocks; i++)
+    {
+      bb = BASIC_BLOCK_FOR_FN (cfun, bb_dfs_preorder[i]);
+      FOR_BB_INSNS (bb, insn)
+	{
+	  operand_rr_info *dest_op_info;
+	  struct du_chain *chain;
+	  unsigned dest_regno;
+	  fma_forest *forest;
+	  du_head_p head;
+	  int i;
+
+	  if (!is_fmul_fmac_insn (insn, true))
+	    continue;
+
+	  /* Search the chain where this instruction is (one of) the root.  */
+	  dest_op_info = insn_rr[INSN_UID (insn)].op_info;
+	  dest_regno = REGNO (SET_DEST (PATTERN (insn)));
+	  for (i = 0; i < dest_op_info->n_chains; i++)
+	    {
+	      /* The register tracked by this chain does not match the
+		 destination register of insn.  */
+	      if (dest_op_info->heads[i]->regno != dest_regno)
+		continue;
+
+	      head = dest_op_info->heads[i];
+	      /* The chain was merged in another, find the new head.  */
+	      if (!head->first)
+		head = regrename_chain_from_id (head->id);
+
+	      /* Search the chain element for this instruction and, if another
+		 FMUL or FMADD/FMSUB instruction was already processed, note
+		 the forest of its tree.  */
+	      forest = NULL;
+	      for (chain = head->first; chain; chain = chain->next_use)
+		{
+		  fma_node **fma_slot;
+
+		  if (!is_fmul_fmac_insn (chain->insn, true))
+		    continue;
+
+		  /* This is a use, continue.  */
+		  if (chain->loc != &SET_DEST (PATTERN (chain->insn)))
+		    continue;
+
+		  if (chain->insn == insn)
+		    break;
+
+		  fma_slot = this->m_insn_fma_head_map->get (chain->insn);
+		  if (fma_slot && (*fma_slot)->get_children ())
+		    forest = (*fma_slot)->get_forest ();
+		}
+	      if (chain)
+		break;
+	    }
+
+	  /* We didn't find a chain with a def for this instruction.  */
+	  gcc_assert (i < dest_op_info->n_chains);
+
+	  this->analyze_fma_fmul_insn (forest, chain, head);
+	}
+    }
+  free (bb_dfs_preorder);
+
+  if (dump_file)
+    this->dfs (dump_forest_info, dump_tree_root_info, dump_tree_node_info,
+	       false);
+}
+
+/* Perform the renaming of all chains with FMUL or FMADD/FMSUB involved with
+   the objective of keeping FPU pipeline balanced in term of instructions and
+   having FMADD/FMSUB with dependencies on previous FMUL or FMADD/FMSUB be
+   scheduled on the same pipeline.  */
+
+void
+func_fma_steering::rename_fma_trees ()
+{
+  this->dfs (dispatch_forest, NULL, rename_fma_node, true);
+
+  if (dump_file && !this->m_fma_forests.empty ())
+    {
+      fprintf (dump_file, "Function %s has ", current_function_name ());
+      if (this->m_fpu_balance == 0)
+	fprintf (dump_file, "perfect balance of FMUL/FMA chains between the "
+		 "two FPU pipelines\n");
+      else if (this->m_fpu_balance > 0)
+	fprintf (dump_file, "%d more FMUL/FMA chains scheduled on the second "
+		 "FPU pipeline\n", this->m_fpu_balance);
+      else /* this->m_fpu_balance < 0 */
+	fprintf (dump_file, "%d more FMUL/FMA chains scheduled on the first "
+		 "FPU pipeline\n", - this->m_fpu_balance);
+    }
+}
+
+/* Execute FMA steering pass.  */
+
+void
+func_fma_steering::execute_fma_steering ()
+{
+  df_set_flags (DF_LR_RUN_DCE);
+  df_note_add_problem ();
+  df_analyze ();
+  df_set_flags (DF_DEFER_INSN_RESCAN);
+
+  regrename_init (true);
+  regrename_analyze (NULL);
+  this->analyze ();
+  this->rename_fma_trees ();
+  regrename_finish ();
+}
+
+const pass_data pass_data_fma_steering =
+{
+  RTL_PASS, /* type */
+  "fma_steering", /* name */
+  OPTGROUP_NONE, /* optinfo_flags */
+  TV_NONE, /* tv_id */
+  0, /* properties_required */
+  0, /* properties_provided */
+  0, /* properties_destroyed */
+  0, /* todo_flags_start */
+  TODO_df_finish, /* todo_flags_finish */
+};
+
+class pass_fma_steering : public rtl_opt_pass
+{
+public:
+  pass_fma_steering (gcc::context *ctxt)
+    : rtl_opt_pass (pass_data_fma_steering, ctxt)
+  {}
+
+  /* opt_pass methods: */
+  virtual bool gate (function *)
+    {
+      return (aarch64_tune_params.extra_tuning_flags
+	      & AARCH64_EXTRA_TUNE_RENAME_FMA_REGS)
+	      && optimize >= 2;
+    }
+
+  virtual unsigned int execute (function *)
+    {
+      func_fma_steering *fma_steering = new func_fma_steering;
+      fma_steering->execute_fma_steering ();
+      delete fma_steering;
+      return 0;
+    }
+
+}; // class pass_fma_steering
+
+/* Create a new fma steering pass instance.  */
+
+static rtl_opt_pass *
+make_pass_fma_steering (gcc::context *ctxt)
+{
+  return new pass_fma_steering (ctxt);
+}
+
+/* Register the FMA steering pass to the pass manager.  */
+
+void
+aarch64_register_fma_steering ()
+{
+  opt_pass *pass_fma_steering = make_pass_fma_steering (g);
+
+  static struct register_pass_info fma_steering_info
+    = { pass_fma_steering, "rnreg", 1, PASS_POS_INSERT_AFTER };
+
+  register_pass (&fma_steering_info);
+}
--- a/src//dev/null
+++ b/src/gcc/config/aarch64/cortex-a57-fma-steering.h
@@ -0,0 +1,22 @@
+/* This file contains declarations for the FMA steering optimization
+   pass for Cortex-A57.
+   Copyright (C) 2015 Free Software Foundation, Inc.
+   Contributed by ARM Ltd.
+
+   This file is part of GCC.
+
+   GCC is free software; you can redistribute it and/or modify it
+   under the terms of the GNU General Public License as published by
+   the Free Software Foundation; either version 3, or (at your option)
+   any later version.
+
+   GCC is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with GCC; see the file COPYING3.  If not see
+   <http://www.gnu.org/licenses/>.  */
+
+void aarch64_register_fma_steering (void);
--- a/src//dev/null
+++ b/src/gcc/config/aarch64/driver-aarch64.c
@@ -0,0 +1,307 @@
+/* Native CPU detection for aarch64.
+   Copyright (C) 2015 Free Software Foundation, Inc.
+
+   This file is part of GCC.
+
+   GCC is free software; you can redistribute it and/or modify
+   it under the terms of the GNU General Public License as published by
+   the Free Software Foundation; either version 3, or (at your option)
+   any later version.
+
+   GCC is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+   GNU General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with GCC; see the file COPYING3.  If not see
+   <http://www.gnu.org/licenses/>.  */
+
+#include "config.h"
+#include "system.h"
+
+struct arch_extension
+{
+  const char *ext;
+  const char *feat_string;
+};
+
+#define AARCH64_OPT_EXTENSION(EXT_NAME, FLAGS_ON, FLAGS_OFF, FEATURE_STRING) \
+  { EXT_NAME, FEATURE_STRING },
+static struct arch_extension ext_to_feat_string[] =
+{
+#include "aarch64-option-extensions.def"
+};
+#undef AARCH64_OPT_EXTENSION
+
+
+struct aarch64_core_data
+{
+  const char* name;
+  const char* arch;
+  const char* implementer_id;
+  const char* part_no;
+};
+
+#define AARCH64_CORE(CORE_NAME, CORE_IDENT, SCHED, ARCH, FLAGS, COSTS, IMP, PART) \
+  { CORE_NAME, #ARCH, IMP, PART },
+
+static struct aarch64_core_data cpu_data [] =
+{
+#include "aarch64-cores.def"
+  { NULL, NULL, NULL, NULL }
+};
+
+#undef AARCH64_CORE
+
+struct aarch64_arch
+{
+  const char* id;
+  const char* name;
+};
+
+#define AARCH64_ARCH(NAME, CORE, ARCH, FLAGS) \
+  { #ARCH, NAME  },
+
+static struct aarch64_arch aarch64_arches [] =
+{
+#include "aarch64-arches.def"
+  {NULL, NULL}
+};
+
+#undef AARCH64_ARCH
+
+/* Return the full architecture name string corresponding to the
+   identifier ID.  */
+
+static const char*
+get_arch_name_from_id (const char* id)
+{
+  unsigned int i = 0;
+
+  for (i = 0; aarch64_arches[i].id != NULL; i++)
+    {
+      if (strcmp (id, aarch64_arches[i].id) == 0)
+        return aarch64_arches[i].name;
+    }
+
+  return NULL;
+}
+
+
+/* Check wether the string CORE contains the same CPU part numbers
+   as BL_STRING.  For example CORE="{0xd03, 0xd07}" and BL_STRING="0xd07.0xd03"
+   should return true.  */
+
+static bool
+valid_bL_string_p (const char** core, const char* bL_string)
+{
+  return strstr (bL_string, core[0]) != NULL
+         && strstr (bL_string, core[1]) != NULL;
+}
+
+/*  Return true iff ARR contains STR in one of its two elements.  */
+
+static bool
+contains_string_p (const char** arr, const char* str)
+{
+  bool res = false;
+
+  if (arr[0] != NULL)
+    {
+      res = strstr (arr[0], str) != NULL;
+      if (res)
+        return res;
+
+      if (arr[1] != NULL)
+        return strstr (arr[1], str) != NULL;
+    }
+
+  return false;
+}
+
+/* This will be called by the spec parser in gcc.c when it sees
+   a %:local_cpu_detect(args) construct.  Currently it will be called
+   with either "arch", "cpu" or "tune" as argument depending on if
+   -march=native, -mcpu=native or -mtune=native is to be substituted.
+
+   It returns a string containing new command line parameters to be
+   put at the place of the above two options, depending on what CPU
+   this is executed.  E.g. "-march=armv8-a" on a Cortex-A57 for
+   -march=native.  If the routine can't detect a known processor,
+   the -march or -mtune option is discarded.
+
+   For -mtune and -mcpu arguments it attempts to detect the CPU or
+   a big.LITTLE system.
+   ARGC and ARGV are set depending on the actual arguments given
+   in the spec.  */
+
+const char *
+host_detect_local_cpu (int argc, const char **argv)
+{
+  const char *arch_id = NULL;
+  const char *res = NULL;
+  static const int num_exts = ARRAY_SIZE (ext_to_feat_string);
+  char buf[128];
+  FILE *f = NULL;
+  bool arch = false;
+  bool tune = false;
+  bool cpu = false;
+  unsigned int i = 0;
+  unsigned int core_idx = 0;
+  const char* imps[2] = { NULL, NULL };
+  const char* cores[2] = { NULL, NULL };
+  unsigned int n_cores = 0;
+  unsigned int n_imps = 0;
+  bool processed_exts = false;
+  const char *ext_string = "";
+
+  gcc_assert (argc);
+
+  if (!argv[0])
+    goto not_found;
+
+  /* Are we processing -march, mtune or mcpu?  */
+  arch = strcmp (argv[0], "arch") == 0;
+  if (!arch)
+    tune = strcmp (argv[0], "tune") == 0;
+
+  if (!arch && !tune)
+    cpu = strcmp (argv[0], "cpu") == 0;
+
+  if (!arch && !tune && !cpu)
+    goto not_found;
+
+  f = fopen ("/proc/cpuinfo", "r");
+
+  if (f == NULL)
+    goto not_found;
+
+  /* Look through /proc/cpuinfo to determine the implementer
+     and then the part number that identifies a particular core.  */
+  while (fgets (buf, sizeof (buf), f) != NULL)
+    {
+      if (strstr (buf, "implementer") != NULL)
+	{
+	  for (i = 0; cpu_data[i].name != NULL; i++)
+	    if (strstr (buf, cpu_data[i].implementer_id) != NULL
+                && !contains_string_p (imps, cpu_data[i].implementer_id))
+	      {
+                if (n_imps == 2)
+                  goto not_found;
+
+                imps[n_imps++] = cpu_data[i].implementer_id;
+
+                break;
+	      }
+          continue;
+	}
+
+      if (strstr (buf, "part") != NULL)
+	{
+	  for (i = 0; cpu_data[i].name != NULL; i++)
+	    if (strstr (buf, cpu_data[i].part_no) != NULL
+                && !contains_string_p (cores, cpu_data[i].part_no))
+	      {
+                if (n_cores == 2)
+                  goto not_found;
+
+                cores[n_cores++] = cpu_data[i].part_no;
+	        core_idx = i;
+	        arch_id = cpu_data[i].arch;
+	        break;
+	      }
+          continue;
+        }
+      if (!tune && !processed_exts && strstr (buf, "Features") != NULL)
+        {
+          for (i = 0; i < num_exts; i++)
+            {
+              bool enabled = true;
+              char *p = NULL;
+              char *feat_string = concat (ext_to_feat_string[i].feat_string, NULL);
+
+              p = strtok (feat_string, " ");
+
+              while (p != NULL)
+                {
+                  if (strstr (buf, p) == NULL)
+                    {
+                      enabled = false;
+                      break;
+                    }
+                  p = strtok (NULL, " ");
+                }
+              ext_string = concat (ext_string, "+", enabled ? "" : "no",
+                                   ext_to_feat_string[i].ext, NULL);
+            }
+          processed_exts = true;
+        }
+    }
+
+  fclose (f);
+  f = NULL;
+
+  /* Weird cpuinfo format that we don't know how to handle.  */
+  if (n_cores == 0 || n_cores > 2 || n_imps != 1)
+    goto not_found;
+
+  if (arch && !arch_id)
+    goto not_found;
+
+  if (arch)
+    {
+      const char* arch_name = get_arch_name_from_id (arch_id);
+
+      /* We got some arch indentifier that's not in aarch64-arches.def?  */
+      if (!arch_name)
+        goto not_found;
+
+      res = concat ("-march=", arch_name, NULL);
+    }
+  /* We have big.LITTLE.  */
+  else if (n_cores == 2)
+    {
+      for (i = 0; cpu_data[i].name != NULL; i++)
+        {
+          if (strchr (cpu_data[i].part_no, '.') != NULL
+              && strncmp (cpu_data[i].implementer_id, imps[0], strlen (imps[0]) - 1) == 0
+              && valid_bL_string_p (cores, cpu_data[i].part_no))
+            {
+              res = concat ("-m", cpu ? "cpu" : "tune", "=", cpu_data[i].name, NULL);
+              break;
+            }
+        }
+      if (!res)
+        goto not_found;
+    }
+  /* The simple, non-big.LITTLE case.  */
+  else
+    {
+      if (strncmp (cpu_data[core_idx].implementer_id, imps[0],
+                   strlen (imps[0]) - 1) != 0)
+        goto not_found;
+
+      res = concat ("-m", cpu ? "cpu" : "tune", "=",
+                      cpu_data[core_idx].name, NULL);
+    }
+
+  if (tune)
+    return res;
+
+  res = concat (res, ext_string, NULL);
+
+  return res;
+
+not_found:
+  {
+   /* If detection fails we ignore the option.
+      Clean up and return empty string.  */
+
+    if (f)
+      fclose (f);
+
+    return "";
+  }
+}
+
--- a/src/gcc/config/aarch64/iterators.md
+++ b/src/gcc/config/aarch64/iterators.md
@@ -537,24 +537,15 @@
 
 (define_mode_attr VRL2 [(V8QI "V32QI") (V4HI "V16HI")
 			(V2SI "V8SI")  (V2SF "V8SF")
-			(DI   "V4DI")  (DF   "V4DF")
-			(V16QI "V32QI") (V8HI "V16HI")
-			(V4SI "V8SI")  (V4SF "V8SF")
-			(V2DI "V4DI")  (V2DF "V4DF")])
+			(DI   "V4DI")  (DF   "V4DF")])
 
 (define_mode_attr VRL3 [(V8QI "V48QI") (V4HI "V24HI")
 			(V2SI "V12SI")  (V2SF "V12SF")
-			(DI   "V6DI")  (DF   "V6DF")
-			(V16QI "V48QI") (V8HI "V24HI")
-			(V4SI "V12SI")  (V4SF "V12SF")
-			(V2DI "V6DI")  (V2DF "V6DF")])
+			(DI   "V6DI")  (DF   "V6DF")])
 
 (define_mode_attr VRL4 [(V8QI "V64QI") (V4HI "V32HI")
 			(V2SI "V16SI")  (V2SF "V16SF")
-			(DI   "V8DI")  (DF   "V8DF")
-			(V16QI "V64QI") (V8HI "V32HI")
-			(V4SI "V16SI")  (V4SF "V16SF")
-			(V2DI "V8DI")  (V2DF "V8DF")])
+			(DI   "V8DI")  (DF   "V8DF")])
 
 (define_mode_attr VSTRUCT_DREG [(OI "TI") (CI "EI") (XI "OI")])
 
--- a/src/gcc/config/aarch64/t-aarch64
+++ b/src/gcc/config/aarch64/t-aarch64
@@ -48,6 +48,16 @@ aarch-common.o: $(srcdir)/config/arm/aarch-common.c $(CONFIG_H) $(SYSTEM_H) \
 	$(COMPILER) -c $(ALL_COMPILERFLAGS) $(ALL_CPPFLAGS) $(INCLUDES) \
 		$(srcdir)/config/arm/aarch-common.c
 
+cortex-a57-fma-steering.o: $(srcdir)/config/aarch64/cortex-a57-fma-steering.c \
+    $(CONFIG_H) $(SYSTEM_H) $(TM_H) $(REGS_H) insn-config.h $(RTL_BASE_H) \
+    dominance.h cfg.h cfganal.h $(BASIC_BLOCK_H) $(INSN_ATTR_H) $(RECOG_H) \
+    output.h hash-map.h $(DF_H) $(OBSTACK_H) $(TARGET_H) $(RTL_H) \
+    $(CONTEXT_H) $(TREE_PASS_H) regrename.h \
+    $(srcdir)/config/aarch64/cortex-a57-fma-steering.h \
+    $(srcdir)/config/aarch64/aarch64-protos.h
+	$(COMPILER) -c $(ALL_COMPILERFLAGS) $(ALL_CPPFLAGS) $(INCLUDES) \
+		$(srcdir)/config/aarch64/cortex-a57-fma-steering.c
+
 comma=,
 MULTILIB_OPTIONS    = $(subst $(comma),/, $(patsubst %, mabi=%, $(subst $(comma),$(comma)mabi=,$(TM_MULTILIB_CONFIG))))
 MULTILIB_DIRNAMES   = $(subst $(comma), ,$(TM_MULTILIB_CONFIG))
--- a/src//dev/null
+++ b/src/gcc/config/aarch64/x-aarch64
@@ -0,0 +1,3 @@
+driver-aarch64.o: $(srcdir)/config/aarch64/driver-aarch64.c \
+  $(CONFIG_H) $(SYSTEM_H)
+	$(COMPILER) -c $(ALL_COMPILERFLAGS) $(ALL_CPPFLAGS) $(INCLUDES) $<
--- a/src/gcc/config/alpha/linux.h
+++ b/src/gcc/config/alpha/linux.h
@@ -61,10 +61,14 @@ along with GCC; see the file COPYING3.  If not see
 #define OPTION_GLIBC  (DEFAULT_LIBC == LIBC_GLIBC)
 #define OPTION_UCLIBC (DEFAULT_LIBC == LIBC_UCLIBC)
 #define OPTION_BIONIC (DEFAULT_LIBC == LIBC_BIONIC)
+#undef OPTION_MUSL
+#define OPTION_MUSL   (DEFAULT_LIBC == LIBC_MUSL)
 #else
 #define OPTION_GLIBC  (linux_libc == LIBC_GLIBC)
 #define OPTION_UCLIBC (linux_libc == LIBC_UCLIBC)
 #define OPTION_BIONIC (linux_libc == LIBC_BIONIC)
+#undef OPTION_MUSL
+#define OPTION_MUSL   (linux_libc == LIBC_MUSL)
 #endif
 
 /* Determine what functions are present at the runtime;
--- a/src/gcc/config/arm/aarch-common-protos.h
+++ b/src/gcc/config/arm/aarch-common-protos.h
@@ -102,6 +102,8 @@ struct mem_cost_table
   const int storef;		/* SFmode.  */
   const int stored;		/* DFmode.  */
   const int store_unaligned;	/* Extra for unaligned stores.  */
+  const int loadv;		/* Vector load.  */
+  const int storev;		/* Vector store.  */
 };
 
 struct fp_cost_table
--- a/src/gcc/config/arm/aarch-cost-tables.h
+++ b/src/gcc/config/arm/aarch-cost-tables.h
@@ -81,7 +81,9 @@ const struct cpu_cost_table generic_extra_costs =
     1,			/* stm_regs_per_insn_subsequent.  */
     COSTS_N_INSNS (2),	/* storef.  */
     COSTS_N_INSNS (3),	/* stored.  */
-    COSTS_N_INSNS (1)  /* store_unaligned.  */
+    COSTS_N_INSNS (1),	/* store_unaligned.  */
+    COSTS_N_INSNS (1),	/* loadv.  */
+    COSTS_N_INSNS (1)	/* storev.  */
   },
   {
     /* FP SFmode */
@@ -130,12 +132,12 @@ const struct cpu_cost_table cortexa53_extra_costs =
     0,			/* arith.  */
     0,			/* logical.  */
     COSTS_N_INSNS (1),	/* shift.  */
-    COSTS_N_INSNS (2),	/* shift_reg.  */
+    0,			/* shift_reg.  */
     COSTS_N_INSNS (1),	/* arith_shift.  */
-    COSTS_N_INSNS (2),	/* arith_shift_reg.  */
+    COSTS_N_INSNS (1),	/* arith_shift_reg.  */
     COSTS_N_INSNS (1),	/* log_shift.  */
-    COSTS_N_INSNS (2),	/* log_shift_reg.  */
-    0,			/* extend.  */
+    COSTS_N_INSNS (1),	/* log_shift_reg.  */
+    COSTS_N_INSNS (1),	/* extend.  */
     COSTS_N_INSNS (1),	/* extend_arith.  */
     COSTS_N_INSNS (1),	/* bfi.  */
     COSTS_N_INSNS (1),	/* bfx.  */
@@ -182,7 +184,9 @@ const struct cpu_cost_table cortexa53_extra_costs =
     2,				/* stm_regs_per_insn_subsequent.  */
     0,				/* storef.  */
     0,				/* stored.  */
-    COSTS_N_INSNS (1)		/* store_unaligned.  */
+    COSTS_N_INSNS (1),		/* store_unaligned.  */
+    COSTS_N_INSNS (1),		/* loadv.  */
+    COSTS_N_INSNS (1)		/* storev.  */
   },
   {
     /* FP SFmode */
@@ -283,7 +287,9 @@ const struct cpu_cost_table cortexa57_extra_costs =
     2,                         /* stm_regs_per_insn_subsequent.  */
     0,                         /* storef.  */
     0,                         /* stored.  */
-    COSTS_N_INSNS (1)          /* store_unaligned.  */
+    COSTS_N_INSNS (1),         /* store_unaligned.  */
+    COSTS_N_INSNS (1),         /* loadv.  */
+    COSTS_N_INSNS (1)          /* storev.  */
   },
   {
     /* FP SFmode */
@@ -385,6 +391,8 @@ const struct cpu_cost_table xgene1_extra_costs =
     0,                         /* storef.  */
     0,                         /* stored.  */
     0,                         /* store_unaligned.  */
+    COSTS_N_INSNS (1),         /* loadv.  */
+    COSTS_N_INSNS (1)          /* storev.  */
   },
   {
     /* FP SFmode */
--- a/src/gcc/config/arm/arm-arches.def
+++ b/src/gcc/config/arm/arm-arches.def
@@ -44,7 +44,8 @@ ARM_ARCH("armv6",   arm1136js,  6,   FL_CO_PROC |             FL_FOR_ARCH6)
 ARM_ARCH("armv6j",  arm1136js,  6J,  FL_CO_PROC |             FL_FOR_ARCH6J)
 ARM_ARCH("armv6k",  mpcore,	6K,  FL_CO_PROC |             FL_FOR_ARCH6K)
 ARM_ARCH("armv6z",  arm1176jzs, 6Z,  FL_CO_PROC |             FL_FOR_ARCH6Z)
-ARM_ARCH("armv6zk", arm1176jzs, 6ZK, FL_CO_PROC |             FL_FOR_ARCH6ZK)
+ARM_ARCH("armv6kz", arm1176jzs, 6KZ, FL_CO_PROC |             FL_FOR_ARCH6KZ)
+ARM_ARCH("armv6zk", arm1176jzs, 6KZ, FL_CO_PROC |             FL_FOR_ARCH6KZ)
 ARM_ARCH("armv6t2", arm1156t2s, 6T2, FL_CO_PROC |             FL_FOR_ARCH6T2)
 ARM_ARCH("armv6-m", cortexm1,	6M,			      FL_FOR_ARCH6M)
 ARM_ARCH("armv6s-m", cortexm1,	6M,			      FL_FOR_ARCH6M)
--- a/src/gcc/config/arm/arm-builtins.c
+++ b/src/gcc/config/arm/arm-builtins.c
@@ -89,7 +89,9 @@ enum arm_type_qualifiers
   /* qualifier_const_pointer | qualifier_map_mode  */
   qualifier_const_pointer_map_mode = 0x86,
   /* Polynomial types.  */
-  qualifier_poly = 0x100
+  qualifier_poly = 0x100,
+  /* Lane indices - must be within range of previous argument = a vector.  */
+  qualifier_lane_index = 0x200
 };
 
 /*  The qualifier_internal allows generation of a unary builtin from
@@ -120,21 +122,40 @@ arm_ternop_qualifiers[SIMD_MAX_BUILTIN_ARGS]
 
 /* T (T, immediate).  */
 static enum arm_type_qualifiers
-arm_getlane_qualifiers[SIMD_MAX_BUILTIN_ARGS]
+arm_binop_imm_qualifiers[SIMD_MAX_BUILTIN_ARGS]
   = { qualifier_none, qualifier_none, qualifier_immediate };
+#define BINOP_IMM_QUALIFIERS (arm_binop_imm_qualifiers)
+
+/* T (T, lane index).  */
+static enum arm_type_qualifiers
+arm_getlane_qualifiers[SIMD_MAX_BUILTIN_ARGS]
+  = { qualifier_none, qualifier_none, qualifier_lane_index };
 #define GETLANE_QUALIFIERS (arm_getlane_qualifiers)
 
 /* T (T, T, T, immediate).  */
 static enum arm_type_qualifiers
-arm_lanemac_qualifiers[SIMD_MAX_BUILTIN_ARGS]
+arm_mac_n_qualifiers[SIMD_MAX_BUILTIN_ARGS]
   = { qualifier_none, qualifier_none, qualifier_none,
       qualifier_none, qualifier_immediate };
-#define LANEMAC_QUALIFIERS (arm_lanemac_qualifiers)
+#define MAC_N_QUALIFIERS (arm_mac_n_qualifiers)
+
+/* T (T, T, T, lane index).  */
+static enum arm_type_qualifiers
+arm_mac_lane_qualifiers[SIMD_MAX_BUILTIN_ARGS]
+  = { qualifier_none, qualifier_none, qualifier_none,
+      qualifier_none, qualifier_lane_index };
+#define MAC_LANE_QUALIFIERS (arm_mac_lane_qualifiers)
 
 /* T (T, T, immediate).  */
 static enum arm_type_qualifiers
-arm_setlane_qualifiers[SIMD_MAX_BUILTIN_ARGS]
+arm_ternop_imm_qualifiers[SIMD_MAX_BUILTIN_ARGS]
   = { qualifier_none, qualifier_none, qualifier_none, qualifier_immediate };
+#define TERNOP_IMM_QUALIFIERS (arm_ternop_imm_qualifiers)
+
+/* T (T, T, lane index).  */
+static enum arm_type_qualifiers
+arm_setlane_qualifiers[SIMD_MAX_BUILTIN_ARGS]
+  = { qualifier_none, qualifier_none, qualifier_none, qualifier_lane_index };
 #define SETLANE_QUALIFIERS (arm_setlane_qualifiers)
 
 /* T (T, T).  */
@@ -525,12 +546,16 @@ enum arm_builtins
 #undef CRYPTO2
 #undef CRYPTO3
 
+  ARM_BUILTIN_NEON_BASE,
+  ARM_BUILTIN_NEON_LANE_CHECK = ARM_BUILTIN_NEON_BASE,
+
 #include "arm_neon_builtins.def"
 
   ARM_BUILTIN_MAX
 };
 
-#define ARM_BUILTIN_NEON_BASE (ARM_BUILTIN_MAX - ARRAY_SIZE (neon_builtin_data))
+#define ARM_BUILTIN_NEON_PATTERN_START \
+    (ARM_BUILTIN_MAX - ARRAY_SIZE (neon_builtin_data))
 
 #undef CF
 #undef VAR1
@@ -889,7 +914,7 @@ arm_init_simd_builtin_scalar_types (void)
 static void
 arm_init_neon_builtins (void)
 {
-  unsigned int i, fcode = ARM_BUILTIN_NEON_BASE;
+  unsigned int i, fcode = ARM_BUILTIN_NEON_PATTERN_START;
 
   arm_init_simd_builtin_types ();
 
@@ -899,6 +924,15 @@ arm_init_neon_builtins (void)
      system.  */
   arm_init_simd_builtin_scalar_types ();
 
+  tree lane_check_fpr = build_function_type_list (void_type_node,
+						  intSI_type_node,
+						  intSI_type_node,
+						  NULL);
+  arm_builtin_decls[ARM_BUILTIN_NEON_LANE_CHECK] =
+      add_builtin_function ("__builtin_arm_lane_check", lane_check_fpr,
+			    ARM_BUILTIN_NEON_LANE_CHECK, BUILT_IN_MD,
+			    NULL, NULL_TREE);
+
   for (i = 0; i < ARRAY_SIZE (neon_builtin_data); i++, fcode++)
     {
       bool print_type_signature_p = false;
@@ -1939,6 +1973,7 @@ arm_expand_unop_builtin (enum insn_code icode,
 typedef enum {
   NEON_ARG_COPY_TO_REG,
   NEON_ARG_CONSTANT,
+  NEON_ARG_LANE_INDEX,
   NEON_ARG_MEMORY,
   NEON_ARG_STOP
 } builtin_arg;
@@ -2055,6 +2090,16 @@ arm_expand_neon_args (rtx target, machine_mode map_mode, int fcode,
 		op[argc] = copy_to_mode_reg (mode[argc], op[argc]);
 	      break;
 
+	    case NEON_ARG_LANE_INDEX:
+	      /* Previous argument must be a vector, which this indexes.  */
+	      gcc_assert (argc > 0);
+	      if (CONST_INT_P (op[argc]))
+		{
+		  enum machine_mode vmode = mode[argc - 1];
+		  neon_lane_bounds (op[argc], 0, GET_MODE_NUNITS (vmode), exp);
+		}
+	      /* Fall through - if the lane index isn't a constant then
+		 the next case will error.  */
 	    case NEON_ARG_CONSTANT:
 	      if (!(*insn_data[icode].operand[opno].predicate)
 		  (op[argc], mode[argc]))
@@ -2151,14 +2196,31 @@ arm_expand_neon_args (rtx target, machine_mode map_mode, int fcode,
   return target;
 }
 
-/* Expand a Neon builtin. These are "special" because they don't have symbolic
+/* Expand a Neon builtin, i.e. those registered only if TARGET_NEON holds.
+   Most of these are "special" because they don't have symbolic
    constants defined per-instruction or per instruction-variant. Instead, the
    required info is looked up in the table neon_builtin_data.  */
 static rtx
 arm_expand_neon_builtin (int fcode, tree exp, rtx target)
 {
+  if (fcode == ARM_BUILTIN_NEON_LANE_CHECK)
+    {
+      /* Builtin is only to check bounds of the lane passed to some intrinsics
+	 that are implemented with gcc vector extensions in arm_neon.h.  */
+
+      tree nlanes = CALL_EXPR_ARG (exp, 0);
+      gcc_assert (TREE_CODE (nlanes) == INTEGER_CST);
+      rtx lane_idx = expand_normal (CALL_EXPR_ARG (exp, 1));
+      if (CONST_INT_P (lane_idx))
+	neon_lane_bounds (lane_idx, 0, TREE_INT_CST_LOW (nlanes), exp);
+      else
+	error ("%Klane index must be a constant immediate", exp);
+      /* Don't generate any RTL.  */
+      return const0_rtx;
+    }
+
   neon_builtin_datum *d =
-		&neon_builtin_data[fcode - ARM_BUILTIN_NEON_BASE];
+		&neon_builtin_data[fcode - ARM_BUILTIN_NEON_PATTERN_START];
   enum insn_code icode = d->code;
   builtin_arg args[SIMD_MAX_BUILTIN_ARGS];
   int num_args = insn_data[d->code].n_operands;
@@ -2182,7 +2244,9 @@ arm_expand_neon_builtin (int fcode, tree exp, rtx target)
       int operands_k = k - is_void;
       int expr_args_k = k - 1;
 
-      if (d->qualifiers[qualifiers_k] & qualifier_immediate)
+      if (d->qualifiers[qualifiers_k] & qualifier_lane_index)
+	args[k] = NEON_ARG_LANE_INDEX;
+      else if (d->qualifiers[qualifiers_k] & qualifier_immediate)
 	args[k] = NEON_ARG_CONSTANT;
       else if (d->qualifiers[qualifiers_k] & qualifier_maybe_immediate)
 	{
--- a/src/gcc/config/arm/arm-c.c
+++ b/src/gcc/config/arm/arm-c.c
@@ -31,7 +31,11 @@
 #include "wide-int.h"
 #include "inchash.h"
 #include "tree.h"
+#include "tm_p.h"
 #include "c-family/c-common.h"
+#include "target.h"
+#include "target-def.h"
+#include "c-family/c-pragma.h"
 
 /* Output C specific EABI object attributes.  These can not be done in
    arm.c because they require information from the C frontend.  */
@@ -51,3 +55,223 @@ arm_lang_object_attributes_init (void)
 {
   arm_lang_output_object_attributes_hook = arm_output_c_attributes;
 }
+
+#define builtin_define(TXT) cpp_define (pfile, TXT)
+#define builtin_assert(TXT) cpp_assert (pfile, TXT)
+
+/* Define or undefine macros based on the current target.  If the user does
+   #pragma GCC target, we need to adjust the macros dynamically.  */
+
+static void
+def_or_undef_macro(struct cpp_reader* pfile, const char *name, bool def_p)
+{
+  if (def_p)
+    cpp_define (pfile, name);
+  else
+    cpp_undef (pfile, name);
+}
+
+static void
+arm_cpu_builtins (struct cpp_reader* pfile)
+{
+  def_or_undef_macro (pfile, "__ARM_FEATURE_DSP", TARGET_DSP_MULTIPLY);
+  def_or_undef_macro (pfile, "__ARM_FEATURE_QBIT", TARGET_ARM_QBIT); 
+  def_or_undef_macro (pfile, "__ARM_FEATURE_SAT", TARGET_ARM_SAT);
+  if (TARGET_CRYPTO)
+    builtin_define ("__ARM_FEATURE_CRYPTO");
+  if (unaligned_access)
+    builtin_define ("__ARM_FEATURE_UNALIGNED");
+  if (TARGET_CRC32)
+    builtin_define ("__ARM_FEATURE_CRC32");
+
+  def_or_undef_macro (pfile, "__ARM_32BIT_STATE", TARGET_32BIT); 
+
+  if (TARGET_ARM_FEATURE_LDREX)
+    builtin_define_with_int_value ("__ARM_FEATURE_LDREX", 
+				   TARGET_ARM_FEATURE_LDREX);
+  else
+    cpp_undef (pfile, "__ARM_FEATURE_LDREX");
+
+  def_or_undef_macro (pfile, "__ARM_FEATURE_CLZ",
+		      ((TARGET_ARM_ARCH >= 5 && !TARGET_THUMB)
+		       || TARGET_ARM_ARCH_ISA_THUMB >=2));
+
+  def_or_undef_macro (pfile, "__ARM_FEATURE_SIMD32", TARGET_INT_SIMD);
+
+  builtin_define_with_int_value ("__ARM_SIZEOF_MINIMAL_ENUM",
+				 flag_short_enums ? 1 : 4);
+  builtin_define_type_sizeof ("__ARM_SIZEOF_WCHAR_T", wchar_type_node);
+  if (TARGET_ARM_ARCH_PROFILE)
+    builtin_define_with_int_value ("__ARM_ARCH_PROFILE",
+				   TARGET_ARM_ARCH_PROFILE);
+
+  /* Define __arm__ even when in thumb mode, for
+     consistency with armcc.  */
+  builtin_define ("__arm__");
+  if (TARGET_ARM_ARCH)
+    builtin_define_with_int_value ("__ARM_ARCH", TARGET_ARM_ARCH);
+  if (arm_arch_notm)
+    builtin_define ("__ARM_ARCH_ISA_ARM");
+  builtin_define ("__APCS_32__");
+
+  def_or_undef_macro (pfile, "__thumb__", TARGET_THUMB);
+  def_or_undef_macro (pfile, "__thumb2__", TARGET_THUMB2);
+  if (TARGET_BIG_END)
+    def_or_undef_macro (pfile, "__THUMBEB__", TARGET_THUMB);
+  else
+    def_or_undef_macro (pfile, "__THUMBEL__", TARGET_THUMB);
+
+  if (TARGET_ARM_ARCH_ISA_THUMB)
+    builtin_define_with_int_value ("__ARM_ARCH_ISA_THUMB",
+				   TARGET_ARM_ARCH_ISA_THUMB);
+
+  if (TARGET_BIG_END)
+    {
+      builtin_define ("__ARMEB__");
+      builtin_define ("__ARM_BIG_ENDIAN");
+    }
+  else
+    {
+      builtin_define ("__ARMEL__");
+    }
+
+  if (TARGET_SOFT_FLOAT)
+    builtin_define ("__SOFTFP__");
+
+  if (TARGET_VFP)
+    builtin_define ("__VFP_FP__");
+	
+  if (TARGET_ARM_FP)
+    builtin_define_with_int_value ("__ARM_FP", TARGET_ARM_FP);
+  if (arm_fp16_format == ARM_FP16_FORMAT_IEEE)
+    builtin_define ("__ARM_FP16_FORMAT_IEEE");
+  if (arm_fp16_format == ARM_FP16_FORMAT_ALTERNATIVE)
+    builtin_define ("__ARM_FP16_FORMAT_ALTERNATIVE");
+  if (TARGET_FMA)
+    builtin_define ("__ARM_FEATURE_FMA");
+
+  if (TARGET_NEON)
+    {
+      builtin_define ("__ARM_NEON__");
+      builtin_define ("__ARM_NEON");
+    }
+  if (TARGET_NEON_FP)
+    builtin_define_with_int_value ("__ARM_NEON_FP", TARGET_NEON_FP);
+  
+  /* Add a define for interworking. Needed when building libgcc.a.  */
+  if (arm_cpp_interwork)
+    builtin_define ("__THUMB_INTERWORK__");
+
+
+  builtin_define (arm_arch_name);
+  if (arm_arch_xscale)
+    builtin_define ("__XSCALE__");
+  if (arm_arch_iwmmxt)
+    {
+      builtin_define ("__IWMMXT__");
+      builtin_define ("__ARM_WMMX");
+    }
+  if (arm_arch_iwmmxt2)
+    builtin_define ("__IWMMXT2__");
+  /* ARMv6KZ was originally identified as the misspelled __ARM_ARCH_6ZK__.  To
+     preserve the existing behaviour, the misspelled feature macro must still be
+     defined.  */
+  if (arm_arch6kz)
+    builtin_define ("__ARM_ARCH_6ZK__");
+  if (TARGET_AAPCS_BASED)
+    {
+      if (arm_pcs_default == ARM_PCS_AAPCS_VFP)
+	builtin_define ("__ARM_PCS_VFP");
+      else if (arm_pcs_default == ARM_PCS_AAPCS)
+	builtin_define ("__ARM_PCS");
+      builtin_define ("__ARM_EABI__");
+    }
+
+  def_or_undef_macro (pfile, "__ARM_ARCH_EXT_IDIV__", TARGET_IDIV);
+  def_or_undef_macro (pfile, "__ARM_FEATURE_IDIV", TARGET_IDIV);
+
+  def_or_undef_macro (pfile, "__ARM_ASM_SYNTAX_UNIFIED__", inline_asm_unified);
+}
+
+void
+arm_cpu_cpp_builtins (struct cpp_reader * pfile)
+{
+  builtin_assert ("cpu=arm");
+  builtin_assert ("machine=arm");
+
+  arm_cpu_builtins (pfile);
+}
+
+/* Hook to validate the current #pragma GCC target and set the arch custom
+   mode state.  If ARGS is NULL, then POP_TARGET is used to reset
+   the options.  */
+static bool
+arm_pragma_target_parse (tree args, tree pop_target)
+{
+  tree prev_tree = build_target_option_node (&global_options);
+  tree cur_tree;
+  struct cl_target_option *prev_opt;
+  struct cl_target_option *cur_opt;
+
+  if (! args)
+    {
+      cur_tree = ((pop_target) ? pop_target : target_option_default_node);
+      cl_target_option_restore (&global_options,
+				TREE_TARGET_OPTION (cur_tree));
+    }
+  else
+    {
+      cur_tree = arm_valid_target_attribute_tree (args, &global_options,
+						  &global_options_set);
+      if (cur_tree == NULL_TREE)
+	{
+	  cl_target_option_restore (&global_options,
+				    TREE_TARGET_OPTION (prev_tree));
+	  return false;
+	}
+    }
+
+  target_option_current_node = cur_tree;
+  arm_reset_previous_fndecl ();
+
+  /* Figure out the previous mode.  */
+  prev_opt  = TREE_TARGET_OPTION (prev_tree);
+  cur_opt   = TREE_TARGET_OPTION (cur_tree);
+
+  gcc_assert (prev_opt);
+  gcc_assert (cur_opt);
+
+  if (cur_opt->x_target_flags != prev_opt->x_target_flags)
+    {
+      /* For the definitions, ensure all newly defined macros are considered
+	 as used for -Wunused-macros.  There is no point warning about the
+	 compiler predefined macros.  */
+      cpp_options *cpp_opts = cpp_get_options (parse_in);
+      unsigned char saved_warn_unused_macros = cpp_opts->warn_unused_macros;
+      cpp_opts->warn_unused_macros = 0;
+
+      /* Update macros.  */
+      gcc_assert (cur_opt->x_target_flags == target_flags);
+      arm_cpu_builtins (parse_in);
+
+      cpp_opts->warn_unused_macros = saved_warn_unused_macros;
+    }
+
+  return true;
+}
+
+/* Register target pragmas.  We need to add the hook for parsing #pragma GCC
+   option here rather than in arm.c since it will pull in various preprocessor
+   functions, and those are not present in languages like fortran without a
+   preprocessor.  */
+
+void
+arm_register_target_pragmas (void)
+{
+  /* Update pragma hook to allow parsing #pragma GCC target.  */
+  targetm.target_option.pragma_parse = arm_pragma_target_parse;
+
+#ifdef REGISTER_SUBTARGET_PRAGMAS
+  REGISTER_SUBTARGET_PRAGMAS ();
+#endif
+}
--- a/src/gcc/config/arm/arm-cores.def
+++ b/src/gcc/config/arm/arm-cores.def
@@ -125,8 +125,8 @@ ARM_CORE("arm1026ej-s",	arm1026ejs, arm1026ejs,	5TEJ, FL_LDSCHED, 9e)
 /* V6 Architecture Processors */
 ARM_CORE("arm1136j-s",		arm1136js, arm1136js,		6J,  FL_LDSCHED, 9e)
 ARM_CORE("arm1136jf-s",		arm1136jfs, arm1136jfs,		6J,  FL_LDSCHED | FL_VFPV2, 9e)
-ARM_CORE("arm1176jz-s",		arm1176jzs, arm1176jzs,		6ZK, FL_LDSCHED, 9e)
-ARM_CORE("arm1176jzf-s",	arm1176jzfs, arm1176jzfs,	6ZK, FL_LDSCHED | FL_VFPV2, 9e)
+ARM_CORE("arm1176jz-s",		arm1176jzs, arm1176jzs,		6KZ, FL_LDSCHED, 9e)
+ARM_CORE("arm1176jzf-s",	arm1176jzfs, arm1176jzfs,	6KZ, FL_LDSCHED | FL_VFPV2, 9e)
 ARM_CORE("mpcorenovfp",		mpcorenovfp, mpcorenovfp,	6K,  FL_LDSCHED, 9e)
 ARM_CORE("mpcore",		mpcore, mpcore,			6K,  FL_LDSCHED | FL_VFPV2, 9e)
 ARM_CORE("arm1156t2-s",		arm1156t2s, arm1156t2s,		6T2, FL_LDSCHED, v6t2)
@@ -158,7 +158,7 @@ ARM_CORE("cortex-r7",		cortexr7, cortexr7,		7R,  FL_LDSCHED | FL_ARM_DIV, cortex
 ARM_CORE("cortex-m7",		cortexm7, cortexm7,		7EM, FL_LDSCHED | FL_NO_VOLATILE_CE, cortex_m7)
 ARM_CORE("cortex-m4",		cortexm4, cortexm4,		7EM, FL_LDSCHED, v7m)
 ARM_CORE("cortex-m3",		cortexm3, cortexm3,		7M,  FL_LDSCHED, v7m)
-ARM_CORE("marvell-pj4",		marvell_pj4, marvell_pj4,	7A,  FL_LDSCHED, 9e)
+ARM_CORE("marvell-pj4",		marvell_pj4, marvell_pj4,	7A,  FL_LDSCHED, marvell_pj4)
 
 /* V7 big.LITTLE implementations */
 ARM_CORE("cortex-a15.cortex-a7", cortexa15cortexa7, cortexa7,	7A,  FL_LDSCHED | FL_THUMB_DIV | FL_ARM_DIV, cortex_a15)
--- a/src/gcc/config/arm/arm-protos.h
+++ b/src/gcc/config/arm/arm-protos.h
@@ -30,6 +30,7 @@ extern void arm_load_pic_register (unsigned long);
 extern int arm_volatile_func (void);
 extern void arm_expand_prologue (void);
 extern void arm_expand_epilogue (bool);
+extern void arm_declare_function_name (FILE *, const char *, tree);
 extern void thumb2_expand_return (bool);
 extern const char *arm_strip_name_encoding (const char *);
 extern void arm_asm_output_labelref (FILE *, const char *);
@@ -66,10 +67,6 @@ extern rtx legitimize_tls_address (rtx, rtx);
 extern bool arm_legitimate_address_p (machine_mode, rtx, bool);
 extern int arm_legitimate_address_outer_p (machine_mode, rtx, RTX_CODE, int);
 extern int thumb_legitimate_offset_p (machine_mode, HOST_WIDE_INT);
-extern bool arm_legitimize_reload_address (rtx *, machine_mode, int, int,
-					   int);
-extern rtx thumb_legitimize_reload_address (rtx *, machine_mode, int, int,
-					    int);
 extern int thumb1_legitimate_address_p (machine_mode, rtx, int);
 extern bool ldm_stm_operation_p (rtx, bool, machine_mode mode,
                                  bool, bool);
@@ -89,7 +86,7 @@ extern void neon_pairwise_reduce (rtx, rtx, machine_mode,
 extern rtx neon_make_constant (rtx);
 extern tree arm_builtin_vectorized_function (tree, tree, tree);
 extern void neon_expand_vector_init (rtx, rtx);
-extern void neon_lane_bounds (rtx, HOST_WIDE_INT, HOST_WIDE_INT);
+extern void neon_lane_bounds (rtx, HOST_WIDE_INT, HOST_WIDE_INT, const_tree);
 extern void neon_const_bounds (rtx, HOST_WIDE_INT, HOST_WIDE_INT);
 extern HOST_WIDE_INT neon_element_bits (machine_mode);
 extern void neon_reinterpret (rtx, rtx);
@@ -185,9 +182,6 @@ extern const char *thumb1_unexpanded_epilogue (void);
 extern void thumb1_expand_prologue (void);
 extern void thumb1_expand_epilogue (void);
 extern const char *thumb1_output_interwork (void);
-#ifdef TREE_CODE
-extern int is_called_in_ARM_mode (tree);
-#endif
 extern int thumb_shiftable_const (unsigned HOST_WIDE_INT);
 #ifdef RTX_CODE
 extern enum arm_cond_code maybe_get_arm_condition_code (rtx);
@@ -216,14 +210,15 @@ extern int arm_dllexport_p (tree);
 extern int arm_dllimport_p (tree);
 extern void arm_mark_dllexport (tree);
 extern void arm_mark_dllimport (tree);
+extern bool arm_change_mode_p (tree);
 #endif
 
+extern tree arm_valid_target_attribute_tree (tree, struct gcc_options *,
+					     struct gcc_options *);
 extern void arm_pr_long_calls (struct cpp_reader *);
 extern void arm_pr_no_long_calls (struct cpp_reader *);
 extern void arm_pr_long_calls_off (struct cpp_reader *);
 
-extern void arm_lang_object_attributes_init(void);
-
 extern const char *arm_mangle_type (const_tree);
 extern const char *arm_mangle_builtin_type (const_tree);
 
@@ -257,13 +252,6 @@ struct cpu_vec_costs {
 
 struct cpu_cost_table;
 
-enum arm_sched_autopref
-  {
-    ARM_SCHED_AUTOPREF_OFF,
-    ARM_SCHED_AUTOPREF_RANK,
-    ARM_SCHED_AUTOPREF_FULL
-  };
-
 /* Dump function ARM_PRINT_TUNE_INFO should be updated whenever this
    structure is modified.  */
 
@@ -272,39 +260,58 @@ struct tune_params
   bool (*rtx_costs) (rtx, RTX_CODE, RTX_CODE, int *, bool);
   const struct cpu_cost_table *insn_extra_cost;
   bool (*sched_adjust_cost) (rtx_insn *, rtx, rtx_insn *, int *);
+  int (*branch_cost) (bool, bool);
+  /* Vectorizer costs.  */
+  const struct cpu_vec_costs* vec_costs;
   int constant_limit;
   /* Maximum number of instructions to conditionalise.  */
   int max_insns_skipped;
-  int num_prefetch_slots;
-  int l1_cache_size;
-  int l1_cache_line_size;
-  bool prefer_constant_pool;
-  int (*branch_cost) (bool, bool);
+  /* Maximum number of instructions to inline calls to memset.  */
+  int max_insns_inline_memset;
+  /* Issue rate of the processor.  */
+  unsigned int issue_rate;
+  /* Explicit prefetch data.  */
+  struct
+    {
+      int num_slots;
+      int l1_cache_size;
+      int l1_cache_line_size;
+    } prefetch;
+  enum {PREF_CONST_POOL_FALSE, PREF_CONST_POOL_TRUE}
+    prefer_constant_pool: 1;
   /* Prefer STRD/LDRD instructions over PUSH/POP/LDM/STM.  */
-  bool prefer_ldrd_strd;
+  enum {PREF_LDRD_FALSE, PREF_LDRD_TRUE} prefer_ldrd_strd: 1;
   /* The preference for non short cirtcuit operation when optimizing for
      performance. The first element covers Thumb state and the second one
      is for ARM state.  */
-  bool logical_op_non_short_circuit[2];
-  /* Vectorizer costs.  */
-  const struct cpu_vec_costs* vec_costs;
-  /* Prefer Neon for 64-bit bitops.  */
-  bool prefer_neon_for_64bits;
+  enum log_op_non_short_circuit {LOG_OP_NON_SHORT_CIRCUIT_FALSE,
+				 LOG_OP_NON_SHORT_CIRCUIT_TRUE};
+  log_op_non_short_circuit logical_op_non_short_circuit_thumb: 1;
+  log_op_non_short_circuit logical_op_non_short_circuit_arm: 1;
   /* Prefer 32-bit encoding instead of flag-setting 16-bit encoding.  */
-  bool disparage_flag_setting_t16_encodings;
-  /* Prefer 32-bit encoding instead of 16-bit encoding where subset of flags
-     would be set.  */
-  bool disparage_partial_flag_setting_t16_encodings;
+  enum {DISPARAGE_FLAGS_NEITHER, DISPARAGE_FLAGS_PARTIAL, DISPARAGE_FLAGS_ALL}
+    disparage_flag_setting_t16_encodings: 2;
+  enum {PREF_NEON_64_FALSE, PREF_NEON_64_TRUE} prefer_neon_for_64bits: 1;
   /* Prefer to inline string operations like memset by using Neon.  */
-  bool string_ops_prefer_neon;
-  /* Maximum number of instructions to inline calls to memset.  */
-  int max_insns_inline_memset;
-  /* Bitfield encoding the fuseable pairs of instructions.  */
-  unsigned int fuseable_ops;
+  enum {PREF_NEON_STRINGOPS_FALSE, PREF_NEON_STRINGOPS_TRUE}
+    string_ops_prefer_neon: 1;
+  /* Bitfield encoding the fusible pairs of instructions.  Use FUSE_OPS
+     in an initializer if multiple fusion operations are supported on a
+     target.  */
+  enum fuse_ops
+  {
+    FUSE_NOTHING   = 0,
+    FUSE_MOVW_MOVT = 1 << 0
+  } fusible_ops: 1;
   /* Depth of scheduling queue to check for L2 autoprefetcher.  */
-  enum arm_sched_autopref sched_autopref;
+  enum {SCHED_AUTOPREF_OFF, SCHED_AUTOPREF_RANK, SCHED_AUTOPREF_FULL}
+    sched_autopref: 2;
 };
 
+/* Smash multiple fusion operations into a type that can be used for an
+   initializer.  */
+#define FUSE_OPS(x) ((tune_params::fuse_ops) (x))
+
 extern const struct tune_params *current_tune;
 extern int vfp3_const_double_for_fract_bits (rtx);
 /* return power of two from operand, otherwise 0.  */
@@ -324,9 +331,16 @@ extern bool arm_autoinc_modes_ok_p (machine_mode, enum arm_auto_incmodes);
 
 extern void arm_emit_eabi_attribute (const char *, int, int);
 
+extern void arm_reset_previous_fndecl (void);
+
 /* Defined in gcc/common/config/arm-common.c.  */
 extern const char *arm_rewrite_selected_cpu (const char *name);
 
+/* Defined in gcc/common/config/arm-c.c.  */
+extern void arm_lang_object_attributes_init (void);
+extern void arm_register_target_pragmas (void);
+extern void arm_cpu_cpp_builtins (struct cpp_reader *);
+
 extern bool arm_is_constant_pool_ref (rtx);
 
 /* Flags used to identify the presence of processor capabilities.  */
@@ -368,6 +382,7 @@ extern bool arm_is_constant_pool_ref (rtx);
 
 #define FL_IWMMXT     (1 << 29)	      /* XScale v2 or "Intel Wireless MMX technology".  */
 #define FL_IWMMXT2    (1 << 30)       /* "Intel Wireless MMX2 technology".  */
+#define FL_ARCH6KZ    (1 << 31)       /* ARMv6KZ architecture.  */
 
 /* Flags that only effect tuning, not available instructions.  */
 #define FL_TUNE		(FL_WBUF | FL_VFPV2 | FL_STRONG | FL_LDSCHED \
@@ -387,7 +402,7 @@ extern bool arm_is_constant_pool_ref (rtx);
 #define FL_FOR_ARCH6J	FL_FOR_ARCH6
 #define FL_FOR_ARCH6K	(FL_FOR_ARCH6 | FL_ARCH6K)
 #define FL_FOR_ARCH6Z	FL_FOR_ARCH6
-#define FL_FOR_ARCH6ZK	FL_FOR_ARCH6K
+#define FL_FOR_ARCH6KZ	(FL_FOR_ARCH6K | FL_ARCH6KZ)
 #define FL_FOR_ARCH6T2	(FL_FOR_ARCH6 | FL_THUMB2)
 #define FL_FOR_ARCH6M	(FL_FOR_ARCH6 & ~FL_NOTM)
 #define FL_FOR_ARCH7	((FL_FOR_ARCH6T2 & ~FL_NOTM) | FL_ARCH7)
@@ -427,6 +442,9 @@ extern int arm_arch6;
 /* Nonzero if this chip supports the ARM 6K extensions.  */
 extern int arm_arch6k;
 
+/* Nonzero if this chip supports the ARM 6KZ extensions.  */
+extern int arm_arch6kz;
+
 /* Nonzero if instructions present in ARMv6-M can be used.  */
 extern int arm_arch6m;
 
@@ -467,12 +485,6 @@ extern int arm_tune_wbuf;
 /* Nonzero if tuning for Cortex-A9.  */
 extern int arm_tune_cortex_a9;
 
-/* Nonzero if generating Thumb instructions.  */
-extern int thumb_code;
-
-/* Nonzero if generating Thumb-1 instructions.  */
-extern int thumb1_code;
-
 /* Nonzero if we should define __THUMB_INTERWORK__ in the
    preprocessor.
    XXX This is a bit of a hack, it's intended to help work around
--- a/src/gcc/config/arm/arm-tables.opt
+++ b/src/gcc/config/arm/arm-tables.opt
@@ -371,46 +371,49 @@ EnumValue
 Enum(arm_arch) String(armv6z) Value(13)
 
 EnumValue
-Enum(arm_arch) String(armv6zk) Value(14)
+Enum(arm_arch) String(armv6kz) Value(14)
 
 EnumValue
-Enum(arm_arch) String(armv6t2) Value(15)
+Enum(arm_arch) String(armv6zk) Value(15)
 
 EnumValue
-Enum(arm_arch) String(armv6-m) Value(16)
+Enum(arm_arch) String(armv6t2) Value(16)
 
 EnumValue
-Enum(arm_arch) String(armv6s-m) Value(17)
+Enum(arm_arch) String(armv6-m) Value(17)
 
 EnumValue
-Enum(arm_arch) String(armv7) Value(18)
+Enum(arm_arch) String(armv6s-m) Value(18)
 
 EnumValue
-Enum(arm_arch) String(armv7-a) Value(19)
+Enum(arm_arch) String(armv7) Value(19)
 
 EnumValue
-Enum(arm_arch) String(armv7ve) Value(20)
+Enum(arm_arch) String(armv7-a) Value(20)
 
 EnumValue
-Enum(arm_arch) String(armv7-r) Value(21)
+Enum(arm_arch) String(armv7ve) Value(21)
 
 EnumValue
-Enum(arm_arch) String(armv7-m) Value(22)
+Enum(arm_arch) String(armv7-r) Value(22)
 
 EnumValue
-Enum(arm_arch) String(armv7e-m) Value(23)
+Enum(arm_arch) String(armv7-m) Value(23)
 
 EnumValue
-Enum(arm_arch) String(armv8-a) Value(24)
+Enum(arm_arch) String(armv7e-m) Value(24)
 
 EnumValue
-Enum(arm_arch) String(armv8-a+crc) Value(25)
+Enum(arm_arch) String(armv8-a) Value(25)
 
 EnumValue
-Enum(arm_arch) String(iwmmxt) Value(26)
+Enum(arm_arch) String(armv8-a+crc) Value(26)
 
 EnumValue
-Enum(arm_arch) String(iwmmxt2) Value(27)
+Enum(arm_arch) String(iwmmxt) Value(27)
+
+EnumValue
+Enum(arm_arch) String(iwmmxt2) Value(28)
 
 Enum
 Name(arm_fpu) Type(int)
--- a/src/gcc/config/arm/arm.c
+++ b/src/gcc/config/arm/arm.c
@@ -94,10 +94,12 @@
 #include "opts.h"
 #include "dumpfile.h"
 #include "gimple-expr.h"
+#include "target-globals.h"
 #include "builtins.h"
 #include "tm-constrs.h"
 #include "rtl-iter.h"
 #include "sched-int.h"
+#include "tree.h"
 
 /* Forward definitions of types.  */
 typedef struct minipool_node    Mnode;
@@ -121,6 +123,7 @@ static int arm_gen_constant (enum rtx_code, machine_mode, rtx,
 static unsigned bit_count (unsigned long);
 static int arm_address_register_rtx_p (rtx, int);
 static int arm_legitimate_index_p (machine_mode, rtx, RTX_CODE, int);
+static bool is_called_in_ARM_mode (tree);
 static int thumb2_legitimate_index_p (machine_mode, rtx, int);
 static int thumb1_base_register_rtx_p (rtx, machine_mode, int);
 static rtx arm_legitimize_address (rtx, rtx, machine_mode);
@@ -231,6 +234,7 @@ static void arm_encode_section_info (tree, rtx, int);
 
 static void arm_file_end (void);
 static void arm_file_start (void);
+static void arm_insert_attributes (tree, tree *);
 
 static void arm_setup_incoming_varargs (cumulative_args_t, machine_mode,
 					tree, int *, int);
@@ -264,6 +268,10 @@ static tree arm_build_builtin_va_list (void);
 static void arm_expand_builtin_va_start (tree, rtx);
 static tree arm_gimplify_va_arg_expr (tree, tree, gimple_seq *, gimple_seq *);
 static void arm_option_override (void);
+static void arm_option_print (FILE *, int, struct cl_target_option *);
+static void arm_set_current_function (tree);
+static bool arm_can_inline_p (tree, tree);
+static bool arm_valid_target_attribute_p (tree, tree, tree, int);
 static unsigned HOST_WIDE_INT arm_shift_truncation_mask (machine_mode);
 static bool arm_macro_fusion_p (void);
 static bool arm_cannot_copy_insn_p (rtx_insn *);
@@ -386,6 +394,9 @@ static const struct attribute_spec arm_attribute_table[] =
 #undef  TARGET_ATTRIBUTE_TABLE
 #define TARGET_ATTRIBUTE_TABLE arm_attribute_table
 
+#undef  TARGET_INSERT_ATTRIBUTES
+#define TARGET_INSERT_ATTRIBUTES arm_insert_attributes
+
 #undef TARGET_ASM_FILE_START
 #define TARGET_ASM_FILE_START arm_file_start
 #undef TARGET_ASM_FILE_END
@@ -412,9 +423,15 @@ static const struct attribute_spec arm_attribute_table[] =
 #undef  TARGET_ASM_FUNCTION_EPILOGUE
 #define TARGET_ASM_FUNCTION_EPILOGUE arm_output_function_epilogue
 
+#undef TARGET_CAN_INLINE_P
+#define TARGET_CAN_INLINE_P arm_can_inline_p
+
 #undef  TARGET_OPTION_OVERRIDE
 #define TARGET_OPTION_OVERRIDE arm_option_override
 
+#undef TARGET_OPTION_PRINT
+#define TARGET_OPTION_PRINT arm_option_print
+
 #undef  TARGET_COMP_TYPE_ATTRIBUTES
 #define TARGET_COMP_TYPE_ATTRIBUTES arm_comp_type_attributes
 
@@ -430,6 +447,12 @@ static const struct attribute_spec arm_attribute_table[] =
 #undef  TARGET_SCHED_ADJUST_COST
 #define TARGET_SCHED_ADJUST_COST arm_adjust_cost
 
+#undef TARGET_SET_CURRENT_FUNCTION
+#define TARGET_SET_CURRENT_FUNCTION arm_set_current_function
+
+#undef TARGET_OPTION_VALID_ATTRIBUTE_P
+#define TARGET_OPTION_VALID_ATTRIBUTE_P arm_valid_target_attribute_p
+
 #undef TARGET_SCHED_REORDER
 #define TARGET_SCHED_REORDER arm_sched_reorder
 
@@ -806,6 +829,9 @@ int arm_arch6 = 0;
 /* Nonzero if this chip supports the ARM 6K extensions.  */
 int arm_arch6k = 0;
 
+/* Nonzero if this chip supports the ARM 6KZ extensions.  */
+int arm_arch6kz = 0;
+
 /* Nonzero if instructions present in ARMv6-M can be used.  */
 int arm_arch6m = 0;
 
@@ -846,12 +872,6 @@ int arm_tune_wbuf = 0;
 /* Nonzero if tuning for Cortex-A9.  */
 int arm_tune_cortex_a9 = 0;
 
-/* Nonzero if generating Thumb instructions.  */
-int thumb_code = 0;
-
-/* Nonzero if generating Thumb-1 instructions.  */
-int thumb1_code = 0;
-
 /* Nonzero if we should define __THUMB_INTERWORK__ in the
    preprocessor.
    XXX This is a bit of a hack, it's intended to help work around
@@ -940,11 +960,13 @@ struct processors
 };
 
 
-#define ARM_PREFETCH_NOT_BENEFICIAL 0, -1, -1
-#define ARM_PREFETCH_BENEFICIAL(prefetch_slots,l1_size,l1_line_size) \
-  prefetch_slots, \
-  l1_size, \
-  l1_line_size
+#define ARM_PREFETCH_NOT_BENEFICIAL { 0, -1, -1 }
+#define ARM_PREFETCH_BENEFICIAL(num_slots,l1_size,l1_line_size) \
+  {								\
+    num_slots,							\
+    l1_size,							\
+    l1_line_size						\
+  }
 
 /* arm generic vectorizer costs.  */
 static const
@@ -1027,7 +1049,9 @@ const struct cpu_cost_table cortexa9_extra_costs =
     2,			/* stm_regs_per_insn_subsequent.  */
     COSTS_N_INSNS (1),	/* storef.  */
     COSTS_N_INSNS (1),	/* stored.  */
-    COSTS_N_INSNS (1)	/* store_unaligned.  */
+    COSTS_N_INSNS (1),	/* store_unaligned.  */
+    COSTS_N_INSNS (1),	/* loadv.  */
+    COSTS_N_INSNS (1)	/* storev.  */
   },
   {
     /* FP SFmode */
@@ -1128,7 +1152,9 @@ const struct cpu_cost_table cortexa8_extra_costs =
     2,			/* stm_regs_per_insn_subsequent.  */
     COSTS_N_INSNS (1),	/* storef.  */
     COSTS_N_INSNS (1),	/* stored.  */
-    COSTS_N_INSNS (1)	/* store_unaligned.  */
+    COSTS_N_INSNS (1),	/* store_unaligned.  */
+    COSTS_N_INSNS (1),	/* loadv.  */
+    COSTS_N_INSNS (1)	/* storev.  */
   },
   {
     /* FP SFmode */
@@ -1230,7 +1256,9 @@ const struct cpu_cost_table cortexa5_extra_costs =
     2,			/* stm_regs_per_insn_subsequent.  */
     COSTS_N_INSNS (2),	/* storef.  */
     COSTS_N_INSNS (2),	/* stored.  */
-    COSTS_N_INSNS (1)	/* store_unaligned.  */
+    COSTS_N_INSNS (1),	/* store_unaligned.  */
+    COSTS_N_INSNS (1),	/* loadv.  */
+    COSTS_N_INSNS (1)	/* storev.  */
   },
   {
     /* FP SFmode */
@@ -1333,7 +1361,9 @@ const struct cpu_cost_table cortexa7_extra_costs =
     2,			/* stm_regs_per_insn_subsequent.  */
     COSTS_N_INSNS (2),	/* storef.  */
     COSTS_N_INSNS (2),	/* stored.  */
-    COSTS_N_INSNS (1)	/* store_unaligned.  */
+    COSTS_N_INSNS (1),	/* store_unaligned.  */
+    COSTS_N_INSNS (1),	/* loadv.  */
+    COSTS_N_INSNS (1)	/* storev.  */
   },
   {
     /* FP SFmode */
@@ -1434,7 +1464,9 @@ const struct cpu_cost_table cortexa12_extra_costs =
     2,			/* stm_regs_per_insn_subsequent.  */
     COSTS_N_INSNS (2),	/* storef.  */
     COSTS_N_INSNS (2),	/* stored.  */
-    0			/* store_unaligned.  */
+    0,			/* store_unaligned.  */
+    COSTS_N_INSNS (1),	/* loadv.  */
+    COSTS_N_INSNS (1)	/* storev.  */
   },
   {
     /* FP SFmode */
@@ -1535,7 +1567,9 @@ const struct cpu_cost_table cortexa15_extra_costs =
     2,			/* stm_regs_per_insn_subsequent.  */
     0,			/* storef.  */
     0,			/* stored.  */
-    0			/* store_unaligned.  */
+    0,			/* store_unaligned.  */
+    COSTS_N_INSNS (1),	/* loadv.  */
+    COSTS_N_INSNS (1)	/* storev.  */
   },
   {
     /* FP SFmode */
@@ -1636,7 +1670,9 @@ const struct cpu_cost_table v7m_extra_costs =
     1,			/* stm_regs_per_insn_subsequent.  */
     COSTS_N_INSNS (2),	/* storef.  */
     COSTS_N_INSNS (3),	/* stored.  */
-    COSTS_N_INSNS (1)  /* store_unaligned.  */
+    COSTS_N_INSNS (1),	/* store_unaligned.  */
+    COSTS_N_INSNS (1),	/* loadv.  */
+    COSTS_N_INSNS (1)	/* storev.  */
   },
   {
     /* FP SFmode */
@@ -1678,49 +1714,50 @@ const struct cpu_cost_table v7m_extra_costs =
   }
 };
 
-#define ARM_FUSE_NOTHING	(0)
-#define ARM_FUSE_MOVW_MOVT	(1 << 0)
-
 const struct tune_params arm_slowmul_tune =
 {
   arm_slowmul_rtx_costs,
-  NULL,
-  NULL,						/* Sched adj cost.  */
+  NULL,					/* Insn extra costs.  */
+  NULL,					/* Sched adj cost.  */
+  arm_default_branch_cost,
+  &arm_default_vec_cost,
   3,						/* Constant limit.  */
   5,						/* Max cond insns.  */
+  8,						/* Memset max inline.  */
+  1,						/* Issue rate.  */
   ARM_PREFETCH_NOT_BENEFICIAL,
-  true,						/* Prefer constant pool.  */
-  arm_default_branch_cost,
-  false,					/* Prefer LDRD/STRD.  */
-  {true, true},					/* Prefer non short circuit.  */
-  &arm_default_vec_cost,                        /* Vectorizer costs.  */
-  false,                                        /* Prefer Neon for 64-bits bitops.  */
-  false, false,                                 /* Prefer 32-bit encodings.  */
-  false,					/* Prefer Neon for stringops.  */
-  8,						/* Maximum insns to inline memset.  */
-  ARM_FUSE_NOTHING,				/* Fuseable pairs of instructions.  */
-  ARM_SCHED_AUTOPREF_OFF			/* Sched L2 autopref.  */
+  tune_params::PREF_CONST_POOL_TRUE,
+  tune_params::PREF_LDRD_FALSE,
+  tune_params::LOG_OP_NON_SHORT_CIRCUIT_TRUE,		/* Thumb.  */
+  tune_params::LOG_OP_NON_SHORT_CIRCUIT_TRUE,		/* ARM.  */
+  tune_params::DISPARAGE_FLAGS_NEITHER,
+  tune_params::PREF_NEON_64_FALSE,
+  tune_params::PREF_NEON_STRINGOPS_FALSE,
+  tune_params::FUSE_NOTHING,
+  tune_params::SCHED_AUTOPREF_OFF
 };
 
 const struct tune_params arm_fastmul_tune =
 {
   arm_fastmul_rtx_costs,
-  NULL,
-  NULL,						/* Sched adj cost.  */
+  NULL,					/* Insn extra costs.  */
+  NULL,					/* Sched adj cost.  */
+  arm_default_branch_cost,
+  &arm_default_vec_cost,
   1,						/* Constant limit.  */
   5,						/* Max cond insns.  */
+  8,						/* Memset max inline.  */
+  1,						/* Issue rate.  */
   ARM_PREFETCH_NOT_BENEFICIAL,
-  true,						/* Prefer constant pool.  */
-  arm_default_branch_cost,
-  false,					/* Prefer LDRD/STRD.  */
-  {true, true},					/* Prefer non short circuit.  */
-  &arm_default_vec_cost,                        /* Vectorizer costs.  */
-  false,                                        /* Prefer Neon for 64-bits bitops.  */
-  false, false,                                 /* Prefer 32-bit encodings.  */
-  false,					/* Prefer Neon for stringops.  */
-  8,						/* Maximum insns to inline memset.  */
-  ARM_FUSE_NOTHING,				/* Fuseable pairs of instructions.  */
-  ARM_SCHED_AUTOPREF_OFF			/* Sched L2 autopref.  */
+  tune_params::PREF_CONST_POOL_TRUE,
+  tune_params::PREF_LDRD_FALSE,
+  tune_params::LOG_OP_NON_SHORT_CIRCUIT_TRUE,		/* Thumb.  */
+  tune_params::LOG_OP_NON_SHORT_CIRCUIT_TRUE,		/* ARM.  */
+  tune_params::DISPARAGE_FLAGS_NEITHER,
+  tune_params::PREF_NEON_64_FALSE,
+  tune_params::PREF_NEON_STRINGOPS_FALSE,
+  tune_params::FUSE_NOTHING,
+  tune_params::SCHED_AUTOPREF_OFF
 };
 
 /* StrongARM has early execution of branches, so a sequence that is worth
@@ -1729,233 +1766,279 @@ const struct tune_params arm_fastmul_tune =
 const struct tune_params arm_strongarm_tune =
 {
   arm_fastmul_rtx_costs,
-  NULL,
-  NULL,						/* Sched adj cost.  */
+  NULL,					/* Insn extra costs.  */
+  NULL,					/* Sched adj cost.  */
+  arm_default_branch_cost,
+  &arm_default_vec_cost,
   1,						/* Constant limit.  */
   3,						/* Max cond insns.  */
+  8,						/* Memset max inline.  */
+  1,						/* Issue rate.  */
   ARM_PREFETCH_NOT_BENEFICIAL,
-  true,						/* Prefer constant pool.  */
-  arm_default_branch_cost,
-  false,					/* Prefer LDRD/STRD.  */
-  {true, true},					/* Prefer non short circuit.  */
-  &arm_default_vec_cost,                        /* Vectorizer costs.  */
-  false,                                        /* Prefer Neon for 64-bits bitops.  */
-  false, false,                                 /* Prefer 32-bit encodings.  */
-  false,					/* Prefer Neon for stringops.  */
-  8,						/* Maximum insns to inline memset.  */
-  ARM_FUSE_NOTHING,				/* Fuseable pairs of instructions.  */
-  ARM_SCHED_AUTOPREF_OFF			/* Sched L2 autopref.  */
+  tune_params::PREF_CONST_POOL_TRUE,
+  tune_params::PREF_LDRD_FALSE,
+  tune_params::LOG_OP_NON_SHORT_CIRCUIT_TRUE,		/* Thumb.  */
+  tune_params::LOG_OP_NON_SHORT_CIRCUIT_TRUE,		/* ARM.  */
+  tune_params::DISPARAGE_FLAGS_NEITHER,
+  tune_params::PREF_NEON_64_FALSE,
+  tune_params::PREF_NEON_STRINGOPS_FALSE,
+  tune_params::FUSE_NOTHING,
+  tune_params::SCHED_AUTOPREF_OFF
 };
 
 const struct tune_params arm_xscale_tune =
 {
   arm_xscale_rtx_costs,
-  NULL,
+  NULL,					/* Insn extra costs.  */
   xscale_sched_adjust_cost,
+  arm_default_branch_cost,
+  &arm_default_vec_cost,
   2,						/* Constant limit.  */
   3,						/* Max cond insns.  */
+  8,						/* Memset max inline.  */
+  1,						/* Issue rate.  */
   ARM_PREFETCH_NOT_BENEFICIAL,
-  true,						/* Prefer constant pool.  */
-  arm_default_branch_cost,
-  false,					/* Prefer LDRD/STRD.  */
-  {true, true},					/* Prefer non short circuit.  */
-  &arm_default_vec_cost,                        /* Vectorizer costs.  */
-  false,                                        /* Prefer Neon for 64-bits bitops.  */
-  false, false,                                 /* Prefer 32-bit encodings.  */
-  false,					/* Prefer Neon for stringops.  */
-  8,						/* Maximum insns to inline memset.  */
-  ARM_FUSE_NOTHING,				/* Fuseable pairs of instructions.  */
-  ARM_SCHED_AUTOPREF_OFF			/* Sched L2 autopref.  */
+  tune_params::PREF_CONST_POOL_TRUE,
+  tune_params::PREF_LDRD_FALSE,
+  tune_params::LOG_OP_NON_SHORT_CIRCUIT_TRUE,		/* Thumb.  */
+  tune_params::LOG_OP_NON_SHORT_CIRCUIT_TRUE,		/* ARM.  */
+  tune_params::DISPARAGE_FLAGS_NEITHER,
+  tune_params::PREF_NEON_64_FALSE,
+  tune_params::PREF_NEON_STRINGOPS_FALSE,
+  tune_params::FUSE_NOTHING,
+  tune_params::SCHED_AUTOPREF_OFF
 };
 
 const struct tune_params arm_9e_tune =
 {
   arm_9e_rtx_costs,
-  NULL,
-  NULL,						/* Sched adj cost.  */
+  NULL,					/* Insn extra costs.  */
+  NULL,					/* Sched adj cost.  */
+  arm_default_branch_cost,
+  &arm_default_vec_cost,
   1,						/* Constant limit.  */
   5,						/* Max cond insns.  */
+  8,						/* Memset max inline.  */
+  1,						/* Issue rate.  */
   ARM_PREFETCH_NOT_BENEFICIAL,
-  true,						/* Prefer constant pool.  */
+  tune_params::PREF_CONST_POOL_TRUE,
+  tune_params::PREF_LDRD_FALSE,
+  tune_params::LOG_OP_NON_SHORT_CIRCUIT_TRUE,		/* Thumb.  */
+  tune_params::LOG_OP_NON_SHORT_CIRCUIT_TRUE,		/* ARM.  */
+  tune_params::DISPARAGE_FLAGS_NEITHER,
+  tune_params::PREF_NEON_64_FALSE,
+  tune_params::PREF_NEON_STRINGOPS_FALSE,
+  tune_params::FUSE_NOTHING,
+  tune_params::SCHED_AUTOPREF_OFF
+};
+
+const struct tune_params arm_marvell_pj4_tune =
+{
+  arm_9e_rtx_costs,
+  NULL,					/* Insn extra costs.  */
+  NULL,					/* Sched adj cost.  */
   arm_default_branch_cost,
-  false,					/* Prefer LDRD/STRD.  */
-  {true, true},					/* Prefer non short circuit.  */
-  &arm_default_vec_cost,                        /* Vectorizer costs.  */
-  false,                                        /* Prefer Neon for 64-bits bitops.  */
-  false, false,                                 /* Prefer 32-bit encodings.  */
-  false,					/* Prefer Neon for stringops.  */
-  8,						/* Maximum insns to inline memset.  */
-  ARM_FUSE_NOTHING,				/* Fuseable pairs of instructions.  */
-  ARM_SCHED_AUTOPREF_OFF			/* Sched L2 autopref.  */
+  &arm_default_vec_cost,
+  1,						/* Constant limit.  */
+  5,						/* Max cond insns.  */
+  8,						/* Memset max inline.  */
+  2,						/* Issue rate.  */
+  ARM_PREFETCH_NOT_BENEFICIAL,
+  tune_params::PREF_CONST_POOL_TRUE,
+  tune_params::PREF_LDRD_FALSE,
+  tune_params::LOG_OP_NON_SHORT_CIRCUIT_TRUE,		/* Thumb.  */
+  tune_params::LOG_OP_NON_SHORT_CIRCUIT_TRUE,		/* ARM.  */
+  tune_params::DISPARAGE_FLAGS_NEITHER,
+  tune_params::PREF_NEON_64_FALSE,
+  tune_params::PREF_NEON_STRINGOPS_FALSE,
+  tune_params::FUSE_NOTHING,
+  tune_params::SCHED_AUTOPREF_OFF
 };
 
 const struct tune_params arm_v6t2_tune =
 {
   arm_9e_rtx_costs,
-  NULL,
-  NULL,						/* Sched adj cost.  */
+  NULL,					/* Insn extra costs.  */
+  NULL,					/* Sched adj cost.  */
+  arm_default_branch_cost,
+  &arm_default_vec_cost,
   1,						/* Constant limit.  */
   5,						/* Max cond insns.  */
+  8,						/* Memset max inline.  */
+  1,						/* Issue rate.  */
   ARM_PREFETCH_NOT_BENEFICIAL,
-  false,					/* Prefer constant pool.  */
-  arm_default_branch_cost,
-  false,					/* Prefer LDRD/STRD.  */
-  {true, true},					/* Prefer non short circuit.  */
-  &arm_default_vec_cost,                        /* Vectorizer costs.  */
-  false,                                        /* Prefer Neon for 64-bits bitops.  */
-  false, false,                                 /* Prefer 32-bit encodings.  */
-  false,					/* Prefer Neon for stringops.  */
-  8,						/* Maximum insns to inline memset.  */
-  ARM_FUSE_NOTHING,				/* Fuseable pairs of instructions.  */
-  ARM_SCHED_AUTOPREF_OFF			/* Sched L2 autopref.  */
+  tune_params::PREF_CONST_POOL_FALSE,
+  tune_params::PREF_LDRD_FALSE,
+  tune_params::LOG_OP_NON_SHORT_CIRCUIT_TRUE,		/* Thumb.  */
+  tune_params::LOG_OP_NON_SHORT_CIRCUIT_TRUE,		/* ARM.  */
+  tune_params::DISPARAGE_FLAGS_NEITHER,
+  tune_params::PREF_NEON_64_FALSE,
+  tune_params::PREF_NEON_STRINGOPS_FALSE,
+  tune_params::FUSE_NOTHING,
+  tune_params::SCHED_AUTOPREF_OFF
 };
 
+
 /* Generic Cortex tuning.  Use more specific tunings if appropriate.  */
 const struct tune_params arm_cortex_tune =
 {
   arm_9e_rtx_costs,
   &generic_extra_costs,
-  NULL,						/* Sched adj cost.  */
+  NULL,					/* Sched adj cost.  */
+  arm_default_branch_cost,
+  &arm_default_vec_cost,
   1,						/* Constant limit.  */
   5,						/* Max cond insns.  */
+  8,						/* Memset max inline.  */
+  2,						/* Issue rate.  */
   ARM_PREFETCH_NOT_BENEFICIAL,
-  false,					/* Prefer constant pool.  */
-  arm_default_branch_cost,
-  false,					/* Prefer LDRD/STRD.  */
-  {true, true},					/* Prefer non short circuit.  */
-  &arm_default_vec_cost,                        /* Vectorizer costs.  */
-  false,                                        /* Prefer Neon for 64-bits bitops.  */
-  false, false,                                 /* Prefer 32-bit encodings.  */
-  false,					/* Prefer Neon for stringops.  */
-  8,						/* Maximum insns to inline memset.  */
-  ARM_FUSE_NOTHING,				/* Fuseable pairs of instructions.  */
-  ARM_SCHED_AUTOPREF_OFF			/* Sched L2 autopref.  */
+  tune_params::PREF_CONST_POOL_FALSE,
+  tune_params::PREF_LDRD_FALSE,
+  tune_params::LOG_OP_NON_SHORT_CIRCUIT_TRUE,		/* Thumb.  */
+  tune_params::LOG_OP_NON_SHORT_CIRCUIT_TRUE,		/* ARM.  */
+  tune_params::DISPARAGE_FLAGS_NEITHER,
+  tune_params::PREF_NEON_64_FALSE,
+  tune_params::PREF_NEON_STRINGOPS_FALSE,
+  tune_params::FUSE_NOTHING,
+  tune_params::SCHED_AUTOPREF_OFF
 };
 
 const struct tune_params arm_cortex_a8_tune =
 {
   arm_9e_rtx_costs,
   &cortexa8_extra_costs,
-  NULL,						/* Sched adj cost.  */
+  NULL,					/* Sched adj cost.  */
+  arm_default_branch_cost,
+  &arm_default_vec_cost,
   1,						/* Constant limit.  */
   5,						/* Max cond insns.  */
+  8,						/* Memset max inline.  */
+  2,						/* Issue rate.  */
   ARM_PREFETCH_NOT_BENEFICIAL,
-  false,					/* Prefer constant pool.  */
-  arm_default_branch_cost,
-  false,					/* Prefer LDRD/STRD.  */
-  {true, true},					/* Prefer non short circuit.  */
-  &arm_default_vec_cost,                        /* Vectorizer costs.  */
-  false,                                        /* Prefer Neon for 64-bits bitops.  */
-  false, false,                                 /* Prefer 32-bit encodings.  */
-  true,						/* Prefer Neon for stringops.  */
-  8,						/* Maximum insns to inline memset.  */
-  ARM_FUSE_NOTHING,				/* Fuseable pairs of instructions.  */
-  ARM_SCHED_AUTOPREF_OFF			/* Sched L2 autopref.  */
+  tune_params::PREF_CONST_POOL_FALSE,
+  tune_params::PREF_LDRD_FALSE,
+  tune_params::LOG_OP_NON_SHORT_CIRCUIT_TRUE,		/* Thumb.  */
+  tune_params::LOG_OP_NON_SHORT_CIRCUIT_TRUE,		/* ARM.  */
+  tune_params::DISPARAGE_FLAGS_NEITHER,
+  tune_params::PREF_NEON_64_FALSE,
+  tune_params::PREF_NEON_STRINGOPS_TRUE,
+  tune_params::FUSE_NOTHING,
+  tune_params::SCHED_AUTOPREF_OFF
 };
 
 const struct tune_params arm_cortex_a7_tune =
 {
   arm_9e_rtx_costs,
   &cortexa7_extra_costs,
-  NULL,
+  NULL,					/* Sched adj cost.  */
+  arm_default_branch_cost,
+  &arm_default_vec_cost,
   1,						/* Constant limit.  */
   5,						/* Max cond insns.  */
+  8,						/* Memset max inline.  */
+  2,						/* Issue rate.  */
   ARM_PREFETCH_NOT_BENEFICIAL,
-  false,					/* Prefer constant pool.  */
-  arm_default_branch_cost,
-  false,					/* Prefer LDRD/STRD.  */
-  {true, true},					/* Prefer non short circuit.  */
-  &arm_default_vec_cost,			/* Vectorizer costs.  */
-  false,					/* Prefer Neon for 64-bits bitops.  */
-  false, false,                                 /* Prefer 32-bit encodings.  */
-  true,						/* Prefer Neon for stringops.  */
-  8,						/* Maximum insns to inline memset.  */
-  ARM_FUSE_NOTHING,				/* Fuseable pairs of instructions.  */
-  ARM_SCHED_AUTOPREF_OFF			/* Sched L2 autopref.  */
+  tune_params::PREF_CONST_POOL_FALSE,
+  tune_params::PREF_LDRD_FALSE,
+  tune_params::LOG_OP_NON_SHORT_CIRCUIT_TRUE,		/* Thumb.  */
+  tune_params::LOG_OP_NON_SHORT_CIRCUIT_TRUE,		/* ARM.  */
+  tune_params::DISPARAGE_FLAGS_NEITHER,
+  tune_params::PREF_NEON_64_FALSE,
+  tune_params::PREF_NEON_STRINGOPS_TRUE,
+  tune_params::FUSE_NOTHING,
+  tune_params::SCHED_AUTOPREF_OFF
 };
 
 const struct tune_params arm_cortex_a15_tune =
 {
   arm_9e_rtx_costs,
   &cortexa15_extra_costs,
-  NULL,						/* Sched adj cost.  */
+  NULL,					/* Sched adj cost.  */
+  arm_default_branch_cost,
+  &arm_default_vec_cost,
   1,						/* Constant limit.  */
   2,						/* Max cond insns.  */
+  8,						/* Memset max inline.  */
+  3,						/* Issue rate.  */
   ARM_PREFETCH_NOT_BENEFICIAL,
-  false,					/* Prefer constant pool.  */
-  arm_default_branch_cost,
-  true,						/* Prefer LDRD/STRD.  */
-  {true, true},					/* Prefer non short circuit.  */
-  &arm_default_vec_cost,                        /* Vectorizer costs.  */
-  false,                                        /* Prefer Neon for 64-bits bitops.  */
-  true, true,                                   /* Prefer 32-bit encodings.  */
-  true,						/* Prefer Neon for stringops.  */
-  8,						/* Maximum insns to inline memset.  */
-  ARM_FUSE_NOTHING,				/* Fuseable pairs of instructions.  */
-  ARM_SCHED_AUTOPREF_FULL			/* Sched L2 autopref.  */
+  tune_params::PREF_CONST_POOL_FALSE,
+  tune_params::PREF_LDRD_TRUE,
+  tune_params::LOG_OP_NON_SHORT_CIRCUIT_TRUE,		/* Thumb.  */
+  tune_params::LOG_OP_NON_SHORT_CIRCUIT_TRUE,		/* ARM.  */
+  tune_params::DISPARAGE_FLAGS_ALL,
+  tune_params::PREF_NEON_64_FALSE,
+  tune_params::PREF_NEON_STRINGOPS_TRUE,
+  tune_params::FUSE_NOTHING,
+  tune_params::SCHED_AUTOPREF_FULL
 };
 
 const struct tune_params arm_cortex_a53_tune =
 {
   arm_9e_rtx_costs,
   &cortexa53_extra_costs,
-  NULL,						/* Scheduler cost adjustment.  */
+  NULL,					/* Sched adj cost.  */
+  arm_default_branch_cost,
+  &arm_default_vec_cost,
   1,						/* Constant limit.  */
   5,						/* Max cond insns.  */
+  8,						/* Memset max inline.  */
+  2,						/* Issue rate.  */
   ARM_PREFETCH_NOT_BENEFICIAL,
-  false,					/* Prefer constant pool.  */
-  arm_default_branch_cost,
-  false,					/* Prefer LDRD/STRD.  */
-  {true, true},					/* Prefer non short circuit.  */
-  &arm_default_vec_cost,			/* Vectorizer costs.  */
-  false,					/* Prefer Neon for 64-bits bitops.  */
-  false, false,                                 /* Prefer 32-bit encodings.  */
-  true,						/* Prefer Neon for stringops.  */
-  8,						/* Maximum insns to inline memset.  */
-  ARM_FUSE_MOVW_MOVT,				/* Fuseable pairs of instructions.  */
-  ARM_SCHED_AUTOPREF_OFF			/* Sched L2 autopref.  */
+  tune_params::PREF_CONST_POOL_FALSE,
+  tune_params::PREF_LDRD_FALSE,
+  tune_params::LOG_OP_NON_SHORT_CIRCUIT_TRUE,		/* Thumb.  */
+  tune_params::LOG_OP_NON_SHORT_CIRCUIT_TRUE,		/* ARM.  */
+  tune_params::DISPARAGE_FLAGS_NEITHER,
+  tune_params::PREF_NEON_64_FALSE,
+  tune_params::PREF_NEON_STRINGOPS_TRUE,
+  FUSE_OPS (tune_params::FUSE_MOVW_MOVT),
+  tune_params::SCHED_AUTOPREF_OFF
 };
 
 const struct tune_params arm_cortex_a57_tune =
 {
   arm_9e_rtx_costs,
   &cortexa57_extra_costs,
-  NULL,                                         /* Scheduler cost adjustment.  */
-  1,                                           /* Constant limit.  */
-  2,                                           /* Max cond insns.  */
-  ARM_PREFETCH_NOT_BENEFICIAL,
-  false,                                       /* Prefer constant pool.  */
+  NULL,					/* Sched adj cost.  */
   arm_default_branch_cost,
-  true,                                       /* Prefer LDRD/STRD.  */
-  {true, true},                                /* Prefer non short circuit.  */
-  &arm_default_vec_cost,                       /* Vectorizer costs.  */
-  false,                                       /* Prefer Neon for 64-bits bitops.  */
-  true, true,                                  /* Prefer 32-bit encodings.  */
-  true,						/* Prefer Neon for stringops.  */
-  8,						/* Maximum insns to inline memset.  */
-  ARM_FUSE_MOVW_MOVT,				/* Fuseable pairs of instructions.  */
-  ARM_SCHED_AUTOPREF_FULL			/* Sched L2 autopref.  */
+  &arm_default_vec_cost,
+  1,						/* Constant limit.  */
+  2,						/* Max cond insns.  */
+  8,						/* Memset max inline.  */
+  3,						/* Issue rate.  */
+  ARM_PREFETCH_NOT_BENEFICIAL,
+  tune_params::PREF_CONST_POOL_FALSE,
+  tune_params::PREF_LDRD_TRUE,
+  tune_params::LOG_OP_NON_SHORT_CIRCUIT_TRUE,		/* Thumb.  */
+  tune_params::LOG_OP_NON_SHORT_CIRCUIT_TRUE,		/* ARM.  */
+  tune_params::DISPARAGE_FLAGS_ALL,
+  tune_params::PREF_NEON_64_FALSE,
+  tune_params::PREF_NEON_STRINGOPS_TRUE,
+  FUSE_OPS (tune_params::FUSE_MOVW_MOVT),
+  tune_params::SCHED_AUTOPREF_FULL
 };
 
 const struct tune_params arm_xgene1_tune =
 {
   arm_9e_rtx_costs,
   &xgene1_extra_costs,
-  NULL,                                        /* Scheduler cost adjustment.  */
-  1,                                           /* Constant limit.  */
-  2,                                           /* Max cond insns.  */
-  ARM_PREFETCH_NOT_BENEFICIAL,
-  false,                                       /* Prefer constant pool.  */
+  NULL,					/* Sched adj cost.  */
   arm_default_branch_cost,
-  true,                                        /* Prefer LDRD/STRD.  */
-  {true, true},                                /* Prefer non short circuit.  */
-  &arm_default_vec_cost,                       /* Vectorizer costs.  */
-  false,                                       /* Prefer Neon for 64-bits bitops.  */
-  true, true,                                  /* Prefer 32-bit encodings.  */
-  false,				       /* Prefer Neon for stringops.  */
-  32,					       /* Maximum insns to inline memset.  */
-  ARM_FUSE_NOTHING,				/* Fuseable pairs of instructions.  */
-  ARM_SCHED_AUTOPREF_OFF			/* Sched L2 autopref.  */
+  &arm_default_vec_cost,
+  1,						/* Constant limit.  */
+  2,						/* Max cond insns.  */
+  32,						/* Memset max inline.  */
+  4,						/* Issue rate.  */
+  ARM_PREFETCH_NOT_BENEFICIAL,
+  tune_params::PREF_CONST_POOL_FALSE,
+  tune_params::PREF_LDRD_TRUE,
+  tune_params::LOG_OP_NON_SHORT_CIRCUIT_TRUE,		/* Thumb.  */
+  tune_params::LOG_OP_NON_SHORT_CIRCUIT_TRUE,		/* ARM.  */
+  tune_params::DISPARAGE_FLAGS_ALL,
+  tune_params::PREF_NEON_64_FALSE,
+  tune_params::PREF_NEON_STRINGOPS_FALSE,
+  tune_params::FUSE_NOTHING,
+  tune_params::SCHED_AUTOPREF_OFF
 };
 
 /* Branches can be dual-issued on Cortex-A5, so conditional execution is
@@ -1965,21 +2048,23 @@ const struct tune_params arm_cortex_a5_tune =
 {
   arm_9e_rtx_costs,
   &cortexa5_extra_costs,
-  NULL,						/* Sched adj cost.  */
+  NULL,					/* Sched adj cost.  */
+  arm_cortex_a5_branch_cost,
+  &arm_default_vec_cost,
   1,						/* Constant limit.  */
   1,						/* Max cond insns.  */
+  8,						/* Memset max inline.  */
+  2,						/* Issue rate.  */
   ARM_PREFETCH_NOT_BENEFICIAL,
-  false,					/* Prefer constant pool.  */
-  arm_cortex_a5_branch_cost,
-  false,					/* Prefer LDRD/STRD.  */
-  {false, false},				/* Prefer non short circuit.  */
-  &arm_default_vec_cost,                        /* Vectorizer costs.  */
-  false,                                        /* Prefer Neon for 64-bits bitops.  */
-  false, false,                                 /* Prefer 32-bit encodings.  */
-  true,						/* Prefer Neon for stringops.  */
-  8,						/* Maximum insns to inline memset.  */
-  ARM_FUSE_NOTHING,				/* Fuseable pairs of instructions.  */
-  ARM_SCHED_AUTOPREF_OFF			/* Sched L2 autopref.  */
+  tune_params::PREF_CONST_POOL_FALSE,
+  tune_params::PREF_LDRD_FALSE,
+  tune_params::LOG_OP_NON_SHORT_CIRCUIT_FALSE,		/* Thumb.  */
+  tune_params::LOG_OP_NON_SHORT_CIRCUIT_FALSE,		/* ARM.  */
+  tune_params::DISPARAGE_FLAGS_NEITHER,
+  tune_params::PREF_NEON_64_FALSE,
+  tune_params::PREF_NEON_STRINGOPS_TRUE,
+  tune_params::FUSE_NOTHING,
+  tune_params::SCHED_AUTOPREF_OFF
 };
 
 const struct tune_params arm_cortex_a9_tune =
@@ -1987,41 +2072,45 @@ const struct tune_params arm_cortex_a9_tune =
   arm_9e_rtx_costs,
   &cortexa9_extra_costs,
   cortex_a9_sched_adjust_cost,
+  arm_default_branch_cost,
+  &arm_default_vec_cost,
   1,						/* Constant limit.  */
   5,						/* Max cond insns.  */
+  8,						/* Memset max inline.  */
+  2,						/* Issue rate.  */
   ARM_PREFETCH_BENEFICIAL(4,32,32),
-  false,					/* Prefer constant pool.  */
-  arm_default_branch_cost,
-  false,					/* Prefer LDRD/STRD.  */
-  {true, true},					/* Prefer non short circuit.  */
-  &arm_default_vec_cost,                        /* Vectorizer costs.  */
-  false,                                        /* Prefer Neon for 64-bits bitops.  */
-  false, false,                                 /* Prefer 32-bit encodings.  */
-  false,					/* Prefer Neon for stringops.  */
-  8,						/* Maximum insns to inline memset.  */
-  ARM_FUSE_NOTHING,				/* Fuseable pairs of instructions.  */
-  ARM_SCHED_AUTOPREF_OFF			/* Sched L2 autopref.  */
+  tune_params::PREF_CONST_POOL_FALSE,
+  tune_params::PREF_LDRD_FALSE,
+  tune_params::LOG_OP_NON_SHORT_CIRCUIT_TRUE,		/* Thumb.  */
+  tune_params::LOG_OP_NON_SHORT_CIRCUIT_TRUE,		/* ARM.  */
+  tune_params::DISPARAGE_FLAGS_NEITHER,
+  tune_params::PREF_NEON_64_FALSE,
+  tune_params::PREF_NEON_STRINGOPS_FALSE,
+  tune_params::FUSE_NOTHING,
+  tune_params::SCHED_AUTOPREF_OFF
 };
 
 const struct tune_params arm_cortex_a12_tune =
 {
   arm_9e_rtx_costs,
   &cortexa12_extra_costs,
-  NULL,						/* Sched adj cost.  */
+  NULL,					/* Sched adj cost.  */
+  arm_default_branch_cost,
+  &arm_default_vec_cost,                        /* Vectorizer costs.  */
   1,						/* Constant limit.  */
   2,						/* Max cond insns.  */
+  8,						/* Memset max inline.  */
+  2,						/* Issue rate.  */
   ARM_PREFETCH_NOT_BENEFICIAL,
-  false,					/* Prefer constant pool.  */
-  arm_default_branch_cost,
-  true,						/* Prefer LDRD/STRD.  */
-  {true, true},					/* Prefer non short circuit.  */
-  &arm_default_vec_cost,                        /* Vectorizer costs.  */
-  false,                                        /* Prefer Neon for 64-bits bitops.  */
-  true, true,                                   /* Prefer 32-bit encodings.  */
-  true,						/* Prefer Neon for stringops.  */
-  8,						/* Maximum insns to inline memset.  */
-  ARM_FUSE_MOVW_MOVT,				/* Fuseable pairs of instructions.  */
-  ARM_SCHED_AUTOPREF_OFF			/* Sched L2 autopref.  */
+  tune_params::PREF_CONST_POOL_FALSE,
+  tune_params::PREF_LDRD_TRUE,
+  tune_params::LOG_OP_NON_SHORT_CIRCUIT_TRUE,		/* Thumb.  */
+  tune_params::LOG_OP_NON_SHORT_CIRCUIT_TRUE,		/* ARM.  */
+  tune_params::DISPARAGE_FLAGS_ALL,
+  tune_params::PREF_NEON_64_FALSE,
+  tune_params::PREF_NEON_STRINGOPS_TRUE,
+  FUSE_OPS (tune_params::FUSE_MOVW_MOVT),
+  tune_params::SCHED_AUTOPREF_OFF
 };
 
 /* armv7m tuning.  On Cortex-M4 cores for example, MOVW/MOVT take a single
@@ -2035,21 +2124,23 @@ const struct tune_params arm_v7m_tune =
 {
   arm_9e_rtx_costs,
   &v7m_extra_costs,
-  NULL,						/* Sched adj cost.  */
+  NULL,					/* Sched adj cost.  */
+  arm_cortex_m_branch_cost,
+  &arm_default_vec_cost,
   1,						/* Constant limit.  */
   2,						/* Max cond insns.  */
+  8,						/* Memset max inline.  */
+  1,						/* Issue rate.  */
   ARM_PREFETCH_NOT_BENEFICIAL,
-  true,						/* Prefer constant pool.  */
-  arm_cortex_m_branch_cost,
-  false,					/* Prefer LDRD/STRD.  */
-  {false, false},				/* Prefer non short circuit.  */
-  &arm_default_vec_cost,                        /* Vectorizer costs.  */
-  false,                                        /* Prefer Neon for 64-bits bitops.  */
-  false, false,                                 /* Prefer 32-bit encodings.  */
-  false,					/* Prefer Neon for stringops.  */
-  8,						/* Maximum insns to inline memset.  */
-  ARM_FUSE_NOTHING,				/* Fuseable pairs of instructions.  */
-  ARM_SCHED_AUTOPREF_OFF			/* Sched L2 autopref.  */
+  tune_params::PREF_CONST_POOL_TRUE,
+  tune_params::PREF_LDRD_FALSE,
+  tune_params::LOG_OP_NON_SHORT_CIRCUIT_FALSE,		/* Thumb.  */
+  tune_params::LOG_OP_NON_SHORT_CIRCUIT_FALSE,		/* ARM.  */
+  tune_params::DISPARAGE_FLAGS_NEITHER,
+  tune_params::PREF_NEON_64_FALSE,
+  tune_params::PREF_NEON_STRINGOPS_FALSE,
+  tune_params::FUSE_NOTHING,
+  tune_params::SCHED_AUTOPREF_OFF
 };
 
 /* Cortex-M7 tuning.  */
@@ -2058,21 +2149,23 @@ const struct tune_params arm_cortex_m7_tune =
 {
   arm_9e_rtx_costs,
   &v7m_extra_costs,
-  NULL,						/* Sched adj cost.  */
+  NULL,					/* Sched adj cost.  */
+  arm_cortex_m7_branch_cost,
+  &arm_default_vec_cost,
   0,						/* Constant limit.  */
   1,						/* Max cond insns.  */
+  8,						/* Memset max inline.  */
+  2,						/* Issue rate.  */
   ARM_PREFETCH_NOT_BENEFICIAL,
-  true,						/* Prefer constant pool.  */
-  arm_cortex_m7_branch_cost,
-  false,					/* Prefer LDRD/STRD.  */
-  {true, true},					/* Prefer non short circuit.  */
-  &arm_default_vec_cost,                        /* Vectorizer costs.  */
-  false,                                        /* Prefer Neon for 64-bits bitops.  */
-  false, false,                                 /* Prefer 32-bit encodings.  */
-  false,					/* Prefer Neon for stringops.  */
-  8,						/* Maximum insns to inline memset.  */
-  ARM_FUSE_NOTHING,				/* Fuseable pairs of instructions.  */
-  ARM_SCHED_AUTOPREF_OFF			/* Sched L2 autopref.  */
+  tune_params::PREF_CONST_POOL_TRUE,
+  tune_params::PREF_LDRD_FALSE,
+  tune_params::LOG_OP_NON_SHORT_CIRCUIT_TRUE,		/* Thumb.  */
+  tune_params::LOG_OP_NON_SHORT_CIRCUIT_TRUE,		/* ARM.  */
+  tune_params::DISPARAGE_FLAGS_NEITHER,
+  tune_params::PREF_NEON_64_FALSE,
+  tune_params::PREF_NEON_STRINGOPS_FALSE,
+  tune_params::FUSE_NOTHING,
+  tune_params::SCHED_AUTOPREF_OFF
 };
 
 /* The arm_v6m_tune is duplicated from arm_cortex_tune, rather than
@@ -2080,43 +2173,47 @@ const struct tune_params arm_cortex_m7_tune =
 const struct tune_params arm_v6m_tune =
 {
   arm_9e_rtx_costs,
-  NULL,
-  NULL,						/* Sched adj cost.  */
+  NULL,					/* Insn extra costs.  */
+  NULL,					/* Sched adj cost.  */
+  arm_default_branch_cost,
+  &arm_default_vec_cost,                        /* Vectorizer costs.  */
   1,						/* Constant limit.  */
   5,						/* Max cond insns.  */
+  8,						/* Memset max inline.  */
+  1,						/* Issue rate.  */
   ARM_PREFETCH_NOT_BENEFICIAL,
-  false,					/* Prefer constant pool.  */
-  arm_default_branch_cost,
-  false,					/* Prefer LDRD/STRD.  */
-  {false, false},				/* Prefer non short circuit.  */
-  &arm_default_vec_cost,                        /* Vectorizer costs.  */
-  false,                                        /* Prefer Neon for 64-bits bitops.  */
-  false, false,                                 /* Prefer 32-bit encodings.  */
-  false,					/* Prefer Neon for stringops.  */
-  8,						/* Maximum insns to inline memset.  */
-  ARM_FUSE_NOTHING,				/* Fuseable pairs of instructions.  */
-  ARM_SCHED_AUTOPREF_OFF			/* Sched L2 autopref.  */
+  tune_params::PREF_CONST_POOL_FALSE,
+  tune_params::PREF_LDRD_FALSE,
+  tune_params::LOG_OP_NON_SHORT_CIRCUIT_FALSE,		/* Thumb.  */
+  tune_params::LOG_OP_NON_SHORT_CIRCUIT_FALSE,		/* ARM.  */
+  tune_params::DISPARAGE_FLAGS_NEITHER,
+  tune_params::PREF_NEON_64_FALSE,
+  tune_params::PREF_NEON_STRINGOPS_FALSE,
+  tune_params::FUSE_NOTHING,
+  tune_params::SCHED_AUTOPREF_OFF
 };
 
 const struct tune_params arm_fa726te_tune =
 {
   arm_9e_rtx_costs,
-  NULL,
+  NULL,					/* Insn extra costs.  */
   fa726te_sched_adjust_cost,
+  arm_default_branch_cost,
+  &arm_default_vec_cost,
   1,						/* Constant limit.  */
   5,						/* Max cond insns.  */
+  8,						/* Memset max inline.  */
+  2,						/* Issue rate.  */
   ARM_PREFETCH_NOT_BENEFICIAL,
-  true,						/* Prefer constant pool.  */
-  arm_default_branch_cost,
-  false,					/* Prefer LDRD/STRD.  */
-  {true, true},					/* Prefer non short circuit.  */
-  &arm_default_vec_cost,                        /* Vectorizer costs.  */
-  false,                                        /* Prefer Neon for 64-bits bitops.  */
-  false, false,                                 /* Prefer 32-bit encodings.  */
-  false,					/* Prefer Neon for stringops.  */
-  8,						/* Maximum insns to inline memset.  */
-  ARM_FUSE_NOTHING,				/* Fuseable pairs of instructions.  */
-  ARM_SCHED_AUTOPREF_OFF			/* Sched L2 autopref.  */
+  tune_params::PREF_CONST_POOL_TRUE,
+  tune_params::PREF_LDRD_FALSE,
+  tune_params::LOG_OP_NON_SHORT_CIRCUIT_TRUE,		/* Thumb.  */
+  tune_params::LOG_OP_NON_SHORT_CIRCUIT_TRUE,		/* ARM.  */
+  tune_params::DISPARAGE_FLAGS_NEITHER,
+  tune_params::PREF_NEON_64_FALSE,
+  tune_params::PREF_NEON_STRINGOPS_FALSE,
+  tune_params::FUSE_NOTHING,
+  tune_params::SCHED_AUTOPREF_OFF
 };
 
 
@@ -2626,6 +2723,171 @@ arm_gimplify_va_arg_expr (tree valist, tree type, gimple_seq *pre_p,
   return std_gimplify_va_arg_expr (valist, type, pre_p, post_p);
 }
 
+/* Check any incompatible options that the user has specified.  */
+static void
+arm_option_check_internal (struct gcc_options *opts)
+{
+  int flags = opts->x_target_flags;
+
+  /* Make sure that the processor choice does not conflict with any of the
+     other command line choices.  */
+  if (TARGET_ARM_P (flags) && !(insn_flags & FL_NOTM))
+    error ("target CPU does not support ARM mode");
+
+  /* TARGET_BACKTRACE calls leaf_function_p, which causes a crash if done
+     from here where no function is being compiled currently.  */
+  if ((TARGET_TPCS_FRAME || TARGET_TPCS_LEAF_FRAME) && TARGET_ARM_P (flags))
+    warning (0, "enabling backtrace support is only meaningful when compiling for the Thumb");
+
+  if (TARGET_ARM_P (flags) && TARGET_CALLEE_INTERWORKING)
+    warning (0, "enabling callee interworking support is only meaningful when compiling for the Thumb");
+
+  /* If this target is normally configured to use APCS frames, warn if they
+     are turned off and debugging is turned on.  */
+  if (TARGET_ARM_P (flags)
+      && write_symbols != NO_DEBUG
+      && !TARGET_APCS_FRAME
+      && (TARGET_DEFAULT & MASK_APCS_FRAME))
+    warning (0, "-g with -mno-apcs-frame may not give sensible debugging");
+
+  /* iWMMXt unsupported under Thumb mode.  */
+  if (TARGET_THUMB_P (flags) && TARGET_IWMMXT)
+    error ("iWMMXt unsupported under Thumb mode");
+
+  if (TARGET_HARD_TP && TARGET_THUMB1_P (flags))
+    error ("can not use -mtp=cp15 with 16-bit Thumb");
+
+  if (TARGET_THUMB_P (flags) && TARGET_VXWORKS_RTP && flag_pic)
+    {
+      error ("RTP PIC is incompatible with Thumb");
+      flag_pic = 0;
+    }
+
+  /* We only support -mslow-flash-data on armv7-m targets.  */
+  if (target_slow_flash_data
+      && ((!(arm_arch7 && !arm_arch_notm) && !arm_arch7em)
+	  || (TARGET_THUMB1_P (flags) || flag_pic || TARGET_NEON)))
+    error ("-mslow-flash-data only supports non-pic code on armv7-m targets");
+}
+
+/* Recompute the global settings depending on target attribute options.  */
+
+static void
+arm_option_params_internal (void)
+{
+  /* If we are not using the default (ARM mode) section anchor offset
+     ranges, then set the correct ranges now.  */
+  if (TARGET_THUMB1)
+    {
+      /* Thumb-1 LDR instructions cannot have negative offsets.
+         Permissible positive offset ranges are 5-bit (for byte loads),
+         6-bit (for halfword loads), or 7-bit (for word loads).
+         Empirical results suggest a 7-bit anchor range gives the best
+         overall code size.  */
+      targetm.min_anchor_offset = 0;
+      targetm.max_anchor_offset = 127;
+    }
+  else if (TARGET_THUMB2)
+    {
+      /* The minimum is set such that the total size of the block
+         for a particular anchor is 248 + 1 + 4095 bytes, which is
+         divisible by eight, ensuring natural spacing of anchors.  */
+      targetm.min_anchor_offset = -248;
+      targetm.max_anchor_offset = 4095;
+    }
+  else
+    {
+      targetm.min_anchor_offset = TARGET_MIN_ANCHOR_OFFSET;
+      targetm.max_anchor_offset = TARGET_MAX_ANCHOR_OFFSET;
+    }
+
+  if (optimize_size)
+    {
+      /* If optimizing for size, bump the number of instructions that we
+         are prepared to conditionally execute (even on a StrongARM).  */
+      max_insns_skipped = 6;
+
+      /* For THUMB2, we limit the conditional sequence to one IT block.  */
+      if (TARGET_THUMB2)
+        max_insns_skipped = arm_restrict_it ? 1 : 4;
+    }
+  else
+    /* When -mrestrict-it is in use tone down the if-conversion.  */
+    max_insns_skipped = (TARGET_THUMB2 && arm_restrict_it)
+      ? 1 : current_tune->max_insns_skipped;
+}
+
+/* True if -mflip-thumb should next add an attribute for the default
+   mode, false if it should next add an attribute for the opposite mode.  */
+static GTY(()) bool thumb_flipper;
+
+/* Options after initial target override.  */
+static GTY(()) tree init_optimize;
+
+/* Reset options between modes that the user has specified.  */
+static void
+arm_option_override_internal (struct gcc_options *opts,
+			      struct gcc_options *opts_set)
+{
+  if (TARGET_THUMB_P (opts->x_target_flags) && !(insn_flags & FL_THUMB))
+    {
+      warning (0, "target CPU does not support THUMB instructions");
+      opts->x_target_flags &= ~MASK_THUMB;
+    }
+
+  if (TARGET_APCS_FRAME && TARGET_THUMB_P (opts->x_target_flags))
+    {
+      /* warning (0, "ignoring -mapcs-frame because -mthumb was used"); */
+      opts->x_target_flags &= ~MASK_APCS_FRAME;
+    }
+
+  /* Callee super interworking implies thumb interworking.  Adding
+     this to the flags here simplifies the logic elsewhere.  */
+  if (TARGET_THUMB_P (opts->x_target_flags) && TARGET_CALLEE_INTERWORKING)
+    opts->x_target_flags |= MASK_INTERWORK;
+
+  /* need to remember initial values so combinaisons of options like
+     -mflip-thumb -mthumb -fno-schedule-insns work for any attribute.  */
+  cl_optimization *to = TREE_OPTIMIZATION (init_optimize);
+
+  if (! opts_set->x_arm_restrict_it)
+    opts->x_arm_restrict_it = arm_arch8;
+
+  if (!TARGET_THUMB2_P (opts->x_target_flags))
+    opts->x_arm_restrict_it = 0;
+
+  /* Don't warn since it's on by default in -O2.  */
+  if (TARGET_THUMB1_P (opts->x_target_flags))
+    opts->x_flag_schedule_insns = 0;
+  else
+    opts->x_flag_schedule_insns = to->x_flag_schedule_insns;
+
+  /* Disable shrink-wrap when optimizing function for size, since it tends to
+     generate additional returns.  */
+  if (optimize_function_for_size_p (cfun)
+      && TARGET_THUMB2_P (opts->x_target_flags))
+    opts->x_flag_shrink_wrap = false;
+  else
+    opts->x_flag_shrink_wrap = to->x_flag_shrink_wrap;
+
+  /* In Thumb1 mode, we emit the epilogue in RTL, but the last insn
+     - epilogue_insns - does not accurately model the corresponding insns
+     emitted in the asm file.  In particular, see the comment in thumb_exit
+     'Find out how many of the (return) argument registers we can corrupt'.
+     As a consequence, the epilogue may clobber registers without fipa-ra
+     finding out about it.  Therefore, disable fipa-ra in Thumb1 mode.
+     TODO: Accurately model clobbers for epilogue_insns and reenable
+     fipa-ra.  */
+  if (TARGET_THUMB1_P (opts->x_target_flags))
+    opts->x_flag_ipa_ra = 0;
+  else
+    opts->x_flag_ipa_ra = to->x_flag_ipa_ra;
+
+  /* Thumb2 inline assembly code should always use unified syntax.
+     This will apply to ARM and Thumb1 eventually.  */
+  opts->x_inline_asm_unified = TARGET_THUMB2_P (opts->x_target_flags);
+}
+
 /* Fix up any incompatible options that the user has specified.  */
 static void
 arm_option_override (void)
@@ -2772,10 +3034,9 @@ arm_option_override (void)
   tune_flags = arm_selected_tune->flags;
   current_tune = arm_selected_tune->tune;
 
-  /* Make sure that the processor choice does not conflict with any of the
-     other command line choices.  */
-  if (TARGET_ARM && !(insn_flags & FL_NOTM))
-    error ("target CPU does not support ARM mode");
+  /* TBD: Dwarf info for apcs frame is not handled yet.  */
+  if (TARGET_APCS_FRAME)
+    flag_shrink_wrap = false;
 
   /* BPABI targets use linker tricks to allow interworking on cores
      without thumb support.  */
@@ -2785,31 +3046,6 @@ arm_option_override (void)
       target_flags &= ~MASK_INTERWORK;
     }
 
-  if (TARGET_THUMB && !(insn_flags & FL_THUMB))
-    {
-      warning (0, "target CPU does not support THUMB instructions");
-      target_flags &= ~MASK_THUMB;
-    }
-
-  if (TARGET_APCS_FRAME && TARGET_THUMB)
-    {
-      /* warning (0, "ignoring -mapcs-frame because -mthumb was used"); */
-      target_flags &= ~MASK_APCS_FRAME;
-    }
-
-  /* Callee super interworking implies thumb interworking.  Adding
-     this to the flags here simplifies the logic elsewhere.  */
-  if (TARGET_THUMB && TARGET_CALLEE_INTERWORKING)
-    target_flags |= MASK_INTERWORK;
-
-  /* TARGET_BACKTRACE calls leaf_function_p, which causes a crash if done
-     from here where no function is being compiled currently.  */
-  if ((TARGET_TPCS_FRAME || TARGET_TPCS_LEAF_FRAME) && TARGET_ARM)
-    warning (0, "enabling backtrace support is only meaningful when compiling for the Thumb");
-
-  if (TARGET_ARM && TARGET_CALLEE_INTERWORKING)
-    warning (0, "enabling callee interworking support is only meaningful when compiling for the Thumb");
-
   if (TARGET_APCS_STACK && !TARGET_APCS_FRAME)
     {
       warning (0, "-mapcs-stack-check incompatible with -mno-apcs-frame");
@@ -2825,14 +3061,6 @@ arm_option_override (void)
   if (TARGET_APCS_REENT)
     warning (0, "APCS reentrant code not supported.  Ignored");
 
-  /* If this target is normally configured to use APCS frames, warn if they
-     are turned off and debugging is turned on.  */
-  if (TARGET_ARM
-      && write_symbols != NO_DEBUG
-      && !TARGET_APCS_FRAME
-      && (TARGET_DEFAULT & MASK_APCS_FRAME))
-    warning (0, "-g with -mno-apcs-frame may not give sensible debugging");
-
   if (TARGET_APCS_FLOAT)
     warning (0, "passing floating point arguments in fp regs not yet supported");
 
@@ -2844,6 +3072,7 @@ arm_option_override (void)
   arm_arch5e = (insn_flags & FL_ARCH5E) != 0;
   arm_arch6 = (insn_flags & FL_ARCH6) != 0;
   arm_arch6k = (insn_flags & FL_ARCH6K) != 0;
+  arm_arch6kz = arm_arch6k && (insn_flags & FL_ARCH6KZ);
   arm_arch_notm = (insn_flags & FL_NOTM) != 0;
   arm_arch6m = arm_arch6 && !arm_arch_notm;
   arm_arch7 = (insn_flags & FL_ARCH7) != 0;
@@ -2854,8 +3083,6 @@ arm_option_override (void)
 
   arm_ld_sched = (tune_flags & FL_LDSCHED) != 0;
   arm_tune_strongarm = (tune_flags & FL_STRONG) != 0;
-  thumb_code = TARGET_ARM == 0;
-  thumb1_code = TARGET_THUMB1 != 0;
   arm_tune_wbuf = (tune_flags & FL_WBUF) != 0;
   arm_tune_xscale = (tune_flags & FL_XSCALE) != 0;
   arm_arch_iwmmxt = (insn_flags & FL_IWMMXT) != 0;
@@ -2866,32 +3093,6 @@ arm_option_override (void)
   arm_tune_cortex_a9 = (arm_tune == cortexa9) != 0;
   arm_arch_crc = (insn_flags & FL_CRC32) != 0;
   arm_m_profile_small_mul = (insn_flags & FL_SMALLMUL) != 0;
-  if (arm_restrict_it == 2)
-    arm_restrict_it = arm_arch8 && TARGET_THUMB2;
-
-  if (!TARGET_THUMB2)
-    arm_restrict_it = 0;
-
-  /* If we are not using the default (ARM mode) section anchor offset
-     ranges, then set the correct ranges now.  */
-  if (TARGET_THUMB1)
-    {
-      /* Thumb-1 LDR instructions cannot have negative offsets.
-         Permissible positive offset ranges are 5-bit (for byte loads),
-         6-bit (for halfword loads), or 7-bit (for word loads).
-         Empirical results suggest a 7-bit anchor range gives the best
-         overall code size.  */
-      targetm.min_anchor_offset = 0;
-      targetm.max_anchor_offset = 127;
-    }
-  else if (TARGET_THUMB2)
-    {
-      /* The minimum is set such that the total size of the block
-         for a particular anchor is 248 + 1 + 4095 bytes, which is
-         divisible by eight, ensuring natural spacing of anchors.  */
-      targetm.min_anchor_offset = -248;
-      targetm.max_anchor_offset = 4095;
-    }
 
   /* V5 code we generate is completely interworking capable, so we turn off
      TARGET_INTERWORK here to avoid many tests later on.  */
@@ -2951,10 +3152,6 @@ arm_option_override (void)
   if (TARGET_IWMMXT && TARGET_NEON)
     error ("iWMMXt and NEON are incompatible");
 
-  /* iWMMXt unsupported under Thumb mode.  */
-  if (TARGET_THUMB && TARGET_IWMMXT)
-    error ("iWMMXt unsupported under Thumb mode");
-
   /* __fp16 support currently assumes the core has ldrh.  */
   if (!arm_arch4 && arm_fp16_format != ARM_FP16_FORMAT_NONE)
     sorry ("__fp16 and no ldrh");
@@ -2999,9 +3196,6 @@ arm_option_override (void)
 	target_thread_pointer = TP_SOFT;
     }
 
-  if (TARGET_HARD_TP && TARGET_THUMB1)
-    error ("can not use -mtp=cp15 with 16-bit Thumb");
-
   /* Override the default structure alignment for AAPCS ABI.  */
   if (!global_options_set.x_arm_structure_size_boundary)
     {
@@ -3024,12 +3218,6 @@ arm_option_override (void)
 	}
     }
 
-  if (!TARGET_ARM && TARGET_VXWORKS_RTP && flag_pic)
-    {
-      error ("RTP PIC is incompatible with Thumb");
-      flag_pic = 0;
-    }
-
   /* If stack checking is disabled, we can use r10 as the PIC register,
      which keeps r9 available.  The EABI specifies r9 as the PIC register.  */
   if (flag_pic && TARGET_SINGLE_PIC_BASE)
@@ -3097,25 +3285,6 @@ arm_option_override (void)
       unaligned_access = 0;
     }
 
-  if (TARGET_THUMB1 && flag_schedule_insns)
-    {
-      /* Don't warn since it's on by default in -O2.  */
-      flag_schedule_insns = 0;
-    }
-
-  if (optimize_size)
-    {
-      /* If optimizing for size, bump the number of instructions that we
-         are prepared to conditionally execute (even on a StrongARM).  */
-      max_insns_skipped = 6;
-
-      /* For THUMB2, we limit the conditional sequence to one IT block.  */
-      if (TARGET_THUMB2)
-	max_insns_skipped = MAX_INSN_PER_IT_BLOCK;
-    }
-  else
-    max_insns_skipped = current_tune->max_insns_skipped;
-
   /* Hot/Cold partitioning is not currently supported, since we can't
      handle literal pool placement in that case.  */
   if (flag_reorder_blocks_and_partition)
@@ -3140,31 +3309,33 @@ arm_option_override (void)
       && abi_version_at_least(2))
     flag_strict_volatile_bitfields = 1;
 
-  /* Enable sw prefetching at -O3 for CPUS that have prefetch, and we have deemed
-     it beneficial (signified by setting num_prefetch_slots to 1 or more.)  */
+  /* Enable sw prefetching at -O3 for CPUS that have prefetch, and we
+     have deemed it beneficial (signified by setting
+     prefetch.num_slots to 1 or more).  */
   if (flag_prefetch_loop_arrays < 0
       && HAVE_prefetch
       && optimize >= 3
-      && current_tune->num_prefetch_slots > 0)
+      && current_tune->prefetch.num_slots > 0)
     flag_prefetch_loop_arrays = 1;
 
-  /* Set up parameters to be used in prefetching algorithm.  Do not override the
-     defaults unless we are tuning for a core we have researched values for.  */
-  if (current_tune->num_prefetch_slots > 0)
+  /* Set up parameters to be used in prefetching algorithm.  Do not
+     override the defaults unless we are tuning for a core we have
+     researched values for.  */
+  if (current_tune->prefetch.num_slots > 0)
     maybe_set_param_value (PARAM_SIMULTANEOUS_PREFETCHES,
-                           current_tune->num_prefetch_slots,
-                           global_options.x_param_values,
-                           global_options_set.x_param_values);
-  if (current_tune->l1_cache_line_size >= 0)
+			   current_tune->prefetch.num_slots,
+			   global_options.x_param_values,
+			   global_options_set.x_param_values);
+  if (current_tune->prefetch.l1_cache_line_size >= 0)
     maybe_set_param_value (PARAM_L1_CACHE_LINE_SIZE,
-                           current_tune->l1_cache_line_size,
-                           global_options.x_param_values,
-                           global_options_set.x_param_values);
-  if (current_tune->l1_cache_size >= 0)
+			   current_tune->prefetch.l1_cache_line_size,
+			   global_options.x_param_values,
+			   global_options_set.x_param_values);
+  if (current_tune->prefetch.l1_cache_size >= 0)
     maybe_set_param_value (PARAM_L1_CACHE_SIZE,
-                           current_tune->l1_cache_size,
-                           global_options.x_param_values,
-                           global_options_set.x_param_values);
+			   current_tune->prefetch.l1_cache_size,
+			   global_options.x_param_values,
+			   global_options_set.x_param_values);
 
   /* Use Neon to perform 64-bits operations rather than core
      registers.  */
@@ -3174,67 +3345,63 @@ arm_option_override (void)
 
   /* Use the alternative scheduling-pressure algorithm by default.  */
   maybe_set_param_value (PARAM_SCHED_PRESSURE_ALGORITHM, SCHED_PRESSURE_MODEL,
-                         global_options.x_param_values,
-                         global_options_set.x_param_values);
+			 global_options.x_param_values,
+			 global_options_set.x_param_values);
 
   /* Look through ready list and all of queue for instructions
      relevant for L2 auto-prefetcher.  */
   int param_sched_autopref_queue_depth;
-  if (current_tune->sched_autopref == ARM_SCHED_AUTOPREF_OFF)
-    param_sched_autopref_queue_depth = -1;
-  else if (current_tune->sched_autopref == ARM_SCHED_AUTOPREF_RANK)
-    param_sched_autopref_queue_depth = 0;
-  else if (current_tune->sched_autopref == ARM_SCHED_AUTOPREF_FULL)
-    param_sched_autopref_queue_depth = max_insn_queue_index + 1;
-  else
-    gcc_unreachable ();
-  maybe_set_param_value (PARAM_SCHED_AUTOPREF_QUEUE_DEPTH,
-			 param_sched_autopref_queue_depth,
-                         global_options.x_param_values,
-                         global_options_set.x_param_values);
 
-  /* Disable shrink-wrap when optimizing function for size, since it tends to
-     generate additional returns.  */
-  if (optimize_function_for_size_p (cfun) && TARGET_THUMB2)
-    flag_shrink_wrap = false;
-  /* TBD: Dwarf info for apcs frame is not handled yet.  */
-  if (TARGET_APCS_FRAME)
-    flag_shrink_wrap = false;
+  switch (current_tune->sched_autopref)
+    {
+    case tune_params::SCHED_AUTOPREF_OFF:
+      param_sched_autopref_queue_depth = -1;
+      break;
+
+    case tune_params::SCHED_AUTOPREF_RANK:
+      param_sched_autopref_queue_depth = 0;
+      break;
+
+    case tune_params::SCHED_AUTOPREF_FULL:
+      param_sched_autopref_queue_depth = max_insn_queue_index + 1;
+      break;
+
+    default:
+      gcc_unreachable ();
+    }
 
-  /* We only support -mslow-flash-data on armv7-m targets.  */
-  if (target_slow_flash_data
-      && ((!(arm_arch7 && !arm_arch_notm) && !arm_arch7em)
-	  || (TARGET_THUMB1 || flag_pic || TARGET_NEON)))
-    error ("-mslow-flash-data only supports non-pic code on armv7-m targets");
+  maybe_set_param_value (PARAM_SCHED_AUTOPREF_QUEUE_DEPTH,
+			 param_sched_autopref_queue_depth,
+			 global_options.x_param_values,
+			 global_options_set.x_param_values);
 
   /* Currently, for slow flash data, we just disable literal pools.  */
   if (target_slow_flash_data)
     arm_disable_literal_pool = true;
 
-  /* Thumb2 inline assembly code should always use unified syntax.
-     This will apply to ARM and Thumb1 eventually.  */
-  if (TARGET_THUMB2)
-    inline_asm_unified = 1;
-
   /* Disable scheduling fusion by default if it's not armv7 processor
      or doesn't prefer ldrd/strd.  */
   if (flag_schedule_fusion == 2
       && (!arm_arch7 || !current_tune->prefer_ldrd_strd))
     flag_schedule_fusion = 0;
 
-  /* In Thumb1 mode, we emit the epilogue in RTL, but the last insn
-     - epilogue_insns - does not accurately model the corresponding insns
-     emitted in the asm file.  In particular, see the comment in thumb_exit
-     'Find out how many of the (return) argument registers we can corrupt'.
-     As a consequence, the epilogue may clobber registers without fipa-ra
-     finding out about it.  Therefore, disable fipa-ra in Thumb1 mode.
-     TODO: Accurately model clobbers for epilogue_insns and reenable
-     fipa-ra.  */
-  if (TARGET_THUMB1)
-    flag_ipa_ra = 0;
+  /* Need to remember initial options before they are overriden.  */
+  init_optimize = build_optimization_node (&global_options);
+
+  arm_option_override_internal (&global_options, &global_options_set);
+  arm_option_check_internal (&global_options);
+  arm_option_params_internal ();
 
   /* Register global variables with the garbage collector.  */
   arm_add_gc_roots ();
+
+  /* Save the initial options in case the user does function specific
+     options.  */
+  target_option_default_node = target_option_current_node
+    = build_target_option_node (&global_options);
+
+  /* Init initial mode for testing.  */
+  thumb_flipper = TARGET_THUMB;
 }
 
 static void
@@ -3388,13 +3555,20 @@ arm_warn_func_return (tree decl)
 static void
 arm_asm_trampoline_template (FILE *f)
 {
+  if (TARGET_UNIFIED_ASM)
+    fprintf (f, "\t.syntax unified\n");
+  else
+    fprintf (f, "\t.syntax divided\n");
+
   if (TARGET_ARM)
     {
+      fprintf (f, "\t.arm\n");
       asm_fprintf (f, "\tldr\t%r, [%r, #0]\n", STATIC_CHAIN_REGNUM, PC_REGNUM);
       asm_fprintf (f, "\tldr\t%r, [%r, #0]\n", PC_REGNUM, PC_REGNUM);
     }
   else if (TARGET_THUMB2)
     {
+      fprintf (f, "\t.thumb\n");
       /* The Thumb-2 trampoline is similar to the arm implementation.
 	 Unlike 16-bit Thumb, we enter the stub in thumb mode.  */
       asm_fprintf (f, "\tldr.w\t%r, [%r, #4]\n",
@@ -7946,236 +8120,6 @@ thumb_legitimize_address (rtx x, rtx orig_x, machine_mode mode)
   return x;
 }
 
-bool
-arm_legitimize_reload_address (rtx *p,
-			       machine_mode mode,
-			       int opnum, int type,
-			       int ind_levels ATTRIBUTE_UNUSED)
-{
-  /* We must recognize output that we have already generated ourselves.  */
-  if (GET_CODE (*p) == PLUS
-      && GET_CODE (XEXP (*p, 0)) == PLUS
-      && REG_P (XEXP (XEXP (*p, 0), 0))
-      && CONST_INT_P (XEXP (XEXP (*p, 0), 1))
-      && CONST_INT_P (XEXP (*p, 1)))
-    {
-      push_reload (XEXP (*p, 0), NULL_RTX, &XEXP (*p, 0), NULL,
-		   MODE_BASE_REG_CLASS (mode), GET_MODE (*p),
-		   VOIDmode, 0, 0, opnum, (enum reload_type) type);
-      return true;
-    }
-
-  if (GET_CODE (*p) == PLUS
-      && REG_P (XEXP (*p, 0))
-      && ARM_REGNO_OK_FOR_BASE_P (REGNO (XEXP (*p, 0)))
-      /* If the base register is equivalent to a constant, let the generic
-	 code handle it.  Otherwise we will run into problems if a future
-	 reload pass decides to rematerialize the constant.  */
-      && !reg_equiv_constant (ORIGINAL_REGNO (XEXP (*p, 0)))
-      && CONST_INT_P (XEXP (*p, 1)))
-    {
-      HOST_WIDE_INT val = INTVAL (XEXP (*p, 1));
-      HOST_WIDE_INT low, high;
-
-      /* Detect coprocessor load/stores.  */
-      bool coproc_p = ((TARGET_HARD_FLOAT
-			&& TARGET_VFP
-			&& (mode == SFmode || mode == DFmode))
-		       || (TARGET_REALLY_IWMMXT
-			   && VALID_IWMMXT_REG_MODE (mode))
-		       || (TARGET_NEON
-			   && (VALID_NEON_DREG_MODE (mode)
-			       || VALID_NEON_QREG_MODE (mode))));
-
-      /* For some conditions, bail out when lower two bits are unaligned.  */
-      if ((val & 0x3) != 0
-	  /* Coprocessor load/store indexes are 8-bits + '00' appended.  */
-	  && (coproc_p
-	      /* For DI, and DF under soft-float: */
-	      || ((mode == DImode || mode == DFmode)
-		  /* Without ldrd, we use stm/ldm, which does not
-		     fair well with unaligned bits.  */
-		  && (! TARGET_LDRD
-		      /* Thumb-2 ldrd/strd is [-1020,+1020] in steps of 4.  */
-		      || TARGET_THUMB2))))
-	return false;
-
-      /* When breaking down a [reg+index] reload address into [(reg+high)+low],
-	 of which the (reg+high) gets turned into a reload add insn,
-	 we try to decompose the index into high/low values that can often
-	 also lead to better reload CSE.
-	 For example:
-	         ldr r0, [r2, #4100]  // Offset too large
-		 ldr r1, [r2, #4104]  // Offset too large
-
-	 is best reloaded as:
-	         add t1, r2, #4096
-		 ldr r0, [t1, #4]
-		 add t2, r2, #4096
-		 ldr r1, [t2, #8]
-
-	 which post-reload CSE can simplify in most cases to eliminate the
-	 second add instruction:
-	         add t1, r2, #4096
-		 ldr r0, [t1, #4]
-		 ldr r1, [t1, #8]
-
-	 The idea here is that we want to split out the bits of the constant
-	 as a mask, rather than as subtracting the maximum offset that the
-	 respective type of load/store used can handle.
-
-	 When encountering negative offsets, we can still utilize it even if
-	 the overall offset is positive; sometimes this may lead to an immediate
-	 that can be constructed with fewer instructions.
-	 For example:
-	         ldr r0, [r2, #0x3FFFFC]
-
-	 This is best reloaded as:
-	         add t1, r2, #0x400000
-		 ldr r0, [t1, #-4]
-
-	 The trick for spotting this for a load insn with N bits of offset
-	 (i.e. bits N-1:0) is to look at bit N; if it is set, then chose a
-	 negative offset that is going to make bit N and all the bits below
-	 it become zero in the remainder part.
-
-	 The SIGN_MAG_LOW_ADDR_BITS macro below implements this, with respect
-	 to sign-magnitude addressing (i.e. separate +- bit, or 1's complement),
-	 used in most cases of ARM load/store instructions.  */
-
-#define SIGN_MAG_LOW_ADDR_BITS(VAL, N)					\
-      (((VAL) & ((1 << (N)) - 1))					\
-       ? (((VAL) & ((1 << ((N) + 1)) - 1)) ^ (1 << (N))) - (1 << (N))	\
-       : 0)
-
-      if (coproc_p)
-	{
-	  low = SIGN_MAG_LOW_ADDR_BITS (val, 10);
-
-	  /* NEON quad-word load/stores are made of two double-word accesses,
-	     so the valid index range is reduced by 8. Treat as 9-bit range if
-	     we go over it.  */
-	  if (TARGET_NEON && VALID_NEON_QREG_MODE (mode) && low >= 1016)
-	    low = SIGN_MAG_LOW_ADDR_BITS (val, 9);
-	}
-      else if (GET_MODE_SIZE (mode) == 8)
-	{
-	  if (TARGET_LDRD)
-	    low = (TARGET_THUMB2
-		   ? SIGN_MAG_LOW_ADDR_BITS (val, 10)
-		   : SIGN_MAG_LOW_ADDR_BITS (val, 8));
-	  else
-	    /* For pre-ARMv5TE (without ldrd), we use ldm/stm(db/da/ib)
-	       to access doublewords. The supported load/store offsets are
-	       -8, -4, and 4, which we try to produce here.  */
-	    low = ((val & 0xf) ^ 0x8) - 0x8;
-	}
-      else if (GET_MODE_SIZE (mode) < 8)
-	{
-	  /* NEON element load/stores do not have an offset.  */
-	  if (TARGET_NEON_FP16 && mode == HFmode)
-	    return false;
-
-	  if (TARGET_THUMB2)
-	    {
-	      /* Thumb-2 has an asymmetrical index range of (-256,4096).
-		 Try the wider 12-bit range first, and re-try if the result
-		 is out of range.  */
-	      low = SIGN_MAG_LOW_ADDR_BITS (val, 12);
-	      if (low < -255)
-		low = SIGN_MAG_LOW_ADDR_BITS (val, 8);
-	    }
-	  else
-	    {
-	      if (mode == HImode || mode == HFmode)
-		{
-		  if (arm_arch4)
-		    low = SIGN_MAG_LOW_ADDR_BITS (val, 8);
-		  else
-		    {
-		      /* The storehi/movhi_bytes fallbacks can use only
-			 [-4094,+4094] of the full ldrb/strb index range.  */
-		      low = SIGN_MAG_LOW_ADDR_BITS (val, 12);
-		      if (low == 4095 || low == -4095)
-			return false;
-		    }
-		}
-	      else
-		low = SIGN_MAG_LOW_ADDR_BITS (val, 12);
-	    }
-	}
-      else
-	return false;
-
-      high = ((((val - low) & (unsigned HOST_WIDE_INT) 0xffffffff)
-	       ^ (unsigned HOST_WIDE_INT) 0x80000000)
-	      - (unsigned HOST_WIDE_INT) 0x80000000);
-      /* Check for overflow or zero */
-      if (low == 0 || high == 0 || (high + low != val))
-	return false;
-
-      /* Reload the high part into a base reg; leave the low part
-	 in the mem.
-	 Note that replacing this gen_rtx_PLUS with plus_constant is
-	 wrong in this case because we rely on the
-	 (plus (plus reg c1) c2) structure being preserved so that
-	 XEXP (*p, 0) in push_reload below uses the correct term.  */
-      *p = gen_rtx_PLUS (GET_MODE (*p),
-			 gen_rtx_PLUS (GET_MODE (*p), XEXP (*p, 0),
-				       GEN_INT (high)),
-			 GEN_INT (low));
-      push_reload (XEXP (*p, 0), NULL_RTX, &XEXP (*p, 0), NULL,
-		   MODE_BASE_REG_CLASS (mode), GET_MODE (*p),
-		   VOIDmode, 0, 0, opnum, (enum reload_type) type);
-      return true;
-    }
-
-  return false;
-}
-
-rtx
-thumb_legitimize_reload_address (rtx *x_p,
-				 machine_mode mode,
-				 int opnum, int type,
-				 int ind_levels ATTRIBUTE_UNUSED)
-{
-  rtx x = *x_p;
-
-  if (GET_CODE (x) == PLUS
-      && GET_MODE_SIZE (mode) < 4
-      && REG_P (XEXP (x, 0))
-      && XEXP (x, 0) == stack_pointer_rtx
-      && CONST_INT_P (XEXP (x, 1))
-      && !thumb_legitimate_offset_p (mode, INTVAL (XEXP (x, 1))))
-    {
-      rtx orig_x = x;
-
-      x = copy_rtx (x);
-      push_reload (orig_x, NULL_RTX, x_p, NULL, MODE_BASE_REG_CLASS (mode),
-		   Pmode, VOIDmode, 0, 0, opnum, (enum reload_type) type);
-      return x;
-    }
-
-  /* If both registers are hi-regs, then it's better to reload the
-     entire expression rather than each register individually.  That
-     only requires one reload register rather than two.  */
-  if (GET_CODE (x) == PLUS
-      && REG_P (XEXP (x, 0))
-      && REG_P (XEXP (x, 1))
-      && !REG_MODE_OK_FOR_REG_BASE_P (XEXP (x, 0), mode)
-      && !REG_MODE_OK_FOR_REG_BASE_P (XEXP (x, 1), mode))
-    {
-      rtx orig_x = x;
-
-      x = copy_rtx (x);
-      push_reload (orig_x, NULL_RTX, x_p, NULL, MODE_BASE_REG_CLASS (mode),
-		   Pmode, VOIDmode, 0, 0, opnum, (enum reload_type) type);
-      return x;
-    }
-
-  return NULL;
-}
-
 /* Return TRUE if X contains any TLS symbol references.  */
 
 bool
@@ -9399,7 +9343,8 @@ static bool
 arm_unspec_cost (rtx x, enum rtx_code /* outer_code */, bool speed_p, int *cost)
 {
   const struct cpu_cost_table *extra_cost = current_tune->insn_extra_cost;
-  gcc_assert (GET_CODE (x) == UNSPEC);
+  rtx_code code = GET_CODE (x);
+  gcc_assert (code == UNSPEC || code == UNSPEC_VOLATILE);
 
   switch (XINT (x, 1))
     {
@@ -9445,7 +9390,7 @@ arm_unspec_cost (rtx x, enum rtx_code /* outer_code */, bool speed_p, int *cost)
       *cost = COSTS_N_INSNS (2);
       break;
     }
-  return false;
+  return true;
 }
 
 /* Cost of a libcall.  We assume one insn per argument, an amount for the
@@ -11008,6 +10953,7 @@ arm_new_rtx_costs (rtx x, enum rtx_code code, enum rtx_code outer_code,
       *cost = LIBCALL_COST (1);
       return false;
 
+    case UNSPEC_VOLATILE:
     case UNSPEC:
       return arm_unspec_cost (x, outer_code, speed_p, cost);
 
@@ -12908,12 +12854,12 @@ neon_expand_vector_init (rtx target, rtx vals)
 }
 
 /* Ensure OPERAND lies between LOW (inclusive) and HIGH (exclusive).  Raise
-   ERR if it doesn't.  FIXME: NEON bounds checks occur late in compilation, so
-   reported source locations are bogus.  */
+   ERR if it doesn't.  EXP indicates the source location, which includes the
+   inlining history for intrinsics.  */
 
 static void
 bounds_check (rtx operand, HOST_WIDE_INT low, HOST_WIDE_INT high,
-	      const char *err)
+	      const_tree exp, const char *desc)
 {
   HOST_WIDE_INT lane;
 
@@ -12922,15 +12868,22 @@ bounds_check (rtx operand, HOST_WIDE_INT low, HOST_WIDE_INT high,
   lane = INTVAL (operand);
 
   if (lane < low || lane >= high)
-    error (err);
+    {
+      if (exp)
+	error ("%K%s %lld out of range %lld - %lld",
+	       exp, desc, lane, low, high - 1);
+      else
+	error ("%s %lld out of range %lld - %lld", desc, lane, low, high - 1);
+    }
 }
 
 /* Bounds-check lanes.  */
 
 void
-neon_lane_bounds (rtx operand, HOST_WIDE_INT low, HOST_WIDE_INT high)
+neon_lane_bounds (rtx operand, HOST_WIDE_INT low, HOST_WIDE_INT high,
+		  const_tree exp)
 {
-  bounds_check (operand, low, high, "lane out of range");
+  bounds_check (operand, low, high, exp, "lane");
 }
 
 /* Bounds-check constants.  */
@@ -12938,7 +12891,7 @@ neon_lane_bounds (rtx operand, HOST_WIDE_INT low, HOST_WIDE_INT high)
 void
 neon_const_bounds (rtx operand, HOST_WIDE_INT low, HOST_WIDE_INT high)
 {
-  bounds_check (operand, low, high, "constant out of range");
+  bounds_check (operand, low, high, NULL_TREE, "constant");
 }
 
 HOST_WIDE_INT
@@ -17287,14 +17240,16 @@ thumb2_reorg (void)
 
   FOR_EACH_BB_FN (bb, cfun)
     {
-      if (current_tune->disparage_flag_setting_t16_encodings
+      if ((current_tune->disparage_flag_setting_t16_encodings
+	   == tune_params::DISPARAGE_FLAGS_ALL)
 	  && optimize_bb_for_speed_p (bb))
 	continue;
 
       rtx_insn *insn;
       Convert_Action action = SKIP;
       Convert_Action action_for_partial_flag_setting
-	= (current_tune->disparage_partial_flag_setting_t16_encodings
+	= ((current_tune->disparage_flag_setting_t16_encodings
+	    != tune_params::DISPARAGE_FLAGS_NEITHER)
 	   && optimize_bb_for_speed_p (bb))
 	  ? SKIP : CONV;
 
@@ -17699,7 +17654,7 @@ arm_output_multireg_pop (rtx *operands, bool return_pc, rtx cond, bool reverse,
     }
 
   conditional = reverse ? "%?%D0" : "%?%d0";
-  if ((regno_base == SP_REGNUM) && TARGET_UNIFIED_ASM)
+  if ((regno_base == SP_REGNUM) && TARGET_THUMB)
     {
       /* Output pop (not stmfd) because it has a shorter encoding.  */
       gcc_assert (update);
@@ -17998,19 +17953,27 @@ output_mov_long_double_arm_from_arm (rtx *operands)
 void
 arm_emit_movpair (rtx dest, rtx src)
  {
+  rtx insn;
+
   /* If the src is an immediate, simplify it.  */
   if (CONST_INT_P (src))
     {
       HOST_WIDE_INT val = INTVAL (src);
       emit_set_insn (dest, GEN_INT (val & 0x0000ffff));
       if ((val >> 16) & 0x0000ffff)
-        emit_set_insn (gen_rtx_ZERO_EXTRACT (SImode, dest, GEN_INT (16),
-                                             GEN_INT (16)),
-                       GEN_INT ((val >> 16) & 0x0000ffff));
+	{
+	  emit_set_insn (gen_rtx_ZERO_EXTRACT (SImode, dest, GEN_INT (16),
+					       GEN_INT (16)),
+			 GEN_INT ((val >> 16) & 0x0000ffff));
+	  insn = get_last_insn ();
+	  set_unique_reg_note (insn, REG_EQUAL, copy_rtx (src));
+	}
       return;
     }
    emit_set_insn (dest, gen_rtx_HIGH (SImode, src));
    emit_set_insn (dest, gen_rtx_LO_SUM (SImode, dest, src));
+   insn = get_last_insn ();
+   set_unique_reg_note (insn, REG_EQUAL, copy_rtx (src));
  }
 
 /* Output a move between double words.  It must be REG<-MEM
@@ -24077,19 +24040,19 @@ thumb_far_jump_used_p (void)
 }
 
 /* Return nonzero if FUNC must be entered in ARM mode.  */
-int
+static bool
 is_called_in_ARM_mode (tree func)
 {
   gcc_assert (TREE_CODE (func) == FUNCTION_DECL);
 
   /* Ignore the problem about functions whose address is taken.  */
   if (TARGET_CALLEE_INTERWORKING && TREE_PUBLIC (func))
-    return TRUE;
+    return true;
 
 #ifdef ARM_PE
   return lookup_attribute ("interfacearm", DECL_ATTRIBUTES (func)) != NULL_TREE;
 #else
-  return FALSE;
+  return false;
 #endif
 }
 
@@ -24375,6 +24338,24 @@ arm_init_expanders (void)
     mark_reg_pointer (arg_pointer_rtx, PARM_BOUNDARY);
 }
 
+/* Check that FUNC is called with a different mode.  */
+
+bool
+arm_change_mode_p (tree func)
+{
+  if (TREE_CODE (func) != FUNCTION_DECL)
+    return false;
+
+  tree callee_tree = DECL_FUNCTION_SPECIFIC_TARGET (func);
+
+  if (!callee_tree)
+    callee_tree = target_option_default_node;
+
+  struct cl_target_option *callee_opts = TREE_TARGET_OPTION (callee_tree);
+  int flags = callee_opts->x_target_flags;
+
+  return (TARGET_THUMB_P (flags) != TARGET_THUMB);
+}
 
 /* Like arm_compute_initial_elimination offset.  Simpler because there
    isn't an ABI specified frame pointer for Thumb.  Instead, we set it
@@ -25660,12 +25641,12 @@ arm_print_tune_info (void)
 	       current_tune->constant_limit);
   asm_fprintf (asm_out_file, "\t\t@max_insns_skipped:\t%d\n",
 	       current_tune->max_insns_skipped);
-  asm_fprintf (asm_out_file, "\t\t@num_prefetch_slots:\t%d\n",
-	       current_tune->num_prefetch_slots);
-  asm_fprintf (asm_out_file, "\t\t@l1_cache_size:\t%d\n",
-	       current_tune->l1_cache_size);
-  asm_fprintf (asm_out_file, "\t\t@l1_cache_line_size:\t%d\n",
-	       current_tune->l1_cache_line_size);
+  asm_fprintf (asm_out_file, "\t\t@prefetch.num_slots:\t%d\n",
+	       current_tune->prefetch.num_slots);
+  asm_fprintf (asm_out_file, "\t\t@prefetch.l1_cache_size:\t%d\n",
+	       current_tune->prefetch.l1_cache_size);
+  asm_fprintf (asm_out_file, "\t\t@prefetch.l1_cache_line_size:\t%d\n",
+	       current_tune->prefetch.l1_cache_line_size);
   asm_fprintf (asm_out_file, "\t\t@prefer_constant_pool:\t%d\n",
 	       (int) current_tune->prefer_constant_pool);
   asm_fprintf (asm_out_file, "\t\t@branch_cost:\t(s:speed, p:predictable)\n");
@@ -25681,23 +25662,19 @@ arm_print_tune_info (void)
   asm_fprintf (asm_out_file, "\t\t@prefer_ldrd_strd:\t%d\n",
 	       (int) current_tune->prefer_ldrd_strd);
   asm_fprintf (asm_out_file, "\t\t@logical_op_non_short_circuit:\t[%d,%d]\n",
-	       (int) current_tune->logical_op_non_short_circuit[0],
-	       (int) current_tune->logical_op_non_short_circuit[1]);
+	       (int) current_tune->logical_op_non_short_circuit_thumb,
+	       (int) current_tune->logical_op_non_short_circuit_arm);
   asm_fprintf (asm_out_file, "\t\t@prefer_neon_for_64bits:\t%d\n",
 	       (int) current_tune->prefer_neon_for_64bits);
   asm_fprintf (asm_out_file,
 	       "\t\t@disparage_flag_setting_t16_encodings:\t%d\n",
 	       (int) current_tune->disparage_flag_setting_t16_encodings);
-  asm_fprintf (asm_out_file,
-	       "\t\t@disparage_partial_flag_setting_t16_encodings:\t%d\n",
-	       (int) current_tune
-	               ->disparage_partial_flag_setting_t16_encodings);
   asm_fprintf (asm_out_file, "\t\t@string_ops_prefer_neon:\t%d\n",
 	       (int) current_tune->string_ops_prefer_neon);
   asm_fprintf (asm_out_file, "\t\t@max_insns_inline_memset:\t%d\n",
 	       current_tune->max_insns_inline_memset);
-  asm_fprintf (asm_out_file, "\t\t@fuseable_ops:\t%u\n",
-	       current_tune->fuseable_ops);
+  asm_fprintf (asm_out_file, "\t\t@fusible_ops:\t%u\n",
+	       current_tune->fusible_ops);
   asm_fprintf (asm_out_file, "\t\t@sched_autopref:\t%d\n",
 	       (int) current_tune->sched_autopref);
 }
@@ -25707,9 +25684,6 @@ arm_file_start (void)
 {
   int val;
 
-  if (TARGET_UNIFIED_ASM)
-    asm_fprintf (asm_out_file, "\t.syntax unified\n");
-
   if (TARGET_BPABI)
     {
       const char *fpu_name;
@@ -26509,7 +26483,7 @@ arm_dbx_register_number (unsigned int regno)
   if (IS_IWMMXT_REGNUM (regno))
     return 112 + regno - FIRST_IWMMXT_REGNUM;
 
-  gcc_unreachable ();
+  return DWARF_FRAME_REGISTERS;
 }
 
 /* Dwarf models VFPv3 registers as 32 64-bit registers.
@@ -27213,40 +27187,12 @@ thumb2_output_casesi (rtx *operands)
     }
 }
 
-/* Most ARM cores are single issue, but some newer ones can dual issue.
-   The scheduler descriptions rely on this being correct.  */
+/* Implement TARGET_SCHED_ISSUE_RATE.  Lookup the issue rate in the
+   per-core tuning structs.  */
 static int
 arm_issue_rate (void)
 {
-  switch (arm_tune)
-    {
-    case xgene1:
-      return 4;
-
-    case cortexa15:
-    case cortexa57:
-    case exynosm1:
-      return 3;
-
-    case cortexm7:
-    case cortexr4:
-    case cortexr4f:
-    case cortexr5:
-    case genericv7a:
-    case cortexa5:
-    case cortexa7:
-    case cortexa8:
-    case cortexa9:
-    case cortexa12:
-    case cortexa17:
-    case cortexa53:
-    case fa726te:
-    case marvell_pj4:
-      return 2;
-
-    default:
-      return 1;
-    }
+  return current_tune->issue_rate;
 }
 
 /* Return how many instructions should scheduler lookahead to choose the
@@ -29411,7 +29357,7 @@ arm_gen_setmem (rtx *operands)
 static bool
 arm_macro_fusion_p (void)
 {
-  return current_tune->fuseable_ops != ARM_FUSE_NOTHING;
+  return current_tune->fusible_ops != tune_params::FUSE_NOTHING;
 }
 
 
@@ -29432,44 +29378,44 @@ aarch_macro_fusion_pair_p (rtx_insn* prev, rtx_insn* curr)
   if (!arm_macro_fusion_p ())
     return false;
 
-  if (current_tune->fuseable_ops & ARM_FUSE_MOVW_MOVT)
+  if (current_tune->fusible_ops & tune_params::FUSE_MOVW_MOVT)
     {
       /* We are trying to fuse
-         movw imm / movt imm
-         instructions as a group that gets scheduled together.  */
+	 movw imm / movt imm
+	 instructions as a group that gets scheduled together.  */
 
       set_dest = SET_DEST (curr_set);
 
       if (GET_MODE (set_dest) != SImode)
-        return false;
+	return false;
 
       /* We are trying to match:
-         prev (movw)  == (set (reg r0) (const_int imm16))
-         curr (movt) == (set (zero_extract (reg r0)
-                                           (const_int 16)
-                                           (const_int 16))
-                             (const_int imm16_1))
-         or
-         prev (movw) == (set (reg r1)
-                              (high (symbol_ref ("SYM"))))
-         curr (movt) == (set (reg r0)
-                             (lo_sum (reg r1)
-                                     (symbol_ref ("SYM"))))  */
+	 prev (movw)  == (set (reg r0) (const_int imm16))
+	 curr (movt) == (set (zero_extract (reg r0)
+					  (const_int 16)
+					   (const_int 16))
+			     (const_int imm16_1))
+	 or
+	 prev (movw) == (set (reg r1)
+			      (high (symbol_ref ("SYM"))))
+	 curr (movt) == (set (reg r0)
+			     (lo_sum (reg r1)
+				     (symbol_ref ("SYM"))))  */
       if (GET_CODE (set_dest) == ZERO_EXTRACT)
-        {
-          if (CONST_INT_P (SET_SRC (curr_set))
-              && CONST_INT_P (SET_SRC (prev_set))
-              && REG_P (XEXP (set_dest, 0))
-              && REG_P (SET_DEST (prev_set))
-              && REGNO (XEXP (set_dest, 0)) == REGNO (SET_DEST (prev_set)))
-            return true;
-        }
+	{
+	  if (CONST_INT_P (SET_SRC (curr_set))
+	      && CONST_INT_P (SET_SRC (prev_set))
+	      && REG_P (XEXP (set_dest, 0))
+	      && REG_P (SET_DEST (prev_set))
+	      && REGNO (XEXP (set_dest, 0)) == REGNO (SET_DEST (prev_set)))
+	    return true;
+	}
       else if (GET_CODE (SET_SRC (curr_set)) == LO_SUM
-               && REG_P (SET_DEST (curr_set))
-               && REG_P (SET_DEST (prev_set))
-               && GET_CODE (SET_SRC (prev_set)) == HIGH
-               && REGNO (SET_DEST (curr_set)) == REGNO (SET_DEST (prev_set)))
-             return true;
+	       && REG_P (SET_DEST (curr_set))
+	       && REG_P (SET_DEST (prev_set))
+	       && GET_CODE (SET_SRC (prev_set)) == HIGH
+	       && REGNO (SET_DEST (curr_set)) == REGNO (SET_DEST (prev_set)))
+	     return true;
     }
   return false;
 }
@@ -29533,6 +29479,273 @@ arm_is_constant_pool_ref (rtx x)
 	  && CONSTANT_POOL_ADDRESS_P (XEXP (x, 0)));
 }
 
+/* Remember the last target of arm_set_current_function.  */
+static GTY(()) tree arm_previous_fndecl;
+
+/* Invalidate arm_previous_fndecl.  */
+void
+arm_reset_previous_fndecl (void)
+{
+  arm_previous_fndecl = NULL_TREE;
+}
+
+/* Establish appropriate back-end context for processing the function
+   FNDECL.  The argument might be NULL to indicate processing at top
+   level, outside of any function scope.  */
+static void
+arm_set_current_function (tree fndecl)
+{
+  if (!fndecl || fndecl == arm_previous_fndecl)
+    return;
+
+  tree old_tree = (arm_previous_fndecl
+		   ? DECL_FUNCTION_SPECIFIC_TARGET (arm_previous_fndecl)
+		   : NULL_TREE);
+
+  tree new_tree = DECL_FUNCTION_SPECIFIC_TARGET (fndecl);
+
+  arm_previous_fndecl = fndecl;
+  if (old_tree == new_tree)
+    return;
+
+  if (new_tree && new_tree != target_option_default_node)
+    {
+      cl_target_option_restore (&global_options,
+				TREE_TARGET_OPTION (new_tree));
+
+      if (TREE_TARGET_GLOBALS (new_tree))
+	restore_target_globals (TREE_TARGET_GLOBALS (new_tree));
+      else
+	TREE_TARGET_GLOBALS (new_tree)
+	  = save_target_globals_default_opts ();
+    }
+
+  else if (old_tree && old_tree != target_option_default_node)
+    {
+      new_tree = target_option_current_node;
+
+      cl_target_option_restore (&global_options,
+				TREE_TARGET_OPTION (new_tree));
+      if (TREE_TARGET_GLOBALS (new_tree))
+	restore_target_globals (TREE_TARGET_GLOBALS (new_tree));
+      else if (new_tree == target_option_default_node)
+	restore_target_globals (&default_target_globals);
+      else
+	TREE_TARGET_GLOBALS (new_tree)
+	  = save_target_globals_default_opts ();
+    }
+
+  arm_option_params_internal ();
+}
+
+/* Implement TARGET_OPTION_PRINT.  */
+
+static void
+arm_option_print (FILE *file, int indent, struct cl_target_option *ptr)
+{
+  int flags = ptr->x_target_flags;
+
+  fprintf (file, "%*sselected arch %s\n", indent, "",
+	   TARGET_THUMB2_P (flags) ? "thumb2" :
+	   TARGET_THUMB_P (flags) ? "thumb1" :
+	   "arm");
+}
+
+/* Hook to determine if one function can safely inline another.  */
+
+static bool
+arm_can_inline_p (tree caller ATTRIBUTE_UNUSED, tree callee ATTRIBUTE_UNUSED)
+{
+  /* Overidde default hook: Always OK to inline between different modes. 
+     Function with mode specific instructions, e.g using asm, must be explicitely 
+     protected with noinline.  */
+  return true;
+}
+
+/* Inner function to process the attribute((target(...))), take an argument and
+   set the current options from the argument.  If we have a list, recursively
+   go over the list.  */
+
+static bool
+arm_valid_target_attribute_rec (tree args, struct gcc_options *opts)
+{
+  if (TREE_CODE (args) == TREE_LIST)
+    {
+      bool ret = true;
+      for (; args; args = TREE_CHAIN (args))
+	if (TREE_VALUE (args)
+	    && !arm_valid_target_attribute_rec (TREE_VALUE (args), opts))
+	  ret = false;
+      return ret;
+    }
+
+  else if (TREE_CODE (args) != STRING_CST)
+    {
+      error ("attribute %<target%> argument not a string");
+      return false;
+    }
+
+  char *argstr = ASTRDUP (TREE_STRING_POINTER (args));
+  while (argstr && *argstr != '\0')
+    {
+      while (ISSPACE (*argstr))
+	argstr++;
+
+      if (!strcmp (argstr, "thumb"))
+	{
+	  opts->x_target_flags |= MASK_THUMB;
+	  arm_option_check_internal (opts);
+	  return true;
+	}
+
+      if (!strcmp (argstr, "arm"))
+	{
+	  opts->x_target_flags &= ~MASK_THUMB;
+	  arm_option_check_internal (opts);
+	  return true;
+	}
+
+      warning (0, "attribute(target(\"%s\")) is unknown", argstr);
+      return false;
+    }
+
+  return false;
+}
+
+/* Return a TARGET_OPTION_NODE tree of the target options listed or NULL.  */
+
+tree
+arm_valid_target_attribute_tree (tree args, struct gcc_options *opts,
+				 struct gcc_options *opts_set)
+{
+  if (!arm_valid_target_attribute_rec (args, opts))
+    return NULL_TREE;
+
+  /* Do any overrides, such as global options arch=xxx.  */
+  arm_option_override_internal (opts, opts_set);
+
+  return build_target_option_node (opts);
+}
+
+static void 
+add_attribute  (const char * mode, tree *attributes)
+{
+  size_t len = strlen (mode);
+  tree value = build_string (len, mode);
+
+  TREE_TYPE (value) = build_array_type (char_type_node,
+					build_index_type (size_int (len)));
+
+  *attributes = tree_cons (get_identifier ("target"),
+			   build_tree_list (NULL_TREE, value),
+			   *attributes);
+}
+
+/* For testing. Insert thumb or arm modes alternatively on functions.  */
+
+static void
+arm_insert_attributes (tree fndecl, tree * attributes)
+{
+  const char *mode;
+
+  if (! TARGET_FLIP_THUMB)
+    return;
+
+  if (TREE_CODE (fndecl) != FUNCTION_DECL || DECL_EXTERNAL(fndecl)
+      || DECL_BUILT_IN (fndecl) || DECL_ARTIFICIAL (fndecl))
+   return;
+
+  /* Nested definitions must inherit mode.  */
+  if (current_function_decl)
+   {
+     mode = TARGET_THUMB ? "thumb" : "arm";      
+     add_attribute (mode, attributes);
+     return;
+   }
+
+  /* If there is already a setting don't change it.  */
+  if (lookup_attribute ("target", *attributes) != NULL)
+    return;
+
+  mode = thumb_flipper ? "thumb" : "arm";
+  add_attribute (mode, attributes);
+
+  thumb_flipper = !thumb_flipper;
+}
+
+/* Hook to validate attribute((target("string"))).  */
+
+static bool
+arm_valid_target_attribute_p (tree fndecl, tree ARG_UNUSED (name),
+			      tree args, int ARG_UNUSED (flags))
+{
+  bool ret = true;
+  struct gcc_options func_options;
+  tree cur_tree, new_optimize;
+  gcc_assert ((fndecl != NULL_TREE) && (args != NULL_TREE));
+
+  /* Get the optimization options of the current function.  */
+  tree func_optimize = DECL_FUNCTION_SPECIFIC_OPTIMIZATION (fndecl);
+
+  /* If the function changed the optimization levels as well as setting target
+     options, start with the optimizations specified.  */
+  if (!func_optimize)
+    func_optimize = optimization_default_node;
+
+  /* Init func_options.  */
+  memset (&func_options, 0, sizeof (func_options));
+  init_options_struct (&func_options, NULL);
+  lang_hooks.init_options_struct (&func_options);
+
+  /* Initialize func_options to the defaults.  */
+  cl_optimization_restore (&func_options,
+			   TREE_OPTIMIZATION (func_optimize));
+
+  cl_target_option_restore (&func_options,
+			    TREE_TARGET_OPTION (target_option_default_node));
+
+  /* Set func_options flags with new target mode.  */
+  cur_tree = arm_valid_target_attribute_tree (args, &func_options,
+					      &global_options_set);
+
+  if (cur_tree == NULL_TREE)
+    ret = false;
+
+  new_optimize = build_optimization_node (&func_options);
+
+  DECL_FUNCTION_SPECIFIC_TARGET (fndecl) = cur_tree;
+
+  DECL_FUNCTION_SPECIFIC_OPTIMIZATION (fndecl) = new_optimize;
+
+  return ret;
+}
+
+void
+arm_declare_function_name (FILE *stream, const char *name, tree decl)
+{
+  if (TARGET_UNIFIED_ASM)
+    fprintf (stream, "\t.syntax unified\n");
+  else
+    fprintf (stream, "\t.syntax divided\n");
+
+  if (TARGET_THUMB)
+    {
+      if (is_called_in_ARM_mode (decl)
+	  || (TARGET_THUMB1 && !TARGET_THUMB1_ONLY
+	      && cfun->is_thunk))
+	fprintf (stream, "\t.code 32\n");
+      else if (TARGET_THUMB1)
+	fprintf (stream, "\t.code\t16\n\t.thumb_func\n");
+      else
+	fprintf (stream, "\t.thumb\n\t.thumb_func\n");
+    }
+  else
+    fprintf (stream, "\t.arm\n");
+
+  if (TARGET_POKE_FUNCTION_NAME)
+    arm_poke_function_name (stream, (const char *) name);
+}
+
 /* If MEM is in the form of [base+offset], extract the two parts
    of address and set to BASE and OFFSET, otherwise return false
    after clearing BASE and OFFSET.  */
--- a/src/gcc/config/arm/arm.h
+++ b/src/gcc/config/arm/arm.h
@@ -45,132 +45,7 @@
 extern char arm_arch_name[];
 
 /* Target CPU builtins.  */
-#define TARGET_CPU_CPP_BUILTINS()			\
-  do							\
-    {							\
-	if (TARGET_DSP_MULTIPLY)			\
-	   builtin_define ("__ARM_FEATURE_DSP");	\
-        if (TARGET_ARM_QBIT)				\
-           builtin_define ("__ARM_FEATURE_QBIT");	\
-        if (TARGET_ARM_SAT)				\
-           builtin_define ("__ARM_FEATURE_SAT");	\
-        if (TARGET_CRYPTO)				\
-	   builtin_define ("__ARM_FEATURE_CRYPTO");	\
-	if (unaligned_access)				\
-	  builtin_define ("__ARM_FEATURE_UNALIGNED");	\
-	if (TARGET_CRC32)				\
-	  builtin_define ("__ARM_FEATURE_CRC32");	\
-	if (TARGET_32BIT)				\
-	  builtin_define ("__ARM_32BIT_STATE");		\
-	if (TARGET_ARM_FEATURE_LDREX)				\
-	  builtin_define_with_int_value (			\
-	    "__ARM_FEATURE_LDREX", TARGET_ARM_FEATURE_LDREX);	\
-	if ((TARGET_ARM_ARCH >= 5 && !TARGET_THUMB)		\
-	     || TARGET_ARM_ARCH_ISA_THUMB >=2)			\
-	  builtin_define ("__ARM_FEATURE_CLZ");			\
-	if (TARGET_INT_SIMD)					\
-	  builtin_define ("__ARM_FEATURE_SIMD32");		\
-								\
-	builtin_define_with_int_value (				\
-	  "__ARM_SIZEOF_MINIMAL_ENUM",				\
-	  flag_short_enums ? 1 : 4);				\
-	builtin_define_type_sizeof ("__ARM_SIZEOF_WCHAR_T",	\
-				    wchar_type_node);		\
-	if (TARGET_ARM_ARCH_PROFILE)				\
-	  builtin_define_with_int_value (			\
-	    "__ARM_ARCH_PROFILE", TARGET_ARM_ARCH_PROFILE);	\
-								\
-	/* Define __arm__ even when in thumb mode, for	\
-	   consistency with armcc.  */			\
-	builtin_define ("__arm__");			\
-	if (TARGET_ARM_ARCH)				\
-	  builtin_define_with_int_value (		\
-	    "__ARM_ARCH", TARGET_ARM_ARCH);		\
-	if (arm_arch_notm)				\
-	  builtin_define ("__ARM_ARCH_ISA_ARM");	\
-	builtin_define ("__APCS_32__");			\
-	if (TARGET_THUMB)				\
-	  builtin_define ("__thumb__");			\
-	if (TARGET_THUMB2)				\
-	  builtin_define ("__thumb2__");		\
-	if (TARGET_ARM_ARCH_ISA_THUMB)			\
-	  builtin_define_with_int_value (		\
-	    "__ARM_ARCH_ISA_THUMB",			\
-	    TARGET_ARM_ARCH_ISA_THUMB);			\
-							\
-	if (TARGET_BIG_END)				\
-	  {						\
-	    builtin_define ("__ARMEB__");		\
-	    builtin_define ("__ARM_BIG_ENDIAN");	\
-	    if (TARGET_THUMB)				\
-	      builtin_define ("__THUMBEB__");		\
-	  }						\
-        else						\
-	  {						\
-	    builtin_define ("__ARMEL__");		\
-	    if (TARGET_THUMB)				\
-	      builtin_define ("__THUMBEL__");		\
-	  }						\
-							\
-	if (TARGET_SOFT_FLOAT)				\
-	  builtin_define ("__SOFTFP__");		\
-							\
-	if (TARGET_VFP)					\
-	  builtin_define ("__VFP_FP__");		\
-							\
-	if (TARGET_ARM_FP)				\
-	  builtin_define_with_int_value (		\
-	    "__ARM_FP", TARGET_ARM_FP);			\
-	if (arm_fp16_format == ARM_FP16_FORMAT_IEEE)		\
-	  builtin_define ("__ARM_FP16_FORMAT_IEEE");		\
-	if (arm_fp16_format == ARM_FP16_FORMAT_ALTERNATIVE)	\
-	  builtin_define ("__ARM_FP16_FORMAT_ALTERNATIVE");	\
-        if (TARGET_FMA)					\
-          builtin_define ("__ARM_FEATURE_FMA");		\
-							\
-	if (TARGET_NEON)				\
-	  {						\
-	    builtin_define ("__ARM_NEON__");		\
-	    builtin_define ("__ARM_NEON");		\
-	  }						\
-	if (TARGET_NEON_FP)				\
-	  builtin_define_with_int_value (		\
-	    "__ARM_NEON_FP", TARGET_NEON_FP);		\
-							\
-	/* Add a define for interworking.		\
-	   Needed when building libgcc.a.  */		\
-	if (arm_cpp_interwork)				\
-	  builtin_define ("__THUMB_INTERWORK__");	\
-							\
-	builtin_assert ("cpu=arm");			\
-	builtin_assert ("machine=arm");			\
-							\
-	builtin_define (arm_arch_name);			\
-	if (arm_arch_xscale)				\
-	  builtin_define ("__XSCALE__");		\
-	if (arm_arch_iwmmxt)				\
-          {						\
-	    builtin_define ("__IWMMXT__");		\
-	    builtin_define ("__ARM_WMMX");		\
-	  }						\
-	if (arm_arch_iwmmxt2)				\
-	  builtin_define ("__IWMMXT2__");		\
-	if (TARGET_AAPCS_BASED)				\
-	  {						\
-	    if (arm_pcs_default == ARM_PCS_AAPCS_VFP)	\
-	      builtin_define ("__ARM_PCS_VFP");		\
-	    else if (arm_pcs_default == ARM_PCS_AAPCS)	\
-	      builtin_define ("__ARM_PCS");		\
-	    builtin_define ("__ARM_EABI__");		\
-	  }						\
-	if (TARGET_IDIV)				\
-         {						\
-            builtin_define ("__ARM_ARCH_EXT_IDIV__");	\
-            builtin_define ("__ARM_FEATURE_IDIV");	\
-         }						\
-	if (inline_asm_unified)				\
-	  builtin_define ("__ARM_ASM_SYNTAX_UNIFIED__");\
-    } while (0)
+#define TARGET_CPU_CPP_BUILTINS() arm_cpu_cpp_builtins (pfile)
 
 #include "config/arm/arm-opts.h"
 
@@ -252,6 +127,11 @@ extern void (*arm_lang_output_object_attributes_hook)(void);
 #define SUBTARGET_CPP_SPEC      ""
 #endif
 
+/* Tree Target Specification.  */
+#define TARGET_ARM_P(flags)    (!TARGET_THUMB_P (flags))
+#define TARGET_THUMB1_P(flags) (TARGET_THUMB_P (flags) && !arm_arch_thumb2)
+#define TARGET_THUMB2_P(flags) (TARGET_THUMB_P (flags) && arm_arch_thumb2)
+
 /* Run-time Target Specification.  */
 #define TARGET_SOFT_FLOAT		(arm_float_abi == ARM_FLOAT_ABI_SOFT)
 /* Use hardware floating point instructions. */
@@ -367,21 +247,21 @@ extern void (*arm_lang_output_object_attributes_hook)(void);
 #define TARGET_HAVE_MEMORY_BARRIER (TARGET_HAVE_DMB || TARGET_HAVE_DMB_MCR)
 
 /* Nonzero if this chip supports ldrex and strex */
-#define TARGET_HAVE_LDREX	((arm_arch6 && TARGET_ARM) || arm_arch7)
+#define TARGET_HAVE_LDREX        ((arm_arch6 && TARGET_ARM) || arm_arch7)
 
 /* Nonzero if this chip supports ldrex{bh} and strex{bh}.  */
-#define TARGET_HAVE_LDREXBH	((arm_arch6k && TARGET_ARM) || arm_arch7)
+#define TARGET_HAVE_LDREXBH ((arm_arch6k && TARGET_ARM) || arm_arch7)
 
 /* Nonzero if this chip supports ldrexd and strexd.  */
-#define TARGET_HAVE_LDREXD	(((arm_arch6k && TARGET_ARM) || arm_arch7) \
-				 && arm_arch_notm)
+#define TARGET_HAVE_LDREXD (((arm_arch6k && TARGET_ARM) \
+			     || arm_arch7) && arm_arch_notm)
 
 /* Nonzero if this chip supports load-acquire and store-release.  */
 #define TARGET_HAVE_LDACQ	(TARGET_ARM_ARCH >= 8)
 
 /* Nonzero if integer division instructions supported.  */
-#define TARGET_IDIV		((TARGET_ARM && arm_arch_arm_hwdiv) \
-				 || (TARGET_THUMB2 && arm_arch_thumb_hwdiv))
+#define TARGET_IDIV	((TARGET_ARM && arm_arch_arm_hwdiv)	\
+			 || (TARGET_THUMB2 && arm_arch_thumb_hwdiv))
 
 /* Nonzero if disallow volatile memory access in IT block.  */
 #define TARGET_NO_VOLATILE_CE		(arm_arch_no_volatile_ce)
@@ -389,6 +269,12 @@ extern void (*arm_lang_output_object_attributes_hook)(void);
 /* Should NEON be used for 64-bits bitops.  */
 #define TARGET_PREFER_NEON_64BITS (prefer_neon_for_64bits)
 
+/* Should constant I be slplit for OP.  */
+#define DONT_EARLY_SPLIT_CONSTANT(i, op) \
+				((optimize >= 2) \
+				 && can_create_pseudo_p () \
+				 && !const_ok_for_op (i, op))
+
 /* True iff the full BPABI is being used.  If TARGET_BPABI is true,
    then TARGET_AAPCS_BASED must be true -- but the converse does not
    hold.  TARGET_BPABI implies the use of the BPABI runtime library,
@@ -473,7 +359,7 @@ enum base_architecture
   BASE_ARCH_5TEJ = 5,
   BASE_ARCH_6 = 6,
   BASE_ARCH_6J = 6,
-  BASE_ARCH_6ZK = 6,
+  BASE_ARCH_6KZ = 6,
   BASE_ARCH_6K = 6,
   BASE_ARCH_6T2 = 6,
   BASE_ARCH_6M = 6,
@@ -528,12 +414,6 @@ extern int arm_arch8;
 /* Nonzero if this chip can benefit from load scheduling.  */
 extern int arm_ld_sched;
 
-/* Nonzero if generating Thumb code, either Thumb-1 or Thumb-2.  */
-extern int thumb_code;
-
-/* Nonzero if generating Thumb-1 code.  */
-extern int thumb1_code;
-
 /* Nonzero if this chip is a StrongARM.  */
 extern int arm_tune_strongarm;
 
@@ -1360,46 +1240,6 @@ enum reg_class
      ? GENERAL_REGS : NO_REGS)					\
     : THUMB_SECONDARY_INPUT_RELOAD_CLASS (CLASS, MODE, X)))
 
-/* Try a machine-dependent way of reloading an illegitimate address
-   operand.  If we find one, push the reload and jump to WIN.  This
-   macro is used in only one place: `find_reloads_address' in reload.c.
-
-   For the ARM, we wish to handle large displacements off a base
-   register by splitting the addend across a MOV and the mem insn.
-   This can cut the number of reloads needed.  */
-#define ARM_LEGITIMIZE_RELOAD_ADDRESS(X, MODE, OPNUM, TYPE, IND, WIN)	   \
-  do									   \
-    {									   \
-      if (arm_legitimize_reload_address (&X, MODE, OPNUM, TYPE, IND))	   \
-	goto WIN;							   \
-    }									   \
-  while (0)
-
-/* XXX If an HImode FP+large_offset address is converted to an HImode
-   SP+large_offset address, then reload won't know how to fix it.  It sees
-   only that SP isn't valid for HImode, and so reloads the SP into an index
-   register, but the resulting address is still invalid because the offset
-   is too big.  We fix it here instead by reloading the entire address.  */
-/* We could probably achieve better results by defining PROMOTE_MODE to help
-   cope with the variances between the Thumb's signed and unsigned byte and
-   halfword load instructions.  */
-/* ??? This should be safe for thumb2, but we may be able to do better.  */
-#define THUMB_LEGITIMIZE_RELOAD_ADDRESS(X, MODE, OPNUM, TYPE, IND_L, WIN)     \
-do {									      \
-  rtx new_x = thumb_legitimize_reload_address (&X, MODE, OPNUM, TYPE, IND_L); \
-  if (new_x)								      \
-    {									      \
-      X = new_x;							      \
-      goto WIN;								      \
-    }									      \
-} while (0)
-
-#define LEGITIMIZE_RELOAD_ADDRESS(X, MODE, OPNUM, TYPE, IND_LEVELS, WIN)   \
-  if (TARGET_ARM)							   \
-    ARM_LEGITIMIZE_RELOAD_ADDRESS (X, MODE, OPNUM, TYPE, IND_LEVELS, WIN); \
-  else									   \
-    THUMB_LEGITIMIZE_RELOAD_ADDRESS (X, MODE, OPNUM, TYPE, IND_LEVELS, WIN)
-
 /* Return the maximum number of consecutive registers
    needed to represent mode MODE in a register of class CLASS.
    ARM regs are UNITS_PER_WORD bits.  
@@ -2096,10 +1936,11 @@ enum arm_auto_incmodes
   (current_tune->branch_cost (speed_p, predictable_p))
 
 /* False if short circuit operation is preferred.  */
-#define LOGICAL_OP_NON_SHORT_CIRCUIT				\
-  ((optimize_size)						\
-   ? (TARGET_THUMB ? false : true)				\
-   : (current_tune->logical_op_non_short_circuit[TARGET_ARM]))
+#define LOGICAL_OP_NON_SHORT_CIRCUIT					\
+  ((optimize_size)							\
+   ? (TARGET_THUMB ? false : true)					\
+   : TARGET_THUMB ? static_cast<bool> (current_tune->logical_op_non_short_circuit_thumb) \
+   : static_cast<bool> (current_tune->logical_op_non_short_circuit_arm))
 
 
 /* Position Independent Code.  */
@@ -2135,7 +1976,8 @@ extern int making_const_table;
   c_register_pragma (0, "long_calls", arm_pr_long_calls);		\
   c_register_pragma (0, "no_long_calls", arm_pr_no_long_calls);		\
   c_register_pragma (0, "long_calls_off", arm_pr_long_calls_off);	\
-  arm_lang_object_attributes_init(); \
+  arm_lang_object_attributes_init();					\
+  arm_register_target_pragmas();                                       \
 } while (0)
 
 /* Condition code information.  */
@@ -2222,23 +2064,7 @@ extern int making_const_table;
    ? 1 : 0)
 
 #define ARM_DECLARE_FUNCTION_NAME(STREAM, NAME, DECL) 	\
-  do							\
-    {							\
-      if (TARGET_THUMB) 				\
-        {						\
-          if (is_called_in_ARM_mode (DECL)		\
-	      || (TARGET_THUMB1 && !TARGET_THUMB1_ONLY	\
-		  && cfun->is_thunk))	\
-            fprintf (STREAM, "\t.code 32\n") ;		\
-          else if (TARGET_THUMB1)			\
-           fprintf (STREAM, "\t.code\t16\n\t.thumb_func\n") ;	\
-          else						\
-           fprintf (STREAM, "\t.thumb\n\t.thumb_func\n") ;	\
-        }						\
-      if (TARGET_POKE_FUNCTION_NAME)			\
-        arm_poke_function_name (STREAM, (const char *) NAME);	\
-    }							\
-  while (0)
+  arm_declare_function_name ((STREAM), (NAME), (DECL));
 
 /* For aliases of functions we use .thumb_set instead.  */
 #define ASM_OUTPUT_DEF_FROM_DECLS(FILE, DECL1, DECL2)		\
@@ -2413,4 +2239,8 @@ extern const char *host_detect_local_cpu (int argc, const char **argv);
 
 #define DRIVER_SELF_SPECS MCPU_MTUNE_NATIVE_SPECS
 #define TARGET_SUPPORTS_WIDE_INT 1
+
+/* For switching between functions with different target attributes.  */
+#define SWITCHABLE_TARGET 1
+
 #endif /* ! GCC_ARM_H */
--- a/src/gcc/config/arm/arm.md
+++ b/src/gcc/config/arm/arm.md
@@ -69,13 +69,17 @@
 ; IS_THUMB is set to 'yes' when we are generating Thumb code, and 'no' when
 ; generating ARM code.  This is used to control the length of some insn
 ; patterns that share the same RTL in both ARM and Thumb code.
-(define_attr "is_thumb" "no,yes" (const (symbol_ref "thumb_code")))
+(define_attr "is_thumb" "yes,no"
+  (const (if_then_else (symbol_ref "TARGET_THUMB")
+		       (const_string "yes") (const_string "no"))))
 
 ; IS_ARCH6 is set to 'yes' when we are generating code form ARMv6.
 (define_attr "is_arch6" "no,yes" (const (symbol_ref "arm_arch6")))
 
 ; IS_THUMB1 is set to 'yes' iff we are generating Thumb-1 code.
-(define_attr "is_thumb1" "no,yes" (const (symbol_ref "thumb1_code")))
+(define_attr "is_thumb1" "yes,no"
+  (const (if_then_else (symbol_ref "TARGET_THUMB1")
+		       (const_string "yes") (const_string "no"))))
 
 ; We use this attribute to disable alternatives that can produce 32-bit
 ; instructions inside an IT-block in Thumb2 state.  ARMv8 deprecates IT blocks
@@ -1164,10 +1168,16 @@
     {
       if (TARGET_32BIT)
         {
-          arm_split_constant (MINUS, SImode, NULL_RTX,
-	                      INTVAL (operands[1]), operands[0],
-	  		      operands[2], optimize && can_create_pseudo_p ());
-          DONE;
+	  if (DONT_EARLY_SPLIT_CONSTANT (INTVAL (operands[1]), MINUS))
+	    operands[1] = force_reg (SImode, operands[1]);
+	  else
+	    {
+	      arm_split_constant (MINUS, SImode, NULL_RTX,
+				  INTVAL (operands[1]), operands[0],
+				  operands[2],
+				  optimize && can_create_pseudo_p ());
+	      DONE;
+	    }
 	}
       else /* TARGET_THUMB1 */
         operands[1] = force_reg (SImode, operands[1]);
@@ -1177,9 +1187,9 @@
 
 ; ??? Check Thumb-2 split length
 (define_insn_and_split "*arm_subsi3_insn"
-  [(set (match_operand:SI           0 "s_register_operand" "=l,l ,l ,l ,r ,r,r,rk,r")
-	(minus:SI (match_operand:SI 1 "reg_or_int_operand" "l ,0 ,l ,Pz,rI,r,r,k ,?n")
-		  (match_operand:SI 2 "reg_or_int_operand" "l ,Py,Pd,l ,r ,I,r,r ,r")))]
+  [(set (match_operand:SI           0 "s_register_operand" "=l,l ,l ,l ,r,r,r,rk,r")
+	(minus:SI (match_operand:SI 1 "reg_or_int_operand" "l ,0 ,l ,Pz,I,r,r,k ,?n")
+		  (match_operand:SI 2 "reg_or_int_operand" "l ,Py,Pd,l ,r,I,r,r ,r")))]
   "TARGET_32BIT"
   "@
    sub%?\\t%0, %1, %2
@@ -2078,14 +2088,19 @@
 	      operands[1] = convert_to_mode (QImode, operands[1], 1);
 	      emit_insn (gen_thumb2_zero_extendqisi2_v6 (operands[0],
 							 operands[1]));
+	      DONE;
 	    }
+	  else if (DONT_EARLY_SPLIT_CONSTANT (INTVAL (operands[2]), AND))
+	    operands[2] = force_reg (SImode, operands[2]);
 	  else
-	    arm_split_constant (AND, SImode, NULL_RTX,
-				INTVAL (operands[2]), operands[0],
-				operands[1],
-				optimize && can_create_pseudo_p ());
+	    {
+	      arm_split_constant (AND, SImode, NULL_RTX,
+				  INTVAL (operands[2]), operands[0],
+				  operands[1],
+				  optimize && can_create_pseudo_p ());
 
-          DONE;
+	      DONE;
+	    }
         }
     }
   else /* TARGET_THUMB1 */
@@ -2768,6 +2783,55 @@
 		      (const_string "logic_shift_reg")))]
 )
 
+;; Shifted bics pattern used to set up CC status register and not reusing
+;; bics output.  Pattern restricts Thumb2 shift operand as bics for Thumb2
+;; does not support shift by register.
+(define_insn "andsi_not_shiftsi_si_scc_no_reuse"
+  [(set (reg:CC_NOOV CC_REGNUM)
+	(compare:CC_NOOV
+		(and:SI (not:SI (match_operator:SI 0 "shift_operator"
+			[(match_operand:SI 1 "s_register_operand" "r")
+			 (match_operand:SI 2 "arm_rhs_operand" "rM")]))
+			(match_operand:SI 3 "s_register_operand" "r"))
+		(const_int 0)))
+   (clobber (match_scratch:SI 4 "=r"))]
+  "TARGET_ARM || (TARGET_THUMB2 && CONST_INT_P (operands[2]))"
+  "bic%.%?\\t%4, %3, %1%S0"
+  [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
+   (set_attr "conds" "set")
+   (set_attr "shift" "1")
+   (set (attr "type") (if_then_else (match_operand 2 "const_int_operand" "")
+		      (const_string "logic_shift_imm")
+		      (const_string "logic_shift_reg")))]
+)
+
+;; Same as andsi_not_shiftsi_si_scc_no_reuse, but the bics result is also
+;; getting reused later.
+(define_insn "andsi_not_shiftsi_si_scc"
+  [(parallel [(set (reg:CC_NOOV CC_REGNUM)
+	(compare:CC_NOOV
+		(and:SI (not:SI (match_operator:SI 0 "shift_operator"
+			[(match_operand:SI 1 "s_register_operand" "r")
+			 (match_operand:SI 2 "arm_rhs_operand" "rM")]))
+			(match_operand:SI 3 "s_register_operand" "r"))
+		(const_int 0)))
+	(set (match_operand:SI 4 "s_register_operand" "=r")
+	     (and:SI (not:SI (match_op_dup 0
+		     [(match_dup 1)
+		      (match_dup 2)]))
+		     (match_dup 3)))])]
+  "TARGET_ARM || (TARGET_THUMB2 && CONST_INT_P (operands[2]))"
+  "bic%.%?\\t%4, %3, %1%S0"
+  [(set_attr "predicable" "yes")
+   (set_attr "predicable_short_it" "no")
+   (set_attr "conds" "set")
+   (set_attr "shift" "1")
+   (set (attr "type") (if_then_else (match_operand 2 "const_int_operand" "")
+		      (const_string "logic_shift_imm")
+		      (const_string "logic_shift_reg")))]
+)
+
 (define_insn "*andsi_notsi_si_compare0"
   [(set (reg:CC_NOOV CC_REGNUM)
 	(compare:CC_NOOV
@@ -2884,10 +2948,16 @@
     {
       if (TARGET_32BIT)
         {
-          arm_split_constant (IOR, SImode, NULL_RTX,
-	                      INTVAL (operands[2]), operands[0], operands[1],
-			      optimize && can_create_pseudo_p ());
-          DONE;
+	  if (DONT_EARLY_SPLIT_CONSTANT (INTVAL (operands[2]), IOR))
+	    operands[2] = force_reg (SImode, operands[2]);
+	  else
+	    {
+	      arm_split_constant (IOR, SImode, NULL_RTX,
+				  INTVAL (operands[2]), operands[0],
+				  operands[1],
+				  optimize && can_create_pseudo_p ());
+	      DONE;
+	    }
 	}
       else /* TARGET_THUMB1 */
         {
@@ -3054,10 +3124,16 @@
     {
       if (TARGET_32BIT)
         {
-          arm_split_constant (XOR, SImode, NULL_RTX,
-	                      INTVAL (operands[2]), operands[0], operands[1],
-			      optimize && can_create_pseudo_p ());
-          DONE;
+	  if (DONT_EARLY_SPLIT_CONSTANT (INTVAL (operands[2]), XOR))
+	    operands[2] = force_reg (SImode, operands[2]);
+	  else
+	    {
+	      arm_split_constant (XOR, SImode, NULL_RTX,
+				  INTVAL (operands[2]), operands[0],
+				  operands[1],
+				  optimize && can_create_pseudo_p ());
+	      DONE;
+	    }
 	}
       else /* TARGET_THUMB1 */
         {
@@ -5076,7 +5152,7 @@
 
 (define_split
   [(set (match_operand:SI 0 "s_register_operand" "")
-	(ior_xor:SI (and:SI (ashift:SI
+	(IOR_XOR:SI (and:SI (ashift:SI
 			     (match_operand:SI 1 "s_register_operand" "")
 			     (match_operand:SI 2 "const_int_operand" ""))
 			    (match_operand:SI 3 "const_int_operand" ""))
@@ -5088,7 +5164,7 @@
        == (GET_MODE_MASK (GET_MODE (operands[5]))
            & (GET_MODE_MASK (GET_MODE (operands[5]))
 	      << (INTVAL (operands[2])))))"
-  [(set (match_dup 0) (ior_xor:SI (ashift:SI (match_dup 1) (match_dup 2))
+  [(set (match_dup 0) (IOR_XOR:SI (ashift:SI (match_dup 1) (match_dup 2))
 				  (match_dup 4)))
    (set (match_dup 0) (zero_extend:SI (match_dup 5)))]
   "operands[5] = gen_lowpart (GET_MODE (operands[5]), operands[0]);"
@@ -5590,10 +5666,18 @@
           && !(const_ok_for_arm (INTVAL (operands[1]))
                || const_ok_for_arm (~INTVAL (operands[1]))))
         {
-           arm_split_constant (SET, SImode, NULL_RTX,
-	                       INTVAL (operands[1]), operands[0], NULL_RTX,
-			       optimize && can_create_pseudo_p ());
-          DONE;
+	   if (DONT_EARLY_SPLIT_CONSTANT (INTVAL (operands[1]), SET))
+	     {
+		emit_insn (gen_rtx_SET (SImode, operands[0], operands[1]));
+		DONE;
+	     }
+	  else
+	     {
+		arm_split_constant (SET, SImode, NULL_RTX,
+	                            INTVAL (operands[1]), operands[0], NULL_RTX,
+			            optimize && can_create_pseudo_p ());
+		DONE;
+	     }
         }
     }
   else /* TARGET_THUMB1...  */
@@ -5667,7 +5751,7 @@
   [(set_attr "predicable" "yes")
    (set_attr "predicable_short_it" "no")
    (set_attr "length" "4")
-   (set_attr "type" "mov_imm")]
+   (set_attr "type" "alu_sreg")]
 )
 
 (define_insn "*arm_movsi_insn"
@@ -6713,7 +6797,7 @@
 
   /* Support only fixed point registers.  */
   if (!CONST_INT_P (operands[2])
-      || INTVAL (operands[2]) > 14
+      || INTVAL (operands[2]) > MAX_LDM_STM_OPS
       || INTVAL (operands[2]) < 2
       || !MEM_P (operands[1])
       || !REG_P (operands[0])
@@ -6738,7 +6822,7 @@
 
   /* Support only fixed point registers.  */
   if (!CONST_INT_P (operands[2])
-      || INTVAL (operands[2]) > 14
+      || INTVAL (operands[2]) > MAX_LDM_STM_OPS
       || INTVAL (operands[2]) < 2
       || !REG_P (operands[1])
       || !MEM_P (operands[0])
@@ -6923,7 +7007,7 @@
   [(set_attr "conds" "set")
    (set_attr "shift" "1")
    (set_attr "arch" "32,a,a")
-   (set_attr "type" "alus_shift_imm,alu_shift_reg,alus_shift_imm")])
+   (set_attr "type" "alus_shift_imm,alus_shift_reg,alus_shift_imm")])
 
 (define_insn "*cmpsi_shiftsi_swp"
   [(set (reg:CC_SWP CC_REGNUM)
@@ -6936,7 +7020,7 @@
   [(set_attr "conds" "set")
    (set_attr "shift" "1")
    (set_attr "arch" "32,a,a")
-   (set_attr "type" "alus_shift_imm,alu_shift_reg,alus_shift_imm")])
+   (set_attr "type" "alus_shift_imm,alus_shift_reg,alus_shift_imm")])
 
 (define_insn "*arm_cmpsi_negshiftsi_si"
   [(set (reg:CC_Z CC_REGNUM)
@@ -7529,10 +7613,10 @@
                                         (const_string "mov_imm")
                                         (const_string "mov_reg"))
                           (const_string "mvn_imm")
-                          (const_string "mov_reg")
-                          (const_string "mov_reg")
-                          (const_string "mov_reg")
-                          (const_string "mov_reg")])]
+                          (const_string "multiple")
+                          (const_string "multiple")
+                          (const_string "multiple")
+                          (const_string "multiple")])]
 )
 
 (define_insn "*movsfcc_soft_insn"
@@ -7755,6 +7839,13 @@
    && !arm_is_long_call_p (SYMBOL_REF_DECL (operands[0]))"
   "*
   {
+   rtx op = operands[0];
+
+   /* Switch mode now when possible.  */
+   if (SYMBOL_REF_DECL (op) && !TREE_PUBLIC (SYMBOL_REF_DECL (op))
+        && arm_arch5 && arm_change_mode_p (SYMBOL_REF_DECL (op)))
+      return NEED_PLT_RELOC ? \"blx%?\\t%a0(PLT)\" : \"blx%?\\t(%a0)\";
+
     return NEED_PLT_RELOC ? \"bl%?\\t%a0(PLT)\" : \"bl%?\\t%a0\";
   }"
   [(set_attr "type" "call")]
@@ -7772,6 +7863,13 @@
    && !arm_is_long_call_p (SYMBOL_REF_DECL (operands[1]))"
   "*
   {
+   rtx op = operands[1];
+
+   /* Switch mode now when possible.  */
+   if (SYMBOL_REF_DECL (op) && !TREE_PUBLIC (SYMBOL_REF_DECL (op))
+        && arm_arch5 && arm_change_mode_p (SYMBOL_REF_DECL (op)))
+      return NEED_PLT_RELOC ? \"blx%?\\t%a1(PLT)\" : \"blx%?\\t(%a1)\";
+
     return NEED_PLT_RELOC ? \"bl%?\\t%a1(PLT)\" : \"bl%?\\t%a1\";
   }"
   [(set_attr "type" "call")]
@@ -7885,7 +7983,7 @@
 )
 
 (define_expand "<return_str>return"
-  [(returns)]
+  [(RETURNS)]
   "(TARGET_ARM || (TARGET_THUMB2
                    && ARM_FUNC_TYPE (arm_current_func_type ()) == ARM_FT_NORMAL
                    && !IS_STACKALIGN (arm_current_func_type ())))
@@ -7923,7 +8021,7 @@
   [(set (pc)
         (if_then_else (match_operator 0 "arm_comparison_operator"
 		       [(match_operand 1 "cc_register" "") (const_int 0)])
-                      (returns)
+                      (RETURNS)
                       (pc)))]
   "TARGET_ARM  <return_cond_true>"
   "*
@@ -7946,7 +8044,7 @@
         (if_then_else (match_operator 0 "arm_comparison_operator"
 		       [(match_operand 1 "cc_register" "") (const_int 0)])
                       (pc)
-		      (returns)))]
+		      (RETURNS)))]
   "TARGET_ARM <return_cond_true>"
   "*
   {
@@ -8280,7 +8378,7 @@
 
 (define_insn "*<arith_shift_insn>_multsi"
   [(set (match_operand:SI 0 "s_register_operand" "=r,r")
-	(shiftable_ops:SI
+	(SHIFTABLE_OPS:SI
 	 (mult:SI (match_operand:SI 2 "s_register_operand" "r,r")
 		  (match_operand:SI 3 "power_of_two_operand" ""))
 	 (match_operand:SI 1 "s_register_operand" "rk,<t2_binop0>")))]
@@ -8294,7 +8392,7 @@
 
 (define_insn "*<arith_shift_insn>_shiftsi"
   [(set (match_operand:SI 0 "s_register_operand" "=r,r,r")
-	(shiftable_ops:SI
+	(SHIFTABLE_OPS:SI
 	 (match_operator:SI 2 "shift_nomul_operator"
 	  [(match_operand:SI 3 "s_register_operand" "r,r,r")
 	   (match_operand:SI 4 "shift_amount_operand" "M,M,r")])
@@ -8690,7 +8788,14 @@
     return \"\";
   "
   [(set_attr "conds" "use")
-   (set_attr "type" "mov_reg,mov_reg,multiple")
+   (set_attr_alternative "type"
+                         [(if_then_else (match_operand 2 "const_int_operand" "")
+                                        (const_string "mov_imm")
+                                        (const_string "mov_reg"))
+                          (if_then_else (match_operand 1 "const_int_operand" "")
+                                        (const_string "mov_imm")
+                                        (const_string "mov_reg"))
+                          (const_string "multiple")])
    (set_attr "length" "4,4,8")]
 )
 
@@ -9486,8 +9591,8 @@
                                         (const_string "alu_imm" )
                                         (const_string "alu_sreg"))
                           (const_string "alu_imm")
-                          (const_string "alu_sreg")
-                          (const_string "alu_sreg")])]
+                          (const_string "multiple")
+                          (const_string "multiple")])]
 )
 
 (define_insn "*ifcompare_move_plus"
@@ -9524,7 +9629,13 @@
    sub%D4\\t%0, %2, #%n3\;mov%d4\\t%0, %1"
   [(set_attr "conds" "use")
    (set_attr "length" "4,4,8,8")
-   (set_attr "type" "alu_sreg,alu_imm,multiple,multiple")]
+   (set_attr_alternative "type"
+                         [(if_then_else (match_operand 3 "const_int_operand" "")
+                                        (const_string "alu_imm" )
+                                        (const_string "alu_sreg"))
+                          (const_string "alu_imm")
+                          (const_string "multiple")
+                          (const_string "multiple")])]
 )
 
 (define_insn "*ifcompare_arith_arith"
@@ -9619,7 +9730,11 @@
    %I5%d4\\t%0, %2, %3\;mov%D4\\t%0, %1"
   [(set_attr "conds" "use")
    (set_attr "length" "4,8")
-   (set_attr "type" "alu_shift_reg,multiple")]
+   (set_attr_alternative "type"
+                         [(if_then_else (match_operand 3 "const_int_operand" "")
+                                        (const_string "alu_shift_imm" )
+                                        (const_string "alu_shift_reg"))
+                          (const_string "multiple")])]
 )
 
 (define_insn "*ifcompare_move_arith"
@@ -9680,7 +9795,11 @@
    %I5%D4\\t%0, %2, %3\;mov%d4\\t%0, %1"
   [(set_attr "conds" "use")
    (set_attr "length" "4,8")
-   (set_attr "type" "alu_shift_reg,multiple")]
+   (set_attr_alternative "type"
+                         [(if_then_else (match_operand 3 "const_int_operand" "")
+                                        (const_string "alu_shift_imm" )
+                                        (const_string "alu_shift_reg"))
+                          (const_string "multiple")])]
 )
 
 (define_insn "*ifcompare_move_not"
@@ -9787,7 +9906,12 @@
   [(set_attr "conds" "use")
    (set_attr "shift" "2")
    (set_attr "length" "4,8,8")
-   (set_attr "type" "mov_shift_reg,multiple,multiple")]
+   (set_attr_alternative "type"
+                         [(if_then_else (match_operand 3 "const_int_operand" "")
+                                        (const_string "mov_shift" )
+                                        (const_string "mov_shift_reg"))
+                          (const_string "multiple")
+                          (const_string "multiple")])]
 )
 
 (define_insn "*ifcompare_move_shift"
@@ -9825,7 +9949,12 @@
   [(set_attr "conds" "use")
    (set_attr "shift" "2")
    (set_attr "length" "4,8,8")
-   (set_attr "type" "mov_shift_reg,multiple,multiple")]
+   (set_attr_alternative "type"
+                         [(if_then_else (match_operand 3 "const_int_operand" "")
+                                        (const_string "mov_shift" )
+                                        (const_string "mov_shift_reg"))
+                          (const_string "multiple")
+                          (const_string "multiple")])]
 )
 
 (define_insn "*ifcompare_shift_shift"
@@ -10906,7 +11035,7 @@
  [(set_attr "predicable" "yes")
   (set_attr "predicable_short_it" "no")
   (set_attr "length" "4")
-  (set_attr "type" "mov_imm")]
+  (set_attr "type" "alu_sreg")]
 )
 
 (define_insn "*arm_rev"
--- a/src/gcc/config/arm/arm.opt
+++ b/src/gcc/config/arm/arm.opt
@@ -122,6 +122,10 @@ Enum(float_abi_type) String(softfp) Value(ARM_FLOAT_ABI_SOFTFP)
 EnumValue
 Enum(float_abi_type) String(hard) Value(ARM_FLOAT_ABI_HARD)
 
+mflip-thumb
+Target Report Var(TARGET_FLIP_THUMB) Undocumented
+Switch ARM/Thumb modes on alternating functions for compiler testing
+
 mfp16-format=
 Target RejectNegative Joined Enum(arm_fp16_format_type) Var(arm_fp16_format) Init(ARM_FP16_FORMAT_NONE)
 Specify the __fp16 floating-point format
@@ -182,7 +186,7 @@ Target RejectNegative Joined UInteger Var(arm_structure_size_boundary) Init(DEFA
 Specify the minimum bit alignment of structures
 
 mthumb
-Target Report RejectNegative Mask(THUMB)
+Target Report RejectNegative Mask(THUMB) Save
 Generate code for Thumb state
 
 mthumb-interwork
@@ -246,7 +250,7 @@ Target Report Var(target_word_relocations) Init(TARGET_DEFAULT_WORD_RELOCATIONS)
 Only generate absolute relocations on word sized values.
 
 mrestrict-it
-Target Report Var(arm_restrict_it) Init(2)
+Target Report Var(arm_restrict_it) Init(2) Save
 Generate IT blocks appropriate for ARMv8.
 
 mold-rtx-costs
@@ -275,5 +279,5 @@ Target Report Var(target_slow_flash_data) Init(0)
 Assume loading data from flash is slower than fetching instructions.
 
 masm-syntax-unified
-Target Report Var(inline_asm_unified) Init(0)
+Target Report Var(inline_asm_unified) Init(0) Save
 Assume unified syntax for Thumb inline assembly code.
--- a/src/gcc/config/arm/arm_neon_builtins.def
+++ b/src/gcc/config/arm/arm_neon_builtins.def
@@ -67,28 +67,28 @@ VAR8 (BINOP, vqshls, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di)
 VAR8 (BINOP, vqshlu, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di)
 VAR8 (BINOP, vqrshls, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di)
 VAR8 (BINOP, vqrshlu, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di)
-VAR8 (GETLANE, vshrs_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di)
-VAR8 (GETLANE, vshru_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di)
-VAR8 (GETLANE, vrshrs_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di)
-VAR8 (GETLANE, vrshru_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di)
-VAR3 (GETLANE, vshrn_n, v8hi, v4si, v2di)
-VAR3 (GETLANE, vrshrn_n, v8hi, v4si, v2di)
-VAR3 (GETLANE, vqshrns_n, v8hi, v4si, v2di)
-VAR3 (GETLANE, vqshrnu_n, v8hi, v4si, v2di)
-VAR3 (GETLANE, vqrshrns_n, v8hi, v4si, v2di)
-VAR3 (GETLANE, vqrshrnu_n, v8hi, v4si, v2di)
-VAR3 (GETLANE, vqshrun_n, v8hi, v4si, v2di)
-VAR3 (GETLANE, vqrshrun_n, v8hi, v4si, v2di)
-VAR8 (GETLANE, vshl_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di)
-VAR8 (GETLANE, vqshl_s_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di)
-VAR8 (GETLANE, vqshl_u_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di)
-VAR8 (GETLANE, vqshlu_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di)
-VAR3 (GETLANE, vshlls_n, v8qi, v4hi, v2si)
-VAR3 (GETLANE, vshllu_n, v8qi, v4hi, v2si)
-VAR8 (SETLANE, vsras_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di)
-VAR8 (SETLANE, vsrau_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di)
-VAR8 (SETLANE, vrsras_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di)
-VAR8 (SETLANE, vrsrau_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di)
+VAR8 (BINOP_IMM, vshrs_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di)
+VAR8 (BINOP_IMM, vshru_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di)
+VAR8 (BINOP_IMM, vrshrs_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di)
+VAR8 (BINOP_IMM, vrshru_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di)
+VAR3 (BINOP_IMM, vshrn_n, v8hi, v4si, v2di)
+VAR3 (BINOP_IMM, vrshrn_n, v8hi, v4si, v2di)
+VAR3 (BINOP_IMM, vqshrns_n, v8hi, v4si, v2di)
+VAR3 (BINOP_IMM, vqshrnu_n, v8hi, v4si, v2di)
+VAR3 (BINOP_IMM, vqrshrns_n, v8hi, v4si, v2di)
+VAR3 (BINOP_IMM, vqrshrnu_n, v8hi, v4si, v2di)
+VAR3 (BINOP_IMM, vqshrun_n, v8hi, v4si, v2di)
+VAR3 (BINOP_IMM, vqrshrun_n, v8hi, v4si, v2di)
+VAR8 (BINOP_IMM, vshl_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di)
+VAR8 (BINOP_IMM, vqshl_s_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di)
+VAR8 (BINOP_IMM, vqshl_u_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di)
+VAR8 (BINOP_IMM, vqshlu_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di)
+VAR3 (BINOP_IMM, vshlls_n, v8qi, v4hi, v2si)
+VAR3 (BINOP_IMM, vshllu_n, v8qi, v4hi, v2si)
+VAR8 (TERNOP_IMM, vsras_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di)
+VAR8 (TERNOP_IMM, vsrau_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di)
+VAR8 (TERNOP_IMM, vrsras_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di)
+VAR8 (TERNOP_IMM, vrsrau_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di)
 VAR2 (BINOP, vsub, v2sf, v4sf)
 VAR3 (BINOP, vsubls, v8qi, v4hi, v2si)
 VAR3 (BINOP, vsublu, v8qi, v4hi, v2si)
@@ -140,8 +140,8 @@ VAR6 (BINOP, vpadals, v8qi, v4hi, v2si, v16qi, v8hi, v4si)
 VAR6 (BINOP, vpadalu, v8qi, v4hi, v2si, v16qi, v8hi, v4si)
 VAR2 (BINOP, vrecps, v2sf, v4sf)
 VAR2 (BINOP, vrsqrts, v2sf, v4sf)
-VAR8 (SETLANE, vsri_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di)
-VAR8 (SETLANE, vsli_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di)
+VAR8 (TERNOP_IMM, vsri_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di)
+VAR8 (TERNOP_IMM, vsli_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di)
 VAR8 (UNOP, vabs, v8qi, v4hi, v2si, v2sf, v16qi, v8hi, v4si, v4sf)
 VAR6 (UNOP, vqabs, v8qi, v4hi, v2si, v16qi, v8hi, v4si)
 VAR8 (UNOP, vneg, v8qi, v4hi, v2si, v2sf, v16qi, v8hi, v4si, v4sf)
@@ -162,7 +162,7 @@ VAR10 (SETLANE, vset_lane,
 VAR5 (UNOP, vcreate, v8qi, v4hi, v2si, v2sf, di)
 VAR10 (UNOP, vdup_n,
 	 v8qi, v4hi, v2si, v2sf, di, v16qi, v8hi, v4si, v4sf, v2di)
-VAR10 (BINOP, vdup_lane,
+VAR10 (GETLANE, vdup_lane,
 	 v8qi, v4hi, v2si, v2sf, di, v16qi, v8hi, v4si, v4sf, v2di)
 VAR5 (COMBINE, vcombine, v8qi, v4hi, v2si, v2sf, di)
 VAR5 (UNOP, vget_high, v16qi, v8hi, v4si, v4sf, v2di)
@@ -174,23 +174,23 @@ VAR3 (UNOP, vqmovun, v8hi, v4si, v2di)
 VAR3 (UNOP, vmovls, v8qi, v4hi, v2si)
 VAR3 (UNOP, vmovlu, v8qi, v4hi, v2si)
 VAR6 (SETLANE, vmul_lane, v4hi, v2si, v2sf, v8hi, v4si, v4sf)
-VAR6 (LANEMAC, vmla_lane, v4hi, v2si, v2sf, v8hi, v4si, v4sf)
-VAR2 (LANEMAC, vmlals_lane, v4hi, v2si)
-VAR2 (LANEMAC, vmlalu_lane, v4hi, v2si)
-VAR2 (LANEMAC, vqdmlal_lane, v4hi, v2si)
-VAR6 (LANEMAC, vmls_lane, v4hi, v2si, v2sf, v8hi, v4si, v4sf)
-VAR2 (LANEMAC, vmlsls_lane, v4hi, v2si)
-VAR2 (LANEMAC, vmlslu_lane, v4hi, v2si)
-VAR2 (LANEMAC, vqdmlsl_lane, v4hi, v2si)
+VAR6 (MAC_LANE, vmla_lane, v4hi, v2si, v2sf, v8hi, v4si, v4sf)
+VAR2 (MAC_LANE, vmlals_lane, v4hi, v2si)
+VAR2 (MAC_LANE, vmlalu_lane, v4hi, v2si)
+VAR2 (MAC_LANE, vqdmlal_lane, v4hi, v2si)
+VAR6 (MAC_LANE, vmls_lane, v4hi, v2si, v2sf, v8hi, v4si, v4sf)
+VAR2 (MAC_LANE, vmlsls_lane, v4hi, v2si)
+VAR2 (MAC_LANE, vmlslu_lane, v4hi, v2si)
+VAR2 (MAC_LANE, vqdmlsl_lane, v4hi, v2si)
 VAR6 (BINOP, vmul_n, v4hi, v2si, v2sf, v8hi, v4si, v4sf)
-VAR6 (LANEMAC, vmla_n, v4hi, v2si, v2sf, v8hi, v4si, v4sf)
-VAR2 (LANEMAC, vmlals_n, v4hi, v2si)
-VAR2 (LANEMAC, vmlalu_n, v4hi, v2si)
-VAR2 (LANEMAC, vqdmlal_n, v4hi, v2si)
-VAR6 (LANEMAC, vmls_n, v4hi, v2si, v2sf, v8hi, v4si, v4sf)
-VAR2 (LANEMAC, vmlsls_n, v4hi, v2si)
-VAR2 (LANEMAC, vmlslu_n, v4hi, v2si)
-VAR2 (LANEMAC, vqdmlsl_n, v4hi, v2si)
+VAR6 (MAC_N, vmla_n, v4hi, v2si, v2sf, v8hi, v4si, v4sf)
+VAR2 (MAC_N, vmlals_n, v4hi, v2si)
+VAR2 (MAC_N, vmlalu_n, v4hi, v2si)
+VAR2 (MAC_N, vqdmlal_n, v4hi, v2si)
+VAR6 (MAC_N, vmls_n, v4hi, v2si, v2sf, v8hi, v4si, v4sf)
+VAR2 (MAC_N, vmlsls_n, v4hi, v2si)
+VAR2 (MAC_N, vmlslu_n, v4hi, v2si)
+VAR2 (MAC_N, vqdmlsl_n, v4hi, v2si)
 VAR10 (SETLANE, vext,
 	 v8qi, v4hi, v2si, v2sf, di, v16qi, v8hi, v4si, v4sf, v2di)
 VAR8 (UNOP, vrev64, v8qi, v4hi, v2si, v2sf, v16qi, v8hi, v4si, v4sf)
--- a/src/gcc/config/arm/cortex-a53.md
+++ b/src/gcc/config/arm/cortex-a53.md
@@ -360,7 +360,7 @@
 ;; Crude Advanced SIMD approximation.
 ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
 
-(define_insn_reservation "cortex_53_advsimd" 4
+(define_insn_reservation "cortex_a53_advsimd" 4
   (and (eq_attr "tune" "cortexa53")
        (eq_attr "is_neon_type" "yes"))
   "cortex_a53_simd0")
--- a/src/gcc/config/arm/driver-arm.c
+++ b/src/gcc/config/arm/driver-arm.c
@@ -35,6 +35,9 @@ static struct vendor_cpu arm_cpu_table[] = {
     {"0xb02", "armv6k", "mpcore"},
     {"0xb36", "armv6j", "arm1136j-s"},
     {"0xb56", "armv6t2", "arm1156t2-s"},
+    /* armv6kz is the correct spelling for ARMv6KZ but may not be supported in
+       the version of binutils used.  The incorrect spelling is supported in
+       legacy and current binutils so that is used instead.  */
     {"0xb76", "armv6zk", "arm1176jz-s"},
     {"0xc05", "armv7-a", "cortex-a5"},
     {"0xc07", "armv7ve", "cortex-a7"},
--- a/src/gcc/config/arm/elf.h
+++ b/src/gcc/config/arm/elf.h
@@ -120,7 +120,6 @@
   { "marm", "mlittle-endian", "mfloat-abi=soft", "mno-thumb-interwork", "fno-leading-underscore" }
 #endif
 
-#define TARGET_ASM_FILE_START_APP_OFF true
 #define TARGET_ASM_FILE_START_FILE_DIRECTIVE true
 
 
--- a/src/gcc/config/arm/iterators.md
+++ b/src/gcc/config/arm/iterators.md
@@ -181,39 +181,53 @@
 ;; compare a second time.
 (define_code_iterator LTUGEU [ltu geu])
 
+;; The signed gt, ge comparisons
+(define_code_iterator GTGE [gt ge])
+
+;; The unsigned gt, ge comparisons
+(define_code_iterator GTUGEU [gtu geu])
+
+;; Comparisons for vc<cmp>
+(define_code_iterator COMPARISONS [eq gt ge le lt])
+
 ;; A list of ...
-(define_code_iterator ior_xor [ior xor])
+(define_code_iterator IOR_XOR [ior xor])
 
 ;; Operations on two halves of a quadword vector.
-(define_code_iterator vqh_ops [plus smin smax umin umax])
+(define_code_iterator VQH_OPS [plus smin smax umin umax])
 
 ;; Operations on two halves of a quadword vector,
 ;; without unsigned variants (for use with *SFmode pattern).
-(define_code_iterator vqhs_ops [plus smin smax])
+(define_code_iterator VQHS_OPS [plus smin smax])
 
 ;; A list of widening operators
 (define_code_iterator SE [sign_extend zero_extend])
 
 ;; Right shifts
-(define_code_iterator rshifts [ashiftrt lshiftrt])
+(define_code_iterator RSHIFTS [ashiftrt lshiftrt])
 
 ;; Iterator for integer conversions
 (define_code_iterator FIXUORS [fix unsigned_fix])
 
 ;; Binary operators whose second operand can be shifted.
-(define_code_iterator shiftable_ops [plus minus ior xor and])
+(define_code_iterator SHIFTABLE_OPS [plus minus ior xor and])
 
-;; plus and minus are the only shiftable_ops for which Thumb2 allows
+;; plus and minus are the only SHIFTABLE_OPS for which Thumb2 allows
 ;; a stack pointer opoerand.  The minus operation is a candidate for an rsub
 ;; and hence only plus is supported.
 (define_code_attr t2_binop0
   [(plus "rk") (minus "r") (ior "r") (xor "r") (and "r")])
 
-;; The instruction to use when a shiftable_ops has a shift operation as
+;; The instruction to use when a SHIFTABLE_OPS has a shift operation as
 ;; its first operand.
 (define_code_attr arith_shift_insn
   [(plus "add") (minus "rsb") (ior "orr") (xor "eor") (and "and")])
 
+(define_code_attr cmp_op [(eq "eq") (gt "gt") (ge "ge") (lt "lt") (le "le")
+                          (gtu "gt") (geu "ge")])
+
+(define_code_attr cmp_type [(eq "i") (gt "s") (ge "s") (lt "s") (le "s")])
+
 ;;----------------------------------------------------------------------------
 ;; Int iterators
 ;;----------------------------------------------------------------------------
@@ -221,6 +235,10 @@
 (define_int_iterator VRINT [UNSPEC_VRINTZ UNSPEC_VRINTP UNSPEC_VRINTM
                             UNSPEC_VRINTR UNSPEC_VRINTX UNSPEC_VRINTA])
 
+(define_int_iterator NEON_VCMP [UNSPEC_VCEQ UNSPEC_VCGT UNSPEC_VCGE UNSPEC_VCLT UNSPEC_VCLE])
+
+(define_int_iterator NEON_VACMP [UNSPEC_VCAGE UNSPEC_VCAGT])
+
 (define_int_iterator VCVT [UNSPEC_VRINTP UNSPEC_VRINTM UNSPEC_VRINTA])
 
 (define_int_iterator NEON_VRINT [UNSPEC_NVRINTP UNSPEC_NVRINTZ UNSPEC_NVRINTM
@@ -677,6 +695,11 @@
 
 ])
 
+(define_int_attr cmp_op_unsp [(UNSPEC_VCEQ "eq") (UNSPEC_VCGT "gt")
+                              (UNSPEC_VCGE "ge") (UNSPEC_VCLE "le")
+                              (UNSPEC_VCLT "lt") (UNSPEC_VCAGE "ge")
+                              (UNSPEC_VCAGT "gt")])
+
 (define_int_attr r [
   (UNSPEC_VRHADD_S "r") (UNSPEC_VRHADD_U "r")
   (UNSPEC_VHADD_S "") (UNSPEC_VHADD_U "")
@@ -774,7 +797,7 @@
                           (UNSPEC_SHA256H2 "V4SI") (UNSPEC_SHA256SU1 "V4SI")])
 
 ;; Both kinds of return insn.
-(define_code_iterator returns [return simple_return])
+(define_code_iterator RETURNS [return simple_return])
 (define_code_attr return_str [(return "") (simple_return "simple_")])
 (define_code_attr return_simple_p [(return "false") (simple_return "true")])
 (define_code_attr return_cond_false [(return " && USE_RETURN_INSN (FALSE)")
--- a/src/gcc/config/arm/iwmmxt.md
+++ b/src/gcc/config/arm/iwmmxt.md
@@ -107,8 +107,8 @@
 )
 
 (define_insn "*iwmmxt_arm_movdi"
-  [(set (match_operand:DI 0 "nonimmediate_di_operand" "=r, r, r, r, m,y,y,yr,y,yrUy,*w, r,*w,*w, *Uv")
-        (match_operand:DI 1 "di_operand"              "rDa,Db,Dc,mi,r,y,yr,y,yrUy,y, r,*w,*w,*Uvi,*w"))]
+  [(set (match_operand:DI 0 "nonimmediate_di_operand" "=r, r, r, r, m,y,y,r, y,Uy,*w, r,*w,*w, *Uv")
+        (match_operand:DI 1 "di_operand"              "rDa,Db,Dc,mi,r,y,r,y,Uy,y,  r,*w,*w,*Uvi,*w"))]
   "TARGET_REALLY_IWMMXT
    && (   register_operand (operands[0], DImode)
        || register_operand (operands[1], DImode))"
--- a/src/gcc/config/arm/linux-eabi.h
+++ b/src/gcc/config/arm/linux-eabi.h
@@ -77,6 +77,23 @@
     %{mfloat-abi=soft*:" GLIBC_DYNAMIC_LINKER_SOFT_FLOAT "} \
     %{!mfloat-abi=*:" GLIBC_DYNAMIC_LINKER_DEFAULT "}"
 
+/* For ARM musl currently supports four dynamic linkers:
+   - ld-musl-arm.so.1 - for the EABI-derived soft-float ABI
+   - ld-musl-armhf.so.1 - for the EABI-derived hard-float ABI
+   - ld-musl-armeb.so.1 - for the EABI-derived soft-float ABI, EB
+   - ld-musl-armebhf.so.1 - for the EABI-derived hard-float ABI, EB
+   musl does not support the legacy OABI mode.
+   All the dynamic linkers live in /lib.
+   We default to soft-float, EL. */
+#undef  MUSL_DYNAMIC_LINKER
+#if TARGET_BIG_ENDIAN_DEFAULT
+#define MUSL_DYNAMIC_LINKER_E "%{mlittle-endian:;:eb}"
+#else
+#define MUSL_DYNAMIC_LINKER_E "%{mbig-endian:eb}"
+#endif
+#define MUSL_DYNAMIC_LINKER \
+  "/lib/ld-musl-arm" MUSL_DYNAMIC_LINKER_E "%{mfloat-abi=hard:hf}.so.1"
+
 /* At this point, bpabi.h will have clobbered LINK_SPEC.  We want to
    use the GNU/Linux version, not the generic BPABI version.  */
 #undef  LINK_SPEC
@@ -107,6 +124,7 @@
 
 #undef	ENDFILE_SPEC
 #define ENDFILE_SPEC \
+  "%{Ofast|ffast-math|funsafe-math-optimizations:crtfastmath.o%s} "	\
   LINUX_OR_ANDROID_LD (GNU_USER_TARGET_ENDFILE_SPEC, ANDROID_ENDFILE_SPEC)
 
 /* Use the default LIBGCC_SPEC, not the version in linux-elf.h, as we
--- a/src/gcc/config/arm/neon.md
+++ b/src/gcc/config/arm/neon.md
@@ -1114,7 +1114,7 @@
 ;; lshrdi3_neon
 (define_insn_and_split "<shift>di3_neon"
   [(set (match_operand:DI 0 "s_register_operand"	     "= w, w,?&r,?r,?w,?w")
-	(rshifts:DI (match_operand:DI 1 "s_register_operand" " 0w, w, 0r, r,0w, w")
+	(RSHIFTS:DI (match_operand:DI 1 "s_register_operand" " 0w, w, 0r, r,0w, w")
 		    (match_operand:SI 2 "reg_or_int_operand" "  r, i,  r, i, r, i")))
    (clobber (match_scratch:SI 3				     "=2r, X, &r, X,2r, X"))
    (clobber (match_scratch:SI 4				     "= X, X, &r, X, X, X"))
@@ -1194,71 +1194,6 @@
   [(set_attr "type" "neon_add_widen")]
 )
 
-;; VEXT can be used to synthesize coarse whole-vector shifts with 8-bit
-;; shift-count granularity. That's good enough for the middle-end's current
-;; needs.
-
-;; Note that it's not safe to perform such an operation in big-endian mode,
-;; due to element-ordering issues.
-
-(define_expand "vec_shr_<mode>"
-  [(match_operand:VDQ 0 "s_register_operand" "")
-   (match_operand:VDQ 1 "s_register_operand" "")
-   (match_operand:SI 2 "const_multiple_of_8_operand" "")]
-  "TARGET_NEON && !BYTES_BIG_ENDIAN"
-{
-  rtx zero_reg;
-  HOST_WIDE_INT num_bits = INTVAL (operands[2]);
-  const int width = GET_MODE_BITSIZE (<MODE>mode);
-  const machine_mode bvecmode = (width == 128) ? V16QImode : V8QImode;
-  rtx (*gen_ext) (rtx, rtx, rtx, rtx) =
-    (width == 128) ? gen_neon_vextv16qi : gen_neon_vextv8qi;
-
-  if (num_bits == width)
-    {
-      emit_move_insn (operands[0], operands[1]);
-      DONE;
-    }
-
-  zero_reg = force_reg (bvecmode, CONST0_RTX (bvecmode));
-  operands[0] = gen_lowpart (bvecmode, operands[0]);
-  operands[1] = gen_lowpart (bvecmode, operands[1]);
-
-  emit_insn (gen_ext (operands[0], operands[1], zero_reg,
-		      GEN_INT (num_bits / BITS_PER_UNIT)));
-  DONE;
-})
-
-(define_expand "vec_shl_<mode>"
-  [(match_operand:VDQ 0 "s_register_operand" "")
-   (match_operand:VDQ 1 "s_register_operand" "")
-   (match_operand:SI 2 "const_multiple_of_8_operand" "")]
-  "TARGET_NEON && !BYTES_BIG_ENDIAN"
-{
-  rtx zero_reg;
-  HOST_WIDE_INT num_bits = INTVAL (operands[2]);
-  const int width = GET_MODE_BITSIZE (<MODE>mode);
-  const machine_mode bvecmode = (width == 128) ? V16QImode : V8QImode;
-  rtx (*gen_ext) (rtx, rtx, rtx, rtx) =
-    (width == 128) ? gen_neon_vextv16qi : gen_neon_vextv8qi;
-
-  if (num_bits == 0)
-    {
-      emit_move_insn (operands[0], CONST0_RTX (<MODE>mode));
-      DONE;
-    }
-
-  num_bits = width - num_bits;
-
-  zero_reg = force_reg (bvecmode, CONST0_RTX (bvecmode));
-  operands[0] = gen_lowpart (bvecmode, operands[0]);
-  operands[1] = gen_lowpart (bvecmode, operands[1]);
-
-  emit_insn (gen_ext (operands[0], zero_reg, operands[1],
-		      GEN_INT (num_bits / BITS_PER_UNIT)));
-  DONE;
-})
-
 ;; Helpers for quad-word reduction operations
 
 ; Add (or smin, smax...) the low N/2 elements of the N-element vector
@@ -1267,7 +1202,7 @@
 
 (define_insn "quad_halves_<code>v4si"
   [(set (match_operand:V2SI 0 "s_register_operand" "=w")
-        (vqh_ops:V2SI
+        (VQH_OPS:V2SI
           (vec_select:V2SI (match_operand:V4SI 1 "s_register_operand" "w")
                            (parallel [(const_int 0) (const_int 1)]))
           (vec_select:V2SI (match_dup 1)
@@ -1280,7 +1215,7 @@
 
 (define_insn "quad_halves_<code>v4sf"
   [(set (match_operand:V2SF 0 "s_register_operand" "=w")
-        (vqhs_ops:V2SF
+        (VQHS_OPS:V2SF
           (vec_select:V2SF (match_operand:V4SF 1 "s_register_operand" "w")
                            (parallel [(const_int 0) (const_int 1)]))
           (vec_select:V2SF (match_dup 1)
@@ -1293,7 +1228,7 @@
 
 (define_insn "quad_halves_<code>v8hi"
   [(set (match_operand:V4HI 0 "s_register_operand" "+w")
-        (vqh_ops:V4HI
+        (VQH_OPS:V4HI
           (vec_select:V4HI (match_operand:V8HI 1 "s_register_operand" "w")
                            (parallel [(const_int 0) (const_int 1)
 				      (const_int 2) (const_int 3)]))
@@ -1308,7 +1243,7 @@
 
 (define_insn "quad_halves_<code>v16qi"
   [(set (match_operand:V8QI 0 "s_register_operand" "+w")
-        (vqh_ops:V8QI
+        (VQH_OPS:V8QI
           (vec_select:V8QI (match_operand:V16QI 1 "s_register_operand" "w")
                            (parallel [(const_int 0) (const_int 1)
 				      (const_int 2) (const_int 3)
@@ -2200,134 +2135,140 @@
   [(set_attr "type" "neon_sub_halve_narrow_q")]
 )
 
-(define_insn "neon_vceq<mode>"
-  [(set (match_operand:<V_cmp_result> 0 "s_register_operand" "=w,w")
-        (unspec:<V_cmp_result>
-	  [(match_operand:VDQW 1 "s_register_operand" "w,w")
-	   (match_operand:VDQW 2 "reg_or_zero_operand" "w,Dz")]
-          UNSPEC_VCEQ))]
+;; These may expand to an UNSPEC pattern when a floating point mode is used
+;; without unsafe math optimizations.
+(define_expand "neon_vc<cmp_op><mode>"
+  [(match_operand:<V_cmp_result> 0 "s_register_operand" "=w,w")
+     (neg:<V_cmp_result>
+       (COMPARISONS:VDQW (match_operand:VDQW 1 "s_register_operand" "w,w")
+                         (match_operand:VDQW 2 "reg_or_zero_operand" "w,Dz")))]
   "TARGET_NEON"
-  "@
-  vceq.<V_if_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2
-  vceq.<V_if_elem>\t%<V_reg>0, %<V_reg>1, #0"
-  [(set (attr "type")
-      (if_then_else (match_test "<Is_float_mode>")
-                    (const_string "neon_fp_compare_s<q>")
-                    (if_then_else (match_operand 2 "zero_operand")
-                      (const_string "neon_compare_zero<q>")
-                      (const_string "neon_compare<q>"))))]
+  {
+    /* For FP comparisons use UNSPECS unless -funsafe-math-optimizations
+       are enabled.  */
+    if (GET_MODE_CLASS (<MODE>mode) == MODE_VECTOR_FLOAT
+        && !flag_unsafe_math_optimizations)
+      {
+        /* We don't just emit a gen_neon_vc<cmp_op><mode>_insn_unspec because
+           we define gen_neon_vceq<mode>_insn_unspec only for float modes
+           whereas this expander iterates over the integer modes as well,
+           but we will never expand to UNSPECs for the integer comparisons.  */
+        switch (<MODE>mode)
+          {
+            case V2SFmode:
+              emit_insn (gen_neon_vc<cmp_op>v2sf_insn_unspec (operands[0],
+                                                              operands[1],
+                                                              operands[2]));
+              break;
+            case V4SFmode:
+              emit_insn (gen_neon_vc<cmp_op>v4sf_insn_unspec (operands[0],
+                                                              operands[1],
+                                                              operands[2]));
+              break;
+            default:
+              gcc_unreachable ();
+          }
+      }
+    else
+      emit_insn (gen_neon_vc<cmp_op><mode>_insn (operands[0],
+                                                 operands[1],
+                                                 operands[2]));
+    DONE;
+  }
 )
 
-(define_insn "neon_vcge<mode>"
+(define_insn "neon_vc<cmp_op><mode>_insn"
   [(set (match_operand:<V_cmp_result> 0 "s_register_operand" "=w,w")
-        (unspec:<V_cmp_result>
-	  [(match_operand:VDQW 1 "s_register_operand" "w,w")
-	   (match_operand:VDQW 2 "reg_or_zero_operand" "w,Dz")]
-          UNSPEC_VCGE))]
-  "TARGET_NEON"
-  "@
-  vcge.<V_s_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2
-  vcge.<V_s_elem>\t%<V_reg>0, %<V_reg>1, #0"
+        (neg:<V_cmp_result>
+          (COMPARISONS:<V_cmp_result>
+            (match_operand:VDQW 1 "s_register_operand" "w,w")
+            (match_operand:VDQW 2 "reg_or_zero_operand" "w,Dz"))))]
+  "TARGET_NEON && !(GET_MODE_CLASS (<MODE>mode) == MODE_VECTOR_FLOAT
+                    && !flag_unsafe_math_optimizations)"
+  {
+    char pattern[100];
+    sprintf (pattern, "vc<cmp_op>.%s%%#<V_sz_elem>\t%%<V_reg>0,"
+                      " %%<V_reg>1, %s",
+                       GET_MODE_CLASS (<MODE>mode) == MODE_VECTOR_FLOAT
+                         ? "f" : "<cmp_type>",
+                       which_alternative == 0
+                         ? "%<V_reg>2" : "#0");
+    output_asm_insn (pattern, operands);
+    return "";
+  }
   [(set (attr "type")
-     (if_then_else (match_test "<Is_float_mode>")
-                   (const_string "neon_fp_compare_s<q>")
-                    (if_then_else (match_operand 2 "zero_operand")
+        (if_then_else (match_operand 2 "zero_operand")
                       (const_string "neon_compare_zero<q>")
-                      (const_string "neon_compare<q>"))))]
+                      (const_string "neon_compare<q>")))]
 )
 
-(define_insn "neon_vcgeu<mode>"
-  [(set (match_operand:<V_cmp_result> 0 "s_register_operand" "=w")
-        (unspec:<V_cmp_result>
-	  [(match_operand:VDQIW 1 "s_register_operand" "w")
-	   (match_operand:VDQIW 2 "s_register_operand" "w")]
-          UNSPEC_VCGEU))]
-  "TARGET_NEON"
-  "vcge.u%#<V_sz_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
-  [(set_attr "type" "neon_compare<q>")]
-)
-
-(define_insn "neon_vcgt<mode>"
+(define_insn "neon_vc<cmp_op_unsp><mode>_insn_unspec"
   [(set (match_operand:<V_cmp_result> 0 "s_register_operand" "=w,w")
         (unspec:<V_cmp_result>
-	  [(match_operand:VDQW 1 "s_register_operand" "w,w")
-	   (match_operand:VDQW 2 "reg_or_zero_operand" "w,Dz")]
-          UNSPEC_VCGT))]
+	  [(match_operand:VCVTF 1 "s_register_operand" "w,w")
+	   (match_operand:VCVTF 2 "reg_or_zero_operand" "w,Dz")]
+          NEON_VCMP))]
   "TARGET_NEON"
-  "@
-  vcgt.<V_s_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2
-  vcgt.<V_s_elem>\t%<V_reg>0, %<V_reg>1, #0"
-  [(set (attr "type")
-     (if_then_else (match_test "<Is_float_mode>")
-                   (const_string "neon_fp_compare_s<q>")
-                    (if_then_else (match_operand 2 "zero_operand")
-                      (const_string "neon_compare_zero<q>")
-                      (const_string "neon_compare<q>"))))]
+  {
+    char pattern[100];
+    sprintf (pattern, "vc<cmp_op_unsp>.f%%#<V_sz_elem>\t%%<V_reg>0,"
+                       " %%<V_reg>1, %s",
+                       which_alternative == 0
+                         ? "%<V_reg>2" : "#0");
+    output_asm_insn (pattern, operands);
+    return "";
+}
+  [(set_attr "type" "neon_fp_compare_s<q>")]
 )
 
-(define_insn "neon_vcgtu<mode>"
+(define_insn "neon_vc<cmp_op>u<mode>"
   [(set (match_operand:<V_cmp_result> 0 "s_register_operand" "=w")
-        (unspec:<V_cmp_result>
-	  [(match_operand:VDQIW 1 "s_register_operand" "w")
-	   (match_operand:VDQIW 2 "s_register_operand" "w")]
-          UNSPEC_VCGTU))]
+        (neg:<V_cmp_result>
+          (GTUGEU:<V_cmp_result>
+	    (match_operand:VDQIW 1 "s_register_operand" "w")
+	    (match_operand:VDQIW 2 "s_register_operand" "w"))))]
   "TARGET_NEON"
-  "vcgt.u%#<V_sz_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
+  "vc<cmp_op>.u%#<V_sz_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
   [(set_attr "type" "neon_compare<q>")]
 )
 
-;; VCLE and VCLT only support comparisons with immediate zero (register
-;; variants are VCGE and VCGT with operands reversed).
-
-(define_insn "neon_vcle<mode>"
-  [(set (match_operand:<V_cmp_result> 0 "s_register_operand" "=w")
-        (unspec:<V_cmp_result>
-	  [(match_operand:VDQW 1 "s_register_operand" "w")
-	   (match_operand:VDQW 2 "zero_operand" "Dz")]
-          UNSPEC_VCLE))]
+(define_expand "neon_vca<cmp_op><mode>"
+  [(set (match_operand:<V_cmp_result> 0 "s_register_operand")
+        (neg:<V_cmp_result>
+          (GTGE:<V_cmp_result>
+            (abs:VCVTF (match_operand:VCVTF 1 "s_register_operand"))
+            (abs:VCVTF (match_operand:VCVTF 2 "s_register_operand")))))]
   "TARGET_NEON"
-  "vcle.<V_s_elem>\t%<V_reg>0, %<V_reg>1, #0"
-  [(set (attr "type")
-      (if_then_else (match_test "<Is_float_mode>")
-                    (const_string "neon_fp_compare_s<q>")
-                    (if_then_else (match_operand 2 "zero_operand")
-                      (const_string "neon_compare_zero<q>")
-                      (const_string "neon_compare<q>"))))]
-)
-
-(define_insn "neon_vclt<mode>"
-  [(set (match_operand:<V_cmp_result> 0 "s_register_operand" "=w")
-        (unspec:<V_cmp_result>
-	  [(match_operand:VDQW 1 "s_register_operand" "w")
-	   (match_operand:VDQW 2 "zero_operand" "Dz")]
-          UNSPEC_VCLT))]
-  "TARGET_NEON"
-  "vclt.<V_s_elem>\t%<V_reg>0, %<V_reg>1, #0"
-  [(set (attr "type")
-      (if_then_else (match_test "<Is_float_mode>")
-                    (const_string "neon_fp_compare_s<q>")
-                    (if_then_else (match_operand 2 "zero_operand")
-                      (const_string "neon_compare_zero<q>")
-                      (const_string "neon_compare<q>"))))]
+  {
+    if (flag_unsafe_math_optimizations)
+      emit_insn (gen_neon_vca<cmp_op><mode>_insn (operands[0], operands[1],
+                                                  operands[2]));
+    else
+      emit_insn (gen_neon_vca<cmp_op><mode>_insn_unspec (operands[0],
+                                                         operands[1],
+                                                         operands[2]));
+    DONE;
+  }
 )
 
-(define_insn "neon_vcage<mode>"
+(define_insn "neon_vca<cmp_op><mode>_insn"
   [(set (match_operand:<V_cmp_result> 0 "s_register_operand" "=w")
-        (unspec:<V_cmp_result> [(match_operand:VCVTF 1 "s_register_operand" "w")
-		                (match_operand:VCVTF 2 "s_register_operand" "w")]
-                               UNSPEC_VCAGE))]
-  "TARGET_NEON"
-  "vacge.<V_if_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
+        (neg:<V_cmp_result>
+          (GTGE:<V_cmp_result>
+            (abs:VCVTF (match_operand:VCVTF 1 "s_register_operand" "w"))
+            (abs:VCVTF (match_operand:VCVTF 2 "s_register_operand" "w")))))]
+  "TARGET_NEON && flag_unsafe_math_optimizations"
+  "vac<cmp_op>.<V_if_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
   [(set_attr "type" "neon_fp_compare_s<q>")]
 )
 
-(define_insn "neon_vcagt<mode>"
+(define_insn "neon_vca<cmp_op_unsp><mode>_insn_unspec"
   [(set (match_operand:<V_cmp_result> 0 "s_register_operand" "=w")
         (unspec:<V_cmp_result> [(match_operand:VCVTF 1 "s_register_operand" "w")
 		                (match_operand:VCVTF 2 "s_register_operand" "w")]
-                               UNSPEC_VCAGT))]
+                               NEON_VACMP))]
   "TARGET_NEON"
-  "vacgt.<V_if_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
+  "vac<cmp_op_unsp>.<V_if_elem>\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
   [(set_attr "type" "neon_fp_compare_s<q>")]
 )
 
@@ -2722,8 +2663,6 @@
    (match_operand:SI 2 "immediate_operand" "")]
   "TARGET_NEON"
 {
-  neon_lane_bounds (operands[2], 0, GET_MODE_NUNITS (<MODE>mode));
-
   if (BYTES_BIG_ENDIAN)
     {
       /* The intrinsics are defined in terms of a model where the
@@ -2753,8 +2692,6 @@
    (match_operand:SI 2 "immediate_operand" "")]
   "TARGET_NEON"
 {
-  neon_lane_bounds (operands[2], 0, GET_MODE_NUNITS (<MODE>mode));
-
   if (BYTES_BIG_ENDIAN)
     {
       /* The intrinsics are defined in terms of a model where the
@@ -2784,7 +2721,6 @@
    (match_operand:SI 2 "immediate_operand" "")]
   "TARGET_NEON"
 {
-  neon_lane_bounds (operands[2], 0, 1);
   emit_move_insn (operands[0], operands[1]);
   DONE;
 })
@@ -2795,18 +2731,11 @@
    (match_operand:SI 2 "immediate_operand" "")]
   "TARGET_NEON"
 {
-  switch (INTVAL (operands[2]))
-    {
-    case 0:
-      emit_move_insn (operands[0], gen_lowpart (DImode, operands[1]));
-      break;
-    case 1:
-      emit_move_insn (operands[0], gen_highpart (DImode, operands[1]));
-      break;
-    default:
-      neon_lane_bounds (operands[2], 0, 1);
-      FAIL;
-    }
+  int lane = INTVAL (operands[2]);
+  gcc_assert ((lane ==0) || (lane == 1));
+  emit_move_insn (operands[0], lane == 0
+				? gen_lowpart (DImode, operands[1])
+				: gen_highpart (DImode, operands[1]));
   DONE;
 })
 
@@ -2818,7 +2747,6 @@
   "TARGET_NEON"
 {
   unsigned int elt = INTVAL (operands[3]);
-  neon_lane_bounds (operands[3], 0, GET_MODE_NUNITS (<MODE>mode));
 
   if (BYTES_BIG_ENDIAN)
     {
@@ -2841,7 +2769,6 @@
    (match_operand:SI 3 "immediate_operand" "i")]
   "TARGET_NEON"
 {
-  neon_lane_bounds (operands[3], 0, 1);
   emit_move_insn (operands[0], operands[1]);
   DONE;
 })
@@ -2923,7 +2850,6 @@
    (match_operand:SI 2 "immediate_operand" "i")]
   "TARGET_NEON"
 {
-  neon_lane_bounds (operands[2], 0, GET_MODE_NUNITS (<V_double_vector_mode>mode));
   if (BYTES_BIG_ENDIAN)
     {
       unsigned int elt = INTVAL (operands[2]);
@@ -2944,7 +2870,6 @@
    (match_operand:SI 2 "immediate_operand" "i")]
   "TARGET_NEON"
 {
-  neon_lane_bounds (operands[2], 0, 1);
   emit_move_insn (operands[0], operands[1]);
   DONE;
 })
@@ -2956,7 +2881,6 @@
    (match_operand:SI 2 "immediate_operand" "i")]
   "TARGET_NEON"
 {
-  neon_lane_bounds (operands[2], 0, 1);
   emit_insn (gen_neon_vdup_nv2di (operands[0], operands[1]));
   DONE;
 })
@@ -3156,7 +3080,6 @@
                     UNSPEC_VMUL_LANE))]
   "TARGET_NEON"
 {
-  neon_lane_bounds (operands[3], 0, GET_MODE_NUNITS (<MODE>mode));
   return "vmul.<V_if_elem>\t%P0, %P1, %P2[%c3]";
 }
   [(set (attr "type")
@@ -3174,7 +3097,6 @@
                     UNSPEC_VMUL_LANE))]
   "TARGET_NEON"
 {
-  neon_lane_bounds (operands[3], 0, GET_MODE_NUNITS (<V_HALF>mode));
   return "vmul.<V_if_elem>\t%q0, %q1, %P2[%c3]";
 }
   [(set (attr "type")
@@ -3192,7 +3114,6 @@
                           VMULL_LANE))]
   "TARGET_NEON"
 {
-  neon_lane_bounds (operands[3], 0, GET_MODE_NUNITS (<MODE>mode));
   return "vmull.<sup>%#<V_sz_elem>\t%q0, %P1, %P2[%c3]";
 }
   [(set_attr "type" "neon_mul_<V_elem_ch>_scalar_long")]
@@ -3207,7 +3128,6 @@
                           UNSPEC_VQDMULL_LANE))]
   "TARGET_NEON"
 {
-  neon_lane_bounds (operands[3], 0, GET_MODE_NUNITS (<MODE>mode));
   return "vqdmull.<V_s_elem>\t%q0, %P1, %P2[%c3]";
 }
   [(set_attr "type" "neon_sat_mul_<V_elem_ch>_scalar_long")]
@@ -3222,7 +3142,6 @@
                       VQDMULH_LANE))]
   "TARGET_NEON"
 {
-  neon_lane_bounds (operands[3], 0, GET_MODE_NUNITS (<MODE>mode));
   return "vq<r>dmulh.<V_s_elem>\t%q0, %q1, %P2[%c3]";
 }
   [(set_attr "type" "neon_sat_mul_<V_elem_ch>_scalar_q")]
@@ -3237,7 +3156,6 @@
                       VQDMULH_LANE))]
   "TARGET_NEON"
 {
-  neon_lane_bounds (operands[3], 0, GET_MODE_NUNITS (<MODE>mode));
   return "vq<r>dmulh.<V_s_elem>\t%P0, %P1, %P2[%c3]";
 }
   [(set_attr "type" "neon_sat_mul_<V_elem_ch>_scalar_q")]
@@ -3253,7 +3171,6 @@
                      UNSPEC_VMLA_LANE))]
   "TARGET_NEON"
 {
-  neon_lane_bounds (operands[4], 0, GET_MODE_NUNITS (<MODE>mode));
   return "vmla.<V_if_elem>\t%P0, %P2, %P3[%c4]";
 }
   [(set (attr "type")
@@ -3272,7 +3189,6 @@
                      UNSPEC_VMLA_LANE))]
   "TARGET_NEON"
 {
-  neon_lane_bounds (operands[4], 0, GET_MODE_NUNITS (<MODE>mode));
   return "vmla.<V_if_elem>\t%q0, %q2, %P3[%c4]";
 }
   [(set (attr "type")
@@ -3291,7 +3207,6 @@
                           VMLAL_LANE))]
   "TARGET_NEON"
 {
-  neon_lane_bounds (operands[4], 0, GET_MODE_NUNITS (<MODE>mode));
   return "vmlal.<sup>%#<V_sz_elem>\t%q0, %P2, %P3[%c4]";
 }
   [(set_attr "type" "neon_mla_<V_elem_ch>_scalar_long")]
@@ -3307,7 +3222,6 @@
                           UNSPEC_VQDMLAL_LANE))]
   "TARGET_NEON"
 {
-  neon_lane_bounds (operands[4], 0, GET_MODE_NUNITS (<MODE>mode));
   return "vqdmlal.<V_s_elem>\t%q0, %P2, %P3[%c4]";
 }
   [(set_attr "type" "neon_sat_mla_<V_elem_ch>_scalar_long")]
@@ -3323,7 +3237,6 @@
                     UNSPEC_VMLS_LANE))]
   "TARGET_NEON"
 {
-  neon_lane_bounds (operands[4], 0, GET_MODE_NUNITS (<MODE>mode));
   return "vmls.<V_if_elem>\t%P0, %P2, %P3[%c4]";
 }
   [(set (attr "type")
@@ -3342,7 +3255,6 @@
                     UNSPEC_VMLS_LANE))]
   "TARGET_NEON"
 {
-  neon_lane_bounds (operands[4], 0, GET_MODE_NUNITS (<MODE>mode));
   return "vmls.<V_if_elem>\t%q0, %q2, %P3[%c4]";
 }
   [(set (attr "type")
@@ -3361,7 +3273,6 @@
                           VMLSL_LANE))]
   "TARGET_NEON"
 {
-  neon_lane_bounds (operands[4], 0, GET_MODE_NUNITS (<MODE>mode));
   return "vmlsl.<sup>%#<V_sz_elem>\t%q0, %P2, %P3[%c4]";
 }
   [(set_attr "type" "neon_mla_<V_elem_ch>_scalar_long")]
@@ -3377,7 +3288,6 @@
                           UNSPEC_VQDMLSL_LANE))]
   "TARGET_NEON"
 {
-  neon_lane_bounds (operands[4], 0, GET_MODE_NUNITS (<MODE>mode));
   return "vqdmlsl.<V_s_elem>\t%q0, %P2, %P3[%c4]";
 }
   [(set_attr "type" "neon_sat_mla_<V_elem_ch>_scalar_long")]
--- a/src/gcc/config/arm/sync.md
+++ b/src/gcc/config/arm/sync.md
@@ -50,14 +50,11 @@
   {
     if (TARGET_HAVE_DMB)
       {
-	/* Note we issue a system level barrier. We should consider issuing
-	   a inner shareabilty zone barrier here instead, ie. "DMB ISH".  */
-	/* ??? Differentiate based on SEQ_CST vs less strict?  */
-	return "dmb\tsy";
+	return "dmb\\tish";
       }
 
     if (TARGET_HAVE_DMB_MCR)
-      return "mcr\tp15, 0, r0, c7, c10, 5";
+      return "mcr\\tp15, 0, r0, c7, c10, 5";
 
     gcc_unreachable ();
   }
--- a/src/gcc/config/arm/thumb2.md
+++ b/src/gcc/config/arm/thumb2.md
@@ -300,7 +300,7 @@
    ldr%?\\t%0, %1
    str%?\\t%1, %0
    str%?\\t%1, %0"
-  [(set_attr "type" "mov_reg,alu_imm,alu_imm,alu_imm,mov_imm,load1,load1,store1,store1")
+  [(set_attr "type" "mov_reg,mov_imm,mov_imm,mvn_imm,mov_imm,load1,load1,store1,store1")
    (set_attr "length" "2,4,2,4,4,4,4,4,4")
    (set_attr "predicable" "yes")
    (set_attr "predicable_short_it" "yes,no,yes,no,no,no,no,no,no")
@@ -486,12 +486,12 @@
 )
 
 (define_insn_and_split "*thumb2_movsicc_insn"
-  [(set (match_operand:SI 0 "s_register_operand" "=l,l,r,r,r,r,r,r,r,r,r")
+  [(set (match_operand:SI 0 "s_register_operand" "=l,l,r,r,r,r,r,r,r,r,r,r")
 	(if_then_else:SI
 	 (match_operator 3 "arm_comparison_operator"
 	  [(match_operand 4 "cc_register" "") (const_int 0)])
-	 (match_operand:SI 1 "arm_not_operand" "0 ,lPy,0 ,0,rI,K,rI,rI,K ,K,r")
-	 (match_operand:SI 2 "arm_not_operand" "lPy,0 ,rI,K,0 ,0,rI,K ,rI,K,r")))]
+	 (match_operand:SI 1 "arm_not_operand" "0 ,lPy,0 ,0,rI,K,I ,r,rI,K ,K,r")
+	 (match_operand:SI 2 "arm_not_operand" "lPy,0 ,rI,K,0 ,0,rI,I,K ,rI,K,r")))]
   "TARGET_THUMB2"
   "@
    it\\t%D3\;mov%D3\\t%0, %2
@@ -504,12 +504,14 @@
    #
    #
    #
+   #
    #"
    ; alt 6: ite\\t%d3\;mov%d3\\t%0, %1\;mov%D3\\t%0, %2
-   ; alt 7: ite\\t%d3\;mov%d3\\t%0, %1\;mvn%D3\\t%0, #%B2
-   ; alt 8: ite\\t%d3\;mvn%d3\\t%0, #%B1\;mov%D3\\t%0, %2
-   ; alt 9: ite\\t%d3\;mvn%d3\\t%0, #%B1\;mvn%D3\\t%0, #%B2
-   ; alt 10: ite\\t%d3\;mov%d3\\t%0, %1\;mov%D3\\t%0, %2
+   ; alt 7: ite\\t%d3\;mov%d3\\t%0, %1\;mov%D3\\t%0, %2
+   ; alt 8: ite\\t%d3\;mov%d3\\t%0, %1\;mvn%D3\\t%0, #%B2
+   ; alt 9: ite\\t%d3\;mvn%d3\\t%0, #%B1\;mov%D3\\t%0, %2
+   ; alt 10: ite\\t%d3\;mvn%d3\\t%0, #%B1\;mvn%D3\\t%0, #%B2
+   ; alt 11: ite\\t%d3\;mov%d3\\t%0, %1\;mov%D3\\t%0, %2
   "&& reload_completed"
   [(const_int 0)]
   {
@@ -540,10 +542,30 @@
                                                operands[2])));
     DONE;
   }
-  [(set_attr "length" "4,4,6,6,6,6,10,10,10,10,6")
-   (set_attr "enabled_for_depr_it" "yes,yes,no,no,no,no,no,no,no,no,yes")
+  [(set_attr "length" "4,4,6,6,6,6,10,8,10,10,10,6")
+   (set_attr "enabled_for_depr_it" "yes,yes,no,no,no,no,no,no,no,no,no,yes")
    (set_attr "conds" "use")
-   (set_attr "type" "multiple")]
+   (set_attr_alternative "type"
+                         [(if_then_else (match_operand 2 "const_int_operand" "")
+                                        (const_string "mov_imm")
+                                        (const_string "mov_reg"))
+                          (if_then_else (match_operand 1 "const_int_operand" "")
+                                        (const_string "mov_imm")
+                                        (const_string "mov_reg"))
+                          (if_then_else (match_operand 2 "const_int_operand" "")
+                                        (const_string "mov_imm")
+                                        (const_string "mov_reg"))
+                          (const_string "mvn_imm")
+                          (if_then_else (match_operand 1 "const_int_operand" "")
+                                        (const_string "mov_imm")
+                                        (const_string "mov_reg"))
+                          (const_string "mvn_imm")
+                          (const_string "multiple")
+                          (const_string "multiple")
+                          (const_string "multiple")
+                          (const_string "multiple")
+                          (const_string "multiple")
+                          (const_string "multiple")])]
 )
 
 (define_insn "*thumb2_movsfcc_soft_insn"
@@ -1182,7 +1204,11 @@
   "
   [(set_attr "predicable" "yes")
    (set_attr "length" "2")
-   (set_attr "type" "alu_sreg")]
+   (set_attr_alternative "type"
+                         [(if_then_else (match_operand 2 "const_int_operand" "")
+                                        (const_string "alu_imm")
+                                        (const_string "alu_sreg"))
+                          (const_string "alu_imm")])]
 )
 
 (define_insn "*thumb2_subsi_short"
@@ -1247,14 +1273,21 @@
   "
   [(set_attr "conds" "set")
    (set_attr "length" "2,2,4")
-   (set_attr "type" "alu_sreg")]
+   (set_attr_alternative "type"
+                         [(if_then_else (match_operand 2 "const_int_operand" "")
+                                        (const_string "alus_imm")
+                                        (const_string "alus_sreg"))
+                          (const_string "alus_imm")
+                          (if_then_else (match_operand 2 "const_int_operand" "")
+                                        (const_string "alus_imm")
+                                        (const_string "alus_sreg"))])]
 )
 
 (define_insn "*thumb2_addsi3_compare0_scratch"
   [(set (reg:CC_NOOV CC_REGNUM)
 	(compare:CC_NOOV
-	  (plus:SI (match_operand:SI 0 "s_register_operand" "l,l,  r,r")
-		   (match_operand:SI 1 "arm_add_operand"    "Pv,l,IL,r"))
+	  (plus:SI (match_operand:SI 0 "s_register_operand" "l,  r")
+		   (match_operand:SI 1 "arm_add_operand"    "lPv,rIL"))
 	  (const_int 0)))]
   "TARGET_THUMB2"
   "*
@@ -1271,8 +1304,10 @@
       return \"cmn\\t%0, %1\";
   "
   [(set_attr "conds" "set")
-   (set_attr "length" "2,2,4,4")
-   (set_attr "type" "alus_imm,alus_sreg,alus_imm,alus_sreg")]
+   (set_attr "length" "2,4")
+   (set (attr "type") (if_then_else (match_operand 1 "const_int_operand" "")
+                                    (const_string "alus_imm")
+                                    (const_string "alus_sreg")))]
 )
 
 (define_insn "*thumb2_mulsi_short"
--- a/src/gcc/config/arm/unknown-elf.h
+++ b/src/gcc/config/arm/unknown-elf.h
@@ -32,7 +32,9 @@
 #define UNKNOWN_ELF_STARTFILE_SPEC	" crti%O%s crtbegin%O%s crt0%O%s"
 
 #undef  STARTFILE_SPEC
-#define STARTFILE_SPEC	UNKNOWN_ELF_STARTFILE_SPEC
+#define STARTFILE_SPEC	\
+  "%{Ofast|ffast-math|funsafe-math-optimizations:crtfastmath.o%s} "	\
+  UNKNOWN_ELF_STARTFILE_SPEC
 
 #define UNKNOWN_ELF_ENDFILE_SPEC	"crtend%O%s crtn%O%s"
 
@@ -80,7 +82,9 @@
 									\
       ASM_OUTPUT_ALIGN (FILE, floor_log2 (ALIGN / BITS_PER_UNIT));	\
       ASM_OUTPUT_LABEL (FILE, NAME);					\
-      fprintf (FILE, "\t.space\t%d\n", SIZE ? (int)(SIZE) : 1);		\
+      fprintf (FILE, "\t.space\t%d\n", SIZE ? (int) SIZE : 1);		\
+      fprintf (FILE, "\t.size\t%s, %d\n",				\
+	       NAME, SIZE ? (int) SIZE : 1);				\
     }									\
   while (0)
 
--- a/src/gcc/config/arm/vxworks.h
+++ b/src/gcc/config/arm/vxworks.h
@@ -40,7 +40,7 @@ see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
       builtin_define ("CPU=ARMARCH5");		\
     else if (arm_arch4)				\
       {						\
-	if (thumb_code)				\
+	if (TARGET_THUMB)			\
 	  builtin_define ("CPU=ARMARCH4_T");	\
 	else					\
 	  builtin_define ("CPU=ARMARCH4");	\
--- a/src/gcc/config/c6x/c6x.c
+++ b/src/gcc/config/c6x/c6x.c
@@ -3532,7 +3532,7 @@ try_rename_operands (rtx_insn *head, rtx_insn *tail, unit_req_table reqs,
   best_reg =
     find_rename_reg (this_head, super_class, &unavailable, old_reg, true);
 
-  regrename_do_replace (this_head, best_reg);
+  gcc_assert (regrename_do_replace (this_head, best_reg));
 
   count_unit_reqs (new_reqs, head, PREV_INSN (tail));
   merge_unit_reqs (new_reqs);
@@ -3545,7 +3545,7 @@ try_rename_operands (rtx_insn *head, rtx_insn *tail, unit_req_table reqs,
 	       unit_req_imbalance (reqs), unit_req_imbalance (new_reqs));
     }
   if (unit_req_imbalance (new_reqs) > unit_req_imbalance (reqs))
-    regrename_do_replace (this_head, old_reg);
+    gcc_assert (regrename_do_replace (this_head, old_reg));
   else
     memcpy (reqs, new_reqs, sizeof (unit_req_table));
 
--- a/src/gcc/config/glibc-stdint.h
+++ b/src/gcc/config/glibc-stdint.h
@@ -22,6 +22,12 @@ a copy of the GCC Runtime Library Exception along with this program;
 see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
 <http://www.gnu.org/licenses/>.  */
 
+/* Systems using musl libc should use this header and make sure
+   OPTION_MUSL is defined correctly before using the TYPE macros. */
+#ifndef OPTION_MUSL
+#define OPTION_MUSL 0
+#endif
+
 #define SIG_ATOMIC_TYPE "int"
 
 #define INT8_TYPE "signed char"
@@ -43,12 +49,12 @@ see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
 #define UINT_LEAST64_TYPE (LONG_TYPE_SIZE == 64 ? "long unsigned int" : "long long unsigned int")
 
 #define INT_FAST8_TYPE "signed char"
-#define INT_FAST16_TYPE (LONG_TYPE_SIZE == 64 ? "long int" : "int")
-#define INT_FAST32_TYPE (LONG_TYPE_SIZE == 64 ? "long int" : "int")
+#define INT_FAST16_TYPE (LONG_TYPE_SIZE == 64 && !OPTION_MUSL ? "long int" : "int")
+#define INT_FAST32_TYPE (LONG_TYPE_SIZE == 64 && !OPTION_MUSL ? "long int" : "int")
 #define INT_FAST64_TYPE (LONG_TYPE_SIZE == 64 ? "long int" : "long long int")
 #define UINT_FAST8_TYPE "unsigned char"
-#define UINT_FAST16_TYPE (LONG_TYPE_SIZE == 64 ? "long unsigned int" : "unsigned int")
-#define UINT_FAST32_TYPE (LONG_TYPE_SIZE == 64 ? "long unsigned int" : "unsigned int")
+#define UINT_FAST16_TYPE (LONG_TYPE_SIZE == 64 && !OPTION_MUSL ? "long unsigned int" : "unsigned int")
+#define UINT_FAST32_TYPE (LONG_TYPE_SIZE == 64 && !OPTION_MUSL ? "long unsigned int" : "unsigned int")
 #define UINT_FAST64_TYPE (LONG_TYPE_SIZE == 64 ? "long unsigned int" : "long long unsigned int")
 
 #define INTPTR_TYPE (LONG_TYPE_SIZE == 64 ? "long int" : "int")
--- a/src/gcc/config/linux.h
+++ b/src/gcc/config/linux.h
@@ -32,10 +32,14 @@ see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
 #define OPTION_GLIBC  (DEFAULT_LIBC == LIBC_GLIBC)
 #define OPTION_UCLIBC (DEFAULT_LIBC == LIBC_UCLIBC)
 #define OPTION_BIONIC (DEFAULT_LIBC == LIBC_BIONIC)
+#undef OPTION_MUSL
+#define OPTION_MUSL   (DEFAULT_LIBC == LIBC_MUSL)
 #else
 #define OPTION_GLIBC  (linux_libc == LIBC_GLIBC)
 #define OPTION_UCLIBC (linux_libc == LIBC_UCLIBC)
 #define OPTION_BIONIC (linux_libc == LIBC_BIONIC)
+#undef OPTION_MUSL
+#define OPTION_MUSL   (linux_libc == LIBC_MUSL)
 #endif
 
 #define GNU_USER_TARGET_OS_CPP_BUILTINS()			\
@@ -50,21 +54,25 @@ see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
     } while (0)
 
 /* Determine which dynamic linker to use depending on whether GLIBC or
-   uClibc or Bionic is the default C library and whether
-   -muclibc or -mglibc or -mbionic has been passed to change the default.  */
+   uClibc or Bionic or musl is the default C library and whether
+   -muclibc or -mglibc or -mbionic or -mmusl has been passed to change
+   the default.  */
 
-#define CHOOSE_DYNAMIC_LINKER1(LIBC1, LIBC2, LIBC3, LD1, LD2, LD3)	\
-  "%{" LIBC2 ":" LD2 ";:%{" LIBC3 ":" LD3 ";:" LD1 "}}"
+#define CHOOSE_DYNAMIC_LINKER1(LIBC1, LIBC2, LIBC3, LIBC4, LD1, LD2, LD3, LD4)	\
+  "%{" LIBC2 ":" LD2 ";:%{" LIBC3 ":" LD3 ";:%{" LIBC4 ":" LD4 ";:" LD1 "}}}"
 
 #if DEFAULT_LIBC == LIBC_GLIBC
-#define CHOOSE_DYNAMIC_LINKER(G, U, B) \
-  CHOOSE_DYNAMIC_LINKER1 ("mglibc", "muclibc", "mbionic", G, U, B)
+#define CHOOSE_DYNAMIC_LINKER(G, U, B, M) \
+  CHOOSE_DYNAMIC_LINKER1 ("mglibc", "muclibc", "mbionic", "mmusl", G, U, B, M)
 #elif DEFAULT_LIBC == LIBC_UCLIBC
-#define CHOOSE_DYNAMIC_LINKER(G, U, B) \
-  CHOOSE_DYNAMIC_LINKER1 ("muclibc", "mglibc", "mbionic", U, G, B)
+#define CHOOSE_DYNAMIC_LINKER(G, U, B, M) \
+  CHOOSE_DYNAMIC_LINKER1 ("muclibc", "mglibc", "mbionic", "mmusl", U, G, B, M)
 #elif DEFAULT_LIBC == LIBC_BIONIC
-#define CHOOSE_DYNAMIC_LINKER(G, U, B) \
-  CHOOSE_DYNAMIC_LINKER1 ("mbionic", "mglibc", "muclibc", B, G, U)
+#define CHOOSE_DYNAMIC_LINKER(G, U, B, M) \
+  CHOOSE_DYNAMIC_LINKER1 ("mbionic", "mglibc", "muclibc", "mmusl", B, G, U, M)
+#elif DEFAULT_LIBC == LIBC_MUSL
+#define CHOOSE_DYNAMIC_LINKER(G, U, B, M) \
+  CHOOSE_DYNAMIC_LINKER1 ("mmusl", "mglibc", "muclibc", "mbionic", M, G, U, B)
 #else
 #error "Unsupported DEFAULT_LIBC"
 #endif /* DEFAULT_LIBC */
@@ -81,24 +89,100 @@ see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
 #define BIONIC_DYNAMIC_LINKER32 "/system/bin/linker"
 #define BIONIC_DYNAMIC_LINKER64 "/system/bin/linker64"
 #define BIONIC_DYNAMIC_LINKERX32 "/system/bin/linkerx32"
+/* Should be redefined for each target that supports musl.  */
+#define MUSL_DYNAMIC_LINKER "/dev/null"
+#define MUSL_DYNAMIC_LINKER32 "/dev/null"
+#define MUSL_DYNAMIC_LINKER64 "/dev/null"
+#define MUSL_DYNAMIC_LINKERX32 "/dev/null"
 
 #define GNU_USER_DYNAMIC_LINKER						\
   CHOOSE_DYNAMIC_LINKER (GLIBC_DYNAMIC_LINKER, UCLIBC_DYNAMIC_LINKER,	\
-			 BIONIC_DYNAMIC_LINKER)
+			 BIONIC_DYNAMIC_LINKER, MUSL_DYNAMIC_LINKER)
 #define GNU_USER_DYNAMIC_LINKER32					\
   CHOOSE_DYNAMIC_LINKER (GLIBC_DYNAMIC_LINKER32, UCLIBC_DYNAMIC_LINKER32, \
-			 BIONIC_DYNAMIC_LINKER32)
+			 BIONIC_DYNAMIC_LINKER32, MUSL_DYNAMIC_LINKER32)
 #define GNU_USER_DYNAMIC_LINKER64					\
   CHOOSE_DYNAMIC_LINKER (GLIBC_DYNAMIC_LINKER64, UCLIBC_DYNAMIC_LINKER64, \
-			 BIONIC_DYNAMIC_LINKER64)
+			 BIONIC_DYNAMIC_LINKER64, MUSL_DYNAMIC_LINKER64)
 #define GNU_USER_DYNAMIC_LINKERX32					\
   CHOOSE_DYNAMIC_LINKER (GLIBC_DYNAMIC_LINKERX32, UCLIBC_DYNAMIC_LINKERX32, \
-			 BIONIC_DYNAMIC_LINKERX32)
+			 BIONIC_DYNAMIC_LINKERX32, MUSL_DYNAMIC_LINKERX32)
 
 /* Whether we have Bionic libc runtime */
 #undef TARGET_HAS_BIONIC
 #define TARGET_HAS_BIONIC (OPTION_BIONIC)
 
+/* musl avoids problematic includes by rearranging the include directories.
+ * Unfortunately, this is mostly duplicated from cppdefault.c */
+#if DEFAULT_LIBC == LIBC_MUSL
+#define INCLUDE_DEFAULTS_MUSL_GPP			\
+    { GPLUSPLUS_INCLUDE_DIR, "G++", 1, 1,		\
+      GPLUSPLUS_INCLUDE_DIR_ADD_SYSROOT, 0 },		\
+    { GPLUSPLUS_TOOL_INCLUDE_DIR, "G++", 1, 1,		\
+      GPLUSPLUS_INCLUDE_DIR_ADD_SYSROOT, 1 },		\
+    { GPLUSPLUS_BACKWARD_INCLUDE_DIR, "G++", 1, 1,	\
+      GPLUSPLUS_INCLUDE_DIR_ADD_SYSROOT, 0 },
+
+#ifdef LOCAL_INCLUDE_DIR
+#define INCLUDE_DEFAULTS_MUSL_LOCAL			\
+    { LOCAL_INCLUDE_DIR, 0, 0, 1, 1, 2 },		\
+    { LOCAL_INCLUDE_DIR, 0, 0, 1, 1, 0 },
+#else
+#define INCLUDE_DEFAULTS_MUSL_LOCAL
+#endif
+
+#ifdef PREFIX_INCLUDE_DIR
+#define INCLUDE_DEFAULTS_MUSL_PREFIX			\
+    { PREFIX_INCLUDE_DIR, 0, 0, 1, 0, 0},
+#else
+#define INCLUDE_DEFAULTS_MUSL_PREFIX
+#endif
+
+#ifdef CROSS_INCLUDE_DIR
+#define INCLUDE_DEFAULTS_MUSL_CROSS			\
+    { CROSS_INCLUDE_DIR, "GCC", 0, 0, 0, 0},
+#else
+#define INCLUDE_DEFAULTS_MUSL_CROSS
+#endif
+
+#ifdef TOOL_INCLUDE_DIR
+#define INCLUDE_DEFAULTS_MUSL_TOOL			\
+    { TOOL_INCLUDE_DIR, "BINUTILS", 0, 1, 0, 0},
+#else
+#define INCLUDE_DEFAULTS_MUSL_TOOL
+#endif
+
+#ifdef NATIVE_SYSTEM_HEADER_DIR
+#define INCLUDE_DEFAULTS_MUSL_NATIVE			\
+    { NATIVE_SYSTEM_HEADER_DIR, 0, 0, 0, 1, 2 },	\
+    { NATIVE_SYSTEM_HEADER_DIR, 0, 0, 0, 1, 0 },
+#else
+#define INCLUDE_DEFAULTS_MUSL_NATIVE
+#endif
+
+#if defined (CROSS_DIRECTORY_STRUCTURE) && !defined (TARGET_SYSTEM_ROOT)
+# undef INCLUDE_DEFAULTS_MUSL_LOCAL
+# define INCLUDE_DEFAULTS_MUSL_LOCAL
+# undef INCLUDE_DEFAULTS_MUSL_NATIVE
+# define INCLUDE_DEFAULTS_MUSL_NATIVE
+#else
+# undef INCLUDE_DEFAULTS_MUSL_CROSS
+# define INCLUDE_DEFAULTS_MUSL_CROSS
+#endif
+
+#undef INCLUDE_DEFAULTS
+#define INCLUDE_DEFAULTS				\
+  {							\
+    INCLUDE_DEFAULTS_MUSL_GPP				\
+    INCLUDE_DEFAULTS_MUSL_PREFIX			\
+    INCLUDE_DEFAULTS_MUSL_CROSS				\
+    INCLUDE_DEFAULTS_MUSL_TOOL				\
+    INCLUDE_DEFAULTS_MUSL_NATIVE			\
+    { GCC_INCLUDE_DIR, "GCC", 0, 1, 0, 0 },		\
+    { 0, 0, 0, 0, 0, 0 }				\
+  }
+#endif
+
 #if (DEFAULT_LIBC == LIBC_UCLIBC) && defined (SINGLE_LIBC) /* uClinux */
 /* This is a *uclinux* target.  We don't define below macros to normal linux
    versions, because doing so would require *uclinux* targets to include
--- a/src/gcc/config/linux.opt
+++ b/src/gcc/config/linux.opt
@@ -28,5 +28,9 @@ Target Report RejectNegative Var(linux_libc,LIBC_GLIBC) Negative(muclibc)
 Use GNU C library
 
 muclibc
-Target Report RejectNegative Var(linux_libc,LIBC_UCLIBC) Negative(mbionic)
+Target Report RejectNegative Var(linux_libc,LIBC_UCLIBC) Negative(mmusl)
 Use uClibc C library
+
+mmusl
+Target Report RejectNegative Var(linux_libc,LIBC_MUSL) Negative(mbionic)
+Use musl C library
--- a/src/gcc/config/mips/linux.h
+++ b/src/gcc/config/mips/linux.h
@@ -37,7 +37,13 @@ along with GCC; see the file COPYING3.  If not see
 #define UCLIBC_DYNAMIC_LINKERN32 \
   "%{mnan=2008:/lib32/ld-uClibc-mipsn8.so.0;:/lib32/ld-uClibc.so.0}"
 
+#undef MUSL_DYNAMIC_LINKER32
+#define MUSL_DYNAMIC_LINKER32 "/lib/ld-musl-mips%{EL:el}%{msoft-float:-sf}.so.1"
+#undef MUSL_DYNAMIC_LINKER64
+#define MUSL_DYNAMIC_LINKER64 "/lib/ld-musl-mips64%{EL:el}%{msoft-float:-sf}.so.1"
+#define MUSL_DYNAMIC_LINKERN32 "/lib/ld-musl-mipsn32%{EL:el}%{msoft-float:-sf}.so.1"
+
 #define BIONIC_DYNAMIC_LINKERN32 "/system/bin/linker32"
 #define GNU_USER_DYNAMIC_LINKERN32 \
   CHOOSE_DYNAMIC_LINKER (GLIBC_DYNAMIC_LINKERN32, UCLIBC_DYNAMIC_LINKERN32, \
-                         BIONIC_DYNAMIC_LINKERN32)
+                         BIONIC_DYNAMIC_LINKERN32, MUSL_DYNAMIC_LINKERN32)
--- a/src/gcc/config/rs6000/linux.h
+++ b/src/gcc/config/rs6000/linux.h
@@ -30,10 +30,14 @@
 #define OPTION_GLIBC  (DEFAULT_LIBC == LIBC_GLIBC)
 #define OPTION_UCLIBC (DEFAULT_LIBC == LIBC_UCLIBC)
 #define OPTION_BIONIC (DEFAULT_LIBC == LIBC_BIONIC)
+#undef OPTION_MUSL
+#define OPTION_MUSL   (DEFAULT_LIBC == LIBC_MUSL)
 #else
 #define OPTION_GLIBC  (linux_libc == LIBC_GLIBC)
 #define OPTION_UCLIBC (linux_libc == LIBC_UCLIBC)
 #define OPTION_BIONIC (linux_libc == LIBC_BIONIC)
+#undef OPTION_MUSL
+#define OPTION_MUSL   (linux_libc == LIBC_MUSL)
 #endif
 
 /* Determine what functions are present at the runtime;
--- a/src/gcc/config/rs6000/linux64.h
+++ b/src/gcc/config/rs6000/linux64.h
@@ -299,10 +299,14 @@ extern int dot_symbols;
 #define OPTION_GLIBC  (DEFAULT_LIBC == LIBC_GLIBC)
 #define OPTION_UCLIBC (DEFAULT_LIBC == LIBC_UCLIBC)
 #define OPTION_BIONIC (DEFAULT_LIBC == LIBC_BIONIC)
+#undef OPTION_MUSL
+#define OPTION_MUSL   (DEFAULT_LIBC == LIBC_MUSL)
 #else
 #define OPTION_GLIBC  (linux_libc == LIBC_GLIBC)
 #define OPTION_UCLIBC (linux_libc == LIBC_UCLIBC)
 #define OPTION_BIONIC (linux_libc == LIBC_BIONIC)
+#undef OPTION_MUSL
+#define OPTION_MUSL   (linux_libc == LIBC_MUSL)
 #endif
 
 /* Determine what functions are present at the runtime;
--- a/src/gcc/configure
+++ b/src/gcc/configure
@@ -767,10 +767,6 @@ REPORT_BUGS_TEXI
 REPORT_BUGS_TO
 PKGVERSION
 CONFIGURE_SPECS
-CROSS_SYSTEM_HEADER_DIR
-TARGET_SYSTEM_ROOT_DEFINE
-TARGET_SYSTEM_ROOT
-SYSROOT_CFLAGS_FOR_TARGET
 enable_shared
 enable_fixed_point
 enable_decimal_float
@@ -809,6 +805,10 @@ LDFLAGS
 CFLAGS
 CC
 GENINSRC
+CROSS_SYSTEM_HEADER_DIR
+TARGET_SYSTEM_ROOT_DEFINE
+TARGET_SYSTEM_ROOT
+SYSROOT_CFLAGS_FOR_TARGET
 target_subdir
 host_subdir
 build_subdir
@@ -870,6 +870,9 @@ ac_user_opts='
 enable_option_checking
 with_build_libsubdir
 with_local_prefix
+with_native_system_header_dir
+with_build_sysroot
+with_sysroot
 with_gxx_include_dir
 with_cpp_install_dir
 enable_generated_files_in_srcdir
@@ -896,9 +899,6 @@ enable_tls
 enable_objc_gc
 with_dwarf2
 enable_shared
-with_native_system_header_dir
-with_build_sysroot
-with_sysroot
 with_specs
 with_pkgversion
 with_bugurl
@@ -1680,6 +1680,12 @@ Optional Packages:
   --without-PACKAGE       do not use PACKAGE (same as --with-PACKAGE=no)
   --with-build-libsubdir=DIR  Directory where to find libraries for build system
   --with-local-prefix=DIR specifies directory to put local include
+  --with-native-system-header-dir=dir
+                          use dir as the directory to look for standard
+                          system header files in.  Defaults to /usr/include.
+  --with-build-sysroot=sysroot
+                          use sysroot as the system root during the build
+  --with-sysroot[=DIR]    search for usr/lib, usr/include, et al, within DIR
   --with-gxx-include-dir=DIR
                           specifies directory to put g++ header files
   --with-cpp-install-dir=DIR
@@ -1692,14 +1698,9 @@ Optional Packages:
   --with-as               arrange to use the specified as (full pathname)
   --with-stabs            arrange to use stabs instead of host debug format
   --with-dwarf2           force the default debug format to be DWARF 2
-  --with-native-system-header-dir=dir
-                          use dir as the directory to look for standard
-                          system header files in.  Defaults to /usr/include.
-  --with-build-sysroot=sysroot
-                          use sysroot as the system root during the build
-  --with-sysroot[=DIR]    search for usr/lib, usr/include, et al, within DIR
   --with-specs=SPECS      add SPECS to driver command-line processing
-  --with-pkgversion=PKG   Use PKG in the version string in place of "GCC"
+  --with-pkgversion=PKG   Use PKG in the version string in place of "Linaro
+                          GCC `cat $srcdir/LINARO-VERSION`"
   --with-bugurl=URL       Direct users to URL to report a bug
   --with-multilib-list    select multilibs (AArch64, SH and x86-64 only)
   --with-gnu-ld           assume the C compiler uses GNU ld default=no
@@ -3339,6 +3340,83 @@ if test x$local_prefix = x; then
 	local_prefix=/usr/local
 fi
 
+
+# Check whether --with-native-system-header-dir was given.
+if test "${with_native_system_header_dir+set}" = set; then :
+  withval=$with_native_system_header_dir;
+ case ${with_native_system_header_dir} in
+ yes|no) as_fn_error "bad value ${withval} given for --with-native-system-header-dir" "$LINENO" 5 ;;
+ /* | [A-Za-z]:[\\/]*) ;;
+ *) as_fn_error "--with-native-system-header-dir argument ${withval} must be an absolute directory" "$LINENO" 5 ;;
+ esac
+ configured_native_system_header_dir="${withval}"
+
+else
+  configured_native_system_header_dir=
+fi
+
+
+
+# Check whether --with-build-sysroot was given.
+if test "${with_build_sysroot+set}" = set; then :
+  withval=$with_build_sysroot; if test x"$withval" != x ; then
+     SYSROOT_CFLAGS_FOR_TARGET="--sysroot=$withval"
+   fi
+else
+  SYSROOT_CFLAGS_FOR_TARGET=
+fi
+
+
+
+if test "x$prefix" = xNONE; then
+ test_prefix=/usr/local
+else
+ test_prefix=$prefix
+fi
+if test "x$exec_prefix" = xNONE; then
+ test_exec_prefix=$test_prefix
+else
+ test_exec_prefix=$exec_prefix
+fi
+
+
+# Check whether --with-sysroot was given.
+if test "${with_sysroot+set}" = set; then :
+  withval=$with_sysroot;
+ case ${with_sysroot} in
+ /) ;;
+ */) with_sysroot=`echo $with_sysroot | sed 's,/$,,'` ;;
+ esac
+ case ${with_sysroot} in
+ yes) TARGET_SYSTEM_ROOT='${exec_prefix}/${target_noncanonical}/sys-root' ;;
+ *) TARGET_SYSTEM_ROOT=$with_sysroot ;;
+ esac
+
+ TARGET_SYSTEM_ROOT_DEFINE='-DTARGET_SYSTEM_ROOT=\"$(TARGET_SYSTEM_ROOT)\"'
+ CROSS_SYSTEM_HEADER_DIR='$(TARGET_SYSTEM_ROOT)$${sysroot_headers_suffix}$(NATIVE_SYSTEM_HEADER_DIR)'
+
+ case ${TARGET_SYSTEM_ROOT} in
+ "${test_prefix}"|"${test_prefix}/"*|\
+ "${test_exec_prefix}"|"${test_exec_prefix}/"*|\
+ '${prefix}'|'${prefix}/'*|\
+ '${exec_prefix}'|'${exec_prefix}/'*)
+   t="$TARGET_SYSTEM_ROOT_DEFINE -DTARGET_SYSTEM_ROOT_RELOCATABLE"
+   TARGET_SYSTEM_ROOT_DEFINE="$t"
+   ;;
+ esac
+
+else
+
+ TARGET_SYSTEM_ROOT=
+ TARGET_SYSTEM_ROOT_DEFINE=
+ CROSS_SYSTEM_HEADER_DIR='$(gcc_tooldir)/sys-include'
+
+fi
+
+
+
+
+
 # Don't set gcc_gxx_include_dir to gxx_include_dir since that's only
 # passed in by the toplevel make and thus we'd get different behavior
 # depending on where we built the sources.
@@ -3372,7 +3450,9 @@ gcc_gxx_include_dir_add_sysroot=0
 if test "${with_sysroot+set}" = set; then
   gcc_gxx_without_sysroot=`expr "${gcc_gxx_include_dir}" : "${with_sysroot}"'\(.*\)'`
   if test "${gcc_gxx_without_sysroot}"; then
-    gcc_gxx_include_dir="${gcc_gxx_without_sysroot}"
+    if test x${with_sysroot} != x/; then
+      gcc_gxx_include_dir="${gcc_gxx_without_sysroot}"
+    fi
     gcc_gxx_include_dir_add_sysroot=1
   fi
 fi
@@ -7269,79 +7349,6 @@ fi
 
 
 
-# Check whether --with-native-system-header-dir was given.
-if test "${with_native_system_header_dir+set}" = set; then :
-  withval=$with_native_system_header_dir;
- case ${with_native_system_header_dir} in
- yes|no) as_fn_error "bad value ${withval} given for --with-native-system-header-dir" "$LINENO" 5 ;;
- /* | [A-Za-z]:[\\/]*) ;;
- *) as_fn_error "--with-native-system-header-dir argument ${withval} must be an absolute directory" "$LINENO" 5 ;;
- esac
- configured_native_system_header_dir="${withval}"
-
-else
-  configured_native_system_header_dir=
-fi
-
-
-
-# Check whether --with-build-sysroot was given.
-if test "${with_build_sysroot+set}" = set; then :
-  withval=$with_build_sysroot; if test x"$withval" != x ; then
-     SYSROOT_CFLAGS_FOR_TARGET="--sysroot=$withval"
-   fi
-else
-  SYSROOT_CFLAGS_FOR_TARGET=
-fi
-
-
-
-if test "x$prefix" = xNONE; then
- test_prefix=/usr/local
-else
- test_prefix=$prefix
-fi
-if test "x$exec_prefix" = xNONE; then
- test_exec_prefix=$test_prefix
-else
- test_exec_prefix=$exec_prefix
-fi
-
-
-# Check whether --with-sysroot was given.
-if test "${with_sysroot+set}" = set; then :
-  withval=$with_sysroot;
- case ${with_sysroot} in
- yes) TARGET_SYSTEM_ROOT='${exec_prefix}/${target_noncanonical}/sys-root' ;;
- *) TARGET_SYSTEM_ROOT=$with_sysroot ;;
- esac
-
- TARGET_SYSTEM_ROOT_DEFINE='-DTARGET_SYSTEM_ROOT=\"$(TARGET_SYSTEM_ROOT)\"'
- CROSS_SYSTEM_HEADER_DIR='$(TARGET_SYSTEM_ROOT)$${sysroot_headers_suffix}$(NATIVE_SYSTEM_HEADER_DIR)'
-
- case ${TARGET_SYSTEM_ROOT} in
- "${test_prefix}"|"${test_prefix}/"*|\
- "${test_exec_prefix}"|"${test_exec_prefix}/"*|\
- '${prefix}'|'${prefix}/'*|\
- '${exec_prefix}'|'${exec_prefix}/'*)
-   t="$TARGET_SYSTEM_ROOT_DEFINE -DTARGET_SYSTEM_ROOT_RELOCATABLE"
-   TARGET_SYSTEM_ROOT_DEFINE="$t"
-   ;;
- esac
-
-else
-
- TARGET_SYSTEM_ROOT=
- TARGET_SYSTEM_ROOT_DEFINE=
- CROSS_SYSTEM_HEADER_DIR='$(gcc_tooldir)/sys-include'
-
-fi
-
-
-
-
-
-
 # Check whether --with-specs was given.
 if test "${with_specs+set}" = set; then :
   withval=$with_specs; CONFIGURE_SPECS=$withval
@@ -7362,7 +7369,7 @@ if test "${with_pkgversion+set}" = set; then :
       *)   PKGVERSION="($withval) " ;;
      esac
 else
-  PKGVERSION="(GCC) "
+  PKGVERSION="(Linaro GCC `cat $srcdir/LINARO-VERSION`) "
 
 fi
 
@@ -18162,7 +18169,7 @@ else
   lt_dlunknown=0; lt_dlno_uscore=1; lt_dlneed_uscore=2
   lt_status=$lt_dlunknown
   cat > conftest.$ac_ext <<_LT_EOF
-#line 18165 "configure"
+#line 18172 "configure"
 #include "confdefs.h"
 
 #if HAVE_DLFCN_H
@@ -18268,7 +18275,7 @@ else
   lt_dlunknown=0; lt_dlno_uscore=1; lt_dlneed_uscore=2
   lt_status=$lt_dlunknown
   cat > conftest.$ac_ext <<_LT_EOF
-#line 18271 "configure"
+#line 18278 "configure"
 #include "confdefs.h"
 
 #if HAVE_DLFCN_H
@@ -27802,6 +27809,9 @@ if test "${gcc_cv_libc_provides_ssp+set}" = set; then :
 else
   gcc_cv_libc_provides_ssp=no
     case "$target" in
+       *-*-musl*)
+	 # All versions of musl provide stack protector
+	 gcc_cv_libc_provides_ssp=yes;;
        *-*-linux* | *-*-kfreebsd*-gnu | *-*-knetbsd*-gnu)
       # glibc 2.4 and later provides __stack_chk_fail and
       # either __stack_chk_guard, or TLS access to stack guard canary.
@@ -27834,6 +27844,7 @@ fi
 	 # <http://gcc.gnu.org/ml/gcc/2008-10/msg00130.html>) and for now
 	 # simply assert that glibc does provide this, which is true for all
 	 # realistically usable GNU/Hurd configurations.
+	 # All supported versions of musl provide it as well
 	 gcc_cv_libc_provides_ssp=yes;;
        *-*-darwin* | *-*-freebsd*)
 	 ac_fn_c_check_func "$LINENO" "__stack_chk_fail" "ac_cv_func___stack_chk_fail"
@@ -27930,6 +27941,9 @@ case "$target" in
       gcc_cv_target_dl_iterate_phdr=no
     fi
     ;;
+  *-linux-musl*)
+    gcc_cv_target_dl_iterate_phdr=yes
+    ;;
 esac
 
 if test x$gcc_cv_target_dl_iterate_phdr = xyes; then
--- a/src/gcc/configure.ac
+++ b/src/gcc/configure.ac
@@ -121,6 +121,73 @@ if test x$local_prefix = x; then
 	local_prefix=/usr/local
 fi
 
+AC_ARG_WITH([native-system-header-dir],
+  [  --with-native-system-header-dir=dir
+                          use dir as the directory to look for standard
+                          system header files in.  Defaults to /usr/include.],
+[
+ case ${with_native_system_header_dir} in
+ yes|no) AC_MSG_ERROR([bad value ${withval} given for --with-native-system-header-dir]) ;;
+ /* | [[A-Za-z]]:[[\\/]]*) ;;
+ *) AC_MSG_ERROR([--with-native-system-header-dir argument ${withval} must be an absolute directory]) ;;
+ esac
+ configured_native_system_header_dir="${withval}"
+], [configured_native_system_header_dir=])
+
+AC_ARG_WITH(build-sysroot, 
+  [AS_HELP_STRING([--with-build-sysroot=sysroot],
+                  [use sysroot as the system root during the build])],
+  [if test x"$withval" != x ; then
+     SYSROOT_CFLAGS_FOR_TARGET="--sysroot=$withval"
+   fi],
+  [SYSROOT_CFLAGS_FOR_TARGET=])
+AC_SUBST(SYSROOT_CFLAGS_FOR_TARGET)
+
+if test "x$prefix" = xNONE; then
+ test_prefix=/usr/local
+else
+ test_prefix=$prefix
+fi
+if test "x$exec_prefix" = xNONE; then
+ test_exec_prefix=$test_prefix
+else
+ test_exec_prefix=$exec_prefix
+fi
+
+AC_ARG_WITH(sysroot,
+[AS_HELP_STRING([[--with-sysroot[=DIR]]],
+		[search for usr/lib, usr/include, et al, within DIR])],
+[
+ case ${with_sysroot} in
+ /) ;;
+ */) with_sysroot=`echo $with_sysroot | sed 's,/$,,'` ;;
+ esac
+ case ${with_sysroot} in
+ yes) TARGET_SYSTEM_ROOT='${exec_prefix}/${target_noncanonical}/sys-root' ;;
+ *) TARGET_SYSTEM_ROOT=$with_sysroot ;;
+ esac
+   
+ TARGET_SYSTEM_ROOT_DEFINE='-DTARGET_SYSTEM_ROOT=\"$(TARGET_SYSTEM_ROOT)\"'
+ CROSS_SYSTEM_HEADER_DIR='$(TARGET_SYSTEM_ROOT)$${sysroot_headers_suffix}$(NATIVE_SYSTEM_HEADER_DIR)'
+	
+ case ${TARGET_SYSTEM_ROOT} in
+ "${test_prefix}"|"${test_prefix}/"*|\
+ "${test_exec_prefix}"|"${test_exec_prefix}/"*|\
+ '${prefix}'|'${prefix}/'*|\
+ '${exec_prefix}'|'${exec_prefix}/'*)
+   t="$TARGET_SYSTEM_ROOT_DEFINE -DTARGET_SYSTEM_ROOT_RELOCATABLE"
+   TARGET_SYSTEM_ROOT_DEFINE="$t"
+   ;;
+ esac
+], [
+ TARGET_SYSTEM_ROOT=
+ TARGET_SYSTEM_ROOT_DEFINE=
+ CROSS_SYSTEM_HEADER_DIR='$(gcc_tooldir)/sys-include'
+])
+AC_SUBST(TARGET_SYSTEM_ROOT)
+AC_SUBST(TARGET_SYSTEM_ROOT_DEFINE)
+AC_SUBST(CROSS_SYSTEM_HEADER_DIR)
+
 # Don't set gcc_gxx_include_dir to gxx_include_dir since that's only
 # passed in by the toplevel make and thus we'd get different behavior
 # depending on where we built the sources.
@@ -152,7 +219,9 @@ gcc_gxx_include_dir_add_sysroot=0
 if test "${with_sysroot+set}" = set; then
   gcc_gxx_without_sysroot=`expr "${gcc_gxx_include_dir}" : "${with_sysroot}"'\(.*\)'`
   if test "${gcc_gxx_without_sysroot}"; then
-    gcc_gxx_include_dir="${gcc_gxx_without_sysroot}"
+    if test x${with_sysroot} != x/; then
+      gcc_gxx_include_dir="${gcc_gxx_without_sysroot}"
+    fi
     gcc_gxx_include_dir_add_sysroot=1
   fi
 fi
@@ -791,69 +860,6 @@ AC_ARG_ENABLE(shared,
 ], [enable_shared=yes])
 AC_SUBST(enable_shared)
 
-AC_ARG_WITH([native-system-header-dir],
-  [  --with-native-system-header-dir=dir
-                          use dir as the directory to look for standard
-                          system header files in.  Defaults to /usr/include.],
-[
- case ${with_native_system_header_dir} in
- yes|no) AC_MSG_ERROR([bad value ${withval} given for --with-native-system-header-dir]) ;;
- /* | [[A-Za-z]]:[[\\/]]*) ;;
- *) AC_MSG_ERROR([--with-native-system-header-dir argument ${withval} must be an absolute directory]) ;;
- esac
- configured_native_system_header_dir="${withval}"
-], [configured_native_system_header_dir=])
-
-AC_ARG_WITH(build-sysroot, 
-  [AS_HELP_STRING([--with-build-sysroot=sysroot],
-                  [use sysroot as the system root during the build])],
-  [if test x"$withval" != x ; then
-     SYSROOT_CFLAGS_FOR_TARGET="--sysroot=$withval"
-   fi],
-  [SYSROOT_CFLAGS_FOR_TARGET=])
-AC_SUBST(SYSROOT_CFLAGS_FOR_TARGET)
-
-if test "x$prefix" = xNONE; then
- test_prefix=/usr/local
-else
- test_prefix=$prefix
-fi
-if test "x$exec_prefix" = xNONE; then
- test_exec_prefix=$test_prefix
-else
- test_exec_prefix=$exec_prefix
-fi
-
-AC_ARG_WITH(sysroot,
-[AS_HELP_STRING([[--with-sysroot[=DIR]]],
-		[search for usr/lib, usr/include, et al, within DIR])],
-[
- case ${with_sysroot} in
- yes) TARGET_SYSTEM_ROOT='${exec_prefix}/${target_noncanonical}/sys-root' ;;
- *) TARGET_SYSTEM_ROOT=$with_sysroot ;;
- esac
-   
- TARGET_SYSTEM_ROOT_DEFINE='-DTARGET_SYSTEM_ROOT=\"$(TARGET_SYSTEM_ROOT)\"'
- CROSS_SYSTEM_HEADER_DIR='$(TARGET_SYSTEM_ROOT)$${sysroot_headers_suffix}$(NATIVE_SYSTEM_HEADER_DIR)'
-	
- case ${TARGET_SYSTEM_ROOT} in
- "${test_prefix}"|"${test_prefix}/"*|\
- "${test_exec_prefix}"|"${test_exec_prefix}/"*|\
- '${prefix}'|'${prefix}/'*|\
- '${exec_prefix}'|'${exec_prefix}/'*)
-   t="$TARGET_SYSTEM_ROOT_DEFINE -DTARGET_SYSTEM_ROOT_RELOCATABLE"
-   TARGET_SYSTEM_ROOT_DEFINE="$t"
-   ;;
- esac
-], [
- TARGET_SYSTEM_ROOT=
- TARGET_SYSTEM_ROOT_DEFINE=
- CROSS_SYSTEM_HEADER_DIR='$(gcc_tooldir)/sys-include'
-])
-AC_SUBST(TARGET_SYSTEM_ROOT)
-AC_SUBST(TARGET_SYSTEM_ROOT_DEFINE)
-AC_SUBST(CROSS_SYSTEM_HEADER_DIR)
-
 AC_ARG_WITH(specs,
   [AS_HELP_STRING([--with-specs=SPECS],
                   [add SPECS to driver command-line processing])],
@@ -862,7 +868,7 @@ AC_ARG_WITH(specs,
 )
 AC_SUBST(CONFIGURE_SPECS)
 
-ACX_PKGVERSION([GCC])
+ACX_PKGVERSION([Linaro GCC `cat $srcdir/LINARO-VERSION`])
 ACX_BUGURL([http://gcc.gnu.org/bugs.html])
 
 # Sanity check enable_languages in case someone does not run the toplevel
@@ -5282,6 +5288,9 @@ AC_CACHE_CHECK(__stack_chk_fail in target C library,
       gcc_cv_libc_provides_ssp,
       [gcc_cv_libc_provides_ssp=no
     case "$target" in
+       *-*-musl*)
+	 # All versions of musl provide stack protector
+	 gcc_cv_libc_provides_ssp=yes;;
        *-*-linux* | *-*-kfreebsd*-gnu | *-*-knetbsd*-gnu)
       # glibc 2.4 and later provides __stack_chk_fail and
       # either __stack_chk_guard, or TLS access to stack guard canary.
@@ -5308,6 +5317,7 @@ AC_CACHE_CHECK(__stack_chk_fail in target C library,
 	 # <http://gcc.gnu.org/ml/gcc/2008-10/msg00130.html>) and for now
 	 # simply assert that glibc does provide this, which is true for all
 	 # realistically usable GNU/Hurd configurations.
+	 # All supported versions of musl provide it as well
 	 gcc_cv_libc_provides_ssp=yes;;
        *-*-darwin* | *-*-freebsd*)
 	 AC_CHECK_FUNC(__stack_chk_fail,[gcc_cv_libc_provides_ssp=yes],
@@ -5381,6 +5391,9 @@ case "$target" in
       gcc_cv_target_dl_iterate_phdr=no
     fi
     ;;
+  *-linux-musl*)
+    gcc_cv_target_dl_iterate_phdr=yes
+    ;;
 esac
 GCC_TARGET_TEMPLATE([TARGET_DL_ITERATE_PHDR])
 if test x$gcc_cv_target_dl_iterate_phdr = xyes; then
--- a/src/gcc/cp/Make-lang.in
+++ b/src/gcc/cp/Make-lang.in
@@ -155,7 +155,7 @@ check-c++-subtargets : check-g++-subtargets
 # List of targets that can use the generic check- rule and its // variant.
 lang_checks += check-g++
 lang_checks_parallelized += check-g++
-# For description see comment above check_gcc_parallelize in gcc/Makefile.in.
+# For description see the check_$lang_parallelize comment in gcc/Makefile.in.
 check_g++_parallelize = 10000
 #
 # Install hooks:
@@ -221,6 +221,7 @@ c++.mostlyclean:
 	-rm -f doc/g++.1
 	-rm -f cp/*$(objext)
 	-rm -f cp/*$(coverageexts)
+	-rm -f xg++$(exeext) g++-cross$(exeext) cc1plus$(exeext)
 c++.clean:
 c++.distclean:
 	-rm -f cp/config.status cp/Makefile
--- a/src/gcc/cppbuiltin.c
+++ b/src/gcc/cppbuiltin.c
@@ -62,18 +62,41 @@ parse_basever (int *major, int *minor, int *patchlevel)
     *patchlevel = s_patchlevel;
 }
 
+/* Parse a LINAROVER version string of the format "M.m-year.month[-spin][~dev]"
+   to create Linaro release number YYYYMM and spin version.  */
+static void
+parse_linarover (int *release, int *spin)
+{
+  static int s_year = -1, s_month, s_spin;
+
+  if (s_year == -1)
+    if (sscanf (LINAROVER, "%*[^-]-%d.%d-%d", &s_year, &s_month, &s_spin) != 3)
+      {
+	sscanf (LINAROVER, "%*[^-]-%d.%d", &s_year, &s_month);
+	s_spin = 0;
+      }
+
+  if (release)
+    *release = s_year * 100 + s_month;
+
+  if (spin)
+    *spin = s_spin;
+}
 
 /* Define __GNUC__, __GNUC_MINOR__, __GNUC_PATCHLEVEL__ and __VERSION__.  */
 static void
 define__GNUC__ (cpp_reader *pfile)
 {
-  int major, minor, patchlevel;
+  int major, minor, patchlevel, linaro_release, linaro_spin;
 
   parse_basever (&major, &minor, &patchlevel);
+  parse_linarover (&linaro_release, &linaro_spin);
   cpp_define_formatted (pfile, "__GNUC__=%d", major);
   cpp_define_formatted (pfile, "__GNUC_MINOR__=%d", minor);
   cpp_define_formatted (pfile, "__GNUC_PATCHLEVEL__=%d", patchlevel);
   cpp_define_formatted (pfile, "__VERSION__=\"%s\"", version_string);
+  cpp_define_formatted (pfile, "__LINARO_RELEASE__=%d", linaro_release);
+  cpp_define_formatted (pfile, "__LINARO_SPIN__=%d", linaro_spin);
   cpp_define_formatted (pfile, "__ATOMIC_RELAXED=%d", MEMMODEL_RELAXED);
   cpp_define_formatted (pfile, "__ATOMIC_SEQ_CST=%d", MEMMODEL_SEQ_CST);
   cpp_define_formatted (pfile, "__ATOMIC_ACQUIRE=%d", MEMMODEL_ACQUIRE);
--- a/src/gcc/cprop.c
+++ b/src/gcc/cprop.c
@@ -285,6 +285,15 @@ cprop_constant_p (const_rtx x)
   return CONSTANT_P (x) && (GET_CODE (x) != CONST || shared_const_p (x));
 }
 
+/* Determine whether the rtx X should be treated as a register that can
+   be propagated.  Any pseudo-register is fine.  */
+
+static bool
+cprop_reg_p (const_rtx x)
+{
+  return REG_P (x) && !HARD_REGISTER_P (x);
+}
+
 /* Scan SET present in INSN and add an entry to the hash TABLE.
    IMPLICIT is true if it's an implicit set, false otherwise.  */
 
@@ -295,8 +304,7 @@ hash_scan_set (rtx set, rtx_insn *insn, struct hash_table_d *table,
   rtx src = SET_SRC (set);
   rtx dest = SET_DEST (set);
 
-  if (REG_P (dest)
-      && ! HARD_REGISTER_P (dest)
+  if (cprop_reg_p (dest)
       && reg_available_p (dest, insn)
       && can_copy_p (GET_MODE (dest)))
     {
@@ -321,9 +329,8 @@ hash_scan_set (rtx set, rtx_insn *insn, struct hash_table_d *table,
 	src = XEXP (note, 0), set = gen_rtx_SET (VOIDmode, dest, src);
 
       /* Record sets for constant/copy propagation.  */
-      if ((REG_P (src)
+      if ((cprop_reg_p (src)
 	   && src != dest
-	   && ! HARD_REGISTER_P (src)
 	   && reg_available_p (src, insn))
 	  || cprop_constant_p (src))
 	insert_set_in_table (dest, src, insn, table, implicit);
@@ -758,12 +765,37 @@ try_replace_reg (rtx from, rtx to, rtx_insn *insn)
   int success = 0;
   rtx set = single_set (insn);
 
+  bool check_rtx_costs = true;
+  bool speed = optimize_bb_for_speed_p (BLOCK_FOR_INSN (insn));
+  int old_cost = set ? set_rtx_cost (set, speed) : 0;
+
+  if ((note != 0
+      && REG_NOTE_KIND (note) == REG_EQUAL
+      && (GET_CODE (XEXP (note, 0)) == CONST
+	  || CONSTANT_P (XEXP (note, 0))))
+      || (set && CONSTANT_P (SET_SRC (set))))
+    check_rtx_costs = false;
+
   /* Usually we substitute easy stuff, so we won't copy everything.
      We however need to take care to not duplicate non-trivial CONST
      expressions.  */
   to = copy_rtx (to);
 
   validate_replace_src_group (from, to, insn);
+
+  /* If TO is a constant, check the cost of the set after propagation
+     to the cost of the set before the propagation.  If the cost is
+     higher, then do not replace FROM with TO.  */
+
+  if (check_rtx_costs
+      && CONSTANT_P (to)
+      && (set_rtx_cost (set, speed) > old_cost))
+    {
+      cancel_changes (0);
+      return false;
+    }
+
+
   if (num_changes_pending () && apply_change_group ())
     success = 1;
 
@@ -821,15 +853,15 @@ try_replace_reg (rtx from, rtx to, rtx_insn *insn)
   return success;
 }
 
-/* Find a set of REGNOs that are available on entry to INSN's block.  Return
-   NULL no such set is found.  */
+/* Find a set of REGNOs that are available on entry to INSN's block.  If found,
+   SET_RET[0] will be assigned a set with a register source and SET_RET[1] a
+   set with a constant source.  If not found the corresponding entry is set to
+   NULL.  */
 
-static struct cprop_expr *
-find_avail_set (int regno, rtx_insn *insn)
+static void
+find_avail_set (int regno, rtx_insn *insn, struct cprop_expr *set_ret[2])
 {
-  /* SET1 contains the last set found that can be returned to the caller for
-     use in a substitution.  */
-  struct cprop_expr *set1 = 0;
+  set_ret[0] = set_ret[1] = NULL;
 
   /* Loops are not possible here.  To get a loop we would need two sets
      available at the start of the block containing INSN.  i.e. we would
@@ -869,8 +901,10 @@ find_avail_set (int regno, rtx_insn *insn)
          If the source operand changed, we may still use it for the next
          iteration of this loop, but we may not use it for substitutions.  */
 
-      if (cprop_constant_p (src) || reg_not_set_p (src, insn))
-	set1 = set;
+      if (cprop_constant_p (src))
+	set_ret[1] = set;
+      else if (reg_not_set_p (src, insn))
+	set_ret[0] = set;
 
       /* If the source of the set is anything except a register, then
 	 we have reached the end of the copy chain.  */
@@ -881,10 +915,6 @@ find_avail_set (int regno, rtx_insn *insn)
 	 and see if we have an available copy into SRC.  */
       regno = REGNO (src);
     }
-
-  /* SET1 holds the last set that was available and anticipatable at
-     INSN.  */
-  return set1;
 }
 
 /* Subroutine of cprop_insn that tries to propagate constants into
@@ -1050,40 +1080,40 @@ cprop_insn (rtx_insn *insn)
   int changed = 0, changed_this_round;
   rtx note;
 
-retry:
-  changed_this_round = 0;
-  reg_use_count = 0;
-  note_uses (&PATTERN (insn), find_used_regs, NULL);
-
-  /* We may win even when propagating constants into notes.  */
-  note = find_reg_equal_equiv_note (insn);
-  if (note)
-    find_used_regs (&XEXP (note, 0), NULL);
-
-  for (i = 0; i < reg_use_count; i++)
+  do
     {
-      rtx reg_used = reg_use_table[i];
-      unsigned int regno = REGNO (reg_used);
-      rtx src;
-      struct cprop_expr *set;
+      changed_this_round = 0;
+      reg_use_count = 0;
+      note_uses (&PATTERN (insn), find_used_regs, NULL);
 
-      /* If the register has already been set in this block, there's
-	 nothing we can do.  */
-      if (! reg_not_set_p (reg_used, insn))
-	continue;
+      /* We may win even when propagating constants into notes.  */
+      note = find_reg_equal_equiv_note (insn);
+      if (note)
+	find_used_regs (&XEXP (note, 0), NULL);
 
-      /* Find an assignment that sets reg_used and is available
-	 at the start of the block.  */
-      set = find_avail_set (regno, insn);
-      if (! set)
-	continue;
+      for (i = 0; i < reg_use_count; i++)
+	{
+	  rtx reg_used = reg_use_table[i];
+	  unsigned int regno = REGNO (reg_used);
+	  rtx src_cst = NULL, src_reg = NULL;
+	  struct cprop_expr *set[2];
 
-      src = set->src;
+	  /* If the register has already been set in this block, there's
+	     nothing we can do.  */
+	  if (! reg_not_set_p (reg_used, insn))
+	    continue;
 
-      /* Constant propagation.  */
-      if (cprop_constant_p (src))
-	{
-          if (constprop_register (reg_used, src, insn))
+	  /* Find an assignment that sets reg_used and is available
+	     at the start of the block.  */
+	  find_avail_set (regno, insn, set);
+	  if (set[0])
+	    src_reg = set[0]->src;
+	  if (set[1])
+	    src_cst = set[1]->src;
+
+	  /* Constant propagation.  */
+	  if (src_cst && cprop_constant_p (src_cst)
+	      && constprop_register (reg_used, src_cst, insn))
 	    {
 	      changed_this_round = changed = 1;
 	      global_const_prop_count++;
@@ -1093,18 +1123,16 @@ retry:
 			   "GLOBAL CONST-PROP: Replacing reg %d in ", regno);
 		  fprintf (dump_file, "insn %d with constant ",
 			   INSN_UID (insn));
-		  print_rtl (dump_file, src);
+		  print_rtl (dump_file, src_cst);
 		  fprintf (dump_file, "\n");
 		}
 	      if (insn->deleted ())
 		return 1;
 	    }
-	}
-      else if (REG_P (src)
-	       && REGNO (src) >= FIRST_PSEUDO_REGISTER
-	       && REGNO (src) != regno)
-	{
-	  if (try_replace_reg (reg_used, src, insn))
+	  /* Copy propagation.  */
+	  else if (src_reg && cprop_reg_p (src_reg)
+		   && REGNO (src_reg) != regno
+		   && try_replace_reg (reg_used, src_reg, insn))
 	    {
 	      changed_this_round = changed = 1;
 	      global_copy_prop_count++;
@@ -1113,7 +1141,7 @@ retry:
 		  fprintf (dump_file,
 			   "GLOBAL COPY-PROP: Replacing reg %d in insn %d",
 			   regno, INSN_UID (insn));
-		  fprintf (dump_file, " with reg %d\n", REGNO (src));
+		  fprintf (dump_file, " with reg %d\n", REGNO (src_reg));
 		}
 
 	      /* The original insn setting reg_used may or may not now be
@@ -1123,12 +1151,10 @@ retry:
 		 and made things worse.  */
 	    }
 	}
-
-      /* If try_replace_reg simplified the insn, the regs found
-	 by find_used_regs may not be valid anymore.  Start over.  */
-      if (changed_this_round)
-	goto retry;
     }
+  /* If try_replace_reg simplified the insn, the regs found by find_used_regs
+     may not be valid anymore.  Start over.  */
+  while (changed_this_round);
 
   if (changed && DEBUG_INSN_P (insn))
     return 0;
@@ -1191,7 +1217,7 @@ do_local_cprop (rtx x, rtx_insn *insn)
   /* Rule out USE instructions and ASM statements as we don't want to
      change the hard registers mentioned.  */
   if (REG_P (x)
-      && (REGNO (x) >= FIRST_PSEUDO_REGISTER
+      && (cprop_reg_p (x)
           || (GET_CODE (PATTERN (insn)) != USE
 	      && asm_noperands (PATTERN (insn)) < 0)))
     {
@@ -1207,7 +1233,7 @@ do_local_cprop (rtx x, rtx_insn *insn)
 
 	  if (cprop_constant_p (this_rtx))
 	    newcnst = this_rtx;
-	  if (REG_P (this_rtx) && REGNO (this_rtx) >= FIRST_PSEUDO_REGISTER
+	  if (cprop_reg_p (this_rtx)
 	      /* Don't copy propagate if it has attached REG_EQUIV note.
 		 At this point this only function parameters should have
 		 REG_EQUIV notes and if the argument slot is used somewhere
@@ -1328,9 +1354,8 @@ implicit_set_cond_p (const_rtx cond)
   if (GET_CODE (cond) != EQ && GET_CODE (cond) != NE)
     return false;
 
-  /* The first operand of COND must be a pseudo-reg.  */
-  if (! REG_P (XEXP (cond, 0))
-      || HARD_REGISTER_P (XEXP (cond, 0)))
+  /* The first operand of COND must be a register we can propagate.  */
+  if (!cprop_reg_p (XEXP (cond, 0)))
     return false;
 
   /* The second operand of COND must be a suitable constant.  */
--- a/src/gcc/cse.c
+++ b/src/gcc/cse.c
@@ -4540,14 +4540,49 @@ cse_insn (rtx_insn *insn)
   canonicalize_insn (insn, &sets, n_sets);
 
   /* If this insn has a REG_EQUAL note, store the equivalent value in SRC_EQV,
-     if different, or if the DEST is a STRICT_LOW_PART.  The latter condition
-     is necessary because SRC_EQV is handled specially for this case, and if
-     it isn't set, then there will be no equivalence for the destination.  */
+     if different, or if the DEST is a STRICT_LOW_PART/ZERO_EXTRACT.  The
+     latter condition is necessary because SRC_EQV is handled specially for
+     this case, and if it isn't set, then there will be no equivalence
+     for the destination.  */
   if (n_sets == 1 && REG_NOTES (insn) != 0
       && (tem = find_reg_note (insn, REG_EQUAL, NULL_RTX)) != 0
-      && (! rtx_equal_p (XEXP (tem, 0), SET_SRC (sets[0].rtl))
-	  || GET_CODE (SET_DEST (sets[0].rtl)) == STRICT_LOW_PART))
-    src_eqv = copy_rtx (XEXP (tem, 0));
+      && (! rtx_equal_p (XEXP (tem, 0), SET_SRC (sets[0].rtl))))
+    {
+      if (GET_CODE (SET_DEST (sets[0].rtl)) == STRICT_LOW_PART)
+	src_eqv = copy_rtx (XEXP (tem, 0));
+
+      /* If DEST is of the form ZERO_EXTACT, as in:
+	 (set (zero_extract:SI (reg:SI 119)
+		  (const_int 16 [0x10])
+		  (const_int 16 [0x10]))
+	      (const_int 51154 [0xc7d2]))
+	 REG_EQUAL note will specify the value of register (reg:SI 119) at this
+	 point.  Note that this is different from SRC_EQV. We can however
+	 calculate SRC_EQV with the position and width of ZERO_EXTRACT.  */
+      else if (GET_CODE (SET_DEST (sets[0].rtl)) == ZERO_EXTRACT
+	       && CONST_INT_P (XEXP (tem, 0))
+	       && CONST_INT_P (XEXP (SET_DEST (sets[0].rtl), 1))
+	       && CONST_INT_P (XEXP (SET_DEST (sets[0].rtl), 2)))
+	{
+	  rtx dest_reg = XEXP (SET_DEST (sets[0].rtl), 0);
+	  rtx width = XEXP (SET_DEST (sets[0].rtl), 1);
+	  rtx pos = XEXP (SET_DEST (sets[0].rtl), 2);
+	  HOST_WIDE_INT val = INTVAL (XEXP (tem, 0));
+	  HOST_WIDE_INT mask;
+	  unsigned int shift;
+	  if (BITS_BIG_ENDIAN)
+	    shift = GET_MODE_PRECISION (GET_MODE (dest_reg))
+	      - INTVAL (pos) - INTVAL (width);
+	  else
+	    shift = INTVAL (pos);
+	  if (INTVAL (width) == HOST_BITS_PER_WIDE_INT)
+	    mask = ~(HOST_WIDE_INT) 0;
+	  else
+	    mask = ((HOST_WIDE_INT) 1 << INTVAL (width)) - 1;
+	  val = (val >> shift) & mask;
+	  src_eqv = GEN_INT (val);
+	}
+    }
 
   /* Set sets[i].src_elt to the class each source belongs to.
      Detect assignments from or to volatile things
--- a/src/gcc/df-core.c
+++ b/src/gcc/df-core.c
@@ -642,7 +642,6 @@ void
 df_finish_pass (bool verify ATTRIBUTE_UNUSED)
 {
   int i;
-  int removed = 0;
 
 #ifdef ENABLE_DF_CHECKING
   int saved_flags;
@@ -658,21 +657,15 @@ df_finish_pass (bool verify ATTRIBUTE_UNUSED)
   saved_flags = df->changeable_flags;
 #endif
 
-  for (i = 0; i < df->num_problems_defined; i++)
+  /* We iterate over problems by index as each problem removed will
+     lead to problems_in_order to be reordered.  */
+  for (i = 0; i < DF_LAST_PROBLEM_PLUS1; i++)
     {
-      struct dataflow *dflow = df->problems_in_order[i];
-      struct df_problem *problem = dflow->problem;
+      struct dataflow *dflow = df->problems_by_index[i];
 
-      if (dflow->optional_p)
-	{
-	  gcc_assert (problem->remove_problem_fun);
-	  (problem->remove_problem_fun) ();
-	  df->problems_in_order[i] = NULL;
-	  df->problems_by_index[problem->id] = NULL;
-	  removed++;
-	}
+      if (dflow && dflow->optional_p)
+	df_remove_problem (dflow);
     }
-  df->num_problems_defined -= removed;
 
   /* Clear all of the flags.  */
   df->changeable_flags = 0;
--- a/src/gcc/emit-rtl.c
+++ b/src/gcc/emit-rtl.c
@@ -5234,7 +5234,8 @@ set_for_reg_notes (rtx insn)
   reg = SET_DEST (pat);
 
   /* Notes apply to the contents of a STRICT_LOW_PART.  */
-  if (GET_CODE (reg) == STRICT_LOW_PART)
+  if (GET_CODE (reg) == STRICT_LOW_PART
+      || GET_CODE (reg) == ZERO_EXTRACT)
     reg = XEXP (reg, 0);
 
   /* Check that we have a register.  */
--- a/src/gcc/expr.c
+++ b/src/gcc/expr.c
@@ -7725,15 +7725,7 @@ expand_expr_addr_expr_1 (tree exp, rtx target, machine_mode tmode,
 	     marked TREE_ADDRESSABLE, which will be either a front-end
 	     or a tree optimizer bug.  */
 
-	  if (TREE_ADDRESSABLE (exp)
-	      && ! MEM_P (result)
-	      && ! targetm.calls.allocate_stack_slots_for_args ())
-	    {
-	      error ("local frame unavailable (naked function?)");
-	      return result;
-	    }
-	  else
-	    gcc_assert (MEM_P (result));
+	  gcc_assert (MEM_P (result));
 	  result = XEXP (result, 0);
 
 	  /* ??? Is this needed anymore?  */
--- a/src/gcc/fortran/Make-lang.in
+++ b/src/gcc/fortran/Make-lang.in
@@ -167,7 +167,7 @@ check-f95-subtargets : check-gfortran-subtargets
 check-fortran-subtargets : check-gfortran-subtargets
 lang_checks += check-gfortran
 lang_checks_parallelized += check-gfortran
-# For description see comment above check_gcc_parallelize in gcc/Makefile.in.
+# For description see the check_$lang_parallelize comment in gcc/Makefile.in.
 check_gfortran_parallelize = 10000
 
 # GFORTRAN documentation.
@@ -275,7 +275,7 @@ fortran.uninstall:
 # We just have to delete files specific to us.
 
 fortran.mostlyclean:
-	-rm -f f951$(exeext)
+	-rm -f gfortran$(exeext) gfortran-cross$(exeext) f951$(exeext)
 	-rm -f fortran/*.o
 
 fortran.clean:
--- a/src/gcc/function.c
+++ b/src/gcc/function.c
@@ -227,6 +227,7 @@ free_after_compilation (struct function *f)
   f->eh = NULL;
   f->machine = NULL;
   f->cfg = NULL;
+  f->curr_properties &= ~PROP_cfg;
 
   regno_reg_rtx = NULL;
 }
@@ -2121,9 +2122,6 @@ aggregate_value_p (const_tree exp, const_tree fntype)
 bool
 use_register_for_decl (const_tree decl)
 {
-  if (!targetm.calls.allocate_stack_slots_for_args ())
-    return true;
-
   /* Honor volatile.  */
   if (TREE_SIDE_EFFECTS (decl))
     return false;
@@ -2151,6 +2149,9 @@ use_register_for_decl (const_tree decl)
   if (flag_float_store && FLOAT_TYPE_P (TREE_TYPE (decl)))
     return false;
 
+  if (!targetm.calls.allocate_stack_slots_for_args ())
+    return true;
+
   /* If we're not interested in tracking debugging information for
      this decl, then we can certainly put it in a register.  */
   if (DECL_IGNORED_P (decl))
--- a/src/gcc/genmatch.c
+++ b/src/gcc/genmatch.c
@@ -2922,7 +2922,12 @@ parser::parse_operation ()
 
   user_id *p = dyn_cast<user_id *> (op);
   if (p && p->is_oper_list)
-    record_operlist (id_tok->src_loc, p);
+    {
+      if (active_fors.length() == 0)
+	record_operlist (id_tok->src_loc, p);
+      else
+	fatal_at (id_tok, "operator-list %s cannot be exapnded inside 'for'", id);
+    }
   return op;
 }
 
@@ -3338,8 +3343,13 @@ parser::parse_for (source_location)
 		      "others with arity %d", oper, idb->nargs, arity);
 
 	  user_id *p = dyn_cast<user_id *> (idb);
-	  if (p && p->is_oper_list)
-	    op->substitutes.safe_splice (p->substitutes);
+	  if (p)
+	    {
+	      if (p->is_oper_list)
+		op->substitutes.safe_splice (p->substitutes);
+	      else
+		fatal_at (token, "iterator cannot be used as operator-list");
+	    }
 	  else 
 	    op->substitutes.safe_push (idb);
 	}
@@ -3436,6 +3446,11 @@ parser::parse_operator_list (source_location)
 	op->substitutes.safe_push (idb);
     }
 
+  // Check that there is no junk after id-list
+  token = peek();
+  if (token->type != CPP_CLOSE_PAREN)
+    fatal_at (token, "expected identifier got %s", cpp_type2name (token->type, 0));
+
   if (op->substitutes.length () == 0)
     fatal_at (token, "operator-list cannot be empty");
 
--- a/src/gcc/genpreds.c
+++ b/src/gcc/genpreds.c
@@ -640,12 +640,14 @@ struct constraint_data
   const char *regclass;  /* for register constraints */
   rtx exp;               /* for other constraints */
   unsigned int lineno;   /* line of definition */
-  unsigned int is_register  : 1;
-  unsigned int is_const_int : 1;
-  unsigned int is_const_dbl : 1;
-  unsigned int is_extra     : 1;
-  unsigned int is_memory    : 1;
-  unsigned int is_address   : 1;
+  unsigned int is_register	: 1;
+  unsigned int is_const_int	: 1;
+  unsigned int is_const_dbl	: 1;
+  unsigned int is_extra		: 1;
+  unsigned int is_memory	: 1;
+  unsigned int is_address	: 1;
+  unsigned int maybe_allows_reg : 1;
+  unsigned int maybe_allows_mem : 1;
 };
 
 /* Overview of all constraints beginning with a given letter.  */
@@ -691,6 +693,9 @@ static unsigned int satisfied_start;
 static unsigned int const_int_start, const_int_end;
 static unsigned int memory_start, memory_end;
 static unsigned int address_start, address_end;
+static unsigned int maybe_allows_none_start, maybe_allows_none_end;
+static unsigned int maybe_allows_reg_start, maybe_allows_reg_end;
+static unsigned int maybe_allows_mem_start, maybe_allows_mem_end;
 
 /* Convert NAME, which contains angle brackets and/or underscores, to
    a string that can be used as part of a C identifier.  The string
@@ -711,6 +716,34 @@ mangle (const char *name)
   return XOBFINISH (rtl_obstack, const char *);
 }
 
+/* Return a bitmask, bit 1 if EXP maybe allows a REG/SUBREG, 2 if EXP
+   maybe allows a MEM.  Bits should be clear only when we are sure it
+   will not allow a REG/SUBREG or a MEM.  */
+static int
+compute_maybe_allows (rtx exp)
+{
+  switch (GET_CODE (exp))
+    {
+    case IF_THEN_ELSE:
+      /* Conservative answer is like IOR, of the THEN and ELSE branches.  */
+      return compute_maybe_allows (XEXP (exp, 1))
+	     | compute_maybe_allows (XEXP (exp, 2));
+    case AND:
+      return compute_maybe_allows (XEXP (exp, 0))
+	     & compute_maybe_allows (XEXP (exp, 1));
+    case IOR:
+      return compute_maybe_allows (XEXP (exp, 0))
+	     | compute_maybe_allows (XEXP (exp, 1));
+    case MATCH_CODE:
+      if (*XSTR (exp, 1) == '\0')
+	return (strstr (XSTR (exp, 0), "reg") != NULL ? 1 : 0)
+	       | (strstr (XSTR (exp, 0), "mem") != NULL ? 2 : 0);
+      /* FALLTHRU */
+    default:
+      return 3;
+    }
+}
+
 /* Add one constraint, of any sort, to the tables.  NAME is its name;
    REGCLASS is the register class, if any; EXP is the expression to
    test, if any;  IS_MEMORY and IS_ADDRESS indicate memory and address
@@ -866,6 +899,11 @@ add_constraint (const char *name, const char *regclass,
   c->is_extra = !(regclass || is_const_int || is_const_dbl);
   c->is_memory = is_memory;
   c->is_address = is_address;
+  int maybe_allows = 3;
+  if (exp)
+    maybe_allows = compute_maybe_allows (exp);
+  c->maybe_allows_reg = (maybe_allows & 1) != 0;
+  c->maybe_allows_mem = (maybe_allows & 2) != 0;
 
   c->next_this_letter = *slot;
   *slot = c;
@@ -940,8 +978,30 @@ choose_enum_order (void)
       enum_order[next++] = c;
   address_end = next;
 
+  maybe_allows_none_start = next;
+  FOR_ALL_CONSTRAINTS (c)
+    if (!c->is_register && !c->is_const_int && !c->is_memory && !c->is_address
+	&& !c->maybe_allows_reg && !c->maybe_allows_mem)
+      enum_order[next++] = c;
+  maybe_allows_none_end = next;
+
+  maybe_allows_reg_start = next;
+  FOR_ALL_CONSTRAINTS (c)
+    if (!c->is_register && !c->is_const_int && !c->is_memory && !c->is_address
+	&& c->maybe_allows_reg && !c->maybe_allows_mem)
+      enum_order[next++] = c;
+  maybe_allows_reg_end = next;
+
+  maybe_allows_mem_start = next;
+  FOR_ALL_CONSTRAINTS (c)
+    if (!c->is_register && !c->is_const_int && !c->is_memory && !c->is_address
+	&& !c->maybe_allows_reg && c->maybe_allows_mem)
+      enum_order[next++] = c;
+  maybe_allows_mem_end = next;
+
   FOR_ALL_CONSTRAINTS (c)
-    if (!c->is_register && !c->is_const_int && !c->is_memory && !c->is_address)
+    if (!c->is_register && !c->is_const_int && !c->is_memory && !c->is_address
+	&& c->maybe_allows_reg && c->maybe_allows_mem)
       enum_order[next++] = c;
   gcc_assert (next == num_constraints);
 }
@@ -1229,6 +1289,41 @@ write_range_function (const char *name, unsigned int start, unsigned int end)
 	    "}\n\n", name);
 }
 
+/* Write a definition for insn_extra_constraint_allows_reg_mem function.  */
+static void
+write_allows_reg_mem_function (void)
+{
+  printf ("static inline void\n"
+	  "insn_extra_constraint_allows_reg_mem (enum constraint_num c,\n"
+	  "\t\t\t\t      bool *allows_reg, bool *allows_mem)\n"
+	  "{\n");
+  if (maybe_allows_none_start != maybe_allows_none_end)
+    printf ("  if (c >= CONSTRAINT_%s && c <= CONSTRAINT_%s)\n"
+	    "    return;\n",
+	    enum_order[maybe_allows_none_start]->c_name,
+	    enum_order[maybe_allows_none_end - 1]->c_name);
+  if (maybe_allows_reg_start != maybe_allows_reg_end)
+    printf ("  if (c >= CONSTRAINT_%s && c <= CONSTRAINT_%s)\n"
+	    "    {\n"
+	    "      *allows_reg = true;\n"
+	    "      return;\n"
+	    "    }\n",
+	    enum_order[maybe_allows_reg_start]->c_name,
+	    enum_order[maybe_allows_reg_end - 1]->c_name);
+  if (maybe_allows_mem_start != maybe_allows_mem_end)
+    printf ("  if (c >= CONSTRAINT_%s && c <= CONSTRAINT_%s)\n"
+	    "    {\n"
+	    "      *allows_mem = true;\n"
+	    "      return;\n"
+	    "    }\n",
+	    enum_order[maybe_allows_mem_start]->c_name,
+	    enum_order[maybe_allows_mem_end - 1]->c_name);
+  printf ("  (void) c;\n"
+	  "  *allows_reg = true;\n"
+	  "  *allows_mem = true;\n"
+	  "}\n\n");
+}
+
 /* VEC is a list of key/value pairs, with the keys being lower bounds
    of a range.  Output a decision tree that handles the keys covered by
    [VEC[START], VEC[END]), returning FALLBACK for keys lower then VEC[START]'s.
@@ -1326,6 +1421,7 @@ write_tm_preds_h (void)
 			    memory_start, memory_end);
       write_range_function ("insn_extra_address_constraint",
 			    address_start, address_end);
+      write_allows_reg_mem_function ();
 
       if (constraint_max_namelen > 1)
         {
--- a/src/gcc/go/Make-lang.in
+++ b/src/gcc/go/Make-lang.in
@@ -197,6 +197,7 @@ go.uninstall:
 go.mostlyclean:
 	-rm -f go/*$(objext)
 	-rm -f go/*$(coverageexts)
+	-rm -f gccgo$(exeext) gccgo-cross$(exeext) go1$(exeext)
 go.clean:
 go.distclean:
 go.maintainer-clean:
--- a/src/gcc/ifcvt.c
+++ b/src/gcc/ifcvt.c
@@ -1046,7 +1046,7 @@ cc_in_cond (rtx cond)
 
 /* Return sequence of instructions generated by if conversion.  This
    function calls end_sequence() to end the current stream, ensures
-   that are instructions are unshared, recognizable non-jump insns.
+   that the instructions are unshared, recognizable non-jump insns.
    On failure, this function returns a NULL_RTX.  */
 
 static rtx_insn *
--- a/src/gcc/ira-costs.c
+++ b/src/gcc/ira-costs.c
@@ -1380,8 +1380,6 @@ record_operand_costs (rtx_insn *insn, enum reg_class *pref)
       rtx dest = SET_DEST (set);
       rtx src = SET_SRC (set);
 
-      dest = SET_DEST (set);
-      src = SET_SRC (set);
       if (GET_CODE (dest) == SUBREG
 	  && (GET_MODE_SIZE (GET_MODE (dest))
 	      == GET_MODE_SIZE (GET_MODE (SUBREG_REG (dest)))))
--- a/src/gcc/jit/Make-lang.in
+++ b/src/gcc/jit/Make-lang.in
@@ -285,6 +285,10 @@ jit.uninstall:
 # We just have to delete files specific to us.
 
 jit.mostlyclean:
+	-rm -f $(LIBGCCJIT_FILENAME) $(LIBGCCJIT_SYMLINK)
+	-rm -f $(LIBGCCJIT_LINKER_NAME_SYMLINK) $(FULL_DRIVER_NAME)
+	-rm -f $(LIBGCCJIT_SONAME)
+	-rm -f $(jit_OBJS)
 
 jit.clean:
 
--- a/src/gcc/loop-invariant.c
+++ b/src/gcc/loop-invariant.c
@@ -696,6 +696,8 @@ find_defs (struct loop *loop)
   df_remove_problem (df_chain);
   df_process_deferred_rescans ();
   df_chain_add_problem (DF_UD_CHAIN);
+  df_live_add_problem ();
+  df_live_set_all_dirty ();
   df_set_flags (DF_RD_PRUNE_DEAD_DEFS);
   df_analyze_loop (loop);
   check_invariant_table_size ();
@@ -740,8 +742,11 @@ create_new_invariant (struct def *def, rtx_insn *insn, bitmap depends_on,
 	 enough to not regress 410.bwaves either (by still moving reg+reg
 	 invariants).
 	 See http://gcc.gnu.org/ml/gcc-patches/2009-10/msg01210.html .  */
-      inv->cheap_address = address_cost (SET_SRC (set), word_mode,
-					 ADDR_SPACE_GENERIC, speed) < 3;
+      if (SCALAR_INT_MODE_P (GET_MODE (SET_DEST (set))))
+	inv->cheap_address = address_cost (SET_SRC (set), word_mode,
+					   ADDR_SPACE_GENERIC, speed) < 3;
+      else
+	inv->cheap_address = false;
     }
   else
     {
@@ -1174,6 +1179,7 @@ get_inv_cost (struct invariant *inv, int *comp_cost, unsigned *regs_needed,
     }
 
   if (!inv->cheap_address
+      || inv->def->n_uses == 0
       || inv->def->n_addr_uses < inv->def->n_uses)
     (*comp_cost) += inv->cost * inv->eqno;
 
@@ -1512,6 +1518,79 @@ replace_uses (struct invariant *inv, rtx reg, bool in_group)
   return 1;
 }
 
+/* Whether invariant INV setting REG can be moved out of LOOP, at the end of
+   the block preceding its header.  */
+
+static bool
+can_move_invariant_reg (struct loop *loop, struct invariant *inv, rtx reg)
+{
+  df_ref def, use;
+  unsigned int dest_regno, defs_in_loop_count = 0;
+  rtx_insn *insn = inv->insn;
+  basic_block bb = BLOCK_FOR_INSN (inv->insn);
+
+  /* We ignore hard register and memory access for cost and complexity reasons.
+     Hard register are few at this stage and expensive to consider as they
+     require building a separate data flow.  Memory access would require using
+     df_simulate_* and can_move_insns_across functions and is more complex.  */
+  if (!REG_P (reg) || HARD_REGISTER_P (reg))
+    return false;
+
+  /* Check whether the set is always executed.  We could omit this condition if
+     we know that the register is unused outside of the loop, but it does not
+     seem worth finding out.  */
+  if (!inv->always_executed)
+    return false;
+
+  /* Check that all uses that would be dominated by def are already dominated
+     by it.  */
+  dest_regno = REGNO (reg);
+  for (use = DF_REG_USE_CHAIN (dest_regno); use; use = DF_REF_NEXT_REG (use))
+    {
+      rtx_insn *use_insn;
+      basic_block use_bb;
+
+      use_insn = DF_REF_INSN (use);
+      use_bb = BLOCK_FOR_INSN (use_insn);
+
+      /* Ignore instruction considered for moving.  */
+      if (use_insn == insn)
+	continue;
+
+      /* Don't consider uses outside loop.  */
+      if (!flow_bb_inside_loop_p (loop, use_bb))
+	continue;
+
+      /* Don't move if a use is not dominated by def in insn.  */
+      if (use_bb == bb && DF_INSN_LUID (insn) >= DF_INSN_LUID (use_insn))
+	return false;
+      if (!dominated_by_p (CDI_DOMINATORS, use_bb, bb))
+	return false;
+    }
+
+  /* Check for other defs.  Any other def in the loop might reach a use
+     currently reached by the def in insn.  */
+  for (def = DF_REG_DEF_CHAIN (dest_regno); def; def = DF_REF_NEXT_REG (def))
+    {
+      basic_block def_bb = DF_REF_BB (def);
+
+      /* Defs in exit block cannot reach a use they weren't already.  */
+      if (single_succ_p (def_bb))
+	{
+	  basic_block def_bb_succ;
+
+	  def_bb_succ = single_succ (def_bb);
+	  if (!flow_bb_inside_loop_p (loop, def_bb_succ))
+	    continue;
+	}
+
+      if (++defs_in_loop_count > 1)
+	return false;
+    }
+
+  return true;
+}
+
 /* Move invariant INVNO out of the LOOP.  Returns true if this succeeds, false
    otherwise.  */
 
@@ -1545,11 +1624,8 @@ move_invariant_reg (struct loop *loop, unsigned invno)
 	    }
 	}
 
-      /* Move the set out of the loop.  If the set is always executed (we could
-	 omit this condition if we know that the register is unused outside of
-	 the loop, but it does not seem worth finding out) and it has no uses
-	 that would not be dominated by it, we may just move it (TODO).
-	 Otherwise we need to create a temporary register.  */
+      /* If possible, just move the set out of the loop.  Otherwise, we
+	 need to create a temporary register.  */
       set = single_set (inv->insn);
       reg = dest = SET_DEST (set);
       if (GET_CODE (reg) == SUBREG)
@@ -1557,20 +1633,27 @@ move_invariant_reg (struct loop *loop, unsigned invno)
       if (REG_P (reg))
 	regno = REGNO (reg);
 
-      reg = gen_reg_rtx_and_attrs (dest);
+      if (!can_move_invariant_reg (loop, inv, dest))
+	{
+	  reg = gen_reg_rtx_and_attrs (dest);
 
-      /* Try replacing the destination by a new pseudoregister.  */
-      validate_change (inv->insn, &SET_DEST (set), reg, true);
+	  /* Try replacing the destination by a new pseudoregister.  */
+	  validate_change (inv->insn, &SET_DEST (set), reg, true);
 
-      /* As well as all the dominated uses.  */
-      replace_uses (inv, reg, true);
+	  /* As well as all the dominated uses.  */
+	  replace_uses (inv, reg, true);
 
-      /* And validate all the changes.  */
-      if (!apply_change_group ())
-	goto fail;
+	  /* And validate all the changes.  */
+	  if (!apply_change_group ())
+	    goto fail;
 
-      emit_insn_after (gen_move_insn (dest, reg), inv->insn);
+	  emit_insn_after (gen_move_insn (dest, reg), inv->insn);
+	}
+      else if (dump_file)
+	fprintf (dump_file, "Invariant %d moved without introducing a new "
+			    "temporary register\n", invno);
       reorder_insns (inv->insn, inv->insn, BB_END (preheader));
+      df_recompute_luids (preheader);
 
       /* If there is a REG_EQUAL note on the insn we just moved, and the
 	 insn is in a basic block that is not always executed or the note
--- a/src/gcc/lra-constraints.c
+++ b/src/gcc/lra-constraints.c
@@ -1656,8 +1656,7 @@ prohibited_class_reg_set_mode_p (enum reg_class rclass,
 {
   HARD_REG_SET temp;
   
-  // ??? Is this assert right
-  // lra_assert (hard_reg_set_subset_p (set, reg_class_contents[rclass]));
+  lra_assert (hard_reg_set_subset_p (reg_class_contents[rclass], set));
   COPY_HARD_REG_SET (temp, set);
   AND_COMPL_HARD_REG_SET (temp, lra_no_alloc_regs);
   return (hard_reg_set_subset_p
--- a/src/gcc/objc/Make-lang.in
+++ b/src/gcc/objc/Make-lang.in
@@ -114,6 +114,7 @@ objc.uninstall:
 objc.mostlyclean:
 	-rm -f objc/*$(objext) objc/xforward objc/fflags
 	-rm -f objc/*$(coverageexts)
+	-rm -f cc1obj$(exeext)
 objc.clean: objc.mostlyclean
 	-rm -rf objc-headers
 objc.distclean:
--- a/src/gcc/objcp/Make-lang.in
+++ b/src/gcc/objcp/Make-lang.in
@@ -142,6 +142,7 @@ obj-c++.uninstall:
 obj-c++.mostlyclean:
 	-rm -f objcp/*$(objext)
 	-rm -f objcp/*$(coverageexts)
+	-rm -f cc1objplus$(exeext)
 obj-c++.clean: obj-c++.mostlyclean
 obj-c++.distclean:
 	-rm -f objcp/config.status objcp/Makefile
--- a/src/gcc/optabs.c
+++ b/src/gcc/optabs.c
@@ -6544,18 +6544,28 @@ vector_compare_rtx (enum tree_code tcode, tree t_op0, tree t_op1,
 {
   struct expand_operand ops[2];
   rtx rtx_op0, rtx_op1;
+  machine_mode m0, m1;
   enum rtx_code rcode = get_rtx_code (tcode, unsignedp);
 
   gcc_assert (TREE_CODE_CLASS (tcode) == tcc_comparison);
 
-  /* Expand operands.  */
+  /* Expand operands.  For vector types with scalar modes, e.g. where int64x1_t
+     has mode DImode, this can produce a constant RTX of mode VOIDmode; in such
+     cases, use the original mode.  */
   rtx_op0 = expand_expr (t_op0, NULL_RTX, TYPE_MODE (TREE_TYPE (t_op0)),
 			 EXPAND_STACK_PARM);
+  m0 = GET_MODE (rtx_op0);
+  if (m0 == VOIDmode)
+    m0 = TYPE_MODE (TREE_TYPE (t_op0));
+
   rtx_op1 = expand_expr (t_op1, NULL_RTX, TYPE_MODE (TREE_TYPE (t_op1)),
 			 EXPAND_STACK_PARM);
+  m1 = GET_MODE (rtx_op1);
+  if (m1 == VOIDmode)
+    m1 = TYPE_MODE (TREE_TYPE (t_op1));
 
-  create_input_operand (&ops[0], rtx_op0, GET_MODE (rtx_op0));
-  create_input_operand (&ops[1], rtx_op1, GET_MODE (rtx_op1));
+  create_input_operand (&ops[0], rtx_op0, m0);
+  create_input_operand (&ops[1], rtx_op1, m1);
   if (!maybe_legitimize_operands (icode, 4, 2, ops))
     gcc_unreachable ();
   return gen_rtx_fmt_ee (rcode, VOIDmode, ops[0].value, ops[1].value);
--- a/src/gcc/params.def
+++ b/src/gcc/params.def
@@ -262,6 +262,14 @@ DEFPARAM(PARAM_MAX_HOIST_DEPTH,
 	 "Maximum depth of search in the dominator tree for expressions to hoist",
 	 30, 0, 0)
 
+
+/* When synthesizing expnonentiation by a real constant operations using square
+   roots, this controls how deep sqrt chains we are willing to generate.  */
+DEFPARAM(PARAM_MAX_POW_SQRT_DEPTH,
+	 "max-pow-sqrt-depth",
+	 "Maximum depth of sqrt chains to use when synthesizing exponentiation by a real constant",
+	 5, 1, 32)
+
 /* This parameter limits the number of insns in a loop that will be unrolled,
    and by how much the loop is unrolled.
 
--- a/src/gcc/regrename.c
+++ b/src/gcc/regrename.c
@@ -505,12 +505,20 @@ rename_chains (void)
 	  continue;
 	}
 
-      if (dump_file)
-	fprintf (dump_file, ", renamed as %s\n", reg_names[best_new_reg]);
-
-      regrename_do_replace (this_head, best_new_reg);
-      tick[best_new_reg] = ++this_tick;
-      df_set_regs_ever_live (best_new_reg, true);
+      if (regrename_do_replace (this_head, best_new_reg))
+	{
+	  if (dump_file)
+	    fprintf (dump_file, ", renamed as %s\n", reg_names[best_new_reg]);
+	  tick[best_new_reg] = ++this_tick;
+	  df_set_regs_ever_live (best_new_reg, true);
+	}
+      else
+	{
+	  if (dump_file)
+	    fprintf (dump_file, ", renaming as %s failed\n",
+		     reg_names[best_new_reg]);
+	  tick[reg] = ++this_tick;
+	}
     }
 }
 
@@ -936,7 +944,13 @@ regrename_analyze (bitmap bb_mask)
     bb->aux = NULL;
 }
 
-void
+/* Attempt to replace all uses of the register in the chain beginning with
+   HEAD with REG.  Returns true on success and false if the replacement is
+   rejected because the insns would not validate.  The latter can happen
+   e.g. if a match_parallel predicate enforces restrictions on register
+   numbering in its subpatterns.  */
+
+bool
 regrename_do_replace (struct du_head *head, int reg)
 {
   struct du_chain *chain;
@@ -950,22 +964,26 @@ regrename_do_replace (struct du_head *head, int reg)
       int reg_ptr = REG_POINTER (*chain->loc);
 
       if (DEBUG_INSN_P (chain->insn) && REGNO (*chain->loc) != base_regno)
-	INSN_VAR_LOCATION_LOC (chain->insn) = gen_rtx_UNKNOWN_VAR_LOC ();
+	validate_change (chain->insn, &(INSN_VAR_LOCATION_LOC (chain->insn)),
+			 gen_rtx_UNKNOWN_VAR_LOC (), true);
       else
 	{
-	  *chain->loc = gen_raw_REG (GET_MODE (*chain->loc), reg);
+	  validate_change (chain->insn, chain->loc, 
+			   gen_raw_REG (GET_MODE (*chain->loc), reg), true);
 	  if (regno >= FIRST_PSEUDO_REGISTER)
 	    ORIGINAL_REGNO (*chain->loc) = regno;
 	  REG_ATTRS (*chain->loc) = attr;
 	  REG_POINTER (*chain->loc) = reg_ptr;
 	}
-
-      df_insn_rescan (chain->insn);
     }
 
+  if (!apply_change_group ())
+    return false;
+
   mode = GET_MODE (*head->first->loc);
   head->regno = reg;
   head->nregs = hard_regno_nregs[reg][mode];
+  return true;
 }
 
 
--- a/src/gcc/regrename.h
+++ b/src/gcc/regrename.h
@@ -91,6 +91,6 @@ extern void regrename_analyze (bitmap);
 extern du_head_p regrename_chain_from_id (unsigned int);
 extern int find_rename_reg (du_head_p, enum reg_class, HARD_REG_SET *, int,
 			    bool);
-extern void regrename_do_replace (du_head_p, int);
+extern bool regrename_do_replace (du_head_p, int);
 
 #endif
--- a/src/gcc/rtlanal.c
+++ b/src/gcc/rtlanal.c
@@ -104,7 +104,10 @@ generic_subrtx_iterator <T>::add_single_to_queue (array_type &array,
 	  return base;
 	}
       gcc_checking_assert (i == LOCAL_ELEMS);
-      vec_safe_grow (array.heap, i + 1);
+      /* A previous iteration might also have moved from the stack to the
+	 heap, in which case the heap array will already be big enough.  */
+      if (vec_safe_length (array.heap) <= i)
+	vec_safe_grow (array.heap, i + 1);
       base = array.heap->address ();
       memcpy (base, array.stack, sizeof (array.stack));
       base[LOCAL_ELEMS] = x;
--- a/src/gcc/sched-deps.c
+++ b/src/gcc/sched-deps.c
@@ -2856,7 +2856,7 @@ sched_analyze_2 (struct deps_desc *deps, rtx x, rtx_insn *insn)
     sched_deps_info->finish_rhs ();
 }
 
-/* Try to group two fuseable insns together to prevent scheduler
+/* Try to group two fusible insns together to prevent scheduler
    from scheduling them apart.  */
 
 static void
--- a/src/gcc/simplify-rtx.c
+++ b/src/gcc/simplify-rtx.c
@@ -978,6 +978,32 @@ simplify_unary_operation_1 (enum rtx_code code, machine_mode mode, rtx op)
       if (GET_CODE (op) == NEG)
 	return XEXP (op, 0);
 
+      /* (neg (x ? (neg y) : y)) == !x ? (neg y) : y.
+	 If comparison is not reversible use
+	 x ? y : (neg y).  */
+      if (GET_CODE (op) == IF_THEN_ELSE)
+	{
+	  rtx cond = XEXP (op, 0);
+	  rtx true_rtx = XEXP (op, 1);
+	  rtx false_rtx = XEXP (op, 2);
+
+	  if ((GET_CODE (true_rtx) == NEG
+	       && rtx_equal_p (XEXP (true_rtx, 0), false_rtx))
+	       || (GET_CODE (false_rtx) == NEG
+		   && rtx_equal_p (XEXP (false_rtx, 0), true_rtx)))
+	    {
+	      if (reversed_comparison_code (cond, NULL_RTX) != UNKNOWN)
+		temp = reversed_comparison (cond, mode);
+	      else
+		{
+		  temp = cond;
+		  std::swap (true_rtx, false_rtx);
+		}
+	      return simplify_gen_ternary (IF_THEN_ELSE, mode,
+					    mode, temp, true_rtx, false_rtx);
+	    }
+	}
+
       /* (neg (plus X 1)) can become (not X).  */
       if (GET_CODE (op) == PLUS
 	  && XEXP (op, 1) == const1_rtx)
@@ -1171,7 +1197,7 @@ simplify_unary_operation_1 (enum rtx_code code, machine_mode mode, rtx op)
          = (float_truncate:SF foo:DF).
 
          (float_truncate:DF (float_extend:XF foo:SF))
-         = (float_extend:SF foo:DF).  */
+         = (float_extend:DF foo:SF).  */
       if ((GET_CODE (op) == FLOAT_TRUNCATE
 	   && flag_unsafe_math_optimizations)
 	  || GET_CODE (op) == FLOAT_EXTEND)
@@ -1183,14 +1209,14 @@ simplify_unary_operation_1 (enum rtx_code code, machine_mode mode, rtx op)
 				   XEXP (op, 0), mode);
 
       /*  (float_truncate (float x)) is (float x)  */
-      if (GET_CODE (op) == FLOAT
+      if ((GET_CODE (op) == FLOAT || GET_CODE (op) == UNSIGNED_FLOAT)
 	  && (flag_unsafe_math_optimizations
 	      || (SCALAR_FLOAT_MODE_P (GET_MODE (op))
 		  && ((unsigned)significand_size (GET_MODE (op))
 		      >= (GET_MODE_PRECISION (GET_MODE (XEXP (op, 0)))
 			  - num_sign_bit_copies (XEXP (op, 0),
 						 GET_MODE (XEXP (op, 0))))))))
-	return simplify_gen_unary (FLOAT, mode,
+	return simplify_gen_unary (GET_CODE (op), mode,
 				   XEXP (op, 0),
 				   GET_MODE (XEXP (op, 0)));
 
@@ -1221,7 +1247,7 @@ simplify_unary_operation_1 (enum rtx_code code, machine_mode mode, rtx op)
 	  rounding can't happen.
           */
       if (GET_CODE (op) == FLOAT_EXTEND
-	  || (GET_CODE (op) == FLOAT
+	  || ((GET_CODE (op) == FLOAT || GET_CODE (op) == UNSIGNED_FLOAT)
 	      && SCALAR_FLOAT_MODE_P (GET_MODE (op))
 	      && ((unsigned)significand_size (GET_MODE (op))
 		  >= (GET_MODE_PRECISION (GET_MODE (XEXP (op, 0)))
--- a/src/gcc/stmt.c
+++ b/src/gcc/stmt.c
@@ -342,13 +342,7 @@ parse_output_constraint (const char **constraint_p, int operand_num,
 	else if (insn_extra_memory_constraint (cn))
 	  *allows_mem = true;
 	else
-	  {
-	    /* Otherwise we can't assume anything about the nature of
-	       the constraint except that it isn't purely registers.
-	       Treat it like "g" and hope for the best.  */
-	    *allows_reg = true;
-	    *allows_mem = true;
-	  }
+	  insn_extra_constraint_allows_reg_mem (cn, allows_reg, allows_mem);
 	break;
       }
 
@@ -465,13 +459,7 @@ parse_input_constraint (const char **constraint_p, int input_num,
 	else if (insn_extra_memory_constraint (cn))
 	  *allows_mem = true;
 	else
-	  {
-	    /* Otherwise we can't assume anything about the nature of
-	       the constraint except that it isn't purely registers.
-	       Treat it like "g" and hope for the best.  */
-	    *allows_reg = true;
-	    *allows_mem = true;
-	  }
+	  insn_extra_constraint_allows_reg_mem (cn, allows_reg, allows_mem);
 	break;
       }
 
--- a/src/gcc/target.def
+++ b/src/gcc/target.def
@@ -1975,7 +1975,7 @@ merging.",
 DEFHOOKPOD
 (attribute_table,
  "If defined, this target hook points to an array of @samp{struct\n\
-attribute_spec} (defined in @file{tree.h}) specifying the machine\n\
+attribute_spec} (defined in @file{tree-core.h}) specifying the machine\n\
 specific attributes for this target and some of the restrictions on the\n\
 entities to which these attributes are applied and the arguments they\n\
 take.",
--- a/src/gcc/testsuite/c-c++-common/Wsizeof-pointer-memaccess1.c
+++ b/src/gcc/testsuite/c-c++-common/Wsizeof-pointer-memaccess1.c
@@ -1,6 +1,7 @@
 /* Test -Wsizeof-pointer-memaccess warnings.  */
 /* { dg-do compile } */
 /* { dg-options "-Wall -Wno-sizeof-array-argument" } */
+/* { dg-options "-Wall -Wno-sizeof-array-argument -Wno-c++-compat" { target c } } */
 
 typedef __SIZE_TYPE__ size_t;
 #ifdef __cplusplus
--- a/src/gcc/testsuite/c-c++-common/Wsizeof-pointer-memaccess2.c
+++ b/src/gcc/testsuite/c-c++-common/Wsizeof-pointer-memaccess2.c
@@ -1,6 +1,7 @@
 /* Test -Wsizeof-pointer-memaccess warnings.  */
 /* { dg-do compile } */
 /* { dg-options "-Wall -O2 -Wno-sizeof-array-argument" } */
+/* { dg-options "-Wall -O2 -Wno-sizeof-array-argument -Wno-c++-compat" {target c} } */
 
 #define bos(ptr) __builtin_object_size (ptr, 1)
 #define bos0(ptr) __builtin_object_size (ptr, 0)
--- a/src/gcc/testsuite/c-c++-common/pr58346-1.c
+++ b/src/gcc/testsuite/c-c++-common/pr58346-1.c
@@ -1,5 +1,6 @@
 /* PR c/58346 */
 /* { dg-do compile } */
+/* { dg-options "-Wno-c++-compat" { target c } } */
 
 struct U {
 #ifdef __cplusplus
--- a/src/gcc/testsuite/c-c++-common/transparent-union-1.c
+++ b/src/gcc/testsuite/c-c++-common/transparent-union-1.c
@@ -1,4 +1,5 @@
 /* PR c++/51228 */
+/* { dg-options "-Wno-c++-compat" { target c } } */
 
 typedef union {} U __attribute__((transparent_union)); /* { dg-warning "ignored" } */
 
--- a/src/gcc/testsuite/g++.dg/ext/pr57735.C
+++ b/src/gcc/testsuite/g++.dg/ext/pr57735.C
@@ -1,4 +1,7 @@
 /* { dg-do compile { target arm*-*-* } } */
+/* { dg-require-effective-target arm_arch_v5te_ok } */
+/* { dg-require-effective-target arm_arm_ok } */
+/* { dg-skip-if "do not override -mfloat-abi" { *-*-* } { "-mfloat-abi=*" } {"-mfloat-abi=soft" } } */
 /* { dg-options "-march=armv5te -marm  -mtune=xscale -mfloat-abi=soft -O1" } */
 
 typedef unsigned int size_t;
--- a/src//dev/null
+++ b/src/gcc/testsuite/g++.dg/tree-ssa/pr66726.c
@@ -0,0 +1,36 @@
+
+/* { dg-do run } */
+/* { dg-options "-O2" } */
+
+/* Execution test for converting VIEW_CONVERT_EXPR.  */
+
+struct cpp_num {
+  bool f;
+};
+
+extern cpp_num  __attribute__((noinline))
+foo (cpp_num lhs,
+     cpp_num rhs)
+{
+  lhs.f = lhs.f || rhs.f;
+  return lhs;
+}
+
+cpp_num lhs, rhs, r;
+
+int main ()
+{
+
+  lhs.f = false;
+  rhs.f = false;
+  r = foo (lhs, rhs);
+  if (r.f)
+    __builtin_abort ();
+
+
+  lhs.f = false;
+  rhs.f = true;
+  r = foo (lhs, rhs);
+  if (!r.f)
+    __builtin_abort ();
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.c-torture/compile/pr66168.c
@@ -0,0 +1,15 @@
+int a, b;
+
+void
+fn1 ()
+{
+  for (;;)
+    {
+      for (b = 0; b < 3; b++)
+	{
+	  char e[2];
+	  char f = e[1];
+	  a ^= f ? 1 / f : 0;
+	}
+    }
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.c-torture/execute/pr65648.c
@@ -0,0 +1,34 @@
+/* PR target/65648 */
+
+int a = 0, *b = 0, c = 0;
+static int d = 0;
+short e = 1;
+static long long f = 0;
+long long *i = &f;
+unsigned char j = 0;
+
+__attribute__((noinline, noclone)) void
+foo (int x, int *y)
+{
+  asm volatile ("" : : "r" (x), "r" (y) : "memory");
+}
+
+__attribute__((noinline, noclone)) void
+bar (const char *x, long long y)
+{
+  asm volatile ("" : : "r" (x), "r" (&y) : "memory");
+  if (y != 0)
+    __builtin_abort ();
+}
+
+int
+main ()
+{
+  int k = 0;
+  b = &k;
+  j = (!a) - (c <= e);
+  *i = j;
+  foo (a, &k);
+  bar ("", f);
+  return 0;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.dg/Wcxx-compat-22.c
@@ -0,0 +1,8 @@
+/* { dg-do compile } */
+/* { dg-options "-Wc++-compat" } */
+struct A {}; /* { dg-warning "empty struct has size 0 in C" } */
+union B {}; /* { dg-warning "empty union has size 0 in C" } */
+struct C { struct D {}; int x; }; /* { dg-warning "empty struct has size 0 in C|declaration does not declare anything" } */
+struct E { union F {}; int x; }; /* { dg-warning "empty union has size 0 in C|declaration does not declare anything" } */
+union G { union H {}; int x; }; /* { dg-warning "empty union has size 0 in C|declaration does not declare anything" } */
+union I { struct J {}; int x; }; /* { dg-warning "empty struct has size 0 in C|declaration does not declare anything" } */
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.dg/dx-test.c
@@ -0,0 +1,5 @@
+/* { dg-do compile } */
+/* { dg-options "-dx" } */
+
+void f(void)
+{}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.dg/loop-8.c
@@ -0,0 +1,24 @@
+/* { dg-do compile } */
+/* { dg-options "-O1 -fdump-rtl-loop2_invariant" } */
+
+void
+f (int *a, int *b)
+{
+  int i;
+
+  for (i = 0; i < 100; i++)
+    {
+      int d = 42;
+
+      a[i] = d;
+      if (i % 2)
+	d = i;
+      b[i] = d;
+    }
+}
+
+/* Load of 42 is moved out of the loop, introducing a new pseudo register.  */
+/* { dg-final { scan-rtl-dump-times "Decided" 1 "loop2_invariant" } } */
+/* { dg-final { scan-rtl-dump-not "without introducing a new temporary register" "loop2_invariant" } } */
+/* { dg-final { cleanup-rtl-dump "loop2_invariant" } } */
+
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.dg/loop-9.c
@@ -0,0 +1,16 @@
+/* { dg-do compile } */
+/* { dg-options "-O1 -fdump-rtl-loop2_invariant" } */
+
+void
+f (double *a)
+{
+  int i;
+  for (i = 0; i < 100; i++)
+    a[i] = 18.4242;
+}
+
+/* Load of x is moved out of the loop.  */
+/* { dg-final { scan-rtl-dump "Decided" "loop2_invariant" } } */
+/* { dg-final { scan-rtl-dump "without introducing a new temporary register" "loop2_invariant" } } */
+/* { dg-final { cleanup-rtl-dump "loop2_invariant" } } */
+
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.dg/loop-invariant.c
@@ -0,0 +1,43 @@
+/* { dg-do compile { target x86_64-*-* } } */
+/* { dg-options "-O2 -fdump-rtl-loop2_invariant" } */
+/* NOTE: The target list above could be extended to other targets that have
+         conditional moves, but don't have zero registers.  */
+
+enum test_type
+{
+  TYPE0,
+  TYPE1
+};
+
+struct type_node
+{
+  enum test_type type;
+};
+
+struct test_ref
+{
+  struct type_node *referring;
+};
+
+struct test_node
+{
+  struct test_node *next;
+};
+
+int iterate (struct test_node *, unsigned, struct test_ref **);
+
+int
+loop_invar (struct test_node *node)
+{
+  struct test_ref *ref;
+
+  for (unsigned i = 0; iterate (node, i, &ref); i++)
+    if (loop_invar ((ref->referring && ref->referring->type == TYPE0)
+                    ? ((struct test_node *) (ref->referring)) : 0))
+      return 1;
+
+  return 0;
+}
+
+/* { dg-final { scan-rtl-dump "Decided to move invariant" "loop2_invariant" } } */
+/* { dg-final { cleanup-rtl-dump "loop2_invariant" } } */
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.dg/pow-sqrt-1.c
@@ -0,0 +1,6 @@
+/* { dg-do run } */
+/* { dg-options "-O2 -ffast-math --param max-pow-sqrt-depth=5" } */
+
+#define EXPN (-6 * (0.5*0.5*0.5*0.5))
+
+#include "pow-sqrt.x"
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.dg/pow-sqrt-2.c
@@ -0,0 +1,5 @@
+/* { dg-do run } */
+/* { dg-options "-O2 -ffast-math --param max-pow-sqrt-depth=5" } */
+
+#define EXPN (-5.875)
+#include "pow-sqrt.x"
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.dg/pow-sqrt-3.c
@@ -0,0 +1,5 @@
+/* { dg-do run } */
+/* { dg-options "-O2 -ffast-math --param max-pow-sqrt-depth=3" } */
+
+#define EXPN (1.25)
+#include "pow-sqrt.x"
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.dg/pow-sqrt-synth-1.c
@@ -0,0 +1,38 @@
+/* { dg-do compile { target sqrt_insn } } */
+/* { dg-options "-fdump-tree-sincos -Ofast --param max-pow-sqrt-depth=8" } */
+/* { dg-additional-options "-mfloat-abi=softfp -mfpu=neon-vfpv4" { target arm*-*-* } } */
+
+double
+foo (double a)
+{
+  return __builtin_pow (a, -5.875);
+}
+
+double
+foof (double a)
+{
+  return __builtin_pow (a, 0.75f);
+}
+
+double
+bar (double a)
+{
+  return __builtin_pow (a, 1.0 + 0.00390625);
+}
+
+double
+baz (double a)
+{
+  return __builtin_pow (a, -1.25) + __builtin_pow (a, 5.75) - __builtin_pow (a, 3.375);
+}
+
+#define N 256
+void
+vecfoo (double *a)
+{
+  for (int i = 0; i < N; i++)
+    a[i] = __builtin_pow (a[i], 1.25);
+}
+
+/* { dg-final { scan-tree-dump-times "synthesizing" 7 "sincos" } } */
+/* { dg-final { cleanup-tree-dump "sincos" } } */
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.dg/pow-sqrt.x
@@ -0,0 +1,30 @@
+
+extern void abort (void);
+
+
+__attribute__((noinline)) double
+real_pow (double x, double pow_exp)
+{
+  return __builtin_pow (x, pow_exp);
+}
+
+#define EPS (0.000000000000000000001)
+
+#define SYNTH_POW(X, Y) __builtin_pow (X, Y)
+volatile double arg;
+
+int
+main (void)
+{
+  double i_arg = 0.1;
+
+  for (arg = i_arg; arg < 100.0; arg += 1.0)
+    {
+      double synth_res = SYNTH_POW (arg, EXPN);
+      double real_res = real_pow (arg, EXPN);
+
+      if (__builtin_abs (SYNTH_POW (arg, EXPN) - real_pow (arg, EXPN)) > EPS)
+	abort ();
+    }
+  return 0;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.dg/pr49551.c
@@ -0,0 +1,7 @@
+/* { dg-do compile } */
+/* { dg-options "-O -fdata-sections" } */
+
+int x = 1;
+int x;
+
+/* { dg-final { scan-assembler-not {comm[\t ]+x} } } */
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.dg/pr67043.c
@@ -0,0 +1,32 @@
+/* { dg-do compile } */
+/* { dg-options "-O3 -fcompare-debug -w" } */
+
+extern void rt_mutex_owner (void);
+extern void rt_mutex_deadlock_account_lock (int);
+extern void signal_pending (void);
+__typeof__ (int *) a;
+int b;
+
+int
+try_to_take_rt_mutex (int p1) {
+  rt_mutex_owner ();
+  if (b)
+    return 0;
+  rt_mutex_deadlock_account_lock (p1);
+  return 1;
+}
+
+void
+__rt_mutex_slowlock (int p1) {
+  int c;
+  for (;;) {
+    c = ({
+      asm ("" : "=r"(a));
+      a;
+    });
+    if (try_to_take_rt_mutex (c))
+      break;
+    if (__builtin_expect (p1 == 0, 0))
+      signal_pending ();
+  }
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.dg/torture/pr66076.c
@@ -0,0 +1,11 @@
+/* { dg-do compile } */
+/* { dg-options "" } */
+/* { dg-options "-mno-prefer-avx128 -march=bdver4" { target i?86-*-* x86_64-*-* } } */
+
+void
+f0a (char *result, char *arg1, char *arg4, char temp_6)
+{
+  int idx = 0;
+  for (idx = 0; idx < 416; idx += 1)
+    result[idx] = (arg1[idx] + arg4[idx]) * temp_6;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.dg/tree-ssa/pr64130.c
@@ -0,0 +1,18 @@
+
+/* { dg-do compile } */
+/* { dg-options "-O2 -fdump-tree-vrp1" } */
+
+int funsigned (unsigned a)
+{
+  return 0x1ffffffffL / a == 0;
+}
+
+int funsigned2 (unsigned a)
+{
+  if (a < 1) return 1;
+  return (-1 * 0x1ffffffffL) / a == 0;
+}
+
+/* { dg-final { scan-tree-dump ": \\\[2, 8589934591\\\]" "vrp1" } } */
+/* { dg-final { scan-tree-dump ": \\\[-8589934591, -2\\\]" "vrp1" } } */
+
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.dg/tree-ssa/pr65447.c
@@ -0,0 +1,54 @@
+/* { dg-do compile } */
+/* { dg-options "-O2 -fdump-tree-ivopts-details" } */
+
+void foo (double *p)
+{
+  int i;
+  for (i = -20000; i < 200000; i+= 40)
+    {
+      p[i+0] = 1.0;
+      p[i+1] = 1.0;
+      p[i+2] = 1.0;
+      p[i+3] = 1.0;
+      p[i+4] = 1.0;
+      p[i+5] = 1.0;
+      p[i+6] = 1.0;
+      p[i+7] = 1.0;
+      p[i+8] = 1.0;
+      p[i+9] = 1.0;
+      p[i+10] = 1.0;
+      p[i+11] = 1.0;
+      p[i+12] = 1.0;
+      p[i+13] = 1.0;
+      p[i+14] = 1.0;
+      p[i+15] = 1.0;
+      p[i+16] = 1.0;
+      p[i+17] = 1.0;
+      p[i+18] = 1.0;
+      p[i+19] = 1.0;
+      p[i+20] = 1.0;
+      p[i+21] = 1.0;
+      p[i+22] = 1.0;
+      p[i+23] = 1.0;
+      p[i+24] = 1.0;
+      p[i+25] = 1.0;
+      p[i+26] = 1.0;
+      p[i+27] = 1.0;
+      p[i+28] = 1.0;
+      p[i+29] = 1.0;
+      p[i+30] = 1.0;
+      p[i+31] = 1.0;
+      p[i+32] = 1.0;
+      p[i+33] = 1.0;
+      p[i+34] = 1.0;
+      p[i+35] = 1.0;
+      p[i+36] = 1.0;
+      p[i+37] = 1.0;
+      p[i+38] = 1.0;
+      p[i+39] = 1.0;
+    }
+}
+
+/* We should groups address type IV uses.  */
+/* { dg-final { scan-tree-dump-not "\\nuse 2\\n" "ivopts" } }  */
+/* { dg-final { cleanup-tree-dump "ivopts" } }  */
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.dg/tree-ssa/pr66726-2.c
@@ -0,0 +1,19 @@
+
+/* { dg-do compile } */
+/* { dg-options "-O2 -fdump-tree-phiopt1-details" } */
+
+extern void bar (char, char);
+int
+foo (char b)
+{
+  char a;
+  a = b;
+  b = 'b';
+  bar (a, b);
+  b = a;
+  if (b == 0)
+    a++;
+  return a + b;
+}
+
+/* { dg-final { scan-tree-dump-times "factor conversion out" 0 "phiopt1" } } */
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.dg/tree-ssa/pr66726.c
@@ -0,0 +1,15 @@
+
+/* { dg-do compile } */
+/* { dg-options "-O2 -fdump-tree-phiopt1-details" } */
+
+extern unsigned short mode_size[];
+
+int
+oof (int mode)
+{
+  return (64 < mode_size[mode] ? 64 : mode_size[mode]);
+}
+
+/* { dg-final { scan-tree-dump-times "factor conversion out" 1 "phiopt1" } } */
+/* { dg-final { scan-tree-dump-times "MIN_EXPR" 1 "phiopt1" } } */
+
--- a/src/gcc/testsuite/gcc.dg/vect/pr59354.c
+++ b/src/gcc/testsuite/gcc.dg/vect/pr59354.c
@@ -1,4 +1,3 @@
-/* { dg-do run } */
 /* { dg-additional-options "-O3" } */
 
 #include "tree-vect.h"
--- a/src/gcc/testsuite/gcc.dg/vect/pr64252.c
+++ b/src/gcc/testsuite/gcc.dg/vect/pr64252.c
@@ -1,6 +1,5 @@
 /* PR target/64252 */
 /* Test correctness of size 3 store groups permutation.  */
-/* { dg-do run } */
 /* { dg-additional-options "-O3" } */
 /* { dg-additional-options "-mavx" { target avx_runtime } } */
 
--- a/src/gcc/testsuite/gcc.dg/vect/pr64404.c
+++ b/src/gcc/testsuite/gcc.dg/vect/pr64404.c
@@ -1,4 +1,3 @@
-/* { dg-do run } */
 /* { dg-additional-options "--param=sccvn-max-alias-queries-per-access=1" } */
 
 #include "tree-vect.h"
--- a/src/gcc/testsuite/gcc.dg/vect/pr64493.c
+++ b/src/gcc/testsuite/gcc.dg/vect/pr64493.c
@@ -1,5 +1,3 @@
-/* { dg-do run } */
-
 #include "tree-vect.h"
 
 int a, b, c, d, e, f, g, h;
--- a/src/gcc/testsuite/gcc.dg/vect/pr64495.c
+++ b/src/gcc/testsuite/gcc.dg/vect/pr64495.c
@@ -1,5 +1,3 @@
-/* { dg-do run } */
-
 #include <assert.h>
 #include "tree-vect.h"
 
--- a/src/gcc/testsuite/gcc.dg/vect/pr64844.c
+++ b/src/gcc/testsuite/gcc.dg/vect/pr64844.c
@@ -1,4 +1,3 @@
-/* { dg-do run } */
 /* { dg-require-effective-target vect_double } */
 /* { dg-additional-options "-ffast-math" } */
 
--- a/src/gcc/testsuite/gcc.dg/vect/pr65518.c
+++ b/src/gcc/testsuite/gcc.dg/vect/pr65518.c
@@ -1,5 +1,3 @@
-/* { dg-do run } */
-
 extern void abort (void);
 
 typedef struct giga
--- a/src/gcc/testsuite/gcc.dg/vect/vect-aggressive-1.c
+++ b/src/gcc/testsuite/gcc.dg/vect/vect-aggressive-1.c
@@ -1,4 +1,3 @@
-/* { dg-do run } */
 /* { dg-require-effective-target vect_condition } */
 /* { dg-require-effective-target vect_simd_clones } */
 /* { dg-additional-options "-fopenmp-simd" } */
--- a/src/gcc/testsuite/gcc.target/aarch64/aapcs64/func-ret-1.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/aapcs64/func-ret-1.c
@@ -12,6 +12,8 @@
 
 vf2_t vf2 = (vf2_t){ 17.f, 18.f };
 vi4_t vi4 = (vi4_t){ 0xdeadbabe, 0xbabecafe, 0xcafebeef, 0xbeefdead };
+vlf1_t vlf1 = (vlf1_t) { 17.0 };
+
 union int128_t qword;
 
 int *int_ptr = (int *)0xabcdef0123456789ULL;
@@ -41,4 +43,5 @@ FUNC_VAL_CHECK (11,   long double, 98765432123456789.987654321L, Q0, flat)
 FUNC_VAL_CHECK (12,         vf2_t,        vf2, D0, f32in64)
 FUNC_VAL_CHECK (13,         vi4_t,        vi4, Q0, i32in128)
 FUNC_VAL_CHECK (14,         int *,    int_ptr, X0, flat)
+FUNC_VAL_CHECK (15,         vlf1_t,    vlf1, Q0, flat)
 #endif
--- a/src/gcc/testsuite/gcc.target/aarch64/aapcs64/type-def.h
+++ b/src/gcc/testsuite/gcc.target/aarch64/aapcs64/type-def.h
@@ -10,6 +10,9 @@ typedef float vf4_t __attribute__((vector_size (16)));
 /* 128-bit vector of 4 ints.  */
 typedef int vi4_t __attribute__((vector_size (16)));
 
+/* 128-bit vector of 1 quad precision float.  */
+typedef long double vlf1_t __attribute__((vector_size (16)));
+
 /* signed quad-word (in an union for the convenience of initialization).  */
 union int128_t
 {
--- a/src/gcc/testsuite/gcc.target/aarch64/abs_1.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/abs_1.c
@@ -7,15 +7,14 @@ extern void abort (void);
 long long
 abs64 (long long a)
 {
-  /* { dg-final { scan-assembler "eor\t" } } */
-  /* { dg-final { scan-assembler "sub\t" } } */
+  /* { dg-final { scan-assembler "csneg\t" } } */
   return llabs (a);
 }
 
 long long
 abs64_in_dreg (long long a)
 {
-  /* { dg-final { scan-assembler "abs\td\[0-9\]+, d\[0-9\]+" } } */
+  /* { dg-final { scan-assembler "csneg\t" } } */
   register long long x asm ("d8") = a;
   register long long y asm ("d9");
   asm volatile ("" : : "w" (x));
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/advsimd-intrinsics.exp
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/advsimd-intrinsics.exp
@@ -27,14 +27,26 @@ load_lib gcc-dg.exp
 
 # Initialize `dg'.
 load_lib c-torture.exp
-load_lib target-supports.exp
-load_lib torture-options.exp
 
 dg-init
 
-if {[istarget arm*-*-*]
-    && ![check_effective_target_arm_neon_ok]} then {
-  return
+# The default action for a test is 'run'.  Save current default.
+global dg-do-what-default
+set save-dg-do-what-default ${dg-do-what-default}
+
+# For ARM, make sure that we have a target compatible with NEON, and do
+# not attempt to run execution tests if the hardware doesn't support it.
+if {[istarget arm*-*-*]} then {
+    if {![check_effective_target_arm_neon_ok]} then {
+      return
+    }
+    if {![is-effective-target arm_neon_hw]} then {
+        set dg-do-what-default compile
+    } else {
+        set dg-do-what-default run
+    }
+} else {
+    set dg-do-what-default run
 }
 
 torture-init
@@ -44,22 +56,10 @@ set-torture-options $C_TORTURE_OPTIONS {{}} $LTO_TORTURE_OPTIONS
 set additional_flags [add_options_for_arm_neon ""]
 
 # Main loop.
-foreach src [lsort [glob -nocomplain $srcdir/$subdir/*.c]] {
-    # If we're only testing specific files and this isn't one of them, skip it.
-    if ![runtest_file_p $runtests $src] then {
-	continue
-    }
-
-    # runtest_file_p is already run above, and the code below can run
-    # runtest_file_p again, make sure everything for this test is
-    # performed if the above runtest_file_p decided this runtest
-    # instance should execute the test
-    gcc_parallel_test_enable 0
-    c-torture-execute $src $additional_flags
-    gcc-dg-runtest $src "" $additional_flags
-    gcc_parallel_test_enable 1
-}
+gcc-dg-runtest [lsort [glob -nocomplain $srcdir/$subdir/*.c]] \
+	       "" ${additional_flags}
 
 # All done.
+set dg-do-what-default ${save-dg-do-what-default}
 torture-finish
 dg-finish
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/arm-neon-ref.h
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/arm-neon-ref.h
@@ -235,7 +235,8 @@ extern ARRAY(expected, hfloat, 64, 2);
 
 typedef union {
   struct {
-    int _xxx:25;
+    int _xxx:24;
+    unsigned int FZ:1;
     unsigned int DN:1;
     unsigned int AHP:1;
     unsigned int QC:1;
@@ -258,7 +259,8 @@ typedef union {
     unsigned int QC:1;
     unsigned int AHP:1;
     unsigned int DN:1;
-    int _dnm:25;
+    unsigned int FZ:1;
+    int _dnm:24;
   } b;
   unsigned int word;
 } _ARM_FPSCR;
@@ -395,10 +397,15 @@ static void clean_results (void)
 #if defined(__aarch64__)
   /* On AArch64, make sure to return DefaultNaN to have the same
      results as on AArch32.  */
-  _ARM_FPSCR _afpscr_for_dn;
-  asm volatile ("mrs %0,fpcr" : "=r" (_afpscr_for_dn));
-  _afpscr_for_dn.b.DN = 1;
-  asm volatile ("msr fpcr,%0" : : "r" (_afpscr_for_dn));
+  _ARM_FPSCR _afpscr;
+  asm volatile ("mrs %0,fpcr" : "=r" (_afpscr));
+  _afpscr.b.DN = 1;
+
+  /* On AArch64, make sure to flush to zero by default, as on
+     AArch32. */
+  _afpscr.b.FZ = 1;
+
+  asm volatile ("msr fpcr,%0" : : "r" (_afpscr));
 #endif
 }
 
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/binary_op.inc
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/binary_op.inc
@@ -55,7 +55,22 @@ void FNNAME (INSN_NAME) (void)
   /* Apply a binary operator named INSN_NAME.  */
   TEST_MACRO_ALL_VARIANTS_1_5(TEST_BINARY_OP, INSN_NAME);
 
-  CHECK_RESULTS (TEST_MSG, "");
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected, "");
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected, "");
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected, "");
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected, "");
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected, "");
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected, "");
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected, "");
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected, "");
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected, "");
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected, "");
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected, "");
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected, "");
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected, "");
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected, "");
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected, "");
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected, "");
 
 #ifdef EXTRA_TESTS
   EXTRA_TESTS();
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/binary_sat_op.inc
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/binary_sat_op.inc
@@ -76,7 +76,22 @@ void FNNAME (INSN_NAME) (void)
   TEST_BINARY_SAT_OP(INSN_NAME, q, uint, u, 32, 4, expected_cumulative_sat, "");
   TEST_BINARY_SAT_OP(INSN_NAME, q, uint, u, 64, 2, expected_cumulative_sat, "");
 
-  CHECK_RESULTS (TEST_MSG, "");
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected, "");
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected, "");
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected, "");
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected, "");
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected, "");
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected, "");
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected, "");
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected, "");
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected, "");
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected, "");
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected, "");
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected, "");
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected, "");
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected, "");
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected, "");
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected, "");
 
 #ifdef EXTRA_TESTS
   EXTRA_TESTS();
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/unary_op.inc
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/unary_op.inc
@@ -57,7 +57,12 @@ void FNNAME (INSN_NAME) (void)
   TEST_UNARY_OP(INSN_NAME, q, int, s, 16, 8);
   TEST_UNARY_OP(INSN_NAME, q, int, s, 32, 4);
 
-  CHECK_RESULTS (TEST_MSG, "");
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected, "");
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected, "");
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected, "");
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected, "");
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected, "");
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected, "");
 
 #ifdef EXTRA_TESTS
   EXTRA_TESTS();
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vXXXl.inc
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vXXXl.inc
@@ -60,7 +60,12 @@ void FNNAME (INSN_NAME) (void)
   TEST_VADDL(INSN_NAME, uint, u, 16, 32, 4);
   TEST_VADDL(INSN_NAME, uint, u, 32, 64, 2);
 
-  CHECK_RESULTS (TEST_MSG, "");
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected, "");
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected, "");
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected, "");
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected, "");
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected, "");
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected, "");
 }
 
 int main (void)
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vXXXw.inc
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vXXXw.inc
@@ -60,7 +60,12 @@ void FNNAME (INSN_NAME) (void)
   TEST_VADDW(INSN_NAME, uint, u, 16, 32, 4);
   TEST_VADDW(INSN_NAME, uint, u, 32, 64, 2);
 
-  CHECK_RESULTS (TEST_MSG, "");
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected, "");
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected, "");
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected, "");
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected, "");
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected, "");
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected, "");
 }
 
 int main (void)
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vaba.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vaba.c
@@ -7,16 +7,10 @@ VECT_VAR_DECL(expected,int,8,8) [] = { 0xf6, 0xf7, 0xf8, 0xf9,
 				       0xfa, 0xfb, 0xfc, 0xfd };
 VECT_VAR_DECL(expected,int,16,4) [] = { 0x16, 0x17, 0x18, 0x19 };
 VECT_VAR_DECL(expected,int,32,2) [] = { 0x20, 0x21 };
-VECT_VAR_DECL(expected,int,64,1) [] = { 0x3333333333333333 };
 VECT_VAR_DECL(expected,uint,8,8) [] = { 0x53, 0x54, 0x55, 0x56,
 					0x57, 0x58, 0x59, 0x5a };
 VECT_VAR_DECL(expected,uint,16,4) [] = { 0x907, 0x908, 0x909, 0x90a };
 VECT_VAR_DECL(expected,uint,32,2) [] = { 0xffffffe7, 0xffffffe8 };
-VECT_VAR_DECL(expected,uint,64,1) [] = { 0x3333333333333333 };
-VECT_VAR_DECL(expected,poly,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,2) [] = { 0x33333333, 0x33333333 };
 VECT_VAR_DECL(expected,int,8,16) [] = { 0x5e, 0x5f, 0x60, 0x61,
 					0x62, 0x63, 0x64, 0x65,
 					0x66, 0x67, 0x68, 0x69,
@@ -24,8 +18,6 @@ VECT_VAR_DECL(expected,int,8,16) [] = { 0x5e, 0x5f, 0x60, 0x61,
 VECT_VAR_DECL(expected,int,16,8) [] = { 0xb9c, 0xb9d, 0xb9e, 0xb9f,
 					0xba0, 0xba1, 0xba2, 0xba3 };
 VECT_VAR_DECL(expected,int,32,4) [] = { 0x26e0, 0x26e1, 0x26e2, 0x26e3 };
-VECT_VAR_DECL(expected,int,64,2) [] = { 0x3333333333333333,
-					0x3333333333333333 };
 VECT_VAR_DECL(expected,uint,8,16) [] = { 0xf8, 0xf9, 0xfa, 0xfb,
 					 0xfc, 0xfd, 0xfe, 0xff,
 					 0x0, 0x1, 0x2, 0x3,
@@ -33,16 +25,6 @@ VECT_VAR_DECL(expected,uint,8,16) [] = { 0xf8, 0xf9, 0xfa, 0xfb,
 VECT_VAR_DECL(expected,uint,16,8) [] = { 0xfff9, 0xfffa, 0xfffb, 0xfffc,
 					 0xfffd, 0xfffe, 0xffff, 0x0 };
 VECT_VAR_DECL(expected,uint,32,4) [] = { 0xc, 0xd, 0xe, 0xf };
-VECT_VAR_DECL(expected,uint,64,2) [] = { 0x3333333333333333,
-					 0x3333333333333333 };
-VECT_VAR_DECL(expected,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,4) [] = { 0x33333333, 0x33333333,
-					   0x33333333, 0x33333333 };
 
 #define TEST_MSG "VABA/VABAQ"
 void exec_vaba (void)
@@ -132,7 +114,18 @@ void exec_vaba (void)
   TEST_VABA(q, uint, u, 16, 8);
   TEST_VABA(q, uint, u, 32, 4);
 
-  CHECK_RESULTS (TEST_MSG, "");
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected, "");
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected, "");
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected, "");
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected, "");
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected, "");
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected, "");
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected, "");
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected, "");
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected, "");
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected, "");
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected, "");
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected, "");
 }
 
 int main (void)
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vabal.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vabal.c
@@ -3,45 +3,15 @@
 #include "compute-ref-data.h"
 
 /* Expected results.  */
-VECT_VAR_DECL(expected,int,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-				       0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,int,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,int,32,2) [] = { 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,int,64,1) [] = { 0x3333333333333333 };
-VECT_VAR_DECL(expected,uint,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,uint,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,uint,32,2) [] = { 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,uint,64,1) [] = { 0x3333333333333333 };
-VECT_VAR_DECL(expected,poly,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,2) [] = { 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,int,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
 VECT_VAR_DECL(expected,int,16,8) [] = { 0xfff6, 0xfff7, 0xfff8, 0xfff9,
 					0xfffa, 0xfffb, 0xfffc, 0xfffd };
 VECT_VAR_DECL(expected,int,32,4) [] = { 0x16, 0x17, 0x18, 0x19 };
 VECT_VAR_DECL(expected,int,64,2) [] = { 0x20, 0x21 };
-VECT_VAR_DECL(expected,uint,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
 VECT_VAR_DECL(expected,uint,16,8) [] = { 0x53, 0x54, 0x55, 0x56,
 					 0x57, 0x58, 0x59, 0x5a };
 VECT_VAR_DECL(expected,uint,32,4) [] = { 0x907, 0x908, 0x909, 0x90a };
 VECT_VAR_DECL(expected,uint,64,2) [] = { 0xffffffe7,
 					 0xffffffe8 };
-VECT_VAR_DECL(expected,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,4) [] = { 0x33333333, 0x33333333,
-					   0x33333333, 0x33333333 };
 
 /* Expected results for cases with input values chosen to test
    possible intermediate overflow.  */
@@ -121,7 +91,12 @@ void exec_vabal (void)
   TEST_VABAL(uint, u, 16, 32, 4);
   TEST_VABAL(uint, u, 32, 64, 2);
 
-  CHECK_RESULTS (TEST_MSG, "");
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected, "");
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected, "");
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected, "");
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected, "");
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected, "");
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected, "");
 
   /* Use values that could lead to overflow intermediate
    * calculations.  */
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vabd.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vabd.c
@@ -8,15 +8,10 @@ VECT_VAR_DECL(expected,int,8,8) [] = { 0x11, 0x10, 0xf, 0xe,
 				       0xd, 0xc, 0xb, 0xa };
 VECT_VAR_DECL(expected,int,16,4) [] = { 0x3, 0x2, 0x1, 0x0 };
 VECT_VAR_DECL(expected,int,32,2) [] = { 0x18, 0x17 };
-VECT_VAR_DECL(expected,int,64,1) [] = { 0x3333333333333333 };
 VECT_VAR_DECL(expected,uint,8,8) [] = { 0xef, 0xf0, 0xf1, 0xf2,
 					0xf3, 0xf4, 0xf5, 0xf6 };
 VECT_VAR_DECL(expected,uint,16,4) [] = { 0xffe3, 0xffe4, 0xffe5, 0xffe6 };
 VECT_VAR_DECL(expected,uint,32,2) [] = { 0xffffffe8, 0xffffffe9 };
-VECT_VAR_DECL(expected,uint,64,1) [] = { 0x3333333333333333 };
-VECT_VAR_DECL(expected,poly,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
 VECT_VAR_DECL(expected,hfloat,32,2) [] = { 0x41c26666, 0x41ba6666 };
 VECT_VAR_DECL(expected,int,8,16) [] = { 0x1a, 0x19, 0x18, 0x17,
 					0x16, 0x15, 0x14, 0x13,
@@ -25,8 +20,6 @@ VECT_VAR_DECL(expected,int,8,16) [] = { 0x1a, 0x19, 0x18, 0x17,
 VECT_VAR_DECL(expected,int,16,8) [] = { 0x4, 0x3, 0x2, 0x1,
 					0x0, 0x1, 0x2, 0x3 };
 VECT_VAR_DECL(expected,int,32,4) [] = { 0x30, 0x2f, 0x2e, 0x2d };
-VECT_VAR_DECL(expected,int,64,2) [] = { 0x3333333333333333,
-					0x3333333333333333 };
 VECT_VAR_DECL(expected,uint,8,16) [] = { 0xe6, 0xe7, 0xe8, 0xe9,
 					 0xea, 0xeb, 0xec, 0xed,
 					 0xee, 0xef, 0xf0, 0xf1,
@@ -35,14 +28,6 @@ VECT_VAR_DECL(expected,uint,16,8) [] = { 0xffe4, 0xffe5, 0xffe6, 0xffe7,
 					 0xffe8, 0xffe9, 0xffea, 0xffeb };
 VECT_VAR_DECL(expected,uint,32,4) [] = { 0xffffffd0, 0xffffffd1,
 					 0xffffffd2, 0xffffffd3 };
-VECT_VAR_DECL(expected,uint,64,2) [] = { 0x3333333333333333,
-					 0x3333333333333333 };
-VECT_VAR_DECL(expected,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					 0x3333, 0x3333, 0x3333, 0x3333 };
 VECT_VAR_DECL(expected,hfloat,32,4) [] = { 0x42407ae1, 0x423c7ae1,
 					   0x42387ae1, 0x42347ae1 };
 
@@ -130,7 +115,20 @@ void exec_vabd (void)
   TEST_VABD(q, uint, u, 32, 4);
   TEST_VABD(q, float, f, 32, 4);
 
-  CHECK_RESULTS (TEST_MSG, "");
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected, "");
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected, "");
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected, "");
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected, "");
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected, "");
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected, "");
+  CHECK_FP(TEST_MSG, float, 32, 2, PRIx32, expected, "");
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected, "");
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected, "");
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected, "");
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected, "");
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected, "");
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected, "");
+  CHECK_FP(TEST_MSG, float, 32, 4, PRIx32, expected, "");
 
 
   /* Extra FP tests with special values (-0.0, ....) */
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vabdl.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vabdl.c
@@ -3,45 +3,15 @@
 #include "compute-ref-data.h"
 
 /* Expected results.  */
-VECT_VAR_DECL(expected,int,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-				       0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,int,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,int,32,2) [] = { 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,int,64,1) [] = { 0x3333333333333333 };
-VECT_VAR_DECL(expected,uint,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,uint,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,uint,32,2) [] = { 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,uint,64,1) [] = { 0x3333333333333333 };
-VECT_VAR_DECL(expected,poly,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,2) [] = { 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,int,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
 VECT_VAR_DECL(expected,int,16,8) [] = { 0x11, 0x10, 0xf, 0xe,
 					0xd, 0xc, 0xb, 0xa };
 VECT_VAR_DECL(expected,int,32,4) [] = { 0x3, 0x2, 0x1, 0x0 };
 VECT_VAR_DECL(expected,int,64,2) [] = { 0x18, 0x17 };
-VECT_VAR_DECL(expected,uint,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
 VECT_VAR_DECL(expected,uint,16,8) [] = { 0xef, 0xf0, 0xf1, 0xf2,
 					 0xf3, 0xf4, 0xf5, 0xf6 };
 VECT_VAR_DECL(expected,uint,32,4) [] = { 0xffe3, 0xffe4, 0xffe5, 0xffe6 };
 VECT_VAR_DECL(expected,uint,64,2) [] = { 0xffffffe8,
 					 0xffffffe9 };
-VECT_VAR_DECL(expected,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,4) [] = { 0x33333333, 0x33333333,
-					   0x33333333, 0x33333333 };
 
 #define TEST_MSG "VABDL"
 void exec_vabdl (void)
@@ -99,7 +69,12 @@ void exec_vabdl (void)
   TEST_VABDL(uint, u, 16, 32, 4);
   TEST_VABDL(uint, u, 32, 64, 2);
 
-  CHECK_RESULTS (TEST_MSG, "");
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected, "");
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected, "");
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected, "");
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected, "");
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected, "");
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected, "");
 }
 
 int main (void)
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vabs.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vabs.c
@@ -12,41 +12,11 @@ VECT_VAR_DECL(expected,int,8,8) [] = { 0x10, 0xf, 0xe, 0xd,
 				       0xc, 0xb, 0xa, 0x9 };
 VECT_VAR_DECL(expected,int,16,4) [] = { 0x10, 0xf, 0xe, 0xd };
 VECT_VAR_DECL(expected,int,32,2) [] = { 0x10, 0xf };
-VECT_VAR_DECL(expected,int,64,1) [] = { 0x3333333333333333 };
-VECT_VAR_DECL(expected,uint,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,uint,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,uint,32,2) [] = { 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,uint,64,1) [] = { 0x3333333333333333 };
-VECT_VAR_DECL(expected,poly,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,2) [] = { 0x33333333, 0x33333333 };
 VECT_VAR_DECL(expected,int,8,16) [] = { 0x10, 0xf, 0xe, 0xd, 0xc, 0xb, 0xa, 0x9,
 					0x8, 0x7, 0x6, 0x5, 0x4, 0x3, 0x2, 0x1 };
 VECT_VAR_DECL(expected,int,16,8) [] = { 0x10, 0xf, 0xe, 0xd,
 					0xc, 0xb, 0xa, 0x9 };
 VECT_VAR_DECL(expected,int,32,4) [] = { 0x10, 0xf, 0xe, 0xd };
-VECT_VAR_DECL(expected,int,64,2) [] = { 0x3333333333333333,
-					0x3333333333333333 };
-VECT_VAR_DECL(expected,uint,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,uint,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,uint,32,4) [] = { 0x33333333, 0x33333333,
-					 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,uint,64,2) [] = { 0x3333333333333333,
-					 0x3333333333333333 };
-VECT_VAR_DECL(expected,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,4) [] = { 0x33333333, 0x33333333,
-					  0x33333333, 0x33333333 };
 
 /* Expected results for float32 variants. Needs to be separated since
    the generic test function does not test floating-point
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vadd.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vadd.c
@@ -18,10 +18,6 @@ VECT_VAR_DECL(expected,uint,8,8) [] = { 0x4, 0x5, 0x6, 0x7,
 VECT_VAR_DECL(expected,uint,16,4) [] = { 0xe, 0xf, 0x10, 0x11 };
 VECT_VAR_DECL(expected,uint,32,2) [] = { 0x18, 0x19 };
 VECT_VAR_DECL(expected,uint,64,1) [] = { 0xfffffffffffffff2 };
-VECT_VAR_DECL(expected,poly,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,2) [] = { 0x33333333, 0x33333333 };
 VECT_VAR_DECL(expected,int,8,16) [] = { 0xe6, 0xe7, 0xe8, 0xe9,
 					0xea, 0xeb, 0xec, 0xed,
 					0xee, 0xef, 0xf0, 0xf1,
@@ -40,14 +36,6 @@ VECT_VAR_DECL(expected,uint,16,8) [] = { 0xfff3, 0xfff4, 0xfff5, 0xfff6,
 VECT_VAR_DECL(expected,uint,32,4) [] = { 0x27, 0x28, 0x29, 0x2a };
 VECT_VAR_DECL(expected,uint,64,2) [] = { 0xfffffffffffffff3,
 					 0xfffffffffffffff4 };
-VECT_VAR_DECL(expected,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,4) [] = { 0x33333333, 0x33333333,
-					   0x33333333, 0x33333333 };
 
 /* Expected results for float32 variants. Needs to be separated since
    the generic test function does not test floating-point
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vaddl.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vaddl.c
@@ -6,46 +6,16 @@
 #define TEST_MSG "VADDL"
 
 /* Expected results.  */
-VECT_VAR_DECL(expected,int,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-				       0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,int,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,int,32,2) [] = { 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,int,64,1) [] = { 0x3333333333333333 };
-VECT_VAR_DECL(expected,uint,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,uint,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,uint,32,2) [] = { 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,uint,64,1) [] = { 0x3333333333333333 };
-VECT_VAR_DECL(expected,poly,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,2) [] = { 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,int,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
 VECT_VAR_DECL(expected,int,16,8) [] = {  0xffe3, 0xffe4, 0xffe5, 0xffe6,
 					 0xffe7, 0xffe8, 0xffe9, 0xffea };
 VECT_VAR_DECL(expected,int,32,4) [] = { 0xffffffe2, 0xffffffe3,
 					0xffffffe4, 0xffffffe5 };
 VECT_VAR_DECL(expected,int,64,2) [] = { 0xffffffffffffffe0,
 					0xffffffffffffffe1 };
-VECT_VAR_DECL(expected,uint,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
 VECT_VAR_DECL(expected,uint,16,8) [] = { 0x1e3, 0x1e4, 0x1e5, 0x1e6,
 					 0x1e7, 0x1e8, 0x1e9, 0x1ea };
 VECT_VAR_DECL(expected,uint,32,4) [] = { 0x1ffe1, 0x1ffe2,
 					 0x1ffe3, 0x1ffe4 };
 VECT_VAR_DECL(expected,uint,64,2) [] = { 0x1ffffffe0, 0x1ffffffe1 };
-VECT_VAR_DECL(expected,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,4) [] = { 0x33333333, 0x33333333,
-					   0x33333333, 0x33333333 };
 
 #include "vXXXl.inc"
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vaddw.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vaddw.c
@@ -6,46 +6,16 @@
 #define TEST_MSG "VADDW"
 
 /* Expected results.  */
-VECT_VAR_DECL(expected,int,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-				       0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,int,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,int,32,2) [] = { 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,int,64,1) [] = { 0x3333333333333333 };
-VECT_VAR_DECL(expected,uint,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,uint,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,uint,32,2) [] = { 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,uint,64,1) [] = { 0x3333333333333333 };
-VECT_VAR_DECL(expected,poly,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,2) [] = { 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,int,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
 VECT_VAR_DECL(expected,int,16,8) [] = {  0xffe3, 0xffe4, 0xffe5, 0xffe6,
 					 0xffe7, 0xffe8, 0xffe9, 0xffea };
 VECT_VAR_DECL(expected,int,32,4) [] = { 0xffffffe2, 0xffffffe3,
 					0xffffffe4, 0xffffffe5 };
 VECT_VAR_DECL(expected,int,64,2) [] = { 0xffffffffffffffe0,
 					0xffffffffffffffe1 };
-VECT_VAR_DECL(expected,uint,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
 VECT_VAR_DECL(expected,uint,16,8) [] = { 0xe3, 0xe4, 0xe5, 0xe6,
 					 0xe7, 0xe8, 0xe9, 0xea };
 VECT_VAR_DECL(expected,uint,32,4) [] = { 0xffe1, 0xffe2,
 					 0xffe3, 0xffe4 };
 VECT_VAR_DECL(expected,uint,64,2) [] = { 0xffffffe0, 0xffffffe1 };
-VECT_VAR_DECL(expected,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,4) [] = { 0x33333333, 0x33333333,
-					   0x33333333, 0x33333333 };
 
 #include "vXXXw.inc"
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vand.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vand.c
@@ -14,10 +14,6 @@ VECT_VAR_DECL(expected,uint,8,8) [] = { 0x10, 0x10, 0x10, 0x10,
 VECT_VAR_DECL(expected,uint,16,4) [] = { 0x10, 0x10, 0x12, 0x12 };
 VECT_VAR_DECL(expected,uint,32,2) [] = { 0x20, 0x20 };
 VECT_VAR_DECL(expected,uint,64,1) [] = { 0x0 };
-VECT_VAR_DECL(expected,poly,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,2) [] = { 0x33333333, 0x33333333 };
 VECT_VAR_DECL(expected,int,8,16) [] = { 0xf0, 0xf0, 0xf2, 0xf2,
 					0xf4, 0xf4, 0xf6, 0xf6,
 					0xf0, 0xf0, 0xf2, 0xf2,
@@ -35,11 +31,3 @@ VECT_VAR_DECL(expected,uint,16,8) [] = { 0x0, 0x1, 0x2, 0x3,
 					 0x0, 0x1, 0x2, 0x3 };
 VECT_VAR_DECL(expected,uint,32,4) [] = { 0x30, 0x31, 0x32, 0x33 };
 VECT_VAR_DECL(expected,uint,64,2) [] = { 0x0, 0x1 };
-VECT_VAR_DECL(expected,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,4) [] = { 0x33333333, 0x33333333,
-					   0x33333333, 0x33333333 };
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vbic.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vbic.c
@@ -14,10 +14,6 @@ VECT_VAR_DECL(expected,uint,8,8) [] = { 0xe0, 0xe1, 0xe2, 0xe3,
 VECT_VAR_DECL(expected,uint,16,4) [] = { 0xffe0, 0xffe1, 0xffe0, 0xffe1 };
 VECT_VAR_DECL(expected,uint,32,2) [] = { 0xffffffd0, 0xffffffd1 };
 VECT_VAR_DECL(expected,uint,64,1) [] = { 0xfffffffffffffff0 };
-VECT_VAR_DECL(expected,poly,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,2) [] = { 0x33333333, 0x33333333 };
 VECT_VAR_DECL(expected,int,8,16) [] = { 0x0, 0x1, 0x0, 0x1,
 					0x0, 0x1, 0x0, 0x1,
 					0x8, 0x9, 0x8, 0x9,
@@ -36,11 +32,3 @@ VECT_VAR_DECL(expected,uint,32,4) [] = { 0xffffffc0, 0xffffffc0,
 					 0xffffffc0, 0xffffffc0 };
 VECT_VAR_DECL(expected,uint,64,2) [] = { 0xfffffffffffffff0,
 					 0xfffffffffffffff0 };
-VECT_VAR_DECL(expected,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,4) [] = { 0x33333333, 0x33333333,
-					   0x33333333, 0x33333333 };
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vcage.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vcage.c
@@ -4,48 +4,9 @@
 #include "cmp_fp_op.inc"
 
 /* Expected results.  */
-VECT_VAR_DECL(expected,int,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-				       0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,int,16,4) [] = { 0x333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,int,32,2) [] = { 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,int,64,1) [] = { 0x3333333333333333 };
-VECT_VAR_DECL(expected,uint,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,uint,16,4) [] = { 0x333, 0x3333, 0x3333, 0x3333 };
 VECT_VAR_DECL(expected,uint,32,2) [] = { 0xffffffff, 0x0 };
-VECT_VAR_DECL(expected,uint,64,1) [] = { 0x3333333333333333 };
-VECT_VAR_DECL(expected,poly,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,2) [] = { 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,int,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,int,16,8) [] = { 0x333, 0x3333, 0x3333, 0x3333,
-					0x333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,int,32,4) [] = { 0x33333333, 0x33333333,
-					0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,int,64,2) [] = { 0x3333333333333333,
-					0x3333333333333333 };
-VECT_VAR_DECL(expected,uint,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,uint,16,8) [] = { 0x333, 0x3333, 0x3333, 0x3333,
-					 0x333, 0x3333, 0x3333, 0x3333 };
 VECT_VAR_DECL(expected,uint,32,4) [] = { 0xffffffff, 0xffffffff,
 					 0xffffffff, 0x0 };
-VECT_VAR_DECL(expected,uint,64,2) [] = { 0x3333333333333333,
-					 0x3333333333333333 };
-VECT_VAR_DECL(expected,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,4) [] = { 0x33333333, 0x33333333,
-					   0x33333333, 0x33333333 };
 
 VECT_VAR_DECL(expected2,uint,32,2) [] = { 0xffffffff, 0xffffffff };
 VECT_VAR_DECL(expected2,uint,32,4) [] = { 0xffffffff, 0xffffffff,
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vcagt.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vcagt.c
@@ -4,47 +4,9 @@
 #include "cmp_fp_op.inc"
 
 /* Expected results.  */
-VECT_VAR_DECL(expected,int,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-				       0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,int,16,4) [] = { 0x333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,int,32,2) [] = { 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,int,64,1) [] = { 0x3333333333333333 };
-VECT_VAR_DECL(expected,uint,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,uint,16,4) [] = { 0x333, 0x3333, 0x3333, 0x3333 };
 VECT_VAR_DECL(expected,uint,32,2) [] = { 0x0, 0x0 };
-VECT_VAR_DECL(expected,uint,64,1) [] = { 0x3333333333333333 };
-VECT_VAR_DECL(expected,poly,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,2) [] = { 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,int,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,int,16,8) [] = { 0x333, 0x3333, 0x3333, 0x3333,
-					0x333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,int,32,4) [] = { 0xffffffff, 0xffffffff, 0x0, 0x0 };
-VECT_VAR_DECL(expected,int,64,2) [] = { 0x3333333333333333,
-					0x3333333333333333 };
-VECT_VAR_DECL(expected,uint,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,uint,16,8) [] = { 0x333, 0x3333, 0x3333, 0x3333,
-					 0x333, 0x3333, 0x3333, 0x3333 };
 VECT_VAR_DECL(expected,uint,32,4) [] = { 0xffffffff, 0xffffffff,
 					 0x0, 0x0 };
-VECT_VAR_DECL(expected,uint,64,2) [] = { 0x3333333333333333,
-					 0x3333333333333333 };
-VECT_VAR_DECL(expected,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,4) [] = { 0x33333333, 0x33333333,
-					   0x33333333, 0x33333333 };
 
 VECT_VAR_DECL(expected2,uint,32,2) [] = { 0xffffffff, 0xffffffff };
 VECT_VAR_DECL(expected2,uint,32,4) [] = { 0xffffffff, 0xffffffff,
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vcale.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vcale.c
@@ -4,46 +4,8 @@
 #include "cmp_fp_op.inc"
 
 /* Expected results.  */
-VECT_VAR_DECL(expected,int,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-				       0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,int,16,4) [] = { 0x333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,int,32,2) [] = { 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,int,64,1) [] = { 0x3333333333333333 };
-VECT_VAR_DECL(expected,uint,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,uint,16,4) [] = { 0x333, 0x3333, 0x3333, 0x3333 };
 VECT_VAR_DECL(expected,uint,32,2) [] = { 0xffffffff, 0xffffffff };
-VECT_VAR_DECL(expected,uint,64,1) [] = { 0x3333333333333333 };
-VECT_VAR_DECL(expected,poly,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,2) [] = { 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,int,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,int,16,8) [] = { 0x333, 0x3333, 0x3333, 0x3333,
-					0x333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,int,32,4) [] = { 0xffffffff, 0xffffffff, 0x0, 0x0 };
-VECT_VAR_DECL(expected,int,64,2) [] = { 0x3333333333333333,
-					0x3333333333333333 };
-VECT_VAR_DECL(expected,uint,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,uint,16,8) [] = { 0x333, 0x3333, 0x3333, 0x3333,
-					 0x333, 0x3333, 0x3333, 0x3333 };
 VECT_VAR_DECL(expected,uint,32,4) [] = { 0x0, 0x0, 0xffffffff, 0xffffffff };
-VECT_VAR_DECL(expected,uint,64,2) [] = { 0x3333333333333333,
-					 0x3333333333333333 };
-VECT_VAR_DECL(expected,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,4) [] = { 0x33333333, 0x33333333,
-					   0x33333333, 0x33333333 };
 
 VECT_VAR_DECL(expected2,uint,32,2) [] = { 0x0, 0x0 };
 VECT_VAR_DECL(expected2,uint,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vcalt.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vcalt.c
@@ -4,46 +4,8 @@
 #include "cmp_fp_op.inc"
 
 /* Expected results.  */
-VECT_VAR_DECL(expected,int,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-				       0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,int,16,4) [] = { 0x333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,int,32,2) [] = { 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,int,64,1) [] = { 0x3333333333333333 };
-VECT_VAR_DECL(expected,uint,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,uint,16,4) [] = { 0x333, 0x3333, 0x3333, 0x3333 };
 VECT_VAR_DECL(expected,uint,32,2) [] = { 0x0, 0xffffffff };
-VECT_VAR_DECL(expected,uint,64,1) [] = { 0x3333333333333333 };
-VECT_VAR_DECL(expected,poly,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,2) [] = { 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,int,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,int,16,8) [] = { 0x333, 0x3333, 0x3333, 0x3333,
-					0x333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,int,32,4) [] = { 0xffffffff, 0xffffffff, 0x0, 0x0 };
-VECT_VAR_DECL(expected,int,64,2) [] = { 0x3333333333333333,
-					0x3333333333333333 };
-VECT_VAR_DECL(expected,uint,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,uint,16,8) [] = { 0x333, 0x3333, 0x3333, 0x3333,
-					 0x333, 0x3333, 0x3333, 0x3333 };
 VECT_VAR_DECL(expected,uint,32,4) [] = { 0x0, 0x0, 0x0, 0xffffffff };
-VECT_VAR_DECL(expected,uint,64,2) [] = { 0x3333333333333333,
-					 0x3333333333333333 };
-VECT_VAR_DECL(expected,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,4) [] = { 0x33333333, 0x33333333,
-					   0x33333333, 0x33333333 };
 
 VECT_VAR_DECL(expected2,uint,32,2) [] = { 0x0, 0x0 };
 VECT_VAR_DECL(expected2,uint,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vceq.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vceq.c
@@ -8,29 +8,9 @@ void exec_vceq_p8(void);
 #include "cmp_op.inc"
 
 /* Expected results.  */
-VECT_VAR_DECL(expected,int,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-				       0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,int,16,4) [] = { 0x333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,int,32,2) [] = { 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,int,64,1) [] = { 0x3333333333333333 };
 VECT_VAR_DECL(expected,uint,8,8) [] = { 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0xff, 0x0 };
 VECT_VAR_DECL(expected,uint,16,4) [] = { 0x0, 0x0, 0xffff, 0x0 };
 VECT_VAR_DECL(expected,uint,32,2) [] = { 0xffffffff, 0x0 };
-VECT_VAR_DECL(expected,uint,64,1) [] = { 0x3333333333333333 };
-VECT_VAR_DECL(expected,poly,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,2) [] = { 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,int,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,int,16,8) [] = { 0x333, 0x3333, 0x3333, 0x3333,
-					0x333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,int,32,4) [] = { 0x33333333, 0x33333333,
-					0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,int,64,2) [] = { 0x3333333333333333,
-					0x3333333333333333 };
 VECT_VAR_DECL(expected,uint,8,16) [] = { 0x0, 0x0, 0x0, 0x0,
 					 0x0, 0x0, 0x0, 0x0,
 					 0x0, 0x0, 0x0, 0x0,
@@ -38,16 +18,6 @@ VECT_VAR_DECL(expected,uint,8,16) [] = { 0x0, 0x0, 0x0, 0x0,
 VECT_VAR_DECL(expected,uint,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
 					 0x0, 0x0, 0xffff, 0x0 };
 VECT_VAR_DECL(expected,uint,32,4) [] = { 0x0, 0x0, 0xffffffff, 0x0 };
-VECT_VAR_DECL(expected,uint,64,2) [] = { 0x3333333333333333,
-					 0x3333333333333333 };
-VECT_VAR_DECL(expected,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,4) [] = { 0x33333333, 0x33333333,
-					   0x33333333, 0x33333333 };
 
 VECT_VAR_DECL(expected_uint,uint,8,8) [] = { 0x0, 0x0, 0x0, 0xff,
 					     0x0, 0x0, 0x0, 0x0 };
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vcge.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vcge.c
@@ -4,29 +4,9 @@
 #include "cmp_op.inc"
 
 /* Expected results.  */
-VECT_VAR_DECL(expected,int,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-				       0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,int,16,4) [] = { 0x333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,int,32,2) [] = { 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,int,64,1) [] = { 0x3333333333333333 };
 VECT_VAR_DECL(expected,uint,8,8) [] = { 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0xff, 0xff };
 VECT_VAR_DECL(expected,uint,16,4) [] = { 0x0, 0x0, 0xffff, 0xffff };
 VECT_VAR_DECL(expected,uint,32,2) [] = { 0xffffffff, 0xffffffff };
-VECT_VAR_DECL(expected,uint,64,1) [] = { 0x3333333333333333 };
-VECT_VAR_DECL(expected,poly,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,2) [] = { 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,int,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,int,16,8) [] = { 0x333, 0x3333, 0x3333, 0x3333,
-					0x333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,int,32,4) [] = { 0x33333333, 0x33333333,
-					0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,int,64,2) [] = { 0x3333333333333333,
-					0x3333333333333333 };
 VECT_VAR_DECL(expected,uint,8,16) [] = { 0x0, 0x0, 0x0, 0x0,
 					 0x0, 0x0, 0x0, 0x0,
 					 0x0, 0x0, 0x0, 0x0,
@@ -34,16 +14,6 @@ VECT_VAR_DECL(expected,uint,8,16) [] = { 0x0, 0x0, 0x0, 0x0,
 VECT_VAR_DECL(expected,uint,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
 					 0x0, 0x0, 0xffff, 0xffff };
 VECT_VAR_DECL(expected,uint,32,4) [] = { 0x0, 0x0, 0xffffffff, 0xffffffff };
-VECT_VAR_DECL(expected,uint,64,2) [] = { 0x3333333333333333,
-					 0x3333333333333333 };
-VECT_VAR_DECL(expected,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,4) [] = { 0x33333333, 0x33333333,
-					   0x33333333, 0x33333333 };
 
 VECT_VAR_DECL(expected_uint,uint,8,8) [] = { 0x0, 0x0, 0x0, 0xff,
 					     0xff, 0xff, 0xff, 0xff };
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vcgt.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vcgt.c
@@ -4,29 +4,9 @@
 #include "cmp_op.inc"
 
 /* Expected results.  */
-VECT_VAR_DECL(expected,int,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-				       0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,int,16,4) [] = { 0x333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,int,32,2) [] = { 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,int,64,1) [] = { 0x3333333333333333 };
 VECT_VAR_DECL(expected,uint,8,8) [] = { 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0xff };
 VECT_VAR_DECL(expected,uint,16,4) [] = { 0x0, 0x0, 0x0, 0xffff };
 VECT_VAR_DECL(expected,uint,32,2) [] = { 0x0, 0xffffffff };
-VECT_VAR_DECL(expected,uint,64,1) [] = { 0x3333333333333333 };
-VECT_VAR_DECL(expected,poly,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,2) [] = { 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,int,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,int,16,8) [] = { 0x333, 0x3333, 0x3333, 0x3333,
-					0x333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,int,32,4) [] = { 0x33333333, 0x33333333,
-					0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,int,64,2) [] = { 0x3333333333333333,
-					0x3333333333333333 };
 VECT_VAR_DECL(expected,uint,8,16) [] = { 0x0, 0x0, 0x0, 0x0,
 					 0x0, 0x0, 0x0, 0x0,
 					 0x0, 0x0, 0x0, 0x0,
@@ -34,16 +14,6 @@ VECT_VAR_DECL(expected,uint,8,16) [] = { 0x0, 0x0, 0x0, 0x0,
 VECT_VAR_DECL(expected,uint,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
 					 0x0, 0x0, 0x0, 0xffff };
 VECT_VAR_DECL(expected,uint,32,4) [] = { 0x0, 0x0, 0x0, 0xffffffff };
-VECT_VAR_DECL(expected,uint,64,2) [] = { 0x3333333333333333,
-					 0x3333333333333333 };
-VECT_VAR_DECL(expected,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,4) [] = { 0x33333333, 0x33333333,
-					   0x33333333, 0x33333333 };
 
 VECT_VAR_DECL(expected_uint,uint,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
 					     0xff, 0xff, 0xff, 0xff };
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vcle.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vcle.c
@@ -4,30 +4,10 @@
 #include "cmp_op.inc"
 
 /* Expected results.  */
-VECT_VAR_DECL(expected,int,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-				       0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,int,16,4) [] = { 0x333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,int,32,2) [] = { 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,int,64,1) [] = { 0x3333333333333333 };
 VECT_VAR_DECL(expected,uint,8,8) [] = { 0xff, 0xff, 0xff, 0xff,
 					0xff, 0xff, 0xff, 0x0 };
 VECT_VAR_DECL(expected,uint,16,4) [] = { 0xffff, 0xffff, 0xffff, 0x0 };
 VECT_VAR_DECL(expected,uint,32,2) [] = { 0xffffffff, 0x0 };
-VECT_VAR_DECL(expected,uint,64,1) [] = { 0x3333333333333333 };
-VECT_VAR_DECL(expected,poly,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,2) [] = { 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,int,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,int,16,8) [] = { 0x333, 0x3333, 0x3333, 0x3333,
-					0x333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,int,32,4) [] = { 0x33333333, 0x33333333,
-					0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,int,64,2) [] = { 0x3333333333333333,
-					0x3333333333333333 };
 VECT_VAR_DECL(expected,uint,8,16) [] = { 0xff, 0xff, 0xff, 0xff,
 					 0xff, 0xff, 0xff, 0xff,
 					 0xff, 0xff, 0xff, 0xff,
@@ -36,16 +16,6 @@ VECT_VAR_DECL(expected,uint,16,8) [] = { 0xffff, 0xffff, 0xffff, 0xffff,
 					 0xffff, 0xffff, 0xffff, 0x0 };
 VECT_VAR_DECL(expected,uint,32,4) [] = { 0xffffffff, 0xffffffff,
 					 0xffffffff, 0x0 };
-VECT_VAR_DECL(expected,uint,64,2) [] = { 0x3333333333333333,
-					 0x3333333333333333 };
-VECT_VAR_DECL(expected,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,4) [] = { 0x33333333, 0x33333333,
-					   0x33333333, 0x33333333 };
 
 VECT_VAR_DECL(expected_uint,uint,8,8) [] = { 0xff, 0xff, 0xff, 0xff,
 					     0x0, 0x0, 0x0, 0x0 };
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vcls.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vcls.c
@@ -6,16 +6,6 @@
 VECT_VAR_DECL(expected,int,8,8) [] = { 0x6, 0x6, 0x6, 0x6, 0x6, 0x6, 0x6, 0x6 };
 VECT_VAR_DECL(expected,int,16,4) [] = { 0x2, 0x2, 0x2, 0x2 };
 VECT_VAR_DECL(expected,int,32,2) [] = { 0x19, 0x19 };
-VECT_VAR_DECL(expected,int,64,1) [] = { 0x3333333333333333 };
-VECT_VAR_DECL(expected,uint,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,uint,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,uint,32,2) [] = { 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,uint,64,1) [] = { 0x3333333333333333 };
-VECT_VAR_DECL(expected,poly,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,2) [] = { 0x33333333, 0x33333333 };
 VECT_VAR_DECL(expected,int,8,16) [] = { 0x7, 0x7, 0x7, 0x7,
 					0x7, 0x7, 0x7, 0x7,
 					0x7, 0x7, 0x7, 0x7,
@@ -23,45 +13,12 @@ VECT_VAR_DECL(expected,int,8,16) [] = { 0x7, 0x7, 0x7, 0x7,
 VECT_VAR_DECL(expected,int,16,8) [] = { 0x2, 0x2, 0x2, 0x2,
 					0x2, 0x2, 0x2, 0x2 };
 VECT_VAR_DECL(expected,int,32,4) [] = { 0x14, 0x14, 0x14, 0x14 };
-VECT_VAR_DECL(expected,int,64,2) [] = { 0x3333333333333333,
-					0x3333333333333333 };
-VECT_VAR_DECL(expected,uint,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,uint,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,uint,32,4) [] = { 0x33333333, 0x33333333,
-					 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,uint,64,2) [] = { 0x3333333333333333,
-					 0x3333333333333333 };
-VECT_VAR_DECL(expected,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,4) [] = { 0x33333333, 0x33333333,
-					   0x33333333, 0x33333333 };
 
 /* Expected results with negative input.  */
 VECT_VAR_DECL(expected_with_negative,int,8,8) [] = { 0x7, 0x7, 0x7, 0x7,
 						     0x7, 0x7, 0x7, 0x7 };
 VECT_VAR_DECL(expected_with_negative,int,16,4) [] = { 0x1, 0x1, 0x1, 0x1 };
 VECT_VAR_DECL(expected_with_negative,int,32,2) [] = { 0x1, 0x1 };
-VECT_VAR_DECL(expected_with_negative,int,64,1) [] = { 0x3333333333333333 };
-VECT_VAR_DECL(expected_with_negative,uint,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-						      0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected_with_negative,uint,16,4) [] = { 0x3333, 0x3333,
-						       0x3333, 0x3333 };
-VECT_VAR_DECL(expected_with_negative,uint,32,2) [] = { 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected_with_negative,uint,64,1) [] = { 0x3333333333333333 };
-VECT_VAR_DECL(expected_with_negative,poly,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-						      0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected_with_negative,poly,16,4) [] = { 0x3333, 0x3333,
-						       0x3333, 0x3333 };
-VECT_VAR_DECL(expected_with_negative,hfloat,32,2) [] = { 0x33333333,
-							 0x33333333 };
 VECT_VAR_DECL(expected_with_negative,int,8,16) [] = { 0x0, 0x0, 0x0, 0x0,
 						      0x0, 0x0, 0x0, 0x0,
 						      0x0, 0x0, 0x0, 0x0,
@@ -69,32 +26,6 @@ VECT_VAR_DECL(expected_with_negative,int,8,16) [] = { 0x0, 0x0, 0x0, 0x0,
 VECT_VAR_DECL(expected_with_negative,int,16,8) [] = { 0x2, 0x2, 0x2, 0x2,
 						      0x2, 0x2, 0x2, 0x2 };
 VECT_VAR_DECL(expected_with_negative,int,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
-VECT_VAR_DECL(expected_with_negative,int,64,2) [] = { 0x3333333333333333,
-						      0x3333333333333333 };
-VECT_VAR_DECL(expected_with_negative,uint,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-						       0x33, 0x33, 0x33, 0x33,
-						       0x33, 0x33, 0x33, 0x33,
-						       0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected_with_negative,uint,16,8) [] = { 0x3333, 0x3333,
-						       0x3333, 0x3333,
-						       0x3333, 0x3333,
-						       0x3333, 0x3333 };
-VECT_VAR_DECL(expected_with_negative,uint,32,4) [] = { 0x33333333, 0x33333333,
-						       0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected_with_negative,uint,64,2) [] = { 0x3333333333333333,
-						       0x3333333333333333 };
-VECT_VAR_DECL(expected_with_negative,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-						       0x33, 0x33, 0x33, 0x33,
-						       0x33, 0x33, 0x33, 0x33,
-						       0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected_with_negative,poly,16,8) [] = { 0x3333, 0x3333,
-						       0x3333, 0x3333,
-						       0x3333, 0x3333,
-						       0x3333, 0x3333 };
-VECT_VAR_DECL(expected_with_negative,hfloat,32,4) [] = { 0x33333333,
-							 0x33333333,
-							 0x33333333,
-							 0x33333333 };
 
 #define INSN_NAME vcls
 #define TEST_MSG "VCLS/VCLSQ"
@@ -146,7 +77,13 @@ FNNAME (INSN_NAME)
   TEST_UNARY_OP(INSN_NAME, q, int, s, 16, 8);
   TEST_UNARY_OP(INSN_NAME, q, int, s, 32, 4);
 
-  CHECK_RESULTS (TEST_MSG, " (positive input)");
+#define MSG_POSITIVE " (positive input)"
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected, MSG_POSITIVE);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected, MSG_POSITIVE);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected, MSG_POSITIVE);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected, MSG_POSITIVE);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected, MSG_POSITIVE);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected, MSG_POSITIVE);
 
   /* Fill input vector with arbitrary values (negative).  */
   VDUP(vector, , int, s, 8, 8, 0xFF);
@@ -164,7 +101,13 @@ FNNAME (INSN_NAME)
   TEST_UNARY_OP(INSN_NAME, q, int, s, 16, 8);
   TEST_UNARY_OP(INSN_NAME, q, int, s, 32, 4);
 
-  CHECK_RESULTS_NAMED (TEST_MSG, expected_with_negative, " (negative input)");
+#define MSG_NEGATIVE " (negative input)"
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_with_negative, MSG_NEGATIVE);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_with_negative, MSG_NEGATIVE);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_with_negative, MSG_NEGATIVE);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_with_negative, MSG_NEGATIVE);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_with_negative, MSG_NEGATIVE);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_with_negative, MSG_NEGATIVE);
 }
 
 int main (void)
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vclt.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vclt.c
@@ -4,30 +4,10 @@
 #include "cmp_op.inc"
 
 /* Expected results.  */
-VECT_VAR_DECL(expected,int,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-				       0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,int,16,4) [] = { 0x333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,int,32,2) [] = { 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,int,64,1) [] = { 0x3333333333333333 };
 VECT_VAR_DECL(expected,uint,8,8) [] = { 0xff, 0xff, 0xff, 0xff,
 					0xff, 0xff, 0x0, 0x0 };
 VECT_VAR_DECL(expected,uint,16,4) [] = { 0xffff, 0xffff, 0x0, 0x0 };
 VECT_VAR_DECL(expected,uint,32,2) [] = { 0x0, 0x0 };
-VECT_VAR_DECL(expected,uint,64,1) [] = { 0x3333333333333333 };
-VECT_VAR_DECL(expected,poly,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,2) [] = { 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,int,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,int,16,8) [] = { 0x333, 0x3333, 0x3333, 0x3333,
-					0x333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,int,32,4) [] = { 0x33333333, 0x33333333,
-					0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,int,64,2) [] = { 0x3333333333333333,
-					0x3333333333333333 };
 VECT_VAR_DECL(expected,uint,8,16) [] = { 0xff, 0xff, 0xff, 0xff,
 					 0xff, 0xff, 0xff, 0xff,
 					 0xff, 0xff, 0xff, 0xff,
@@ -35,16 +15,6 @@ VECT_VAR_DECL(expected,uint,8,16) [] = { 0xff, 0xff, 0xff, 0xff,
 VECT_VAR_DECL(expected,uint,16,8) [] = { 0xffff, 0xffff, 0xffff, 0xffff,
 					 0xffff, 0xffff, 0x0, 0x0 };
 VECT_VAR_DECL(expected,uint,32,4) [] = { 0xffffffff, 0xffffffff, 0x0, 0x0 };
-VECT_VAR_DECL(expected,uint,64,2) [] = { 0x3333333333333333,
-					 0x3333333333333333 };
-VECT_VAR_DECL(expected,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,4) [] = { 0x33333333, 0x33333333,
-					   0x33333333, 0x33333333 };
 
 VECT_VAR_DECL(expected_uint,uint,8,8) [] = { 0xff, 0xff, 0xff, 0x0,
 					     0x0, 0x0, 0x0, 0x0 };
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vclz.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vclz.c
@@ -6,36 +6,18 @@
 VECT_VAR_DECL(expected,int,8,8) [] = { 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0 };
 VECT_VAR_DECL(expected,int,16,4) [] = { 0x3, 0x3, 0x3, 0x3 };
 VECT_VAR_DECL(expected,int,32,2) [] = { 0x11, 0x11 };
-VECT_VAR_DECL(expected,int,64,1) [] = { 0x3333333333333333 };
 VECT_VAR_DECL(expected,uint,8,8) [] = { 0x2, 0x2, 0x2, 0x2, 0x2, 0x2, 0x2, 0x2 };
 VECT_VAR_DECL(expected,uint,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
 VECT_VAR_DECL(expected,uint,32,2) [] = { 0x5, 0x5 };
-VECT_VAR_DECL(expected,uint,64,1) [] = { 0x3333333333333333 };
-VECT_VAR_DECL(expected,poly,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,2) [] = { 0x33333333, 0x33333333 };
 VECT_VAR_DECL(expected,int,8,16) [] = { 0x2, 0x2, 0x2, 0x2, 0x2, 0x2, 0x2, 0x2,
 					0x2, 0x2, 0x2, 0x2, 0x2, 0x2, 0x2, 0x2 };
 VECT_VAR_DECL(expected,int,16,8) [] = { 0x3, 0x3, 0x3, 0x3, 0x3, 0x3, 0x3, 0x3 };
 VECT_VAR_DECL(expected,int,32,4) [] = { 0x3, 0x3, 0x3, 0x3 };
-VECT_VAR_DECL(expected,int,64,2) [] = { 0x3333333333333333,
-					0x3333333333333333 };
 VECT_VAR_DECL(expected,uint,8,16) [] = { 0x3, 0x3, 0x3, 0x3, 0x3, 0x3, 0x3, 0x3,
 					 0x3, 0x3, 0x3, 0x3, 0x3, 0x3, 0x3, 0x3 };
 VECT_VAR_DECL(expected,uint,16,8) [] = { 0xd, 0xd, 0xd, 0xd,
 					 0xd, 0xd, 0xd, 0xd };
 VECT_VAR_DECL(expected,uint,32,4) [] = { 0x1f, 0x1f, 0x1f, 0x1f };
-VECT_VAR_DECL(expected,uint,64,2) [] = { 0x3333333333333333,
-					 0x3333333333333333 };
-VECT_VAR_DECL(expected,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,4) [] = { 0x33333333, 0x33333333,
-					   0x33333333, 0x33333333 };
 
 
 /* Expected results with input=0.  */
@@ -43,16 +25,10 @@ VECT_VAR_DECL(expected_with_0,int,8,8) [] = { 0x8, 0x8, 0x8, 0x8,
 					      0x8, 0x8, 0x8, 0x8 };
 VECT_VAR_DECL(expected_with_0,int,16,4) [] = { 0x10, 0x10, 0x10, 0x10 };
 VECT_VAR_DECL(expected_with_0,int,32,2) [] = { 0x20, 0x20 };
-VECT_VAR_DECL(expected_with_0,int,64,1) [] = { 0x3333333333333333 };
 VECT_VAR_DECL(expected_with_0,uint,8,8) [] = { 0x8, 0x8, 0x8, 0x8,
 					       0x8, 0x8, 0x8, 0x8 };
 VECT_VAR_DECL(expected_with_0,uint,16,4) [] = { 0x10, 0x10, 0x10, 0x10 };
 VECT_VAR_DECL(expected_with_0,uint,32,2) [] = { 0x20, 0x20 };
-VECT_VAR_DECL(expected_with_0,uint,64,1) [] = { 0x3333333333333333 };
-VECT_VAR_DECL(expected_with_0,poly,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-					       0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected_with_0,poly,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected_with_0,hfloat,32,2) [] = { 0x33333333, 0x33333333 };
 VECT_VAR_DECL(expected_with_0,int,8,16) [] = { 0x8, 0x8, 0x8, 0x8,
 					       0x8, 0x8, 0x8, 0x8,
 					       0x8, 0x8, 0x8, 0x8,
@@ -60,8 +36,6 @@ VECT_VAR_DECL(expected_with_0,int,8,16) [] = { 0x8, 0x8, 0x8, 0x8,
 VECT_VAR_DECL(expected_with_0,int,16,8) [] = { 0x10, 0x10, 0x10, 0x10,
 					       0x10, 0x10, 0x10, 0x10 };
 VECT_VAR_DECL(expected_with_0,int,32,4) [] = { 0x20, 0x20, 0x20, 0x20 };
-VECT_VAR_DECL(expected_with_0,int,64,2) [] = { 0x3333333333333333,
-					       0x3333333333333333 };
 VECT_VAR_DECL(expected_with_0,uint,8,16) [] = { 0x8, 0x8, 0x8, 0x8,
 						0x8, 0x8, 0x8, 0x8,
 						0x8, 0x8, 0x8, 0x8,
@@ -69,16 +43,6 @@ VECT_VAR_DECL(expected_with_0,uint,8,16) [] = { 0x8, 0x8, 0x8, 0x8,
 VECT_VAR_DECL(expected_with_0,uint,16,8) [] = { 0x10, 0x10, 0x10, 0x10,
 						0x10, 0x10, 0x10, 0x10 };
 VECT_VAR_DECL(expected_with_0,uint,32,4) [] = { 0x20, 0x20, 0x20, 0x20 };
-VECT_VAR_DECL(expected_with_0,uint,64,2) [] = { 0x3333333333333333,
-						0x3333333333333333 };
-VECT_VAR_DECL(expected_with_0,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected_with_0,poly,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-						0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected_with_0,hfloat,32,4) [] = { 0x33333333, 0x33333333,
-						  0x33333333, 0x33333333 };
 
 #define INSN_NAME vclz
 #define TEST_MSG "VCLZ/VCLZQ"
@@ -154,7 +118,18 @@ FNNAME (INSN_NAME)
   TEST_UNARY_OP(INSN_NAME, q, uint, u, 16, 8);
   TEST_UNARY_OP(INSN_NAME, q, uint, u, 32, 4);
 
-  CHECK_RESULTS (TEST_MSG, "");
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected, "");
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected, "");
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected, "");
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected, "");
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected, "");
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected, "");
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected, "");
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected, "");
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected, "");
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected, "");
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected, "");
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected, "");
 
   /* Test with zero as input.  */
   VDUP(vector, , int, s, 8, 8, 0);
@@ -184,7 +159,19 @@ FNNAME (INSN_NAME)
   TEST_UNARY_OP(INSN_NAME, q, uint, u, 16, 8);
   TEST_UNARY_OP(INSN_NAME, q, uint, u, 32, 4);
 
-  CHECK_RESULTS_NAMED (TEST_MSG, expected_with_0, " (input=0)");
+#define MSG_ZERO " (input=0)"
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_with_0, MSG_ZERO);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_with_0, MSG_ZERO);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_with_0, MSG_ZERO);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_with_0, MSG_ZERO);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_with_0, MSG_ZERO);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_with_0, MSG_ZERO);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_with_0, MSG_ZERO);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_with_0, MSG_ZERO);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_with_0, MSG_ZERO);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_with_0, MSG_ZERO);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_with_0, MSG_ZERO);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_with_0, MSG_ZERO);
 }
 
 int main (void)
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vcnt.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vcnt.c
@@ -4,37 +4,14 @@
 
 /* Expected results.  */
 VECT_VAR_DECL(expected,int,8,8) [] = { 0x8, 0x8, 0x8, 0x8, 0x8, 0x8, 0x8, 0x8 };
-VECT_VAR_DECL(expected,int,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,int,32,2) [] = { 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,int,64,1) [] = { 0x3333333333333333 };
 VECT_VAR_DECL(expected,uint,8,8) [] = { 0x4, 0x4, 0x4, 0x4, 0x4, 0x4, 0x4, 0x4 };
-VECT_VAR_DECL(expected,uint,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,uint,32,2) [] = { 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,uint,64,1) [] = { 0x3333333333333333 };
 VECT_VAR_DECL(expected,poly,8,8) [] = { 0x4, 0x4, 0x4, 0x4, 0x4, 0x4, 0x4, 0x4 };
-VECT_VAR_DECL(expected,poly,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,2) [] = { 0x33333333, 0x33333333 };
 VECT_VAR_DECL(expected,int,8,16) [] = { 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
 					0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0 };
-VECT_VAR_DECL(expected,int,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,int,32,4) [] = { 0x33333333, 0x33333333,
-					0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,int,64,2) [] = { 0x3333333333333333, 0x3333333333333333 };
 VECT_VAR_DECL(expected,uint,8,16) [] = { 0x6, 0x6, 0x6, 0x6, 0x6, 0x6, 0x6, 0x6,
 					 0x6, 0x6, 0x6, 0x6, 0x6, 0x6, 0x6, 0x6 };
-VECT_VAR_DECL(expected,uint,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,uint,32,4) [] = { 0x33333333, 0x33333333,
-					 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,uint,64,2) [] = { 0x3333333333333333,
-					 0x3333333333333333 };
 VECT_VAR_DECL(expected,poly,8,16) [] = { 0x6, 0x6, 0x6, 0x6, 0x6, 0x6, 0x6, 0x6,
 					 0x6, 0x6, 0x6, 0x6, 0x6, 0x6, 0x6, 0x6 };
-VECT_VAR_DECL(expected,poly,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,4) [] = { 0x33333333, 0x33333333,
-					   0x33333333, 0x33333333 };
 
 #define INSN_NAME vcnt
 #define TEST_MSG "VCNT/VCNTQ"
@@ -86,7 +63,12 @@ FNNAME (INSN_NAME)
   TEST_UNARY_OP(INSN_NAME, q, uint, u, 8, 16);
   TEST_UNARY_OP(INSN_NAME, q, poly, p, 8, 16);
 
-  CHECK_RESULTS (TEST_MSG, "");
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected, "");
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected, "");
+  CHECK(TEST_MSG, poly, 8, 8, PRIx8, expected, "");
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected, "");
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected, "");
+  CHECK(TEST_MSG, poly, 8, 16, PRIx8, expected, "");
 }
 
 int main (void)
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vcombine.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vcombine.c
@@ -3,20 +3,6 @@
 #include "compute-ref-data.h"
 
 /* Expected results.  */
-VECT_VAR_DECL(expected,int,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-				       0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,int,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,int,32,2) [] = { 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,int,64,1) [] = { 0x3333333333333333 };
-VECT_VAR_DECL(expected,uint,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,uint,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,uint,32,2) [] = { 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,uint,64,1) [] = { 0x3333333333333333 };
-VECT_VAR_DECL(expected,poly,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,2) [] = { 0x33333333, 0x33333333 };
 VECT_VAR_DECL(expected,int,8,16) [] = { 0xf0, 0xf1, 0xf2, 0xf3,
 					0xf4, 0xf5, 0xf6, 0xf7,
 					0x11, 0x11, 0x11, 0x11,
@@ -88,7 +74,17 @@ void exec_vcombine (void)
   TEST_VCOMBINE(poly, p, 16, 4, 8);
   TEST_VCOMBINE(float, f, 32, 2, 4);
 
-  CHECK_RESULTS (TEST_MSG, "");
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected, "");
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected, "");
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected, "");
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected, "");
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected, "");
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected, "");
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected, "");
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected, "");
+  CHECK(TEST_MSG, poly, 8, 16, PRIx8, expected, "");
+  CHECK(TEST_MSG, poly, 16, 8, PRIx16, expected, "");
+  CHECK_FP(TEST_MSG, float, 32, 4, PRIx32, expected, "");
 }
 
 int main (void)
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vcreate.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vcreate.c
@@ -17,34 +17,6 @@ VECT_VAR_DECL(expected,poly,8,8) [] = { 0xf0, 0xde, 0xbc, 0x9a,
 					0x78, 0x56, 0x34, 0x12 };
 VECT_VAR_DECL(expected,poly,16,4) [] = { 0xdef0, 0x9abc, 0x5678, 0x1234 };
 VECT_VAR_DECL(expected,hfloat,32,2) [] = { 0x9abcdef0, 0x12345678 };
-VECT_VAR_DECL(expected,int,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,int,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,int,32,4) [] = { 0x33333333, 0x33333333,
-					0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,int,64,2) [] = { 0x3333333333333333,
-					0x3333333333333333 };
-VECT_VAR_DECL(expected,uint,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,uint,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,uint,32,4) [] = { 0x33333333, 0x33333333,
-					 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,uint,64,2) [] = { 0x3333333333333333,
-					 0x3333333333333333 };
-VECT_VAR_DECL(expected,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,4) [] = { 0x33333333, 0x33333333,
-					   0x33333333, 0x33333333 };
 
 #define INSN_NAME vcreate
 #define TEST_MSG "VCREATE"
@@ -113,7 +85,17 @@ FNNAME (INSN_NAME)
   TEST_VCREATE(poly, p, 8, 8);
   TEST_VCREATE(poly, p, 16, 4);
 
-  CHECK_RESULTS (TEST_MSG, "");
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected, "");
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected, "");
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected, "");
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected, "");
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected, "");
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected, "");
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected, "");
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected, "");
+  CHECK(TEST_MSG, poly, 8, 8, PRIx8, expected, "");
+  CHECK(TEST_MSG, poly, 16, 4, PRIx16, expected, "");
+  CHECK_FP(TEST_MSG, float, 32, 2, PRIx32, expected, "");
 }
 
 int main (void)
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/veor.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/veor.c
@@ -14,10 +14,6 @@ VECT_VAR_DECL(expected,uint,8,8) [] = { 0xe4, 0xe5, 0xe6, 0xe7,
 VECT_VAR_DECL(expected,uint,16,4) [] = { 0xffee, 0xffef, 0xffec, 0xffed };
 VECT_VAR_DECL(expected,uint,32,2) [] = { 0xffffffd8, 0xffffffd9 };
 VECT_VAR_DECL(expected,uint,64,1) [] = { 0xfffffffffffffff2 };
-VECT_VAR_DECL(expected,poly,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,2) [] = { 0x33333333, 0x33333333 };
 VECT_VAR_DECL(expected,int,8,16) [] = { 0x6, 0x7, 0x4, 0x5,
 					0x2, 0x3, 0x0, 0x1,
 					0xe, 0xf, 0xc, 0xd,
@@ -37,11 +33,3 @@ VECT_VAR_DECL(expected,uint,32,4) [] = { 0xffffffc7, 0xffffffc6,
 					 0xffffffc5, 0xffffffc4 };
 VECT_VAR_DECL(expected,uint,64,2) [] = { 0xfffffffffffffff3,
 					 0xfffffffffffffff2 };
-VECT_VAR_DECL(expected,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,4) [] = { 0x33333333, 0x33333333,
-					   0x33333333, 0x33333333 };
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vget_high.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vget_high.c
@@ -17,34 +17,6 @@ VECT_VAR_DECL(expected,poly,8,8) [] = { 0xf8, 0xf9, 0xfa, 0xfb,
 					0xfc, 0xfd, 0xfe, 0xff };
 VECT_VAR_DECL(expected,poly,16,4) [] = { 0xfff4, 0xfff5, 0xfff6, 0xfff7 };
 VECT_VAR_DECL(expected,hfloat,32,2) [] = { 0xc1600000, 0xc1500000 };
-VECT_VAR_DECL(expected,int,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,int,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,int,32,4) [] = { 0x33333333, 0x33333333,
-					0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,int,64,2) [] = { 0x3333333333333333,
-					0x3333333333333333 };
-VECT_VAR_DECL(expected,uint,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,uint,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,uint,32,4) [] = { 0x33333333, 0x33333333,
-					 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,uint,64,2) [] = { 0x3333333333333333,
-					 0x3333333333333333 };
-VECT_VAR_DECL(expected,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,4) [] = { 0x33333333, 0x33333333,
-					   0x33333333, 0x33333333 };
 
 #define TEST_MSG "VGET_HIGH"
 void exec_vget_high (void)
@@ -76,7 +48,17 @@ void exec_vget_high (void)
   TEST_VGET_HIGH(poly, p, 16, 4, 8);
   TEST_VGET_HIGH(float, f, 32, 2, 4);
 
-  CHECK_RESULTS (TEST_MSG, "");
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected, "");
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected, "");
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected, "");
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected, "");
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected, "");
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected, "");
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected, "");
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected, "");
+  CHECK(TEST_MSG, poly, 8, 8, PRIx8, expected, "");
+  CHECK(TEST_MSG, poly, 16, 4, PRIx16, expected, "");
+  CHECK_FP(TEST_MSG, float, 32, 2, PRIx32, expected, "");
 }
 
 int main (void)
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vget_lane.c
@@ -0,0 +1,125 @@
+#include <arm_neon.h>
+#include "arm-neon-ref.h"
+#include "compute-ref-data.h"
+
+/* Expected results.  */
+int8_t     expected_s8   = 0xf7;
+int16_t    expected_s16  = 0xfff3;
+int32_t    expected_s32  = 0xfffffff1;
+int64_t    expected_s64  = 0xfffffffffffffff0;
+uint8_t    expected_u8   = 0xf6;
+uint16_t   expected_u16  = 0xfff2;
+uint32_t   expected_u32  = 0xfffffff1;
+uint64_t   expected_u64  = 0xfffffffffffffff0;
+poly8_t    expected_p8   = 0xf6;
+poly16_t   expected_p16  = 0xfff2;
+hfloat32_t expected_f32  = 0xc1700000;
+
+int8_t     expectedq_s8  = 0xff;
+int16_t    expectedq_s16 = 0xfff5;
+int32_t    expectedq_s32 = 0xfffffff3;
+int64_t    expectedq_s64 = 0xfffffffffffffff1;
+uint8_t    expectedq_u8  = 0xfe;
+uint16_t   expectedq_u16 = 0xfff6;
+uint32_t   expectedq_u32 = 0xfffffff2;
+uint64_t   expectedq_u64 = 0xfffffffffffffff1;
+poly8_t    expectedq_p8  = 0xfe;
+poly16_t   expectedq_p16 = 0xfff6;
+hfloat32_t expectedq_f32 = 0xc1500000;
+
+int error_found = 0;
+
+#define TEST_MSG "VGET_LANE"
+void exec_vget_lane (void)
+{
+  /* vec=vget_lane(vec, lane), then store the result.  */
+#define TEST_VGET_LANE(Q, T1, T2, W, N, L)				   \
+  VAR(var, T1, W) = vget##Q##_lane_##T2##W(VECT_VAR(vector, T1, W, N), L); \
+  if (VAR(var, T1, W) != expected##Q##_##T2##W) {			   \
+    fprintf(stderr,							   \
+	    "ERROR in %s (%s line %d in result '%s') at type %s "	   \
+	    "got 0x%" PRIx##W " != 0x%" PRIx##W "\n",			   \
+	    TEST_MSG, __FILE__, __LINE__,				   \
+	    STR(expected##Q##_##T2##W),					   \
+	    STR(VECT_NAME(T1, W, N)),					   \
+	    VAR(var, T1, W),						   \
+	    expected##Q##_##T2##W);					   \
+    error_found = 1;							   \
+  }
+
+  /* Special variant for floating-point.  */
+  union {
+    uint32_t var_int32;
+    float32_t var_float32;
+  } var_int32_float32;
+
+#define TEST_VGET_LANE_FP(Q, T1, T2, W, N, L)				   \
+  VAR(var, T1, W) = vget##Q##_lane_##T2##W(VECT_VAR(vector, T1, W, N), L); \
+  var_int##W##_float##W.var_float##W = VAR(var, T1, W);			   \
+  if (var_int##W##_float##W.var_int##W != expected##Q##_##T2##W) {	   \
+    fprintf(stderr,							   \
+	    "ERROR in %s (%s line %d in result '%s') at type %s "	   \
+	    "got 0x%" PRIx##W " != 0x%" PRIx##W "\n",			   \
+	    TEST_MSG, __FILE__, __LINE__,				   \
+	    STR(expected##Q##_##T2##W),					   \
+	    STR(VECT_NAME(T1, W, N)),					   \
+	    var_int##W##_float##W.var_int##W,				   \
+	    expected##Q##_##T2##W);					   \
+    error_found = 1;							   \
+  }
+
+  DECL_VARIABLE_ALL_VARIANTS(vector);
+
+  /* Scalar variables.  */
+  VAR_DECL(var, int, 8);
+  VAR_DECL(var, int, 16);
+  VAR_DECL(var, int, 32);
+  VAR_DECL(var, int, 64);
+  VAR_DECL(var, uint, 8);
+  VAR_DECL(var, uint, 16);
+  VAR_DECL(var, uint, 32);
+  VAR_DECL(var, uint, 64);
+  VAR_DECL(var, poly, 8);
+  VAR_DECL(var, poly, 16);
+  VAR_DECL(var, float, 32);
+
+  /* Initialize input values.  */
+  TEST_MACRO_ALL_VARIANTS_2_5(VLOAD, vector, buffer);
+  VLOAD(vector, buffer, , float, f, 32, 2);
+  VLOAD(vector, buffer, q, float, f, 32, 4);
+
+  /* Choose lane arbitrarily.  */
+  TEST_VGET_LANE(, int, s, 8, 8, 7);
+  TEST_VGET_LANE(, int, s, 16, 4, 3);
+  TEST_VGET_LANE(, int, s, 32, 2, 1);
+  TEST_VGET_LANE(, int, s, 64, 1, 0);
+  TEST_VGET_LANE(, uint, u, 8, 8, 6);
+  TEST_VGET_LANE(, uint, u, 16, 4, 2);
+  TEST_VGET_LANE(, uint, u, 32, 2, 1);
+  TEST_VGET_LANE(, uint, u, 64, 1, 0);
+  TEST_VGET_LANE(, poly, p, 8, 8, 6);
+  TEST_VGET_LANE(, poly, p, 16, 4, 2);
+  TEST_VGET_LANE_FP(, float, f, 32, 2, 1);
+
+  TEST_VGET_LANE(q, int, s, 8, 16, 15);
+  TEST_VGET_LANE(q, int, s, 16, 8, 5);
+  TEST_VGET_LANE(q, int, s, 32, 4, 3);
+  TEST_VGET_LANE(q, int, s, 64, 2, 1);
+  TEST_VGET_LANE(q, uint, u, 8, 16, 14);
+  TEST_VGET_LANE(q, uint, u, 16, 8, 6);
+  TEST_VGET_LANE(q, uint, u, 32, 4, 2);
+  TEST_VGET_LANE(q, uint, u, 64, 2, 1);
+  TEST_VGET_LANE(q, poly, p, 8, 16, 14);
+  TEST_VGET_LANE(q, poly, p, 16, 8, 6);
+  TEST_VGET_LANE_FP(q, float, f, 32, 4, 3);
+}
+
+int main (void)
+{
+  exec_vget_lane ();
+
+  if (error_found)
+    abort();
+
+  return 0;
+}
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vget_low.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vget_low.c
@@ -17,34 +17,6 @@ VECT_VAR_DECL(expected,poly,8,8) [] = { 0xf0, 0xf1, 0xf2, 0xf3,
 					0xf4, 0xf5, 0xf6, 0xf7 };
 VECT_VAR_DECL(expected,poly,16,4) [] = { 0xfff0, 0xfff1, 0xfff2, 0xfff3 };
 VECT_VAR_DECL(expected,hfloat,32,2) [] = { 0xc1800000, 0xc1700000 };
-VECT_VAR_DECL(expected,int,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,int,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,int,32,4) [] = { 0x33333333, 0x33333333,
-					0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,int,64,2) [] = { 0x3333333333333333,
-					0x3333333333333333 };
-VECT_VAR_DECL(expected,uint,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,uint,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,uint,32,4) [] = { 0x33333333, 0x33333333,
-					 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,uint,64,2) [] = { 0x3333333333333333,
-					 0x3333333333333333 };
-VECT_VAR_DECL(expected,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,4) [] = { 0x33333333, 0x33333333,
-					   0x33333333, 0x33333333 };
 
 #define TEST_MSG "VGET_LOW"
 void exec_vget_low (void)
@@ -76,7 +48,17 @@ void exec_vget_low (void)
   TEST_VGET_LOW(poly, p, 16, 4, 8);
   TEST_VGET_LOW(float, f, 32, 2, 4);
 
-  CHECK_RESULTS (TEST_MSG, "");
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected, "");
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected, "");
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected, "");
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected, "");
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected, "");
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected, "");
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected, "");
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected, "");
+  CHECK(TEST_MSG, poly, 8, 8, PRIx8, expected, "");
+  CHECK(TEST_MSG, poly, 16, 4, PRIx16, expected, "");
+  CHECK_FP(TEST_MSG, float, 32, 2, PRIx32, expected, "");
 }
 
 int main (void)
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld2_lane_f32_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+float32x2x2_t
+f_vld2_lane_f32 (float32_t * p, float32x2x2_t v)
+{
+  float32x2x2_t res;
+  /* { dg-error "lane 2 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  res = vld2_lane_f32 (p, v, 2);
+  /* { dg-error "lane -1 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  res = vld2_lane_f32 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld2_lane_f64_indices_1.c
@@ -0,0 +1,17 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+/* { dg-skip-if "" { arm*-*-* } } */
+
+float64x1x2_t
+f_vld2_lane_f64 (float64_t * p, float64x1x2_t v)
+{
+  float64x1x2_t res;
+  /* { dg-error "lane 1 out of range 0 - 0" "" { xfail arm*-*-* } 0 } */
+  res = vld2_lane_f64 (p, v, 1);
+  /* { dg-error "lane -1 out of range 0 - 0" "" { xfail arm*-*-* } 0 } */
+  res = vld2_lane_f64 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld2_lane_p8_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+poly8x8x2_t
+f_vld2_lane_p8 (poly8_t * p, poly8x8x2_t v)
+{
+  poly8x8x2_t res;
+  /* { dg-error "lane 8 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  res = vld2_lane_p8 (p, v, 8);
+  /* { dg-error "lane -1 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  res = vld2_lane_p8 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld2_lane_s16_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+int16x4x2_t
+f_vld2_lane_s16 (int16_t * p, int16x4x2_t v)
+{
+  int16x4x2_t res;
+  /* { dg-error "lane 4 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  res = vld2_lane_s16 (p, v, 4);
+  /* { dg-error "lane -1 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  res = vld2_lane_s16 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld2_lane_s32_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+int32x2x2_t
+f_vld2_lane_s32 (int32_t * p, int32x2x2_t v)
+{
+  int32x2x2_t res;
+  /* { dg-error "lane 2 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  res = vld2_lane_s32 (p, v, 2);
+  /* { dg-error "lane -1 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  res = vld2_lane_s32 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld2_lane_s64_indices_1.c
@@ -0,0 +1,17 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+/* { dg-skip-if "" { arm*-*-* } } */
+
+int64x1x2_t
+f_vld2_lane_s64 (int64_t * p, int64x1x2_t v)
+{
+  int64x1x2_t res;
+  /* { dg-error "lane 1 out of range 0 - 0" "" { xfail arm*-*-* } 0 } */
+  res = vld2_lane_s64 (p, v, 1);
+  /* { dg-error "lane -1 out of range 0 - 0" "" { xfail arm*-*-* } 0 } */
+  res = vld2_lane_s64 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld2_lane_s8_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+int8x8x2_t
+f_vld2_lane_s8 (int8_t * p, int8x8x2_t v)
+{
+  int8x8x2_t res;
+  /* { dg-error "lane 8 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  res = vld2_lane_s8 (p, v, 8);
+  /* { dg-error "lane -1 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  res = vld2_lane_s8 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld2_lane_u16_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+uint16x4x2_t
+f_vld2_lane_u16 (uint16_t * p, uint16x4x2_t v)
+{
+  uint16x4x2_t res;
+  /* { dg-error "lane 4 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  res = vld2_lane_u16 (p, v, 4);
+  /* { dg-error "lane -1 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  res = vld2_lane_u16 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld2_lane_u32_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+uint32x2x2_t
+f_vld2_lane_u32 (uint32_t * p, uint32x2x2_t v)
+{
+  uint32x2x2_t res;
+  /* { dg-error "lane 2 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  res = vld2_lane_u32 (p, v, 2);
+  /* { dg-error "lane -1 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  res = vld2_lane_u32 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld2_lane_u64_indices_1.c
@@ -0,0 +1,17 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+/* { dg-skip-if "" { arm*-*-* } } */
+
+uint64x1x2_t
+f_vld2_lane_u64 (uint64_t * p, uint64x1x2_t v)
+{
+  uint64x1x2_t res;
+  /* { dg-error "lane 1 out of range 0 - 0" "" { xfail arm*-*-* } 0 } */
+  res = vld2_lane_u64 (p, v, 1);
+  /* { dg-error "lane -1 out of range 0 - 0" "" { xfail arm*-*-* } 0 } */
+  res = vld2_lane_u64 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld2_lane_u8_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+uint8x8x2_t
+f_vld2_lane_u8 (uint8_t * p, uint8x8x2_t v)
+{
+  uint8x8x2_t res;
+  /* { dg-error "lane 8 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  res = vld2_lane_u8 (p, v, 8);
+  /* { dg-error "lane -1 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  res = vld2_lane_u8 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld2q_lane_f32_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+float32x4x2_t
+f_vld2q_lane_f32 (float32_t * p, float32x4x2_t v)
+{
+  float32x4x2_t res;
+  /* { dg-error "lane 4 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  res = vld2q_lane_f32 (p, v, 4);
+  /* { dg-error "lane -1 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  res = vld2q_lane_f32 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld2q_lane_f64_indices_1.c
@@ -0,0 +1,17 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+/* { dg-skip-if "" { arm*-*-* } } */
+
+float64x2x2_t
+f_vld2q_lane_f64 (float64_t * p, float64x2x2_t v)
+{
+  float64x2x2_t res;
+  /* { dg-error "lane 2 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  res = vld2q_lane_f64 (p, v, 2);
+  /* { dg-error "lane -1 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  res = vld2q_lane_f64 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld2q_lane_p8_indices_1.c
@@ -0,0 +1,17 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+/* { dg-skip-if "" { arm*-*-* } } */
+
+poly8x16x2_t
+f_vld2q_lane_p8 (poly8_t * p, poly8x16x2_t v)
+{
+  poly8x16x2_t res;
+  /* { dg-error "lane 16 out of range 0 - 15" "" { xfail arm*-*-* } 0 } */
+  res = vld2q_lane_p8 (p, v, 16);
+  /* { dg-error "lane -1 out of range 0 - 15" "" { xfail arm*-*-* } 0 } */
+  res = vld2q_lane_p8 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld2q_lane_s16_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+int16x8x2_t
+f_vld2q_lane_s16 (int16_t * p, int16x8x2_t v)
+{
+  int16x8x2_t res;
+  /* { dg-error "lane 8 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  res = vld2q_lane_s16 (p, v, 8);
+  /* { dg-error "lane -1 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  res = vld2q_lane_s16 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld2q_lane_s32_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+int32x4x2_t
+f_vld2q_lane_s32 (int32_t * p, int32x4x2_t v)
+{
+  int32x4x2_t res;
+  /* { dg-error "lane 4 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  res = vld2q_lane_s32 (p, v, 4);
+  /* { dg-error "lane -1 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  res = vld2q_lane_s32 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld2q_lane_s64_indices_1.c
@@ -0,0 +1,17 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+/* { dg-skip-if "" { arm*-*-* } } */
+
+int64x2x2_t
+f_vld2q_lane_s64 (int64_t * p, int64x2x2_t v)
+{
+  int64x2x2_t res;
+  /* { dg-error "lane 2 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  res = vld2q_lane_s64 (p, v, 2);
+  /* { dg-error "lane -1 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  res = vld2q_lane_s64 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld2q_lane_s8_indices_1.c
@@ -0,0 +1,17 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+/* { dg-skip-if "" { arm*-*-* } } */
+
+int8x16x2_t
+f_vld2q_lane_s8 (int8_t * p, int8x16x2_t v)
+{
+  int8x16x2_t res;
+  /* { dg-error "lane 16 out of range 0 - 15" "" { xfail arm*-*-* } 0 } */
+  res = vld2q_lane_s8 (p, v, 16);
+  /* { dg-error "lane -1 out of range 0 - 15" "" { xfail arm*-*-* } 0 } */
+  res = vld2q_lane_s8 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld2q_lane_u16_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+uint16x8x2_t
+f_vld2q_lane_u16 (uint16_t * p, uint16x8x2_t v)
+{
+  uint16x8x2_t res;
+  /* { dg-error "lane 8 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  res = vld2q_lane_u16 (p, v, 8);
+  /* { dg-error "lane -1 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  res = vld2q_lane_u16 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld2q_lane_u32_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+uint32x4x2_t
+f_vld2q_lane_u32 (uint32_t * p, uint32x4x2_t v)
+{
+  uint32x4x2_t res;
+  /* { dg-error "lane 4 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  res = vld2q_lane_u32 (p, v, 4);
+  /* { dg-error "lane -1 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  res = vld2q_lane_u32 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld2q_lane_u64_indices_1.c
@@ -0,0 +1,17 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+/* { dg-skip-if "" { arm*-*-* } } */
+
+uint64x2x2_t
+f_vld2q_lane_u64 (uint64_t * p, uint64x2x2_t v)
+{
+  uint64x2x2_t res;
+  /* { dg-error "lane 2 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  res = vld2q_lane_u64 (p, v, 2);
+  /* { dg-error "lane -1 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  res = vld2q_lane_u64 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld2q_lane_u8_indices_1.c
@@ -0,0 +1,17 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+/* { dg-skip-if "" { arm*-*-* } } */
+
+uint8x16x2_t
+f_vld2q_lane_u8 (uint8_t * p, uint8x16x2_t v)
+{
+  uint8x16x2_t res;
+  /* { dg-error "lane 16 out of range 0 - 15" "" { xfail arm*-*-* } 0 } */
+  res = vld2q_lane_u8 (p, v, 16);
+  /* { dg-error "lane -1 out of range 0 - 15" "" { xfail arm*-*-* } 0 } */
+  res = vld2q_lane_u8 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld3_lane_f32_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+float32x2x3_t
+f_vld3_lane_f32 (float32_t * p, float32x2x3_t v)
+{
+  float32x2x3_t res;
+  /* { dg-error "lane 2 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  res = vld3_lane_f32 (p, v, 2);
+  /* { dg-error "lane -1 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  res = vld3_lane_f32 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld3_lane_f64_indices_1.c
@@ -0,0 +1,17 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+/* { dg-skip-if "" { arm*-*-* } } */
+
+float64x1x3_t
+f_vld3_lane_f64 (float64_t * p, float64x1x3_t v)
+{
+  float64x1x3_t res;
+  /* { dg-error "lane 1 out of range 0 - 0" "" { xfail arm*-*-* } 0 } */
+  res = vld3_lane_f64 (p, v, 1);
+  /* { dg-error "lane -1 out of range 0 - 0" "" { xfail arm*-*-* } 0 } */
+  res = vld3_lane_f64 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld3_lane_p8_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+poly8x8x3_t
+f_vld3_lane_p8 (poly8_t * p, poly8x8x3_t v)
+{
+  poly8x8x3_t res;
+  /* { dg-error "lane 8 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  res = vld3_lane_p8 (p, v, 8);
+  /* { dg-error "lane -1 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  res = vld3_lane_p8 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld3_lane_s16_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+int16x4x3_t
+f_vld3_lane_s16 (int16_t * p, int16x4x3_t v)
+{
+  int16x4x3_t res;
+  /* { dg-error "lane 4 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  res = vld3_lane_s16 (p, v, 4);
+  /* { dg-error "lane -1 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  res = vld3_lane_s16 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld3_lane_s32_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+int32x2x3_t
+f_vld3_lane_s32 (int32_t * p, int32x2x3_t v)
+{
+  int32x2x3_t res;
+  /* { dg-error "lane 2 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  res = vld3_lane_s32 (p, v, 2);
+  /* { dg-error "lane -1 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  res = vld3_lane_s32 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld3_lane_s64_indices_1.c
@@ -0,0 +1,17 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+/* { dg-skip-if "" { arm*-*-* } } */
+
+int64x1x3_t
+f_vld3_lane_s64 (int64_t * p, int64x1x3_t v)
+{
+  int64x1x3_t res;
+  /* { dg-error "lane 1 out of range 0 - 0" "" { xfail arm*-*-* } 0 } */
+  res = vld3_lane_s64 (p, v, 1);
+  /* { dg-error "lane -1 out of range 0 - 0" "" { xfail arm*-*-* } 0 } */
+  res = vld3_lane_s64 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld3_lane_s8_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+int8x8x3_t
+f_vld3_lane_s8 (int8_t * p, int8x8x3_t v)
+{
+  int8x8x3_t res;
+  /* { dg-error "lane 8 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  res = vld3_lane_s8 (p, v, 8);
+  /* { dg-error "lane -1 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  res = vld3_lane_s8 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld3_lane_u16_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+uint16x4x3_t
+f_vld3_lane_u16 (uint16_t * p, uint16x4x3_t v)
+{
+  uint16x4x3_t res;
+  /* { dg-error "lane 4 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  res = vld3_lane_u16 (p, v, 4);
+  /* { dg-error "lane -1 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  res = vld3_lane_u16 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld3_lane_u32_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+uint32x2x3_t
+f_vld3_lane_u32 (uint32_t * p, uint32x2x3_t v)
+{
+  uint32x2x3_t res;
+  /* { dg-error "lane 2 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  res = vld3_lane_u32 (p, v, 2);
+  /* { dg-error "lane -1 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  res = vld3_lane_u32 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld3_lane_u64_indices_1.c
@@ -0,0 +1,17 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+/* { dg-skip-if "" { arm*-*-* } } */
+
+uint64x1x3_t
+f_vld3_lane_u64 (uint64_t * p, uint64x1x3_t v)
+{
+  uint64x1x3_t res;
+  /* { dg-error "lane 1 out of range 0 - 0" "" { xfail arm*-*-* } 0 } */
+  res = vld3_lane_u64 (p, v, 1);
+  /* { dg-error "lane -1 out of range 0 - 0" "" { xfail arm*-*-* } 0 } */
+  res = vld3_lane_u64 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld3_lane_u8_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+uint8x8x3_t
+f_vld3_lane_u8 (uint8_t * p, uint8x8x3_t v)
+{
+  uint8x8x3_t res;
+  /* { dg-error "lane 8 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  res = vld3_lane_u8 (p, v, 8);
+  /* { dg-error "lane -1 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  res = vld3_lane_u8 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld3q_lane_f32_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+float32x4x3_t
+f_vld3q_lane_f32 (float32_t * p, float32x4x3_t v)
+{
+  float32x4x3_t res;
+  /* { dg-error "lane 4 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  res = vld3q_lane_f32 (p, v, 4);
+  /* { dg-error "lane -1 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  res = vld3q_lane_f32 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld3q_lane_f64_indices_1.c
@@ -0,0 +1,17 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+/* { dg-skip-if "" { arm*-*-* } } */
+
+float64x2x3_t
+f_vld3q_lane_f64 (float64_t * p, float64x2x3_t v)
+{
+  float64x2x3_t res;
+  /* { dg-error "lane 2 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  res = vld3q_lane_f64 (p, v, 2);
+  /* { dg-error "lane -1 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  res = vld3q_lane_f64 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld3q_lane_p8_indices_1.c
@@ -0,0 +1,17 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+/* { dg-skip-if "" { arm*-*-* } } */
+
+poly8x16x3_t
+f_vld3q_lane_p8 (poly8_t * p, poly8x16x3_t v)
+{
+  poly8x16x3_t res;
+  /* { dg-error "lane 16 out of range 0 - 15" "" { xfail arm*-*-* } 0 } */
+  res = vld3q_lane_p8 (p, v, 16);
+  /* { dg-error "lane -1 out of range 0 - 15" "" { xfail arm*-*-* } 0 } */
+  res = vld3q_lane_p8 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld3q_lane_s16_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+int16x8x3_t
+f_vld3q_lane_s16 (int16_t * p, int16x8x3_t v)
+{
+  int16x8x3_t res;
+  /* { dg-error "lane 8 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  res = vld3q_lane_s16 (p, v, 8);
+  /* { dg-error "lane -1 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  res = vld3q_lane_s16 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld3q_lane_s32_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+int32x4x3_t
+f_vld3q_lane_s32 (int32_t * p, int32x4x3_t v)
+{
+  int32x4x3_t res;
+  /* { dg-error "lane 4 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  res = vld3q_lane_s32 (p, v, 4);
+  /* { dg-error "lane -1 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  res = vld3q_lane_s32 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld3q_lane_s64_indices_1.c
@@ -0,0 +1,17 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+/* { dg-skip-if "" { arm*-*-* } } */
+
+int64x2x3_t
+f_vld3q_lane_s64 (int64_t * p, int64x2x3_t v)
+{
+  int64x2x3_t res;
+  /* { dg-error "lane 2 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  res = vld3q_lane_s64 (p, v, 2);
+  /* { dg-error "lane -1 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  res = vld3q_lane_s64 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld3q_lane_s8_indices_1.c
@@ -0,0 +1,17 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+/* { dg-skip-if "" { arm*-*-* } } */
+
+int8x16x3_t
+f_vld3q_lane_s8 (int8_t * p, int8x16x3_t v)
+{
+  int8x16x3_t res;
+  /* { dg-error "lane 16 out of range 0 - 15" "" { xfail arm*-*-* } 0 } */
+  res = vld3q_lane_s8 (p, v, 16);
+  /* { dg-error "lane -1 out of range 0 - 15" "" { xfail arm*-*-* } 0 } */
+  res = vld3q_lane_s8 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld3q_lane_u16_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+uint16x8x3_t
+f_vld3q_lane_u16 (uint16_t * p, uint16x8x3_t v)
+{
+  uint16x8x3_t res;
+  /* { dg-error "lane 8 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  res = vld3q_lane_u16 (p, v, 8);
+  /* { dg-error "lane -1 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  res = vld3q_lane_u16 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld3q_lane_u32_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+uint32x4x3_t
+f_vld3q_lane_u32 (uint32_t * p, uint32x4x3_t v)
+{
+  uint32x4x3_t res;
+  /* { dg-error "lane 4 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  res = vld3q_lane_u32 (p, v, 4);
+  /* { dg-error "lane -1 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  res = vld3q_lane_u32 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld3q_lane_u64_indices_1.c
@@ -0,0 +1,17 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+/* { dg-skip-if "" { arm*-*-* } } */
+
+uint64x2x3_t
+f_vld3q_lane_u64 (uint64_t * p, uint64x2x3_t v)
+{
+  uint64x2x3_t res;
+  /* { dg-error "lane 2 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  res = vld3q_lane_u64 (p, v, 2);
+  /* { dg-error "lane -1 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  res = vld3q_lane_u64 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld3q_lane_u8_indices_1.c
@@ -0,0 +1,17 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+/* { dg-skip-if "" { arm*-*-* } } */
+
+uint8x16x3_t
+f_vld3q_lane_u8 (uint8_t * p, uint8x16x3_t v)
+{
+  uint8x16x3_t res;
+  /* { dg-error "lane 16 out of range 0 - 15" "" { xfail arm*-*-* } 0 } */
+  res = vld3q_lane_u8 (p, v, 16);
+  /* { dg-error "lane -1 out of range 0 - 15" "" { xfail arm*-*-* } 0 } */
+  res = vld3q_lane_u8 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld4_lane_f32_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+float32x2x4_t
+f_vld4_lane_f32 (float32_t * p, float32x2x4_t v)
+{
+  float32x2x4_t res;
+  /* { dg-error "lane 2 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  res = vld4_lane_f32 (p, v, 2);
+  /* { dg-error "lane -1 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  res = vld4_lane_f32 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld4_lane_f64_indices_1.c
@@ -0,0 +1,17 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+/* { dg-skip-if "" { arm*-*-* } } */
+
+float64x1x4_t
+f_vld4_lane_f64 (float64_t * p, float64x1x4_t v)
+{
+  float64x1x4_t res;
+  /* { dg-error "lane 1 out of range 0 - 0" "" { xfail arm*-*-* } 0 } */
+  res = vld4_lane_f64 (p, v, 1);
+  /* { dg-error "lane -1 out of range 0 - 0" "" { xfail arm*-*-* } 0 } */
+  res = vld4_lane_f64 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld4_lane_p8_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+poly8x8x4_t
+f_vld4_lane_p8 (poly8_t * p, poly8x8x4_t v)
+{
+  poly8x8x4_t res;
+  /* { dg-error "lane 8 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  res = vld4_lane_p8 (p, v, 8);
+  /* { dg-error "lane -1 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  res = vld4_lane_p8 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld4_lane_s16_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+int16x4x4_t
+f_vld4_lane_s16 (int16_t * p, int16x4x4_t v)
+{
+  int16x4x4_t res;
+  /* { dg-error "lane 4 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  res = vld4_lane_s16 (p, v, 4);
+  /* { dg-error "lane -1 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  res = vld4_lane_s16 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld4_lane_s32_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+int32x2x4_t
+f_vld4_lane_s32 (int32_t * p, int32x2x4_t v)
+{
+  int32x2x4_t res;
+  /* { dg-error "lane 2 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  res = vld4_lane_s32 (p, v, 2);
+  /* { dg-error "lane -1 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  res = vld4_lane_s32 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld4_lane_s64_indices_1.c
@@ -0,0 +1,17 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+/* { dg-skip-if "" { arm*-*-* } } */
+
+int64x1x4_t
+f_vld4_lane_s64 (int64_t * p, int64x1x4_t v)
+{
+  int64x1x4_t res;
+  /* { dg-error "lane 1 out of range 0 - 0" "" { xfail arm*-*-* } 0 } */
+  res = vld4_lane_s64 (p, v, 1);
+  /* { dg-error "lane -1 out of range 0 - 0" "" { xfail arm*-*-* } 0 } */
+  res = vld4_lane_s64 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld4_lane_s8_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+int8x8x4_t
+f_vld4_lane_s8 (int8_t * p, int8x8x4_t v)
+{
+  int8x8x4_t res;
+  /* { dg-error "lane 8 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  res = vld4_lane_s8 (p, v, 8);
+  /* { dg-error "lane -1 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  res = vld4_lane_s8 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld4_lane_u16_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+uint16x4x4_t
+f_vld4_lane_u16 (uint16_t * p, uint16x4x4_t v)
+{
+  uint16x4x4_t res;
+  /* { dg-error "lane 4 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  res = vld4_lane_u16 (p, v, 4);
+  /* { dg-error "lane -1 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  res = vld4_lane_u16 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld4_lane_u32_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+uint32x2x4_t
+f_vld4_lane_u32 (uint32_t * p, uint32x2x4_t v)
+{
+  uint32x2x4_t res;
+  /* { dg-error "lane 2 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  res = vld4_lane_u32 (p, v, 2);
+  /* { dg-error "lane -1 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  res = vld4_lane_u32 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld4_lane_u64_indices_1.c
@@ -0,0 +1,17 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+/* { dg-skip-if "" { arm*-*-* } } */
+
+uint64x1x4_t
+f_vld4_lane_u64 (uint64_t * p, uint64x1x4_t v)
+{
+  uint64x1x4_t res;
+  /* { dg-error "lane 1 out of range 0 - 0" "" { xfail arm*-*-* } 0 } */
+  res = vld4_lane_u64 (p, v, 1);
+  /* { dg-error "lane -1 out of range 0 - 0" "" { xfail arm*-*-* } 0 } */
+  res = vld4_lane_u64 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld4_lane_u8_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+uint8x8x4_t
+f_vld4_lane_u8 (uint8_t * p, uint8x8x4_t v)
+{
+  uint8x8x4_t res;
+  /* { dg-error "lane 8 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  res = vld4_lane_u8 (p, v, 8);
+  /* { dg-error "lane -1 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  res = vld4_lane_u8 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld4q_lane_f32_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+float32x4x4_t
+f_vld4q_lane_f32 (float32_t * p, float32x4x4_t v)
+{
+  float32x4x4_t res;
+  /* { dg-error "lane 4 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  res = vld4q_lane_f32 (p, v, 4);
+  /* { dg-error "lane -1 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  res = vld4q_lane_f32 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld4q_lane_f64_indices_1.c
@@ -0,0 +1,17 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+/* { dg-skip-if "" { arm*-*-* } } */
+
+float64x2x4_t
+f_vld4q_lane_f64 (float64_t * p, float64x2x4_t v)
+{
+  float64x2x4_t res;
+  /* { dg-error "lane 2 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  res = vld4q_lane_f64 (p, v, 2);
+  /* { dg-error "lane -1 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  res = vld4q_lane_f64 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld4q_lane_p8_indices_1.c
@@ -0,0 +1,17 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+/* { dg-skip-if "" { arm*-*-* } } */
+
+poly8x16x4_t
+f_vld4q_lane_p8 (poly8_t * p, poly8x16x4_t v)
+{
+  poly8x16x4_t res;
+  /* { dg-error "lane 16 out of range 0 - 15" "" { xfail arm*-*-* } 0 } */
+  res = vld4q_lane_p8 (p, v, 16);
+  /* { dg-error "lane -1 out of range 0 - 15" "" { xfail arm*-*-* } 0 } */
+  res = vld4q_lane_p8 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld4q_lane_s16_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+int16x8x4_t
+f_vld4q_lane_s16 (int16_t * p, int16x8x4_t v)
+{
+  int16x8x4_t res;
+  /* { dg-error "lane 8 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  res = vld4q_lane_s16 (p, v, 8);
+  /* { dg-error "lane -1 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  res = vld4q_lane_s16 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld4q_lane_s32_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+int32x4x4_t
+f_vld4q_lane_s32 (int32_t * p, int32x4x4_t v)
+{
+  int32x4x4_t res;
+  /* { dg-error "lane 4 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  res = vld4q_lane_s32 (p, v, 4);
+  /* { dg-error "lane -1 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  res = vld4q_lane_s32 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld4q_lane_s64_indices_1.c
@@ -0,0 +1,17 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+/* { dg-skip-if "" { arm*-*-* } } */
+
+int64x2x4_t
+f_vld4q_lane_s64 (int64_t * p, int64x2x4_t v)
+{
+  int64x2x4_t res;
+  /* { dg-error "lane 2 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  res = vld4q_lane_s64 (p, v, 2);
+  /* { dg-error "lane -1 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  res = vld4q_lane_s64 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld4q_lane_s8_indices_1.c
@@ -0,0 +1,17 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+/* { dg-skip-if "" { arm*-*-* } } */
+
+int8x16x4_t
+f_vld4q_lane_s8 (int8_t * p, int8x16x4_t v)
+{
+  int8x16x4_t res;
+  /* { dg-error "lane 16 out of range 0 - 15" "" { xfail arm*-*-* } 0 } */
+  res = vld4q_lane_s8 (p, v, 16);
+  /* { dg-error "lane -1 out of range 0 - 15" "" { xfail arm*-*-* } 0 } */
+  res = vld4q_lane_s8 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld4q_lane_u16_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+uint16x8x4_t
+f_vld4q_lane_u16 (uint16_t * p, uint16x8x4_t v)
+{
+  uint16x8x4_t res;
+  /* { dg-error "lane 8 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  res = vld4q_lane_u16 (p, v, 8);
+  /* { dg-error "lane -1 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  res = vld4q_lane_u16 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld4q_lane_u32_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+uint32x4x4_t
+f_vld4q_lane_u32 (uint32_t * p, uint32x4x4_t v)
+{
+  uint32x4x4_t res;
+  /* { dg-error "lane 4 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  res = vld4q_lane_u32 (p, v, 4);
+  /* { dg-error "lane -1 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  res = vld4q_lane_u32 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld4q_lane_u64_indices_1.c
@@ -0,0 +1,17 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+/* { dg-skip-if "" { arm*-*-* } } */
+
+uint64x2x4_t
+f_vld4q_lane_u64 (uint64_t * p, uint64x2x4_t v)
+{
+  uint64x2x4_t res;
+  /* { dg-error "lane 2 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  res = vld4q_lane_u64 (p, v, 2);
+  /* { dg-error "lane -1 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  res = vld4q_lane_u64 (p, v, -1);
+  return res;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld4q_lane_u8_indices_1.c
@@ -0,0 +1,17 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+/* { dg-skip-if "" { arm*-*-* } } */
+
+uint8x16x4_t
+f_vld4q_lane_u8 (uint8_t * p, uint8x16x4_t v)
+{
+  uint8x16x4_t res;
+  /* { dg-error "lane 16 out of range 0 - 15" "" { xfail arm*-*-* } 0 } */
+  res = vld4q_lane_u8 (p, v, 16);
+  /* { dg-error "lane -1 out of range 0 - 15" "" { xfail arm*-*-* } 0 } */
+  res = vld4q_lane_u8 (p, v, -1);
+  return res;
+}
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vldX.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vldX.c
@@ -27,8 +27,6 @@ VECT_VAR_DECL(expected_vld2_0,int,16,8) [] = { 0xfff0, 0xfff1, 0xfff2, 0xfff3,
 					       0xfff4, 0xfff5, 0xfff6, 0xfff7 };
 VECT_VAR_DECL(expected_vld2_0,int,32,4) [] = { 0xfffffff0, 0xfffffff1,
 					       0xfffffff2, 0xfffffff3 };
-VECT_VAR_DECL(expected_vld2_0,int,64,2) [] = { 0x3333333333333333,
-					       0x3333333333333333 };
 VECT_VAR_DECL(expected_vld2_0,uint,8,16) [] = { 0xf0, 0xf1, 0xf2, 0xf3,
 						0xf4, 0xf5, 0xf6, 0xf7,
 						0xf8, 0xf9, 0xfa, 0xfb,
@@ -37,8 +35,6 @@ VECT_VAR_DECL(expected_vld2_0,uint,16,8) [] = { 0xfff0, 0xfff1, 0xfff2, 0xfff3,
 						0xfff4, 0xfff5, 0xfff6, 0xfff7 };
 VECT_VAR_DECL(expected_vld2_0,uint,32,4) [] = { 0xfffffff0, 0xfffffff1,
 						0xfffffff2, 0xfffffff3 };
-VECT_VAR_DECL(expected_vld2_0,uint,64,2) [] = { 0x3333333333333333,
-						0x3333333333333333 };
 VECT_VAR_DECL(expected_vld2_0,poly,8,16) [] = { 0xf0, 0xf1, 0xf2, 0xf3,
 						0xf4, 0xf5, 0xf6, 0xf7,
 						0xf8, 0xf9, 0xfa, 0xfb,
@@ -71,8 +67,6 @@ VECT_VAR_DECL(expected_vld2_1,int,16,8) [] = { 0xfff8, 0xfff9, 0xfffa, 0xfffb,
 					       0xfffc, 0xfffd, 0xfffe, 0xffff };
 VECT_VAR_DECL(expected_vld2_1,int,32,4) [] = { 0xfffffff4, 0xfffffff5,
 					       0xfffffff6, 0xfffffff7 };
-VECT_VAR_DECL(expected_vld2_1,int,64,2) [] = { 0x3333333333333333,
-					       0x3333333333333333 };
 VECT_VAR_DECL(expected_vld2_1,uint,8,16) [] = { 0x0, 0x1, 0x2, 0x3,
 						0x4, 0x5, 0x6, 0x7,
 						0x8, 0x9, 0xa, 0xb,
@@ -81,8 +75,6 @@ VECT_VAR_DECL(expected_vld2_1,uint,16,8) [] = { 0xfff8, 0xfff9, 0xfffa, 0xfffb,
 						0xfffc, 0xfffd, 0xfffe, 0xffff };
 VECT_VAR_DECL(expected_vld2_1,uint,32,4) [] = { 0xfffffff4, 0xfffffff5,
 						0xfffffff6, 0xfffffff7 };
-VECT_VAR_DECL(expected_vld2_1,uint,64,2) [] = { 0x3333333333333333,
-						0x3333333333333333 };
 VECT_VAR_DECL(expected_vld2_1,poly,8,16) [] = { 0x0, 0x1, 0x2, 0x3,
 						0x4, 0x5, 0x6, 0x7,
 						0x8, 0x9, 0xa, 0xb,
@@ -115,8 +107,6 @@ VECT_VAR_DECL(expected_vld3_0,int,16,8) [] = { 0xfff0, 0xfff1, 0xfff2, 0xfff3,
 					       0xfff4, 0xfff5, 0xfff6, 0xfff7 };
 VECT_VAR_DECL(expected_vld3_0,int,32,4) [] = { 0xfffffff0, 0xfffffff1,
 					       0xfffffff2, 0xfffffff3 };
-VECT_VAR_DECL(expected_vld3_0,int,64,2) [] = { 0x3333333333333333,
-					       0x3333333333333333 };
 VECT_VAR_DECL(expected_vld3_0,uint,8,16) [] = { 0xf0, 0xf1, 0xf2, 0xf3,
 						0xf4, 0xf5, 0xf6, 0xf7,
 						0xf8, 0xf9, 0xfa, 0xfb,
@@ -125,8 +115,6 @@ VECT_VAR_DECL(expected_vld3_0,uint,16,8) [] = { 0xfff0, 0xfff1, 0xfff2, 0xfff3,
 						0xfff4, 0xfff5, 0xfff6, 0xfff7 };
 VECT_VAR_DECL(expected_vld3_0,uint,32,4) [] = { 0xfffffff0, 0xfffffff1,
 						0xfffffff2, 0xfffffff3 };
-VECT_VAR_DECL(expected_vld3_0,uint,64,2) [] = { 0x3333333333333333,
-						0x3333333333333333 };
 VECT_VAR_DECL(expected_vld3_0,poly,8,16) [] = { 0xf0, 0xf1, 0xf2, 0xf3,
 						0xf4, 0xf5, 0xf6, 0xf7,
 						0xf8, 0xf9, 0xfa, 0xfb,
@@ -159,8 +147,6 @@ VECT_VAR_DECL(expected_vld3_1,int,16,8) [] = { 0xfff8, 0xfff9, 0xfffa, 0xfffb,
 					       0xfffc, 0xfffd, 0xfffe, 0xffff };
 VECT_VAR_DECL(expected_vld3_1,int,32,4) [] = { 0xfffffff4, 0xfffffff5,
 					       0xfffffff6, 0xfffffff7 };
-VECT_VAR_DECL(expected_vld3_1,int,64,2) [] = { 0x3333333333333333,
-					       0x3333333333333333 };
 VECT_VAR_DECL(expected_vld3_1,uint,8,16) [] = { 0x0, 0x1, 0x2, 0x3,
 						0x4, 0x5, 0x6, 0x7,
 						0x8, 0x9, 0xa, 0xb,
@@ -169,8 +155,6 @@ VECT_VAR_DECL(expected_vld3_1,uint,16,8) [] = { 0xfff8, 0xfff9, 0xfffa, 0xfffb,
 						0xfffc, 0xfffd, 0xfffe, 0xffff };
 VECT_VAR_DECL(expected_vld3_1,uint,32,4) [] = { 0xfffffff4, 0xfffffff5,
 						0xfffffff6, 0xfffffff7 };
-VECT_VAR_DECL(expected_vld3_1,uint,64,2) [] = { 0x3333333333333333,
-						0x3333333333333333 };
 VECT_VAR_DECL(expected_vld3_1,poly,8,16) [] = { 0x0, 0x1, 0x2, 0x3,
 						0x4, 0x5, 0x6, 0x7,
 						0x8, 0x9, 0xa, 0xb,
@@ -206,8 +190,6 @@ VECT_VAR_DECL(expected_vld3_2,int,16,8) [] = { 0x0, 0x1, 0x2, 0x3,
 					       0x4, 0x5, 0x6, 0x7 };
 VECT_VAR_DECL(expected_vld3_2,int,32,4) [] = { 0xfffffff8, 0xfffffff9,
 					       0xfffffffa, 0xfffffffb };
-VECT_VAR_DECL(expected_vld3_2,int,64,2) [] = { 0x3333333333333333,
-					       0x3333333333333333 };
 VECT_VAR_DECL(expected_vld3_2,uint,8,16) [] = { 0x10, 0x11, 0x12, 0x13,
 						0x14, 0x15, 0x16, 0x17,
 						0x18, 0x19, 0x1a, 0x1b,
@@ -216,8 +198,6 @@ VECT_VAR_DECL(expected_vld3_2,uint,16,8) [] = { 0x0, 0x1, 0x2, 0x3,
 						0x4, 0x5, 0x6, 0x7 };
 VECT_VAR_DECL(expected_vld3_2,uint,32,4) [] = { 0xfffffff8, 0xfffffff9,
 						0xfffffffa, 0xfffffffb };
-VECT_VAR_DECL(expected_vld3_2,uint,64,2) [] = { 0x3333333333333333,
-						0x3333333333333333 };
 VECT_VAR_DECL(expected_vld3_2,poly,8,16) [] = { 0x10, 0x11, 0x12, 0x13,
 						0x14, 0x15, 0x16, 0x17,
 						0x18, 0x19, 0x1a, 0x1b,
@@ -252,8 +232,6 @@ VECT_VAR_DECL(expected_vld4_0,int,16,8) [] = { 0xfff0, 0xfff1, 0xfff2, 0xfff3,
 					       0xfff4, 0xfff5, 0xfff6, 0xfff7 };
 VECT_VAR_DECL(expected_vld4_0,int,32,4) [] = { 0xfffffff0, 0xfffffff1,
 					       0xfffffff2, 0xfffffff3 };
-VECT_VAR_DECL(expected_vld4_0,int,64,2) [] = { 0x3333333333333333,
-					       0x3333333333333333 };
 VECT_VAR_DECL(expected_vld4_0,uint,8,16) [] = { 0xf0, 0xf1, 0xf2, 0xf3,
 						0xf4, 0xf5, 0xf6, 0xf7,
 						0xf8, 0xf9, 0xfa, 0xfb,
@@ -262,8 +240,6 @@ VECT_VAR_DECL(expected_vld4_0,uint,16,8) [] = { 0xfff0, 0xfff1, 0xfff2, 0xfff3,
 						0xfff4, 0xfff5, 0xfff6, 0xfff7 };
 VECT_VAR_DECL(expected_vld4_0,uint,32,4) [] = { 0xfffffff0, 0xfffffff1,
 						0xfffffff2, 0xfffffff3 };
-VECT_VAR_DECL(expected_vld4_0,uint,64,2) [] = { 0x3333333333333333,
-						0x3333333333333333 };
 VECT_VAR_DECL(expected_vld4_0,poly,8,16) [] = { 0xf0, 0xf1, 0xf2, 0xf3,
 						0xf4, 0xf5, 0xf6, 0xf7,
 						0xf8, 0xf9, 0xfa, 0xfb,
@@ -296,8 +272,6 @@ VECT_VAR_DECL(expected_vld4_1,int,16,8) [] = { 0xfff8, 0xfff9, 0xfffa, 0xfffb,
 					       0xfffc, 0xfffd, 0xfffe, 0xffff };
 VECT_VAR_DECL(expected_vld4_1,int,32,4) [] = { 0xfffffff4, 0xfffffff5,
 					       0xfffffff6, 0xfffffff7 };
-VECT_VAR_DECL(expected_vld4_1,int,64,2) [] = { 0x3333333333333333,
-					       0x3333333333333333 };
 VECT_VAR_DECL(expected_vld4_1,uint,8,16) [] = { 0x0, 0x1, 0x2, 0x3,
 						0x4, 0x5, 0x6, 0x7,
 						0x8, 0x9, 0xa, 0xb,
@@ -306,8 +280,6 @@ VECT_VAR_DECL(expected_vld4_1,uint,16,8) [] = { 0xfff8, 0xfff9, 0xfffa, 0xfffb,
 						0xfffc, 0xfffd, 0xfffe, 0xffff };
 VECT_VAR_DECL(expected_vld4_1,uint,32,4) [] = { 0xfffffff4, 0xfffffff5,
 						0xfffffff6, 0xfffffff7 };
-VECT_VAR_DECL(expected_vld4_1,uint,64,2) [] = { 0x3333333333333333,
-						0x3333333333333333 };
 VECT_VAR_DECL(expected_vld4_1,poly,8,16) [] = { 0x0, 0x1, 0x2, 0x3,
 						0x4, 0x5, 0x6, 0x7,
 						0x8, 0x9, 0xa, 0xb,
@@ -340,8 +312,6 @@ VECT_VAR_DECL(expected_vld4_2,int,16,8) [] = { 0x0, 0x1, 0x2, 0x3,
 					       0x4, 0x5, 0x6, 0x7 };
 VECT_VAR_DECL(expected_vld4_2,int,32,4) [] = { 0xfffffff8, 0xfffffff9,
 					       0xfffffffa, 0xfffffffb };
-VECT_VAR_DECL(expected_vld4_2,int,64,2) [] = { 0x3333333333333333,
-					       0x3333333333333333 };
 VECT_VAR_DECL(expected_vld4_2,uint,8,16) [] = { 0x10, 0x11, 0x12, 0x13,
 						0x14, 0x15, 0x16, 0x17,
 						0x18, 0x19, 0x1a, 0x1b,
@@ -350,8 +320,6 @@ VECT_VAR_DECL(expected_vld4_2,uint,16,8) [] = { 0x0, 0x1, 0x2, 0x3,
 						0x4, 0x5, 0x6, 0x7 };
 VECT_VAR_DECL(expected_vld4_2,uint,32,4) [] = { 0xfffffff8, 0xfffffff9,
 						0xfffffffa, 0xfffffffb };
-VECT_VAR_DECL(expected_vld4_2,uint,64,2) [] = { 0x3333333333333333,
-						0x3333333333333333 };
 VECT_VAR_DECL(expected_vld4_2,poly,8,16) [] = { 0x10, 0x11, 0x12, 0x13,
 						0x14, 0x15, 0x16, 0x17,
 						0x18, 0x19, 0x1a, 0x1b,
@@ -384,8 +352,6 @@ VECT_VAR_DECL(expected_vld4_3,int,16,8) [] = { 0x8, 0x9, 0xa, 0xb,
 					       0xc, 0xd, 0xe, 0xf };
 VECT_VAR_DECL(expected_vld4_3,int,32,4) [] = { 0xfffffffc, 0xfffffffd,
 					       0xfffffffe, 0xffffffff };
-VECT_VAR_DECL(expected_vld4_3,int,64,2) [] = { 0x3333333333333333,
-					       0x3333333333333333 };
 VECT_VAR_DECL(expected_vld4_3,uint,8,16) [] = { 0x20, 0x21, 0x22, 0x23,
 						0x24, 0x25, 0x26, 0x27,
 						0x28, 0x29, 0x2a, 0x2b,
@@ -394,8 +360,6 @@ VECT_VAR_DECL(expected_vld4_3,uint,16,8) [] = { 0x8, 0x9, 0xa, 0xb,
 						0xc, 0xd, 0xe, 0xf };
 VECT_VAR_DECL(expected_vld4_3,uint,32,4) [] = { 0xfffffffc, 0xfffffffd,
 						0xfffffffe, 0xffffffff };
-VECT_VAR_DECL(expected_vld4_3,uint,64,2) [] = { 0x3333333333333333,
-						0x3333333333333333 };
 VECT_VAR_DECL(expected_vld4_3,poly,8,16) [] = { 0x20, 0x21, 0x22, 0x23,
 						0x24, 0x25, 0x26, 0x27,
 						0x28, 0x29, 0x2a, 0x2b,
@@ -500,6 +464,32 @@ void exec_vldX (void)
   TEST_EXTRA_CHUNK(poly, 16, 8, X, Y);		\
   TEST_EXTRA_CHUNK(float, 32, 4, X, Y)
 
+  /* vldX supports all vector types except [u]int64x2.  */
+#define CHECK_RESULTS_VLDX(test_name,EXPECTED,comment)			\
+  {									\
+    CHECK(test_name, int, 8, 8, PRIx8, EXPECTED, comment);		\
+    CHECK(test_name, int, 16, 4, PRIx16, EXPECTED, comment);		\
+    CHECK(test_name, int, 32, 2, PRIx32, EXPECTED, comment);		\
+    CHECK(test_name, int, 64, 1, PRIx64, EXPECTED, comment);		\
+    CHECK(test_name, uint, 8, 8, PRIx8, EXPECTED, comment);		\
+    CHECK(test_name, uint, 16, 4, PRIx16, EXPECTED, comment);		\
+    CHECK(test_name, uint, 32, 2, PRIx32, EXPECTED, comment);		\
+    CHECK(test_name, uint, 64, 1, PRIx64, EXPECTED, comment);		\
+    CHECK(test_name, poly, 8, 8, PRIx8, EXPECTED, comment);		\
+    CHECK(test_name, poly, 16, 4, PRIx16, EXPECTED, comment);		\
+    CHECK_FP(test_name, float, 32, 2, PRIx32, EXPECTED, comment);	\
+									\
+    CHECK(test_name, int, 8, 16, PRIx8, EXPECTED, comment);		\
+    CHECK(test_name, int, 16, 8, PRIx16, EXPECTED, comment);		\
+    CHECK(test_name, int, 32, 4, PRIx32, EXPECTED, comment);		\
+    CHECK(test_name, uint, 8, 16, PRIx8, EXPECTED, comment);		\
+    CHECK(test_name, uint, 16, 8, PRIx16, EXPECTED, comment);		\
+    CHECK(test_name, uint, 32, 4, PRIx32, EXPECTED, comment);		\
+    CHECK(test_name, poly, 8, 16, PRIx8, EXPECTED, comment);		\
+    CHECK(test_name, poly, 16, 8, PRIx16, EXPECTED, comment);		\
+    CHECK_FP(test_name, float, 32, 4, PRIx32, EXPECTED, comment);	\
+  }									\
+
   DECL_ALL_VLDX(2);
   DECL_ALL_VLDX(3);
   DECL_ALL_VLDX(4);
@@ -650,39 +640,39 @@ void exec_vldX (void)
   clean_results ();
 #define TEST_MSG "VLD2/VLD2Q"
   TEST_ALL_VLDX(2);
-  CHECK_RESULTS_NAMED (TEST_MSG, expected_vld2_0, "chunk 0");
+  CHECK_RESULTS_VLDX (TEST_MSG, expected_vld2_0, "chunk 0");
 
   TEST_ALL_EXTRA_CHUNKS(2, 1);
-  CHECK_RESULTS_NAMED (TEST_MSG, expected_vld2_1, "chunk 1");
+  CHECK_RESULTS_VLDX (TEST_MSG, expected_vld2_1, "chunk 1");
 
   /* Check vld3/vld3q.  */
   clean_results ();
 #undef TEST_MSG
 #define TEST_MSG "VLD3/VLD3Q"
   TEST_ALL_VLDX(3);
-  CHECK_RESULTS_NAMED (TEST_MSG, expected_vld3_0, "chunk 0");
+  CHECK_RESULTS_VLDX (TEST_MSG, expected_vld3_0, "chunk 0");
 
   TEST_ALL_EXTRA_CHUNKS(3, 1);
-  CHECK_RESULTS_NAMED (TEST_MSG, expected_vld3_1, "chunk 1");
+  CHECK_RESULTS_VLDX (TEST_MSG, expected_vld3_1, "chunk 1");
 
   TEST_ALL_EXTRA_CHUNKS(3, 2);
-  CHECK_RESULTS_NAMED (TEST_MSG, expected_vld3_2, "chunk 2");
+  CHECK_RESULTS_VLDX (TEST_MSG, expected_vld3_2, "chunk 2");
 
   /* Check vld4/vld4q.  */
   clean_results ();
 #undef TEST_MSG
 #define TEST_MSG "VLD4/VLD4Q"
   TEST_ALL_VLDX(4);
-  CHECK_RESULTS_NAMED (TEST_MSG, expected_vld4_0, "chunk 0");
+  CHECK_RESULTS_VLDX (TEST_MSG, expected_vld4_0, "chunk 0");
 
   TEST_ALL_EXTRA_CHUNKS(4, 1);
-  CHECK_RESULTS_NAMED (TEST_MSG, expected_vld4_1, "chunk 1");
+  CHECK_RESULTS_VLDX (TEST_MSG, expected_vld4_1, "chunk 1");
 
   TEST_ALL_EXTRA_CHUNKS(4, 2);
-  CHECK_RESULTS_NAMED (TEST_MSG, expected_vld4_2, "chunk 2");
+  CHECK_RESULTS_VLDX (TEST_MSG, expected_vld4_2, "chunk 2");
 
   TEST_ALL_EXTRA_CHUNKS(4, 3);
-  CHECK_RESULTS_NAMED (TEST_MSG, expected_vld4_3, "chunk 3");
+  CHECK_RESULTS_VLDX (TEST_MSG, expected_vld4_3, "chunk 3");
 }
 
 int main (void)
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vldX_dup.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vldX_dup.c
@@ -19,34 +19,6 @@ VECT_VAR_DECL(expected_vld2_0,poly,8,8) [] = { 0xf0, 0xf1, 0xf0, 0xf1,
 					0xf0, 0xf1, 0xf0, 0xf1 };
 VECT_VAR_DECL(expected_vld2_0,poly,16,4) [] = { 0xfff0, 0xfff1, 0xfff0, 0xfff1 };
 VECT_VAR_DECL(expected_vld2_0,hfloat,32,2) [] = { 0xc1800000, 0xc1700000 };
-VECT_VAR_DECL(expected_vld2_0,int,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected_vld2_0,int,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected_vld2_0,int,32,4) [] = { 0x33333333, 0x33333333,
-					0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected_vld2_0,int,64,2) [] = { 0x3333333333333333,
-					0x3333333333333333 };
-VECT_VAR_DECL(expected_vld2_0,uint,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected_vld2_0,uint,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected_vld2_0,uint,32,4) [] = { 0x33333333, 0x33333333,
-					 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected_vld2_0,uint,64,2) [] = { 0x3333333333333333,
-					 0x3333333333333333 };
-VECT_VAR_DECL(expected_vld2_0,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected_vld2_0,poly,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected_vld2_0,hfloat,32,4) [] = { 0x33333333, 0x33333333,
-					   0x33333333, 0x33333333 };
 
 /* vld2_dup/chunk 1.  */
 VECT_VAR_DECL(expected_vld2_1,int,8,8) [] = { 0xf0, 0xf1, 0xf0, 0xf1,
@@ -64,34 +36,6 @@ VECT_VAR_DECL(expected_vld2_1,poly,8,8) [] = { 0xf0, 0xf1, 0xf0, 0xf1,
 VECT_VAR_DECL(expected_vld2_1,poly,16,4) [] = { 0xfff0, 0xfff1,
 						0xfff0, 0xfff1 };
 VECT_VAR_DECL(expected_vld2_1,hfloat,32,2) [] = { 0xc1800000, 0xc1700000 };
-VECT_VAR_DECL(expected_vld2_1,int,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					       0x33, 0x33, 0x33, 0x33,
-					       0x33, 0x33, 0x33, 0x33,
-					       0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected_vld2_1,int,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					       0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected_vld2_1,int,32,4) [] = { 0x33333333, 0x33333333,
-					       0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected_vld2_1,int,64,2) [] = { 0x3333333333333333,
-					       0x3333333333333333 };
-VECT_VAR_DECL(expected_vld2_1,uint,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected_vld2_1,uint,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-						0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected_vld2_1,uint,32,4) [] = { 0x33333333, 0x33333333,
-						0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected_vld2_1,uint,64,2) [] = { 0x3333333333333333,
-						0x3333333333333333 };
-VECT_VAR_DECL(expected_vld2_1,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected_vld2_1,poly,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-						0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected_vld2_1,hfloat,32,4) [] = { 0x33333333, 0x33333333,
-						  0x33333333, 0x33333333 };
 
 /* vld3_dup/chunk 0.  */
 VECT_VAR_DECL(expected_vld3_0,int,8,8) [] = { 0xf0, 0xf1, 0xf2, 0xf0,
@@ -111,34 +55,6 @@ VECT_VAR_DECL(expected_vld3_0,poly,8,8) [] = { 0xf0, 0xf1, 0xf2, 0xf0,
 VECT_VAR_DECL(expected_vld3_0,poly,16,4) [] = { 0xfff0, 0xfff1,
 						0xfff2, 0xfff0 };
 VECT_VAR_DECL(expected_vld3_0,hfloat,32,2) [] = { 0xc1800000, 0xc1700000 };
-VECT_VAR_DECL(expected_vld3_0,int,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					       0x33, 0x33, 0x33, 0x33,
-					       0x33, 0x33, 0x33, 0x33,
-					       0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected_vld3_0,int,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					       0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected_vld3_0,int,32,4) [] = { 0x33333333, 0x33333333,
-					       0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected_vld3_0,int,64,2) [] = { 0x3333333333333333,
-					       0x3333333333333333 };
-VECT_VAR_DECL(expected_vld3_0,uint,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected_vld3_0,uint,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-						0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected_vld3_0,uint,32,4) [] = { 0x33333333, 0x33333333,
-						0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected_vld3_0,uint,64,2) [] = { 0x3333333333333333,
-						0x3333333333333333 };
-VECT_VAR_DECL(expected_vld3_0,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected_vld3_0,poly,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-						0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected_vld3_0,hfloat,32,4) [] = { 0x33333333, 0x33333333,
-						  0x33333333, 0x33333333 };
 
 /* vld3_dup/chunk 1.  */
 VECT_VAR_DECL(expected_vld3_1,int,8,8) [] = { 0xf2, 0xf0, 0xf1, 0xf2,
@@ -158,34 +74,6 @@ VECT_VAR_DECL(expected_vld3_1,poly,8,8) [] = { 0xf2, 0xf0, 0xf1, 0xf2,
 VECT_VAR_DECL(expected_vld3_1,poly,16,4) [] = { 0xfff1, 0xfff2,
 						0xfff0, 0xfff1 };
 VECT_VAR_DECL(expected_vld3_1,hfloat,32,2) [] = { 0xc1600000, 0xc1800000 };
-VECT_VAR_DECL(expected_vld3_1,int,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					       0x33, 0x33, 0x33, 0x33,
-					       0x33, 0x33, 0x33, 0x33,
-					       0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected_vld3_1,int,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					       0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected_vld3_1,int,32,4) [] = { 0x33333333, 0x33333333,
-					       0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected_vld3_1,int,64,2) [] = { 0x3333333333333333,
-					       0x3333333333333333 };
-VECT_VAR_DECL(expected_vld3_1,uint,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected_vld3_1,uint,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-						0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected_vld3_1,uint,32,4) [] = { 0x33333333, 0x33333333,
-						0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected_vld3_1,uint,64,2) [] = { 0x3333333333333333,
-						0x3333333333333333 };
-VECT_VAR_DECL(expected_vld3_1,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected_vld3_1,poly,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-						0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected_vld3_1,hfloat,32,4) [] = { 0x33333333, 0x33333333,
-						  0x33333333, 0x33333333 };
 
 /* vld3_dup/chunk 2.  */
 VECT_VAR_DECL(expected_vld3_2,int,8,8) [] = { 0xf1, 0xf2, 0xf0, 0xf1,
@@ -205,34 +93,6 @@ VECT_VAR_DECL(expected_vld3_2,poly,8,8) [] = { 0xf1, 0xf2, 0xf0, 0xf1,
 VECT_VAR_DECL(expected_vld3_2,poly,16,4) [] = { 0xfff2, 0xfff0,
 						0xfff1, 0xfff2 };
 VECT_VAR_DECL(expected_vld3_2,hfloat,32,2) [] = { 0xc1700000, 0xc1600000 };
-VECT_VAR_DECL(expected_vld3_2,int,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					       0x33, 0x33, 0x33, 0x33,
-					       0x33, 0x33, 0x33, 0x33,
-					       0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected_vld3_2,int,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					       0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected_vld3_2,int,32,4) [] = { 0x33333333, 0x33333333,
-					       0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected_vld3_2,int,64,2) [] = { 0x3333333333333333,
-					       0x3333333333333333 };
-VECT_VAR_DECL(expected_vld3_2,uint,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected_vld3_2,uint,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-						0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected_vld3_2,uint,32,4) [] = { 0x33333333, 0x33333333,
-						0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected_vld3_2,uint,64,2) [] = { 0x3333333333333333,
-						0x3333333333333333 };
-VECT_VAR_DECL(expected_vld3_2,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected_vld3_2,poly,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-						0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected_vld3_2,hfloat,32,4) [] = { 0x33333333, 0x33333333,
-						  0x33333333, 0x33333333 };
 
 /* vld4_dup/chunk 0.  */
 VECT_VAR_DECL(expected_vld4_0,int,8,8) [] = { 0xf0, 0xf1, 0xf2, 0xf3,
@@ -250,34 +110,6 @@ VECT_VAR_DECL(expected_vld4_0,poly,8,8) [] = { 0xf0, 0xf1, 0xf2, 0xf3,
 					       0xf0, 0xf1, 0xf2, 0xf3 };
 VECT_VAR_DECL(expected_vld4_0,poly,16,4) [] = { 0xfff0, 0xfff1, 0xfff2, 0xfff3 };
 VECT_VAR_DECL(expected_vld4_0,hfloat,32,2) [] = { 0xc1800000, 0xc1700000 };
-VECT_VAR_DECL(expected_vld4_0,int,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					       0x33, 0x33, 0x33, 0x33,
-					       0x33, 0x33, 0x33, 0x33,
-					       0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected_vld4_0,int,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					       0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected_vld4_0,int,32,4) [] = { 0x33333333, 0x33333333,
-					       0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected_vld4_0,int,64,2) [] = { 0x3333333333333333,
-					       0x3333333333333333 };
-VECT_VAR_DECL(expected_vld4_0,uint,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected_vld4_0,uint,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-						0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected_vld4_0,uint,32,4) [] = { 0x33333333, 0x33333333,
-						0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected_vld4_0,uint,64,2) [] = { 0x3333333333333333,
-						0x3333333333333333 };
-VECT_VAR_DECL(expected_vld4_0,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected_vld4_0,poly,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-						0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected_vld4_0,hfloat,32,4) [] = { 0x33333333, 0x33333333,
-						  0x33333333, 0x33333333 };
 
 /* vld4_dup/chunk 1.  */
 VECT_VAR_DECL(expected_vld4_1,int,8,8) [] = { 0xf0, 0xf1, 0xf2, 0xf3,
@@ -294,34 +126,6 @@ VECT_VAR_DECL(expected_vld4_1,poly,8,8) [] = { 0xf0, 0xf1, 0xf2, 0xf3,
 					       0xf0, 0xf1, 0xf2, 0xf3 };
 VECT_VAR_DECL(expected_vld4_1,poly,16,4) [] = { 0xfff0, 0xfff1, 0xfff2, 0xfff3 };
 VECT_VAR_DECL(expected_vld4_1,hfloat,32,2) [] = { 0xc1600000, 0xc1500000 };
-VECT_VAR_DECL(expected_vld4_1,int,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					       0x33, 0x33, 0x33, 0x33,
-					       0x33, 0x33, 0x33, 0x33,
-					       0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected_vld4_1,int,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					       0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected_vld4_1,int,32,4) [] = { 0x33333333, 0x33333333,
-					       0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected_vld4_1,int,64,2) [] = { 0x3333333333333333,
-					       0x3333333333333333 };
-VECT_VAR_DECL(expected_vld4_1,uint,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected_vld4_1,uint,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-						0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected_vld4_1,uint,32,4) [] = { 0x33333333, 0x33333333,
-						0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected_vld4_1,uint,64,2) [] = { 0x3333333333333333,
-						0x3333333333333333 };
-VECT_VAR_DECL(expected_vld4_1,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected_vld4_1,poly,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-						0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected_vld4_1,hfloat,32,4) [] = { 0x33333333, 0x33333333,
-						  0x33333333, 0x33333333 };
 
 /* vld4_dup/chunk 2.  */
 VECT_VAR_DECL(expected_vld4_2,int,8,8) [] = { 0xf0, 0xf1, 0xf2, 0xf3,
@@ -338,34 +142,6 @@ VECT_VAR_DECL(expected_vld4_2,poly,8,8) [] = { 0xf0, 0xf1, 0xf2, 0xf3,
 					       0xf0, 0xf1, 0xf2, 0xf3 };
 VECT_VAR_DECL(expected_vld4_2,poly,16,4) [] = { 0xfff0, 0xfff1, 0xfff2, 0xfff3 };
 VECT_VAR_DECL(expected_vld4_2,hfloat,32,2) [] = { 0xc1800000, 0xc1700000 };
-VECT_VAR_DECL(expected_vld4_2,int,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					       0x33, 0x33, 0x33, 0x33,
-					       0x33, 0x33, 0x33, 0x33,
-					       0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected_vld4_2,int,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					       0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected_vld4_2,int,32,4) [] = { 0x33333333, 0x33333333,
-					       0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected_vld4_2,int,64,2) [] = { 0x3333333333333333,
-					       0x3333333333333333 };
-VECT_VAR_DECL(expected_vld4_2,uint,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected_vld4_2,uint,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-						0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected_vld4_2,uint,32,4) [] = { 0x33333333, 0x33333333,
-						0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected_vld4_2,uint,64,2) [] = { 0x3333333333333333,
-						0x3333333333333333 };
-VECT_VAR_DECL(expected_vld4_2,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected_vld4_2,poly,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-						0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected_vld4_2,hfloat,32,4) [] = { 0x33333333, 0x33333333,
-						  0x33333333, 0x33333333 };
 
 /* vld4_dup/chunk3.  */
 VECT_VAR_DECL(expected_vld4_3,int,8,8) [] = { 0xf0, 0xf1, 0xf2, 0xf3,
@@ -382,33 +158,6 @@ VECT_VAR_DECL(expected_vld4_3,poly,8,8) [] = { 0xf0, 0xf1, 0xf2, 0xf3,
 					       0xf0, 0xf1, 0xf2, 0xf3 };
 VECT_VAR_DECL(expected_vld4_3,poly,16,4) [] = { 0xfff0, 0xfff1, 0xfff2, 0xfff3 };
 VECT_VAR_DECL(expected_vld4_3,hfloat,32,2) [] = { 0xc1600000, 0xc1500000 };
-VECT_VAR_DECL(expected_vld4_3,int,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					       0x33, 0x33, 0x33, 0x33,
-					       0x33, 0x33, 0x33, 0x33,
-					       0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected_vld4_3,int,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					       0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected_vld4_3,int,32,4) [] = { 0x33333333, 0x33333333,
-					       0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected_vld4_3,int,64,2) [] = { 0x3333333333333333, 0x3333333333333333 };
-VECT_VAR_DECL(expected_vld4_3,uint,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected_vld4_3,uint,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-						0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected_vld4_3,uint,32,4) [] = { 0x33333333, 0x33333333,
-						0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected_vld4_3,uint,64,2) [] = { 0x3333333333333333,
-						0x3333333333333333 };
-VECT_VAR_DECL(expected_vld4_3,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected_vld4_3,poly,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-						0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected_vld4_3,hfloat,32,4) [] = { 0x33333333, 0x33333333,
-						  0x33333333, 0x33333333 };
 
 void exec_vldX_dup (void)
 {
@@ -478,6 +227,21 @@ void exec_vldX_dup (void)
   TEST_EXTRA_CHUNK(poly, 16, 4, X, Y);		\
   TEST_EXTRA_CHUNK(float, 32, 2, X, Y)
 
+  /* vldX_dup supports only 64-bit inputs.  */
+#define CHECK_RESULTS_VLDX_DUP(test_name,EXPECTED,comment)		\
+  {									\
+    CHECK(test_name, int, 8, 8, PRIx8, EXPECTED, comment);		\
+    CHECK(test_name, int, 16, 4, PRIx16, EXPECTED, comment);		\
+    CHECK(test_name, int, 32, 2, PRIx32, EXPECTED, comment);		\
+    CHECK(test_name, int, 64, 1, PRIx64, EXPECTED, comment);		\
+    CHECK(test_name, uint, 8, 8, PRIx8, EXPECTED, comment);		\
+    CHECK(test_name, uint, 16, 4, PRIx16, EXPECTED, comment);		\
+    CHECK(test_name, uint, 32, 2, PRIx32, EXPECTED, comment);		\
+    CHECK(test_name, uint, 64, 1, PRIx64, EXPECTED, comment);		\
+    CHECK(test_name, poly, 8, 8, PRIx8, EXPECTED, comment);		\
+    CHECK(test_name, poly, 16, 4, PRIx16, EXPECTED, comment);		\
+    CHECK_FP(test_name, float, 32, 2, PRIx32, EXPECTED, comment);	\
+  }									\
 
   DECL_ALL_VLDX_DUP(2);
   DECL_ALL_VLDX_DUP(3);
@@ -629,39 +393,39 @@ void exec_vldX_dup (void)
   clean_results ();
 #define TEST_MSG "VLD2_DUP/VLD2Q_DUP"
   TEST_ALL_VLDX_DUP(2);
-  CHECK_RESULTS_NAMED (TEST_MSG, expected_vld2_0, "chunk 0");
+  CHECK_RESULTS_VLDX_DUP (TEST_MSG, expected_vld2_0, "chunk 0");
 
   TEST_ALL_EXTRA_CHUNKS(2, 1);
-  CHECK_RESULTS_NAMED (TEST_MSG, expected_vld2_1, "chunk 1");
+  CHECK_RESULTS_VLDX_DUP (TEST_MSG, expected_vld2_1, "chunk 1");
 
   /* Check vld3_dup/vld3q_dup.  */
   clean_results ();
 #undef TEST_MSG
 #define TEST_MSG "VLD3_DUP/VLD3Q_DUP"
   TEST_ALL_VLDX_DUP(3);
-  CHECK_RESULTS_NAMED (TEST_MSG, expected_vld3_0, "chunk 0");
+  CHECK_RESULTS_VLDX_DUP (TEST_MSG, expected_vld3_0, "chunk 0");
 
   TEST_ALL_EXTRA_CHUNKS(3, 1);
-  CHECK_RESULTS_NAMED (TEST_MSG, expected_vld3_1, "chunk 1");
+  CHECK_RESULTS_VLDX_DUP (TEST_MSG, expected_vld3_1, "chunk 1");
 
   TEST_ALL_EXTRA_CHUNKS(3, 2);
-  CHECK_RESULTS_NAMED (TEST_MSG, expected_vld3_2, "chunk 2");
+  CHECK_RESULTS_VLDX_DUP (TEST_MSG, expected_vld3_2, "chunk 2");
 
   /* Check vld4_dup/vld4q_dup */
   clean_results ();
 #undef TEST_MSG
 #define TEST_MSG "VLD4_DUP/VLD4Q_DUP"
   TEST_ALL_VLDX_DUP(4);
-  CHECK_RESULTS_NAMED (TEST_MSG, expected_vld4_0, "chunk 0");
+  CHECK_RESULTS_VLDX_DUP (TEST_MSG, expected_vld4_0, "chunk 0");
 
   TEST_ALL_EXTRA_CHUNKS(4, 1);
-  CHECK_RESULTS_NAMED (TEST_MSG, expected_vld4_1, "chunk 1");
+  CHECK_RESULTS_VLDX_DUP (TEST_MSG, expected_vld4_1, "chunk 1");
 
   TEST_ALL_EXTRA_CHUNKS(4, 2);
-  CHECK_RESULTS_NAMED (TEST_MSG, expected_vld4_2, "chunk 2");
+  CHECK_RESULTS_VLDX_DUP (TEST_MSG, expected_vld4_2, "chunk 2");
 
   TEST_ALL_EXTRA_CHUNKS(4, 3);
-  CHECK_RESULTS_NAMED (TEST_MSG, expected_vld4_3, "chunk 3");
+  CHECK_RESULTS_VLDX_DUP (TEST_MSG, expected_vld4_3, "chunk 3");
 }
 
 int main (void)
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vldX_lane.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vldX_lane.c
@@ -9,42 +9,24 @@ VECT_VAR_DECL(expected_vld2_0,int,8,8) [] = { 0xaa, 0xaa, 0xaa, 0xaa,
 					      0xaa, 0xaa, 0xaa, 0xaa };
 VECT_VAR_DECL(expected_vld2_0,int,16,4) [] = { 0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa };
 VECT_VAR_DECL(expected_vld2_0,int,32,2) [] = { 0xfffffff0, 0xfffffff1 };
-VECT_VAR_DECL(expected_vld2_0,int,64,1) [] = { 0x3333333333333333 };
 VECT_VAR_DECL(expected_vld2_0,uint,8,8) [] = { 0xaa, 0xaa, 0xaa, 0xaa,
 					       0xaa, 0xaa, 0xaa, 0xaa };
 VECT_VAR_DECL(expected_vld2_0,uint,16,4) [] = { 0xaaaa, 0xaaaa,
 						0xaaaa, 0xaaaa };
 VECT_VAR_DECL(expected_vld2_0,uint,32,2) [] = { 0xaaaaaaaa, 0xaaaaaaaa };
-VECT_VAR_DECL(expected_vld2_0,uint,64,1) [] = { 0x3333333333333333 };
 VECT_VAR_DECL(expected_vld2_0,poly,8,8) [] = { 0xaa, 0xaa, 0xaa, 0xaa,
 					       0xaa, 0xaa, 0xaa, 0xaa };
 VECT_VAR_DECL(expected_vld2_0,poly,16,4) [] = { 0xaaaa, 0xaaaa,
 						0xaaaa, 0xaaaa };
 VECT_VAR_DECL(expected_vld2_0,hfloat,32,2) [] = { 0xc1800000, 0xc1700000 };
-VECT_VAR_DECL(expected_vld2_0,int,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					       0x33, 0x33, 0x33, 0x33,
-					       0x33, 0x33, 0x33, 0x33,
-					       0x33, 0x33, 0x33, 0x33 };
 VECT_VAR_DECL(expected_vld2_0,int,16,8) [] = { 0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa,
 					       0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa };
 VECT_VAR_DECL(expected_vld2_0,int,32,4) [] = { 0xaaaaaaaa, 0xaaaaaaaa,
 					       0xaaaaaaaa, 0xaaaaaaaa };
-VECT_VAR_DECL(expected_vld2_0,int,64,2) [] = { 0x3333333333333333,
-					       0x3333333333333333 };
-VECT_VAR_DECL(expected_vld2_0,uint,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33 };
 VECT_VAR_DECL(expected_vld2_0,uint,16,8) [] = { 0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa,
 						0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa };
 VECT_VAR_DECL(expected_vld2_0,uint,32,4) [] = { 0xfffffff0, 0xfffffff1,
 						0xaaaaaaaa, 0xaaaaaaaa };
-VECT_VAR_DECL(expected_vld2_0,uint,64,2) [] = { 0x3333333333333333,
-						0x3333333333333333 };
-VECT_VAR_DECL(expected_vld2_0,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33 };
 VECT_VAR_DECL(expected_vld2_0,poly,16,8) [] = { 0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa,
 						0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa };
 VECT_VAR_DECL(expected_vld2_0,hfloat,32,4) [] = { 0xaaaaaaaa, 0xaaaaaaaa,
@@ -55,40 +37,22 @@ VECT_VAR_DECL(expected_vld2_1,int,8,8) [] = { 0xaa, 0xaa, 0xaa, 0xaa,
 					      0xaa, 0xaa, 0xf0, 0xf1 };
 VECT_VAR_DECL(expected_vld2_1,int,16,4) [] = { 0xfff0, 0xfff1, 0xaaaa, 0xaaaa };
 VECT_VAR_DECL(expected_vld2_1,int,32,2) [] = { 0xaaaaaaaa, 0xaaaaaaaa };
-VECT_VAR_DECL(expected_vld2_1,int,64,1) [] = { 0x3333333333333333 };
 VECT_VAR_DECL(expected_vld2_1,uint,8,8) [] = { 0xf0, 0xf1, 0xaa, 0xaa,
 					       0xaa, 0xaa, 0xaa, 0xaa };
 VECT_VAR_DECL(expected_vld2_1,uint,16,4) [] = { 0xaaaa, 0xaaaa, 0xfff0, 0xfff1 };
 VECT_VAR_DECL(expected_vld2_1,uint,32,2) [] = { 0xfffffff0, 0xfffffff1 };
-VECT_VAR_DECL(expected_vld2_1,uint,64,1) [] = { 0x3333333333333333 };
 VECT_VAR_DECL(expected_vld2_1,poly,8,8) [] = { 0xf0, 0xf1, 0xaa, 0xaa,
 					       0xaa, 0xaa, 0xaa, 0xaa };
 VECT_VAR_DECL(expected_vld2_1,poly,16,4) [] = { 0xaaaa, 0xaaaa, 0xfff0, 0xfff1 };
 VECT_VAR_DECL(expected_vld2_1,hfloat,32,2) [] = { 0xaaaaaaaa, 0xaaaaaaaa };
-VECT_VAR_DECL(expected_vld2_1,int,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					       0x33, 0x33, 0x33, 0x33,
-					       0x33, 0x33, 0x33, 0x33,
-					       0x33, 0x33, 0x33, 0x33 };
 VECT_VAR_DECL(expected_vld2_1,int,16,8) [] = { 0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa,
 					       0xfff0, 0xfff1, 0xaaaa, 0xaaaa };
 VECT_VAR_DECL(expected_vld2_1,int,32,4) [] = { 0xfffffff0, 0xfffffff1,
 					       0xaaaaaaaa, 0xaaaaaaaa };
-VECT_VAR_DECL(expected_vld2_1,int,64,2) [] = { 0x3333333333333333,
-					       0x3333333333333333 };
-VECT_VAR_DECL(expected_vld2_1,uint,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33 };
 VECT_VAR_DECL(expected_vld2_1,uint,16,8) [] = { 0xaaaa, 0xaaaa, 0xfff0, 0xfff1,
 						0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa };
 VECT_VAR_DECL(expected_vld2_1,uint,32,4) [] = { 0xaaaaaaaa, 0xaaaaaaaa,
 						0xaaaaaaaa, 0xaaaaaaaa };
-VECT_VAR_DECL(expected_vld2_1,uint,64,2) [] = { 0x3333333333333333,
-						0x3333333333333333 };
-VECT_VAR_DECL(expected_vld2_1,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33 };
 VECT_VAR_DECL(expected_vld2_1,poly,16,8) [] = { 0xaaaa, 0xaaaa, 0xfff0, 0xfff1,
 						0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa };
 VECT_VAR_DECL(expected_vld2_1,hfloat,32,4) [] = { 0xc1800000, 0xc1700000,
@@ -99,40 +63,22 @@ VECT_VAR_DECL(expected_vld3_0,int,8,8) [] = { 0xaa, 0xaa, 0xaa, 0xaa,
 					      0xaa, 0xaa, 0xaa, 0xaa };
 VECT_VAR_DECL(expected_vld3_0,int,16,4) [] = { 0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa };
 VECT_VAR_DECL(expected_vld3_0,int,32,2) [] = { 0xfffffff0, 0xfffffff1 };
-VECT_VAR_DECL(expected_vld3_0,int,64,1) [] = { 0x3333333333333333 };
 VECT_VAR_DECL(expected_vld3_0,uint,8,8) [] = { 0xaa, 0xaa, 0xaa, 0xaa,
 					       0xaa, 0xaa, 0xaa, 0xaa };
 VECT_VAR_DECL(expected_vld3_0,uint,16,4) [] = { 0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa };
 VECT_VAR_DECL(expected_vld3_0,uint,32,2) [] = { 0xaaaaaaaa, 0xaaaaaaaa };
-VECT_VAR_DECL(expected_vld3_0,uint,64,1) [] = { 0x3333333333333333 };
 VECT_VAR_DECL(expected_vld3_0,poly,8,8) [] = { 0xaa, 0xaa, 0xaa, 0xaa,
 					       0xaa, 0xaa, 0xaa, 0xaa };
 VECT_VAR_DECL(expected_vld3_0,poly,16,4) [] = { 0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa };
 VECT_VAR_DECL(expected_vld3_0,hfloat,32,2) [] = { 0xc1800000, 0xc1700000 };
-VECT_VAR_DECL(expected_vld3_0,int,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					       0x33, 0x33, 0x33, 0x33,
-					       0x33, 0x33, 0x33, 0x33,
-					       0x33, 0x33, 0x33, 0x33 };
 VECT_VAR_DECL(expected_vld3_0,int,16,8) [] = { 0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa,
 					       0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa };
 VECT_VAR_DECL(expected_vld3_0,int,32,4) [] = { 0xaaaaaaaa, 0xaaaaaaaa,
 					       0xaaaaaaaa, 0xaaaaaaaa };
-VECT_VAR_DECL(expected_vld3_0,int,64,2) [] = { 0x3333333333333333,
-					       0x3333333333333333 };
-VECT_VAR_DECL(expected_vld3_0,uint,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33 };
 VECT_VAR_DECL(expected_vld3_0,uint,16,8) [] = { 0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa,
 						0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa };
 VECT_VAR_DECL(expected_vld3_0,uint,32,4) [] = { 0xfffffff0, 0xfffffff1,
 						0xfffffff2, 0xaaaaaaaa };
-VECT_VAR_DECL(expected_vld3_0,uint,64,2) [] = { 0x3333333333333333,
-						0x3333333333333333 };
-VECT_VAR_DECL(expected_vld3_0,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33 };
 VECT_VAR_DECL(expected_vld3_0,poly,16,8) [] = { 0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa,
 						0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa };
 VECT_VAR_DECL(expected_vld3_0,hfloat,32,4) [] = { 0xaaaaaaaa, 0xaaaaaaaa,
@@ -143,40 +89,22 @@ VECT_VAR_DECL(expected_vld3_1,int,8,8) [] = { 0xaa, 0xaa, 0xaa, 0xaa,
 					      0xaa, 0xaa, 0xaa, 0xaa };
 VECT_VAR_DECL(expected_vld3_1,int,16,4) [] = { 0xaaaa, 0xaaaa, 0xfff0, 0xfff1 };
 VECT_VAR_DECL(expected_vld3_1,int,32,2) [] = { 0xfffffff2, 0xaaaaaaaa };
-VECT_VAR_DECL(expected_vld3_1,int,64,1) [] = { 0x3333333333333333 };
 VECT_VAR_DECL(expected_vld3_1,uint,8,8) [] = { 0xaa, 0xaa, 0xaa, 0xaa,
 					       0xf0, 0xf1, 0xf2, 0xaa };
 VECT_VAR_DECL(expected_vld3_1,uint,16,4) [] = { 0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa };
 VECT_VAR_DECL(expected_vld3_1,uint,32,2) [] = { 0xaaaaaaaa, 0xfffffff0 };
-VECT_VAR_DECL(expected_vld3_1,uint,64,1) [] = { 0x3333333333333333 };
 VECT_VAR_DECL(expected_vld3_1,poly,8,8) [] = { 0xaa, 0xaa, 0xaa, 0xaa,
 					       0xf0, 0xf1, 0xf2, 0xaa };
 VECT_VAR_DECL(expected_vld3_1,poly,16,4) [] = { 0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa };
 VECT_VAR_DECL(expected_vld3_1,hfloat,32,2) [] = { 0xc1600000, 0xaaaaaaaa };
-VECT_VAR_DECL(expected_vld3_1,int,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					       0x33, 0x33, 0x33, 0x33,
-					       0x33, 0x33, 0x33, 0x33,
-					       0x33, 0x33, 0x33, 0x33 };
 VECT_VAR_DECL(expected_vld3_1,int,16,8) [] = { 0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa,
 					       0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa };
 VECT_VAR_DECL(expected_vld3_1,int,32,4) [] = { 0xaaaaaaaa, 0xaaaaaaaa,
 					       0xfffffff0, 0xfffffff1 };
-VECT_VAR_DECL(expected_vld3_1,int,64,2) [] = { 0x3333333333333333,
-					       0x3333333333333333 };
-VECT_VAR_DECL(expected_vld3_1,uint,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33 };
 VECT_VAR_DECL(expected_vld3_1,uint,16,8) [] = { 0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa,
 						0xaaaa, 0xaaaa, 0xaaaa, 0xfff0 };
 VECT_VAR_DECL(expected_vld3_1,uint,32,4) [] = { 0xaaaaaaaa, 0xaaaaaaaa,
 						0xaaaaaaaa, 0xaaaaaaaa };
-VECT_VAR_DECL(expected_vld3_1,uint,64,2) [] = { 0x3333333333333333,
-						0x3333333333333333 };
-VECT_VAR_DECL(expected_vld3_1,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33 };
 VECT_VAR_DECL(expected_vld3_1,poly,16,8) [] = { 0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa,
 						0xaaaa, 0xaaaa, 0xaaaa, 0xfff0 };
 VECT_VAR_DECL(expected_vld3_1,hfloat,32,4) [] = { 0xaaaaaaaa, 0xaaaaaaaa,
@@ -187,40 +115,22 @@ VECT_VAR_DECL(expected_vld3_2,int,8,8) [] = { 0xaa, 0xaa, 0xaa, 0xaa,
 					      0xaa, 0xf0, 0xf1, 0xf2 };
 VECT_VAR_DECL(expected_vld3_2,int,16,4) [] = { 0xfff2, 0xaaaa, 0xaaaa, 0xaaaa };
 VECT_VAR_DECL(expected_vld3_2,int,32,2) [] = { 0xaaaaaaaa, 0xaaaaaaaa };
-VECT_VAR_DECL(expected_vld3_2,int,64,1) [] = { 0x3333333333333333 };
 VECT_VAR_DECL(expected_vld3_2,uint,8,8) [] = { 0xaa, 0xaa, 0xaa, 0xaa,
 					       0xaa, 0xaa, 0xaa, 0xaa };
 VECT_VAR_DECL(expected_vld3_2,uint,16,4) [] = { 0xaaaa, 0xfff0, 0xfff1, 0xfff2 };
 VECT_VAR_DECL(expected_vld3_2,uint,32,2) [] = { 0xfffffff1, 0xfffffff2 };
-VECT_VAR_DECL(expected_vld3_2,uint,64,1) [] = { 0x3333333333333333 };
 VECT_VAR_DECL(expected_vld3_2,poly,8,8) [] = { 0xaa, 0xaa, 0xaa, 0xaa,
 					       0xaa, 0xaa, 0xaa, 0xaa };
 VECT_VAR_DECL(expected_vld3_2,poly,16,4) [] = { 0xaaaa, 0xfff0, 0xfff1, 0xfff2 };
 VECT_VAR_DECL(expected_vld3_2,hfloat,32,2) [] = { 0xaaaaaaaa, 0xaaaaaaaa };
-VECT_VAR_DECL(expected_vld3_2,int,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					       0x33, 0x33, 0x33, 0x33,
-					       0x33, 0x33, 0x33, 0x33,
-					       0x33, 0x33, 0x33, 0x33 };
 VECT_VAR_DECL(expected_vld3_2,int,16,8) [] = { 0xaaaa, 0xaaaa, 0xfff0, 0xfff1,
 					       0xfff2, 0xaaaa, 0xaaaa, 0xaaaa };
 VECT_VAR_DECL(expected_vld3_2,int,32,4) [] = { 0xfffffff2, 0xaaaaaaaa,
 					       0xaaaaaaaa, 0xaaaaaaaa };
-VECT_VAR_DECL(expected_vld3_2,int,64,2) [] = { 0x3333333333333333,
-					       0x3333333333333333 };
-VECT_VAR_DECL(expected_vld3_2,uint,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33 };
 VECT_VAR_DECL(expected_vld3_2,uint,16,8) [] = { 0xfff1, 0xfff2, 0xaaaa, 0xaaaa,
 						0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa };
 VECT_VAR_DECL(expected_vld3_2,uint,32,4) [] = { 0xaaaaaaaa, 0xaaaaaaaa,
 						0xaaaaaaaa, 0xaaaaaaaa };
-VECT_VAR_DECL(expected_vld3_2,uint,64,2) [] = { 0x3333333333333333,
-						0x3333333333333333 };
-VECT_VAR_DECL(expected_vld3_2,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33 };
 VECT_VAR_DECL(expected_vld3_2,poly,16,8) [] = { 0xfff1, 0xfff2, 0xaaaa, 0xaaaa,
 						0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa };
 VECT_VAR_DECL(expected_vld3_2,hfloat,32,4) [] = { 0xc1600000, 0xaaaaaaaa,
@@ -231,40 +141,22 @@ VECT_VAR_DECL(expected_vld4_0,int,8,8) [] = { 0xaa, 0xaa, 0xaa, 0xaa,
 					      0xaa, 0xaa, 0xaa, 0xaa };
 VECT_VAR_DECL(expected_vld4_0,int,16,4) [] = { 0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa };
 VECT_VAR_DECL(expected_vld4_0,int,32,2) [] = { 0xfffffff0, 0xfffffff1 };
-VECT_VAR_DECL(expected_vld4_0,int,64,1) [] = { 0x3333333333333333 };
 VECT_VAR_DECL(expected_vld4_0,uint,8,8) [] = { 0xaa, 0xaa, 0xaa, 0xaa,
 					       0xaa, 0xaa, 0xaa, 0xaa };
 VECT_VAR_DECL(expected_vld4_0,uint,16,4) [] = { 0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa };
 VECT_VAR_DECL(expected_vld4_0,uint,32,2) [] = { 0xaaaaaaaa, 0xaaaaaaaa };
-VECT_VAR_DECL(expected_vld4_0,uint,64,1) [] = { 0x3333333333333333 };
 VECT_VAR_DECL(expected_vld4_0,poly,8,8) [] = { 0xaa, 0xaa, 0xaa, 0xaa,
 					       0xaa, 0xaa, 0xaa, 0xaa };
 VECT_VAR_DECL(expected_vld4_0,poly,16,4) [] = { 0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa };
 VECT_VAR_DECL(expected_vld4_0,hfloat,32,2) [] = { 0xc1800000, 0xc1700000 };
-VECT_VAR_DECL(expected_vld4_0,int,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					       0x33, 0x33, 0x33, 0x33,
-					       0x33, 0x33, 0x33, 0x33,
-					       0x33, 0x33, 0x33, 0x33 };
 VECT_VAR_DECL(expected_vld4_0,int,16,8) [] = { 0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa,
 					       0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa };
 VECT_VAR_DECL(expected_vld4_0,int,32,4) [] = { 0xaaaaaaaa, 0xaaaaaaaa,
 					       0xaaaaaaaa, 0xaaaaaaaa };
-VECT_VAR_DECL(expected_vld4_0,int,64,2) [] = { 0x3333333333333333,
-					       0x3333333333333333 };
-VECT_VAR_DECL(expected_vld4_0,uint,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33 };
 VECT_VAR_DECL(expected_vld4_0,uint,16,8) [] = { 0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa,
 						0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa };
 VECT_VAR_DECL(expected_vld4_0,uint,32,4) [] = { 0xfffffff0, 0xfffffff1,
 						0xfffffff2, 0xfffffff3 };
-VECT_VAR_DECL(expected_vld4_0,uint,64,2) [] = { 0x3333333333333333,
-						0x3333333333333333 };
-VECT_VAR_DECL(expected_vld4_0,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33 };
 VECT_VAR_DECL(expected_vld4_0,poly,16,8) [] = { 0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa,
 						0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa };
 VECT_VAR_DECL(expected_vld4_0,hfloat,32,4) [] = { 0xaaaaaaaa, 0xaaaaaaaa,
@@ -275,40 +167,22 @@ VECT_VAR_DECL(expected_vld4_1,int,8,8) [] = { 0xaa, 0xaa, 0xaa, 0xaa,
 					      0xaa, 0xaa, 0xaa, 0xaa };
 VECT_VAR_DECL(expected_vld4_1,int,16,4) [] = { 0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa };
 VECT_VAR_DECL(expected_vld4_1,int,32,2) [] = { 0xfffffff2, 0xfffffff3 };
-VECT_VAR_DECL(expected_vld4_1,int,64,1) [] = { 0x3333333333333333 };
 VECT_VAR_DECL(expected_vld4_1,uint,8,8) [] = { 0xaa, 0xaa, 0xaa, 0xaa,
 					       0xaa, 0xaa, 0xaa, 0xaa };
 VECT_VAR_DECL(expected_vld4_1,uint,16,4) [] = { 0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa };
 VECT_VAR_DECL(expected_vld4_1,uint,32,2) [] = { 0xaaaaaaaa, 0xaaaaaaaa };
-VECT_VAR_DECL(expected_vld4_1,uint,64,1) [] = { 0x3333333333333333 };
 VECT_VAR_DECL(expected_vld4_1,poly,8,8) [] = { 0xaa, 0xaa, 0xaa, 0xaa,
 					       0xaa, 0xaa, 0xaa, 0xaa };
 VECT_VAR_DECL(expected_vld4_1,poly,16,4) [] = { 0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa };
 VECT_VAR_DECL(expected_vld4_1,hfloat,32,2) [] = { 0xc1600000, 0xc1500000 };
-VECT_VAR_DECL(expected_vld4_1,int,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					       0x33, 0x33, 0x33, 0x33,
-					       0x33, 0x33, 0x33, 0x33,
-					       0x33, 0x33, 0x33, 0x33 };
 VECT_VAR_DECL(expected_vld4_1,int,16,8) [] = { 0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa,
 					       0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa };
 VECT_VAR_DECL(expected_vld4_1,int,32,4) [] = { 0xaaaaaaaa, 0xaaaaaaaa,
 					       0xaaaaaaaa, 0xaaaaaaaa };
-VECT_VAR_DECL(expected_vld4_1,int,64,2) [] = { 0x3333333333333333,
-					       0x3333333333333333 };
-VECT_VAR_DECL(expected_vld4_1,uint,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33 };
 VECT_VAR_DECL(expected_vld4_1,uint,16,8) [] = { 0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa,
 						0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa };
 VECT_VAR_DECL(expected_vld4_1,uint,32,4) [] = { 0xaaaaaaaa, 0xaaaaaaaa,
 						0xaaaaaaaa, 0xaaaaaaaa };
-VECT_VAR_DECL(expected_vld4_1,uint,64,2) [] = { 0x3333333333333333,
-						0x3333333333333333 };
-VECT_VAR_DECL(expected_vld4_1,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33 };
 VECT_VAR_DECL(expected_vld4_1,poly,16,8) [] = { 0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa,
 						0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa };
 VECT_VAR_DECL(expected_vld4_1,hfloat,32,4) [] = { 0xaaaaaaaa, 0xaaaaaaaa,
@@ -319,40 +193,22 @@ VECT_VAR_DECL(expected_vld4_2,int,8,8) [] = { 0xaa, 0xaa, 0xaa, 0xaa,
 					      0xaa, 0xaa, 0xaa, 0xaa };
 VECT_VAR_DECL(expected_vld4_2,int,16,4) [] = { 0xfff0, 0xfff1, 0xfff2, 0xfff3 };
 VECT_VAR_DECL(expected_vld4_2,int,32,2) [] = { 0xaaaaaaaa, 0xaaaaaaaa };
-VECT_VAR_DECL(expected_vld4_2,int,64,1) [] = { 0x3333333333333333 };
 VECT_VAR_DECL(expected_vld4_2,uint,8,8) [] = { 0xf0, 0xf1, 0xf2, 0xf3,
 					       0xaa, 0xaa, 0xaa, 0xaa };
 VECT_VAR_DECL(expected_vld4_2,uint,16,4) [] = { 0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa };
 VECT_VAR_DECL(expected_vld4_2,uint,32,2) [] = { 0xfffffff0, 0xfffffff1 };
-VECT_VAR_DECL(expected_vld4_2,uint,64,1) [] = { 0x3333333333333333 };
 VECT_VAR_DECL(expected_vld4_2,poly,8,8) [] = { 0xf0, 0xf1, 0xf2, 0xf3,
 					       0xaa, 0xaa, 0xaa, 0xaa };
 VECT_VAR_DECL(expected_vld4_2,poly,16,4) [] = { 0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa };
 VECT_VAR_DECL(expected_vld4_2,hfloat,32,2) [] = { 0xaaaaaaaa, 0xaaaaaaaa };
-VECT_VAR_DECL(expected_vld4_2,int,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					       0x33, 0x33, 0x33, 0x33,
-					       0x33, 0x33, 0x33, 0x33,
-					       0x33, 0x33, 0x33, 0x33 };
 VECT_VAR_DECL(expected_vld4_2,int,16,8) [] = { 0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa,
 					       0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa };
 VECT_VAR_DECL(expected_vld4_2,int,32,4) [] = { 0xfffffff0, 0xfffffff1,
 					       0xfffffff2, 0xfffffff3 };
-VECT_VAR_DECL(expected_vld4_2,int,64,2) [] = { 0x3333333333333333,
-					       0x3333333333333333 };
-VECT_VAR_DECL(expected_vld4_2,uint,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33 };
 VECT_VAR_DECL(expected_vld4_2,uint,16,8) [] = { 0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa,
 						0xfff0, 0xfff1, 0xfff2, 0xfff3 };
 VECT_VAR_DECL(expected_vld4_2,uint,32,4) [] = { 0xaaaaaaaa, 0xaaaaaaaa,
 						0xaaaaaaaa, 0xaaaaaaaa };
-VECT_VAR_DECL(expected_vld4_2,uint,64,2) [] = { 0x3333333333333333,
-						0x3333333333333333 };
-VECT_VAR_DECL(expected_vld4_2,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33 };
 VECT_VAR_DECL(expected_vld4_2,poly,16,8) [] = { 0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa,
 						0xfff0, 0xfff1, 0xfff2, 0xfff3 };
 VECT_VAR_DECL(expected_vld4_2,hfloat,32,4) [] = { 0xc1800000, 0xc1700000,
@@ -363,40 +219,22 @@ VECT_VAR_DECL(expected_vld4_3,int,8,8) [] = { 0xaa, 0xaa, 0xaa, 0xaa,
 					      0xf0, 0xf1, 0xf2, 0xf3 };
 VECT_VAR_DECL(expected_vld4_3,int,16,4) [] = { 0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa };
 VECT_VAR_DECL(expected_vld4_3,int,32,2) [] = { 0xaaaaaaaa, 0xaaaaaaaa };
-VECT_VAR_DECL(expected_vld4_3,int,64,1) [] = { 0x3333333333333333 };
 VECT_VAR_DECL(expected_vld4_3,uint,8,8) [] = { 0xaa, 0xaa, 0xaa, 0xaa,
 					       0xaa, 0xaa, 0xaa, 0xaa };
 VECT_VAR_DECL(expected_vld4_3,uint,16,4) [] = { 0xfff0, 0xfff1, 0xfff2, 0xfff3 };
 VECT_VAR_DECL(expected_vld4_3,uint,32,2) [] = { 0xfffffff2, 0xfffffff3 };
-VECT_VAR_DECL(expected_vld4_3,uint,64,1) [] = { 0x3333333333333333 };
 VECT_VAR_DECL(expected_vld4_3,poly,8,8) [] = { 0xaa, 0xaa, 0xaa, 0xaa,
 					       0xaa, 0xaa, 0xaa, 0xaa };
 VECT_VAR_DECL(expected_vld4_3,poly,16,4) [] = { 0xfff0, 0xfff1, 0xfff2, 0xfff3 };
 VECT_VAR_DECL(expected_vld4_3,hfloat,32,2) [] = { 0xaaaaaaaa, 0xaaaaaaaa };
-VECT_VAR_DECL(expected_vld4_3,int,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					       0x33, 0x33, 0x33, 0x33,
-					       0x33, 0x33, 0x33, 0x33,
-					       0x33, 0x33, 0x33, 0x33 };
 VECT_VAR_DECL(expected_vld4_3,int,16,8) [] = { 0xfff0, 0xfff1, 0xfff2, 0xfff3,
 					       0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa };
 VECT_VAR_DECL(expected_vld4_3,int,32,4) [] = { 0xaaaaaaaa, 0xaaaaaaaa,
 					       0xaaaaaaaa, 0xaaaaaaaa };
-VECT_VAR_DECL(expected_vld4_3,int,64,2) [] = { 0x3333333333333333,
-					       0x3333333333333333 };
-VECT_VAR_DECL(expected_vld4_3,uint,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33 };
 VECT_VAR_DECL(expected_vld4_3,uint,16,8) [] = { 0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa,
 						0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa };
 VECT_VAR_DECL(expected_vld4_3,uint,32,4) [] = { 0xaaaaaaaa, 0xaaaaaaaa,
 						0xaaaaaaaa, 0xaaaaaaaa };
-VECT_VAR_DECL(expected_vld4_3,uint,64,2) [] = { 0x3333333333333333,
-						0x3333333333333333 };
-VECT_VAR_DECL(expected_vld4_3,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33,
-						0x33, 0x33, 0x33, 0x33 };
 VECT_VAR_DECL(expected_vld4_3,poly,16,8) [] = { 0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa,
 						0xaaaa, 0xaaaa, 0xaaaa, 0xaaaa };
 VECT_VAR_DECL(expected_vld4_3,hfloat,32,4) [] = { 0xaaaaaaaa, 0xaaaaaaaa,
@@ -542,6 +380,26 @@ void exec_vldX_lane (void)
   TEST_EXTRA_CHUNK(float, 32, 2, X, Y);		\
   TEST_EXTRA_CHUNK(float, 32, 4, X, Y)
 
+  /* vldX_lane supports only a subset of all variants.  */
+#define CHECK_RESULTS_VLDX_LANE(test_name,EXPECTED,comment)		\
+  {									\
+    CHECK(test_name, int, 8, 8, PRIx8, EXPECTED, comment);		\
+    CHECK(test_name, int, 16, 4, PRIx16, EXPECTED, comment);		\
+    CHECK(test_name, int, 32, 2, PRIx32, EXPECTED, comment);		\
+    CHECK(test_name, uint, 8, 8, PRIx8, EXPECTED, comment);		\
+    CHECK(test_name, uint, 16, 4, PRIx16, EXPECTED, comment);		\
+    CHECK(test_name, uint, 32, 2, PRIx32, EXPECTED, comment);		\
+    CHECK(test_name, poly, 8, 8, PRIx8, EXPECTED, comment);		\
+    CHECK(test_name, poly, 16, 4, PRIx16, EXPECTED, comment);		\
+    CHECK_FP(test_name, float, 32, 2, PRIx32, EXPECTED, comment);	\
+    CHECK(test_name, int, 16, 8, PRIx16, EXPECTED, comment);		\
+    CHECK(test_name, int, 32, 4, PRIx32, EXPECTED, comment);		\
+    CHECK(test_name, uint, 16, 8, PRIx16, EXPECTED, comment);		\
+    CHECK(test_name, uint, 32, 4, PRIx32, EXPECTED, comment);		\
+    CHECK(test_name, poly, 16, 8, PRIx16, EXPECTED, comment);		\
+    CHECK_FP(test_name, float, 32, 4, PRIx32, EXPECTED, comment);	\
+  }									\
+
   /* Declare the temporary buffers / variables.  */
   DECL_ALL_VLDX_LANE(2);
   DECL_ALL_VLDX_LANE(3);
@@ -568,39 +426,39 @@ void exec_vldX_lane (void)
   clean_results ();
 #define TEST_MSG "VLD2_LANE/VLD2Q_LANE"
   TEST_ALL_VLDX_LANE(2);
-  CHECK_RESULTS_NAMED (TEST_MSG, expected_vld2_0, " chunk 0");
+  CHECK_RESULTS_VLDX_LANE (TEST_MSG, expected_vld2_0, " chunk 0");
 
   TEST_ALL_EXTRA_CHUNKS(2, 1);
-  CHECK_RESULTS_NAMED (TEST_MSG, expected_vld2_1, " chunk 1");
+  CHECK_RESULTS_VLDX_LANE (TEST_MSG, expected_vld2_1, " chunk 1");
 
   /* Check vld3_lane/vld3q_lane.  */
   clean_results ();
 #undef TEST_MSG
 #define TEST_MSG "VLD3_LANE/VLD3Q_LANE"
   TEST_ALL_VLDX_LANE(3);
-  CHECK_RESULTS_NAMED (TEST_MSG, expected_vld3_0, " chunk 0");
+  CHECK_RESULTS_VLDX_LANE (TEST_MSG, expected_vld3_0, " chunk 0");
 
   TEST_ALL_EXTRA_CHUNKS(3, 1);
-  CHECK_RESULTS_NAMED (TEST_MSG, expected_vld3_1, " chunk 1");
+  CHECK_RESULTS_VLDX_LANE (TEST_MSG, expected_vld3_1, " chunk 1");
 
   TEST_ALL_EXTRA_CHUNKS(3, 2);
-  CHECK_RESULTS_NAMED (TEST_MSG, expected_vld3_2, " chunk 2");
+  CHECK_RESULTS_VLDX_LANE (TEST_MSG, expected_vld3_2, " chunk 2");
 
   /* Check vld4_lane/vld4q_lane.  */
   clean_results ();
 #undef TEST_MSG
 #define TEST_MSG "VLD4_LANE/VLD4Q_LANE"
   TEST_ALL_VLDX_LANE(4);
-  CHECK_RESULTS_NAMED (TEST_MSG, expected_vld4_0, " chunk 0");
+  CHECK_RESULTS_VLDX_LANE (TEST_MSG, expected_vld4_0, " chunk 0");
 
   TEST_ALL_EXTRA_CHUNKS(4, 1);
-  CHECK_RESULTS_NAMED (TEST_MSG, expected_vld4_1, " chunk 1");
+  CHECK_RESULTS_VLDX_LANE (TEST_MSG, expected_vld4_1, " chunk 1");
   TEST_ALL_EXTRA_CHUNKS(4, 2);
 
-  CHECK_RESULTS_NAMED (TEST_MSG, expected_vld4_2, " chunk 2");
+  CHECK_RESULTS_VLDX_LANE (TEST_MSG, expected_vld4_2, " chunk 2");
 
   TEST_ALL_EXTRA_CHUNKS(4, 3);
-  CHECK_RESULTS_NAMED (TEST_MSG, expected_vld4_3, " chunk 3");
+  CHECK_RESULTS_VLDX_LANE (TEST_MSG, expected_vld4_3, " chunk 3");
 }
 
 int main (void)
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vmul.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vmul.c
@@ -7,15 +7,12 @@ VECT_VAR_DECL(expected,int,8,8) [] = { 0xf0, 0x1, 0x12, 0x23,
 				       0x34, 0x45, 0x56, 0x67 };
 VECT_VAR_DECL(expected,int,16,4) [] = { 0xfde0, 0xfe02, 0xfe24, 0xfe46 };
 VECT_VAR_DECL(expected,int,32,2) [] = { 0xfffffcd0, 0xfffffd03 };
-VECT_VAR_DECL(expected,int,64,1) [] = { 0x3333333333333333 };
 VECT_VAR_DECL(expected,uint,8,8) [] = { 0xc0, 0x4, 0x48, 0x8c,
 					0xd0, 0x14, 0x58, 0x9c };
 VECT_VAR_DECL(expected,uint,16,4) [] = { 0xfab0, 0xfb05, 0xfb5a, 0xfbaf };
 VECT_VAR_DECL(expected,uint,32,2) [] = { 0xfffff9a0, 0xfffffa06 };
-VECT_VAR_DECL(expected,uint,64,1) [] = { 0x3333333333333333 };
 VECT_VAR_DECL(expected,poly,8,8) [] = { 0xc0, 0x84, 0x48, 0xc,
 					0xd0, 0x94, 0x58, 0x1c };
-VECT_VAR_DECL(expected,poly,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
 VECT_VAR_DECL(expected,hfloat,32,2) [] = { 0xc4053333, 0xc3f9c000 };
 VECT_VAR_DECL(expected,int,8,16) [] = { 0x90, 0x7, 0x7e, 0xf5,
 					0x6c, 0xe3, 0x5a, 0xd1,
@@ -25,8 +22,6 @@ VECT_VAR_DECL(expected,int,16,8) [] = { 0xf780, 0xf808, 0xf890, 0xf918,
 					0xf9a0, 0xfa28, 0xfab0, 0xfb38 };
 VECT_VAR_DECL(expected,int,32,4) [] = { 0xfffff670, 0xfffff709,
 					0xfffff7a2, 0xfffff83b };
-VECT_VAR_DECL(expected,int,64,2) [] = { 0x3333333333333333,
-					0x3333333333333333 };
 VECT_VAR_DECL(expected,uint,8,16) [] = { 0x60, 0xa, 0xb4, 0x5e,
 					 0x8, 0xb2, 0x5c, 0x6,
 					 0xb0, 0x5a, 0x4, 0xae,
@@ -35,14 +30,10 @@ VECT_VAR_DECL(expected,uint,16,8) [] = { 0xf450, 0xf50b, 0xf5c6, 0xf681,
 					 0xf73c, 0xf7f7, 0xf8b2, 0xf96d };
 VECT_VAR_DECL(expected,uint,32,4) [] = { 0xfffff340, 0xfffff40c,
 					 0xfffff4d8, 0xfffff5a4 };
-VECT_VAR_DECL(expected,uint,64,2) [] = { 0x3333333333333333,
-					 0x3333333333333333 };
 VECT_VAR_DECL(expected,poly,8,16) [] = { 0x60, 0xca, 0x34, 0x9e,
 					 0xc8, 0x62, 0x9c, 0x36,
 					 0x30, 0x9a, 0x64, 0xce,
 					 0x98, 0x32, 0xcc, 0x66 };
-VECT_VAR_DECL(expected,poly,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					 0x3333, 0x3333, 0x3333, 0x3333 };
 VECT_VAR_DECL(expected,hfloat,32,4) [] = { 0xc4c73333, 0xc4bac000,
 					   0xc4ae4ccd, 0xc4a1d999 };
 
@@ -145,7 +136,22 @@ void FNNAME (INSN_NAME) (void)
   TEST_VMUL(INSN_NAME, q, poly, p, 8, 16);
   TEST_VMUL(INSN_NAME, q, float, f, 32, 4);
 
-  CHECK_RESULTS (TEST_MSG, "");
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected, "");
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected, "");
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected, "");
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected, "");
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected, "");
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected, "");
+  CHECK(TEST_MSG, poly, 8, 8, PRIx8, expected, "");
+  CHECK_FP(TEST_MSG, float, 32, 2, PRIx32, expected, "");
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected, "");
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected, "");
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected, "");
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected, "");
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected, "");
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected, "");
+  CHECK(TEST_MSG, poly, 8, 16, PRIx8, expected, "");
+  CHECK_FP(TEST_MSG, float, 32, 4, PRIx32, expected, "");
 }
 
 int main (void)
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vneg.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vneg.c
@@ -12,41 +12,11 @@ VECT_VAR_DECL(expected,int,8,8) [] = { 0x10, 0xf, 0xe, 0xd,
 				       0xc, 0xb, 0xa, 0x9 };
 VECT_VAR_DECL(expected,int,16,4) [] = { 0x10, 0xf, 0xe, 0xd };
 VECT_VAR_DECL(expected,int,32,2) [] = { 0x10, 0xf };
-VECT_VAR_DECL(expected,int,64,1) [] = { 0x3333333333333333 };
-VECT_VAR_DECL(expected,uint,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,uint,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,uint,32,2) [] = { 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,uint,64,1) [] = { 0x3333333333333333 };
-VECT_VAR_DECL(expected,poly,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,2) [] = { 0x33333333, 0x33333333 };
 VECT_VAR_DECL(expected,int,8,16) [] = { 0x10, 0xf, 0xe, 0xd, 0xc, 0xb, 0xa, 0x9,
 					0x8, 0x7, 0x6, 0x5, 0x4, 0x3, 0x2, 0x1 };
 VECT_VAR_DECL(expected,int,16,8) [] = { 0x10, 0xf, 0xe, 0xd,
 					0xc, 0xb, 0xa, 0x9 };
 VECT_VAR_DECL(expected,int,32,4) [] = { 0x10, 0xf, 0xe, 0xd };
-VECT_VAR_DECL(expected,int,64,2) [] = { 0x3333333333333333,
-					0x3333333333333333 };
-VECT_VAR_DECL(expected,uint,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,uint,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,uint,32,4) [] = { 0x33333333, 0x33333333,
-					 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,uint,64,2) [] = { 0x3333333333333333,
-					 0x3333333333333333 };
-VECT_VAR_DECL(expected,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,4) [] = { 0x33333333, 0x33333333,
-					   0x33333333, 0x33333333 };
 
 /* Expected results for float32 variants. Needs to be separated since
    the generic test function does not test floating-point
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vorn.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vorn.c
@@ -14,10 +14,6 @@ VECT_VAR_DECL(expected,uint,8,8) [] = { 0xfb, 0xfb, 0xfb, 0xfb,
 VECT_VAR_DECL(expected,uint,16,4) [] = { 0xfff1, 0xfff1, 0xfff3, 0xfff3 };
 VECT_VAR_DECL(expected,uint,32,2) [] = { 0xfffffff7, 0xfffffff7 };
 VECT_VAR_DECL(expected,uint,64,1) [] = { 0xfffffffffffffffd };
-VECT_VAR_DECL(expected,poly,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,2) [] = { 0x33333333, 0x33333333 };
 VECT_VAR_DECL(expected,int,8,16) [] = { 0xf9, 0xf9, 0xfb, 0xfb,
 					0xfd, 0xfd, 0xff, 0xff,
 					0xf9, 0xf9, 0xfb, 0xfb,
@@ -38,11 +34,3 @@ VECT_VAR_DECL(expected,uint,32,4) [] = { 0xfffffff8, 0xfffffff9,
 					 0xfffffffa, 0xfffffffb };
 VECT_VAR_DECL(expected,uint,64,2) [] = { 0xfffffffffffffffc,
 					 0xfffffffffffffffd };
-VECT_VAR_DECL(expected,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,4) [] = { 0x33333333, 0x33333333,
-					   0x33333333, 0x33333333 };
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vorr.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vorr.c
@@ -14,10 +14,6 @@ VECT_VAR_DECL(expected,uint,8,8) [] = { 0xf4, 0xf5, 0xf6, 0xf7,
 VECT_VAR_DECL(expected,uint,16,4) [] = { 0xfffe, 0xffff, 0xfffe, 0xffff };
 VECT_VAR_DECL(expected,uint,32,2) [] = { 0xfffffff8, 0xfffffff9 };
 VECT_VAR_DECL(expected,uint,64,1) [] = { 0xfffffffffffffff2 };
-VECT_VAR_DECL(expected,poly,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,2) [] = { 0x33333333, 0x33333333 };
 VECT_VAR_DECL(expected,int,8,16) [] = { 0xf6, 0xf7, 0xf6, 0xf7,
 					0xf6, 0xf7, 0xf6, 0xf7,
 					0xfe, 0xff, 0xfe, 0xff,
@@ -38,11 +34,3 @@ VECT_VAR_DECL(expected,uint,32,4) [] = { 0xfffffff7, 0xfffffff7,
 					 0xfffffff7, 0xfffffff7 };
 VECT_VAR_DECL(expected,uint,64,2) [] = { 0xfffffffffffffff3,
 					 0xfffffffffffffff3 };
-VECT_VAR_DECL(expected,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,4) [] = { 0x33333333, 0x33333333,
-					   0x33333333, 0x33333333 };
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vqabs.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vqabs.c
@@ -11,16 +11,6 @@ void vqabs_extra(void);
 VECT_VAR_DECL(expected,int,8,8) [] = { 0x10, 0xf, 0xe, 0xd, 0xc, 0xb, 0xa, 0x9 };
 VECT_VAR_DECL(expected,int,16,4) [] = { 0x10, 0xf, 0xe, 0xd };
 VECT_VAR_DECL(expected,int,32,2) [] = { 0x10, 0xf };
-VECT_VAR_DECL(expected,int,64,1) [] = { 0x3333333333333333 };
-VECT_VAR_DECL(expected,uint,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,uint,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,uint,32,2) [] = { 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,uint,64,1) [] = { 0x3333333333333333 };
-VECT_VAR_DECL(expected,poly,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,2) [] = { 0x33333333, 0x33333333 };
 VECT_VAR_DECL(expected,int,8,16) [] = { 0x10, 0xf, 0xe, 0xd,
 					0xc, 0xb, 0xa, 0x9,
 					0x8, 0x7, 0x6, 0x5,
@@ -28,25 +18,6 @@ VECT_VAR_DECL(expected,int,8,16) [] = { 0x10, 0xf, 0xe, 0xd,
 VECT_VAR_DECL(expected,int,16,8) [] = { 0x10, 0xf, 0xe, 0xd,
 					0xc, 0xb, 0xa, 0x9 };
 VECT_VAR_DECL(expected,int,32,4) [] = { 0x10, 0xf, 0xe, 0xd };
-VECT_VAR_DECL(expected,int,64,2) [] = { 0x3333333333333333, 0x3333333333333333 };
-VECT_VAR_DECL(expected,uint,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,uint,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,uint,32,4) [] = { 0x33333333, 0x33333333,
-					 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,uint,64,2) [] = { 0x3333333333333333,
-					 0x3333333333333333 };
-VECT_VAR_DECL(expected,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,4) [] = { 0x33333333, 0x33333333,
-					   0x33333333, 0x33333333 };
 
 /* Expected values of cumulative_saturation flag.  */
 int VECT_VAR(expected_cumulative_sat,int,8,8) = 0;
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vqadd.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vqadd.c
@@ -39,10 +39,6 @@ VECT_VAR_DECL(expected,uint,8,8) [] = { 0xff, 0xff, 0xff, 0xff,
 VECT_VAR_DECL(expected,uint,16,4) [] = { 0xffff, 0xffff, 0xffff, 0xffff };
 VECT_VAR_DECL(expected,uint,32,2) [] = { 0xffffffff, 0xffffffff };
 VECT_VAR_DECL(expected,uint,64,1) [] = { 0xffffffffffffffff };
-VECT_VAR_DECL(expected,poly,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,2) [] = { 0x33333333, 0x33333333 };
 VECT_VAR_DECL(expected,int,8,16) [] = { 0x1, 0x2, 0x3, 0x4,
 					0x5, 0x6, 0x7, 0x8,
 					0x9, 0xa, 0xb, 0xc,
@@ -61,14 +57,6 @@ VECT_VAR_DECL(expected,uint,32,4) [] = { 0xffffffff, 0xffffffff,
 					 0xffffffff, 0xffffffff };
 VECT_VAR_DECL(expected,uint,64,2) [] = { 0xffffffffffffffff,
 					 0xffffffffffffffff };
-VECT_VAR_DECL(expected,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,4) [] = { 0x33333333, 0x33333333,
-					   0x33333333, 0x33333333 };
 
 
 /* 64-bits types, with 0 as second input.  */
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vqmovn.c
@@ -0,0 +1,134 @@
+#include <arm_neon.h>
+#include "arm-neon-ref.h"
+#include "compute-ref-data.h"
+
+/* Expected values of cumulative_saturation flag.  */
+int VECT_VAR(expected_cumulative_sat,int,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat,int,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat,int,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat,uint,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat,uint,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat,uint,32,2) = 0;
+
+/* Expected results.  */
+VECT_VAR_DECL(expected,int,8,8) [] = { 0x12, 0x12, 0x12, 0x12,
+				       0x12, 0x12, 0x12, 0x12 };
+VECT_VAR_DECL(expected,int,16,4) [] = { 0x1278, 0x1278, 0x1278, 0x1278 };
+VECT_VAR_DECL(expected,int,32,2) [] = { 0x12345678, 0x12345678 };
+VECT_VAR_DECL(expected,uint,8,8) [] = { 0x82, 0x82, 0x82, 0x82,
+					0x82, 0x82, 0x82, 0x82 };
+VECT_VAR_DECL(expected,uint,16,4) [] = { 0x8765, 0x8765, 0x8765, 0x8765 };
+VECT_VAR_DECL(expected,uint,32,2) [] = { 0x87654321, 0x87654321 };
+
+/* Expected values of cumulative_saturation flag when saturation occurs.  */
+int VECT_VAR(expected_cumulative_sat1,int,8,8) = 1;
+int VECT_VAR(expected_cumulative_sat1,int,16,4) = 1;
+int VECT_VAR(expected_cumulative_sat1,int,32,2) = 1;
+int VECT_VAR(expected_cumulative_sat1,uint,8,8) = 1;
+int VECT_VAR(expected_cumulative_sat1,uint,16,4) = 1;
+int VECT_VAR(expected_cumulative_sat1,uint,32,2) = 1;
+
+/* Expected results when saturation occurs.  */
+VECT_VAR_DECL(expected1,int,8,8) [] = { 0x7f, 0x7f, 0x7f, 0x7f,
+					0x7f, 0x7f, 0x7f, 0x7f };
+VECT_VAR_DECL(expected1,int,16,4) [] = { 0x7fff, 0x7fff, 0x7fff, 0x7fff };
+VECT_VAR_DECL(expected1,int,32,2) [] = { 0x7fffffff, 0x7fffffff };
+VECT_VAR_DECL(expected1,uint,8,8) [] = { 0xff, 0xff, 0xff, 0xff,
+					 0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected1,uint,16,4) [] = { 0xffff, 0xffff, 0xffff, 0xffff };
+VECT_VAR_DECL(expected1,uint,32,2) [] = { 0xffffffff, 0xffffffff };
+
+#define INSN_NAME vqmovn
+#define TEST_MSG "VQMOVN"
+
+#define FNNAME1(NAME) void exec_ ## NAME (void)
+#define FNNAME(NAME) FNNAME1(NAME)
+
+FNNAME (INSN_NAME)
+{
+  /* Basic test: y=OP(x), then store the result.  */
+#define TEST_UNARY_OP1(INSN, T1, T2, W, W2, N, EXPECTED_CUMULATIVE_SAT, CMT) \
+  Set_Neon_Cumulative_Sat(0, VECT_VAR(vector_res, T1, W, N));		\
+  VECT_VAR(vector_res, T1, W, N) =					\
+    INSN##_##T2##W2(VECT_VAR(vector, T1, W2, N));			\
+  vst1##_##T2##W(VECT_VAR(result, T1, W, N),				\
+		 VECT_VAR(vector_res, T1, W, N));			\
+  CHECK_CUMULATIVE_SAT(TEST_MSG, T1, W, N, EXPECTED_CUMULATIVE_SAT, CMT)
+
+#define TEST_UNARY_OP(INSN, T1, T2, W, W2, N, EXPECTED_CUMULATIVE_SAT, CMT) \
+  TEST_UNARY_OP1(INSN, T1, T2, W, W2, N, EXPECTED_CUMULATIVE_SAT, CMT)
+
+  /* No need for 64 bits variants.  */
+  DECL_VARIABLE(vector, int, 16, 8);
+  DECL_VARIABLE(vector, int, 32, 4);
+  DECL_VARIABLE(vector, int, 64, 2);
+  DECL_VARIABLE(vector, uint, 16, 8);
+  DECL_VARIABLE(vector, uint, 32, 4);
+  DECL_VARIABLE(vector, uint, 64, 2);
+
+  DECL_VARIABLE(vector_res, int, 8, 8);
+  DECL_VARIABLE(vector_res, int, 16, 4);
+  DECL_VARIABLE(vector_res, int, 32, 2);
+  DECL_VARIABLE(vector_res, uint, 8, 8);
+  DECL_VARIABLE(vector_res, uint, 16, 4);
+  DECL_VARIABLE(vector_res, uint, 32, 2);
+
+  clean_results ();
+
+  /* Fill input vector with arbitrary values.  */
+  VDUP(vector, q, int, s, 16, 8, 0x12);
+  VDUP(vector, q, int, s, 32, 4, 0x1278);
+  VDUP(vector, q, int, s, 64, 2, 0x12345678);
+  VDUP(vector, q, uint, u, 16, 8, 0x82);
+  VDUP(vector, q, uint, u, 32, 4, 0x8765);
+  VDUP(vector, q, uint, u, 64, 2, 0x87654321);
+
+  /* Apply a unary operator named INSN_NAME.  */
+#define CMT ""
+  TEST_UNARY_OP(INSN_NAME, int, s, 8, 16, 8, expected_cumulative_sat, CMT);
+  TEST_UNARY_OP(INSN_NAME, int, s, 16, 32, 4, expected_cumulative_sat, CMT);
+  TEST_UNARY_OP(INSN_NAME, int, s, 32, 64, 2, expected_cumulative_sat, CMT);
+  TEST_UNARY_OP(INSN_NAME, uint, u, 8, 16, 8, expected_cumulative_sat, CMT);
+  TEST_UNARY_OP(INSN_NAME, uint, u, 16, 32, 4, expected_cumulative_sat, CMT);
+  TEST_UNARY_OP(INSN_NAME, uint, u, 32, 64, 2, expected_cumulative_sat, CMT);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected, CMT);
+
+
+  /* Fill input vector with arbitrary values which cause cumulative
+     saturation.  */
+  VDUP(vector, q, int, s, 16, 8, 0x1234);
+  VDUP(vector, q, int, s, 32, 4, 0x12345678);
+  VDUP(vector, q, int, s, 64, 2, 0x1234567890ABLL);
+  VDUP(vector, q, uint, u, 16, 8, 0x8234);
+  VDUP(vector, q, uint, u, 32, 4, 0x87654321);
+  VDUP(vector, q, uint, u, 64, 2, 0x8765432187654321ULL);
+
+  /* Apply a unary operator named INSN_NAME.  */
+#undef CMT
+#define CMT " (with saturation)"
+  TEST_UNARY_OP(INSN_NAME, int, s, 8, 16, 8, expected_cumulative_sat1, CMT);
+  TEST_UNARY_OP(INSN_NAME, int, s, 16, 32, 4, expected_cumulative_sat1, CMT);
+  TEST_UNARY_OP(INSN_NAME, int, s, 32, 64, 2, expected_cumulative_sat1, CMT);
+  TEST_UNARY_OP(INSN_NAME, uint, u, 8, 16, 8, expected_cumulative_sat1, CMT);
+  TEST_UNARY_OP(INSN_NAME, uint, u, 16, 32, 4, expected_cumulative_sat1, CMT);
+  TEST_UNARY_OP(INSN_NAME, uint, u, 32, 64, 2, expected_cumulative_sat1, CMT);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected1, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected1, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected1, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected1, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected1, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected1, CMT);
+}
+
+int main (void)
+{
+  exec_vqmovn ();
+  return 0;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vqmovun.c
@@ -0,0 +1,93 @@
+#include <arm_neon.h>
+#include "arm-neon-ref.h"
+#include "compute-ref-data.h"
+
+/* Expected values of cumulative_saturation flag.  */
+int VECT_VAR(expected_cumulative_sat,uint,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat,uint,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat,uint,32,2) = 0;
+
+/* Expected results.  */
+VECT_VAR_DECL(expected,uint,8,8) [] = { 0x34, 0x34, 0x34, 0x34,
+					0x34, 0x34, 0x34, 0x34 };
+VECT_VAR_DECL(expected,uint,16,4) [] = { 0x5678, 0x5678, 0x5678, 0x5678 };
+VECT_VAR_DECL(expected,uint,32,2) [] = { 0x12345678, 0x12345678 };
+
+/* Expected values of cumulative_saturation flag with negative input.  */
+int VECT_VAR(expected_cumulative_sat_neg,uint,8,8) = 1;
+int VECT_VAR(expected_cumulative_sat_neg,uint,16,4) = 1;
+int VECT_VAR(expected_cumulative_sat_neg,uint,32,2) = 1;
+
+/* Expected results with negative input.  */
+VECT_VAR_DECL(expected_neg,uint,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					    0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_neg,uint,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_neg,uint,32,2) [] = { 0x0, 0x0 };
+
+#define INSN_NAME vqmovun
+#define TEST_MSG "VQMOVUN"
+
+#define FNNAME1(NAME) void exec_ ## NAME (void)
+#define FNNAME(NAME) FNNAME1(NAME)
+
+FNNAME (INSN_NAME)
+{
+  /* Basic test: y=OP(x), then store the result.  */
+#define TEST_UNARY_OP1(INSN, T1, T2, W, W2, N, EXPECTED_CUMULATIVE_SAT, CMT) \
+  Set_Neon_Cumulative_Sat(0, VECT_VAR(vector_res, T1, W, N));		\
+  VECT_VAR(vector_res, T1, W, N) =					\
+    INSN##_s##W2(VECT_VAR(vector, int, W2, N));				\
+  vst1##_##T2##W(VECT_VAR(result, T1, W, N),				\
+		 VECT_VAR(vector_res, T1, W, N));			\
+  CHECK_CUMULATIVE_SAT(TEST_MSG, T1, W, N, EXPECTED_CUMULATIVE_SAT, CMT)
+
+#define TEST_UNARY_OP(INSN, T1, T2, W, W2, N, EXPECTED_CUMULATIVE_SAT, CMT) \
+  TEST_UNARY_OP1(INSN, T1, T2, W, W2, N, EXPECTED_CUMULATIVE_SAT, CMT)
+
+  DECL_VARIABLE(vector, int, 16, 8);
+  DECL_VARIABLE(vector, int, 32, 4);
+  DECL_VARIABLE(vector, int, 64, 2);
+
+  DECL_VARIABLE(vector_res, uint, 8, 8);
+  DECL_VARIABLE(vector_res, uint, 16, 4);
+  DECL_VARIABLE(vector_res, uint, 32, 2);
+
+  clean_results ();
+
+  /* Fill input vector with arbitrary values.  */
+  VDUP(vector, q, int, s, 16, 8, 0x34);
+  VDUP(vector, q, int, s, 32, 4, 0x5678);
+  VDUP(vector, q, int, s, 64, 2, 0x12345678);
+
+  /* Apply a unary operator named INSN_NAME.  */
+#define CMT ""
+  TEST_UNARY_OP(INSN_NAME, uint, u, 8, 16, 8, expected_cumulative_sat, CMT);
+  TEST_UNARY_OP(INSN_NAME, uint, u, 16, 32, 4, expected_cumulative_sat, CMT);
+  TEST_UNARY_OP(INSN_NAME, uint, u, 32, 64, 2, expected_cumulative_sat, CMT);
+
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected, CMT);
+
+  /* Fill input vector with negative values.  */
+  VDUP(vector, q, int, s, 16, 8, 0x8234);
+  VDUP(vector, q, int, s, 32, 4, 0x87654321);
+  VDUP(vector, q, int, s, 64, 2, 0x8765432187654321LL);
+
+  /* Apply a unary operator named INSN_NAME.  */
+#undef CMT
+#define CMT " (negative input)"
+  TEST_UNARY_OP(INSN_NAME, uint, u, 8, 16, 8, expected_cumulative_sat_neg, CMT);
+  TEST_UNARY_OP(INSN_NAME, uint, u, 16, 32, 4, expected_cumulative_sat_neg, CMT);
+  TEST_UNARY_OP(INSN_NAME, uint, u, 32, 64, 2, expected_cumulative_sat_neg, CMT);
+
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_neg, CMT);
+}
+
+int main (void)
+{
+  exec_vqmovun ();
+  return 0;
+}
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vqneg.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vqneg.c
@@ -11,16 +11,6 @@ void vqneg_extra(void);
 VECT_VAR_DECL(expected,int,8,8) [] = { 0x10, 0xf, 0xe, 0xd, 0xc, 0xb, 0xa, 0x9 };
 VECT_VAR_DECL(expected,int,16,4) [] = { 0x10, 0xf, 0xe, 0xd };
 VECT_VAR_DECL(expected,int,32,2) [] = { 0x10, 0xf };
-VECT_VAR_DECL(expected,int,64,1) [] = { 0x3333333333333333 };
-VECT_VAR_DECL(expected,uint,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,uint,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,uint,32,2) [] = { 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,uint,64,1) [] = { 0x3333333333333333 };
-VECT_VAR_DECL(expected,poly,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,2) [] = { 0x33333333, 0x33333333 };
 VECT_VAR_DECL(expected,int,8,16) [] = { 0x10, 0xf, 0xe, 0xd,
 					0xc, 0xb, 0xa, 0x9,
 					0x8, 0x7, 0x6, 0x5,
@@ -28,25 +18,6 @@ VECT_VAR_DECL(expected,int,8,16) [] = { 0x10, 0xf, 0xe, 0xd,
 VECT_VAR_DECL(expected,int,16,8) [] = { 0x10, 0xf, 0xe, 0xd,
 					0xc, 0xb, 0xa, 0x9 };
 VECT_VAR_DECL(expected,int,32,4) [] = { 0x10, 0xf, 0xe, 0xd };
-VECT_VAR_DECL(expected,int,64,2) [] = { 0x3333333333333333, 0x3333333333333333 };
-VECT_VAR_DECL(expected,uint,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,uint,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,uint,32,4) [] = { 0x33333333, 0x33333333,
-					 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,uint,64,2) [] = { 0x3333333333333333,
-					 0x3333333333333333 };
-VECT_VAR_DECL(expected,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,4) [] = { 0x33333333, 0x33333333,
-					   0x33333333, 0x33333333 };
 
 /* Expected values of cumulative_saturation flag.  */
 int VECT_VAR(expected_cumulative_sat,int,8,8) = 0;
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vqrdmulh.c
@@ -0,0 +1,161 @@
+#include <arm_neon.h>
+#include "arm-neon-ref.h"
+#include "compute-ref-data.h"
+
+/* Expected values of cumulative_saturation flag.  */
+int VECT_VAR(expected_cumulative_sat,int,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat,int,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat,int,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat,int,32,4) = 0;
+
+/* Expected results.  */
+VECT_VAR_DECL(expected,int,16,4) [] = { 0xfff5, 0xfff6, 0xfff7, 0xfff7 };
+VECT_VAR_DECL(expected,int,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected,int,16,8) [] = { 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected,int,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+
+/* Expected values of cumulative_saturation flag when multiplication
+   saturates.  */
+int VECT_VAR(expected_cumulative_sat_mul,int,16,4) = 1;
+int VECT_VAR(expected_cumulative_sat_mul,int,32,2) = 1;
+int VECT_VAR(expected_cumulative_sat_mul,int,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat_mul,int,32,4) = 1;
+
+/* Expected results when multiplication saturates.  */
+VECT_VAR_DECL(expected_mul,int,16,4) [] = { 0x7fff, 0x7fff, 0x7fff, 0x7fff };
+VECT_VAR_DECL(expected_mul,int,32,2) [] = { 0x7fffffff, 0x7fffffff };
+VECT_VAR_DECL(expected_mul,int,16,8) [] = { 0x7fff, 0x7fff, 0x7fff, 0x7fff,
+					    0x7fff, 0x7fff, 0x7fff, 0x7fff };
+VECT_VAR_DECL(expected_mul,int,32,4) [] = { 0x7fffffff, 0x7fffffff,
+					    0x7fffffff, 0x7fffffff };
+
+/* Expected values of cumulative_saturation flag when rounding
+   should not cause saturation.  */
+int VECT_VAR(expected_cumulative_sat_round,int,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat_round,int,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat_round,int,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_round,int,32,4) = 0;
+
+/* Expected results when rounding should not cause saturation.  */
+VECT_VAR_DECL(expected_round,int,16,4) [] = { 0x7fff, 0x7fff, 0x7fff, 0x7fff };
+VECT_VAR_DECL(expected_round,int,32,2) [] = { 0x7fffffff, 0x7fffffff };
+VECT_VAR_DECL(expected_round,int,16,8) [] = { 0x7fff, 0x7fff, 0x7fff, 0x7fff,
+					      0x7fff, 0x7fff, 0x7fff, 0x7fff };
+VECT_VAR_DECL(expected_round,int,32,4) [] = { 0x7fffffff, 0x7fffffff,
+					      0x7fffffff, 0x7fffffff };
+
+#define INSN vqrdmulh
+#define TEST_MSG "VQRDMULH"
+
+#define FNNAME1(NAME) void exec_ ## NAME (void)
+#define FNNAME(NAME) FNNAME1(NAME)
+
+FNNAME (INSN)
+{
+  /* vector_res = vqrdmulh(vector,vector2), then store the result.  */
+#define TEST_VQRDMULH2(INSN, Q, T1, T2, W, N, EXPECTED_CUMULATIVE_SAT, CMT) \
+  Set_Neon_Cumulative_Sat(0, VECT_VAR(vector_res, T1, W, N));		\
+  VECT_VAR(vector_res, T1, W, N) =					\
+    INSN##Q##_##T2##W(VECT_VAR(vector, T1, W, N),			\
+		      VECT_VAR(vector2, T1, W, N));			\
+  vst1##Q##_##T2##W(VECT_VAR(result, T1, W, N),				\
+		    VECT_VAR(vector_res, T1, W, N));			\
+  CHECK_CUMULATIVE_SAT(TEST_MSG, T1, W, N, EXPECTED_CUMULATIVE_SAT, CMT)
+
+  /* Two auxliary macros are necessary to expand INSN */
+#define TEST_VQRDMULH1(INSN, Q, T1, T2, W, N, EXPECTED_CUMULATIVE_SAT, CMT) \
+  TEST_VQRDMULH2(INSN, Q, T1, T2, W, N, EXPECTED_CUMULATIVE_SAT, CMT)
+
+#define TEST_VQRDMULH(Q, T1, T2, W, N, EXPECTED_CUMULATIVE_SAT, CMT)	\
+  TEST_VQRDMULH1(INSN, Q, T1, T2, W, N, EXPECTED_CUMULATIVE_SAT, CMT)
+
+
+  DECL_VARIABLE(vector, int, 16, 4);
+  DECL_VARIABLE(vector, int, 32, 2);
+  DECL_VARIABLE(vector, int, 16, 8);
+  DECL_VARIABLE(vector, int, 32, 4);
+
+  DECL_VARIABLE(vector_res, int, 16, 4);
+  DECL_VARIABLE(vector_res, int, 32, 2);
+  DECL_VARIABLE(vector_res, int, 16, 8);
+  DECL_VARIABLE(vector_res, int, 32, 4);
+
+  DECL_VARIABLE(vector2, int, 16, 4);
+  DECL_VARIABLE(vector2, int, 32, 2);
+  DECL_VARIABLE(vector2, int, 16, 8);
+  DECL_VARIABLE(vector2, int, 32, 4);
+
+  clean_results ();
+
+  VLOAD(vector, buffer, , int, s, 16, 4);
+  VLOAD(vector, buffer, , int, s, 32, 2);
+  VLOAD(vector, buffer, q, int, s, 16, 8);
+  VLOAD(vector, buffer, q, int, s, 32, 4);
+
+  /* Initialize vector2.  */
+  VDUP(vector2, , int, s, 16, 4, 0x5555);
+  VDUP(vector2, , int, s, 32, 2, 0xBB);
+  VDUP(vector2, q, int, s, 16, 8, 0x33);
+  VDUP(vector2, q, int, s, 32, 4, 0x22);
+
+#define CMT ""
+  TEST_VQRDMULH(, int, s, 16, 4, expected_cumulative_sat, CMT);
+  TEST_VQRDMULH(, int, s, 32, 2, expected_cumulative_sat, CMT);
+  TEST_VQRDMULH(q, int, s, 16, 8, expected_cumulative_sat, CMT);
+  TEST_VQRDMULH(q, int, s, 32, 4, expected_cumulative_sat, CMT);
+
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected, CMT);
+
+  /* Now use input values such that the multiplication causes
+     saturation.  */
+#define TEST_MSG_MUL " (check mul cumulative saturation)"
+  VDUP(vector, , int, s, 16, 4, 0x8000);
+  VDUP(vector, , int, s, 32, 2, 0x80000000);
+  VDUP(vector, q, int, s, 16, 8, 0x8000);
+  VDUP(vector, q, int, s, 32, 4, 0x80000000);
+  VDUP(vector2, , int, s, 16, 4, 0x8000);
+  VDUP(vector2, , int, s, 32, 2, 0x80000000);
+  VDUP(vector2, q, int, s, 16, 8, 0x8000);
+  VDUP(vector2, q, int, s, 32, 4, 0x80000000);
+
+  TEST_VQRDMULH(, int, s, 16, 4, expected_cumulative_sat_mul, TEST_MSG_MUL);
+  TEST_VQRDMULH(, int, s, 32, 2, expected_cumulative_sat_mul, TEST_MSG_MUL);
+  TEST_VQRDMULH(q, int, s, 16, 8, expected_cumulative_sat_mul, TEST_MSG_MUL);
+  TEST_VQRDMULH(q, int, s, 32, 4, expected_cumulative_sat_mul, TEST_MSG_MUL);
+
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_mul, TEST_MSG_MUL);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_mul, TEST_MSG_MUL);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_mul, TEST_MSG_MUL);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_mul, TEST_MSG_MUL);
+
+  /* Use input values where rounding produces a result equal to the
+     saturation value, but does not set the saturation flag.  */
+#define TEST_MSG_ROUND " (check rounding)"
+  VDUP(vector, , int, s, 16, 4, 0x8000);
+  VDUP(vector, , int, s, 32, 2, 0x80000000);
+  VDUP(vector, q, int, s, 16, 8, 0x8000);
+  VDUP(vector, q, int, s, 32, 4, 0x80000000);
+  VDUP(vector2, , int, s, 16, 4, 0x8001);
+  VDUP(vector2, , int, s, 32, 2, 0x80000001);
+  VDUP(vector2, q, int, s, 16, 8, 0x8001);
+  VDUP(vector2, q, int, s, 32, 4, 0x80000001);
+
+  TEST_VQRDMULH(, int, s, 16, 4, expected_cumulative_sat_round, TEST_MSG_ROUND);
+  TEST_VQRDMULH(, int, s, 32, 2, expected_cumulative_sat_round, TEST_MSG_ROUND);
+  TEST_VQRDMULH(q, int, s, 16, 8, expected_cumulative_sat_round, TEST_MSG_ROUND);
+  TEST_VQRDMULH(q, int, s, 32, 4, expected_cumulative_sat_round, TEST_MSG_ROUND);
+
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_round, TEST_MSG_ROUND);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_round, TEST_MSG_ROUND);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_round, TEST_MSG_ROUND);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_round, TEST_MSG_ROUND);
+}
+
+int main (void)
+{
+  exec_vqrdmulh ();
+  return 0;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vqrdmulh_lane.c
@@ -0,0 +1,169 @@
+#include <arm_neon.h>
+#include "arm-neon-ref.h"
+#include "compute-ref-data.h"
+
+/* Expected values of cumulative_saturation flag.  */
+int VECT_VAR(expected_cumulative_sat,int,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat,int,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat,int,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat,int,32,4) = 0;
+
+/* Expected results.  */
+VECT_VAR_DECL(expected,int,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected,int,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected,int,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected,int,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+
+/* Expected values of cumulative_saturation flag when multiplication
+   saturates.  */
+int VECT_VAR(expected_cumulative_sat_mul,int,16,4) = 1;
+int VECT_VAR(expected_cumulative_sat_mul,int,32,2) = 1;
+int VECT_VAR(expected_cumulative_sat_mul,int,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat_mul,int,32,4) = 1;
+
+/* Expected results when multiplication saturates.  */
+VECT_VAR_DECL(expected_mul,int,16,4) [] = { 0x7fff, 0x7fff, 0x7fff, 0x7fff };
+VECT_VAR_DECL(expected_mul,int,32,2) [] = { 0x7fffffff, 0x7fffffff };
+VECT_VAR_DECL(expected_mul,int,16,8) [] = { 0x7fff, 0x7fff, 0x7fff, 0x7fff,
+					    0x7fff, 0x7fff, 0x7fff, 0x7fff };
+VECT_VAR_DECL(expected_mul,int,32,4) [] = { 0x7fffffff, 0x7fffffff,
+					    0x7fffffff, 0x7fffffff };
+
+/* Expected values of cumulative_saturation flag when rounding
+   should not cause saturation.  */
+int VECT_VAR(expected_cumulative_sat_round,int,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat_round,int,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat_round,int,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_round,int,32,4) = 0;
+
+/* Expected results when rounding should not cause saturation.  */
+VECT_VAR_DECL(expected_round,int,16,4) [] = { 0x7fff, 0x7fff, 0x7fff, 0x7fff };
+VECT_VAR_DECL(expected_round,int,32,2) [] = { 0x7fffffff, 0x7fffffff };
+VECT_VAR_DECL(expected_round,int,16,8) [] = { 0x7fff, 0x7fff, 0x7fff, 0x7fff,
+					      0x7fff, 0x7fff, 0x7fff, 0x7fff };
+VECT_VAR_DECL(expected_round,int,32,4) [] = { 0x7fffffff, 0x7fffffff,
+					      0x7fffffff, 0x7fffffff };
+
+#define INSN vqrdmulh
+#define TEST_MSG "VQRDMULH_LANE"
+
+#define FNNAME1(NAME) void exec_ ## NAME ## _lane (void)
+#define FNNAME(NAME) FNNAME1(NAME)
+
+FNNAME (INSN)
+{
+  /* vector_res = vqrdmulh_lane(vector,vector2,lane), then store the result.  */
+#define TEST_VQRDMULH_LANE2(INSN, Q, T1, T2, W, N, N2, L, EXPECTED_CUMULATIVE_SAT, CMT) \
+  Set_Neon_Cumulative_Sat(0, VECT_VAR(vector_res, T1, W, N));		\
+  VECT_VAR(vector_res, T1, W, N) =					\
+    INSN##Q##_lane_##T2##W(VECT_VAR(vector, T1, W, N),			\
+			   VECT_VAR(vector2, T1, W, N2),		\
+			   L);						\
+  vst1##Q##_##T2##W(VECT_VAR(result, T1, W, N),				\
+		    VECT_VAR(vector_res, T1, W, N));			\
+  CHECK_CUMULATIVE_SAT(TEST_MSG, T1, W, N, EXPECTED_CUMULATIVE_SAT, CMT)
+
+  /* Two auxliary macros are necessary to expand INSN */
+#define TEST_VQRDMULH_LANE1(INSN, Q, T1, T2, W, N, N2, L, EXPECTED_CUMULATIVE_SAT, CMT) \
+  TEST_VQRDMULH_LANE2(INSN, Q, T1, T2, W, N, N2, L, EXPECTED_CUMULATIVE_SAT, CMT)
+
+#define TEST_VQRDMULH_LANE(Q, T1, T2, W, N, N2, L, EXPECTED_CUMULATIVE_SAT, CMT) \
+  TEST_VQRDMULH_LANE1(INSN, Q, T1, T2, W, N, N2, L, EXPECTED_CUMULATIVE_SAT, CMT)
+
+
+  DECL_VARIABLE(vector, int, 16, 4);
+  DECL_VARIABLE(vector, int, 32, 2);
+  DECL_VARIABLE(vector, int, 16, 8);
+  DECL_VARIABLE(vector, int, 32, 4);
+
+  DECL_VARIABLE(vector_res, int, 16, 4);
+  DECL_VARIABLE(vector_res, int, 32, 2);
+  DECL_VARIABLE(vector_res, int, 16, 8);
+  DECL_VARIABLE(vector_res, int, 32, 4);
+
+  /* vector2: vqrdmulh_lane and vqrdmulhq_lane have a 2nd argument with
+     the same number of elements, so we need only one variable of each
+     type.  */
+  DECL_VARIABLE(vector2, int, 16, 4);
+  DECL_VARIABLE(vector2, int, 32, 2);
+
+  clean_results ();
+
+  VLOAD(vector, buffer, , int, s, 16, 4);
+  VLOAD(vector, buffer, , int, s, 32, 2);
+
+  VLOAD(vector, buffer, q, int, s, 16, 8);
+  VLOAD(vector, buffer, q, int, s, 32, 4);
+
+  /* Initialize vector2.  */
+  VDUP(vector2, , int, s, 16, 4, 0x55);
+  VDUP(vector2, , int, s, 32, 2, 0xBB);
+
+  /* Choose lane arbitrarily.  */
+#define CMT ""
+  TEST_VQRDMULH_LANE(, int, s, 16, 4, 4, 2, expected_cumulative_sat, CMT);
+  TEST_VQRDMULH_LANE(, int, s, 32, 2, 2, 1, expected_cumulative_sat, CMT);
+  TEST_VQRDMULH_LANE(q, int, s, 16, 8, 4, 3, expected_cumulative_sat, CMT);
+  TEST_VQRDMULH_LANE(q, int, s, 32, 4, 2, 0, expected_cumulative_sat, CMT);
+
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected, CMT);
+
+  /* Now use input values such that the multiplication causes
+     saturation.  */
+#define TEST_MSG_MUL " (check mul cumulative saturation)"
+  VDUP(vector, , int, s, 16, 4, 0x8000);
+  VDUP(vector, , int, s, 32, 2, 0x80000000);
+  VDUP(vector, q, int, s, 16, 8, 0x8000);
+  VDUP(vector, q, int, s, 32, 4, 0x80000000);
+  VDUP(vector2, , int, s, 16, 4, 0x8000);
+  VDUP(vector2, , int, s, 32, 2, 0x80000000);
+
+  TEST_VQRDMULH_LANE(, int, s, 16, 4, 4, 2, expected_cumulative_sat_mul,
+		     TEST_MSG_MUL);
+  TEST_VQRDMULH_LANE(, int, s, 32, 2, 2, 1, expected_cumulative_sat_mul,
+		     TEST_MSG_MUL);
+  TEST_VQRDMULH_LANE(q, int, s, 16, 8, 4, 3, expected_cumulative_sat_mul,
+		     TEST_MSG_MUL);
+  TEST_VQRDMULH_LANE(q, int, s, 32, 4, 2, 0, expected_cumulative_sat_mul,
+		     TEST_MSG_MUL);
+
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_mul, TEST_MSG_MUL);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_mul, TEST_MSG_MUL);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_mul, TEST_MSG_MUL);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_mul, TEST_MSG_MUL);
+
+  VDUP(vector, , int, s, 16, 4, 0x8000);
+  VDUP(vector, , int, s, 32, 2, 0x80000000);
+  VDUP(vector, q, int, s, 16, 8, 0x8000);
+  VDUP(vector, q, int, s, 32, 4, 0x80000000);
+  VDUP(vector2, , int, s, 16, 4, 0x8001);
+  VDUP(vector2, , int, s, 32, 2, 0x80000001);
+
+  /* Use input values where rounding produces a result equal to the
+     saturation value, but does not set the saturation flag.  */
+#define TEST_MSG_ROUND " (check rounding)"
+  TEST_VQRDMULH_LANE(, int, s, 16, 4, 4, 2, expected_cumulative_sat_round,
+		     TEST_MSG_ROUND);
+  TEST_VQRDMULH_LANE(, int, s, 32, 2, 2, 1, expected_cumulative_sat_round,
+		     TEST_MSG_ROUND);
+  TEST_VQRDMULH_LANE(q, int, s, 16, 8, 4, 3, expected_cumulative_sat_round,
+		     TEST_MSG_ROUND);
+  TEST_VQRDMULH_LANE(q, int, s, 32, 4, 2, 0, expected_cumulative_sat_round,
+		     TEST_MSG_ROUND);
+
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_round, TEST_MSG_ROUND);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_round, TEST_MSG_ROUND);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_round, TEST_MSG_ROUND);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_round, TEST_MSG_ROUND);
+}
+
+int main (void)
+{
+  exec_vqrdmulh_lane ();
+  return 0;
+}
+
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vqrdmulh_n.c
@@ -0,0 +1,155 @@
+#include <arm_neon.h>
+#include "arm-neon-ref.h"
+#include "compute-ref-data.h"
+
+/* Expected values of cumulative_saturation flag.  */
+int VECT_VAR(expected_cumulative_sat,int,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat,int,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat,int,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat,int,32,4) = 0;
+
+/* Expected results.  */
+VECT_VAR_DECL(expected,int,16,4) [] = { 0xfffc, 0xfffc, 0xfffc, 0xfffd };
+VECT_VAR_DECL(expected,int,32,2) [] = { 0xfffffffe, 0xfffffffe };
+VECT_VAR_DECL(expected,int,16,8) [] = { 0x6, 0x6, 0x6, 0x5,
+					0x5, 0x4, 0x4, 0x4 };
+VECT_VAR_DECL(expected,int,32,4) [] = { 0xfffffffe, 0xfffffffe,
+					0xfffffffe, 0xfffffffe };
+
+/* Expected values of cumulative_saturation flag when multiplication
+   saturates.  */
+int VECT_VAR(expected_cumulative_sat_mul,int,16,4) = 1;
+int VECT_VAR(expected_cumulative_sat_mul,int,32,2) = 1;
+int VECT_VAR(expected_cumulative_sat_mul,int,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat_mul,int,32,4) = 1;
+
+/* Expected results when multiplication saturates.  */
+VECT_VAR_DECL(expected_mul,int,16,4) [] = { 0x7fff, 0x7fff, 0x7fff, 0x7fff };
+VECT_VAR_DECL(expected_mul,int,32,2) [] = { 0x7fffffff, 0x7fffffff };
+VECT_VAR_DECL(expected_mul,int,16,8) [] = { 0x7fff, 0x7fff, 0x7fff, 0x7fff,
+					    0x7fff, 0x7fff, 0x7fff, 0x7fff };
+VECT_VAR_DECL(expected_mul,int,32,4) [] = { 0x7fffffff, 0x7fffffff,
+					    0x7fffffff, 0x7fffffff };
+
+/* Expected values of cumulative_saturation flag when rounding
+   should not cause saturation.  */
+int VECT_VAR(expected_cumulative_sat_round,int,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat_round,int,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat_round,int,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_round,int,32,4) = 0;
+
+/* Expected results when rounding should not cause saturation.  */
+VECT_VAR_DECL(expected_round,int,16,4) [] = { 0x7fff, 0x7fff, 0x7fff, 0x7fff };
+VECT_VAR_DECL(expected_round,int,32,2) [] = { 0x7fffffff, 0x7fffffff };
+VECT_VAR_DECL(expected_round,int,16,8) [] = { 0x7fff, 0x7fff, 0x7fff, 0x7fff,
+					      0x7fff, 0x7fff, 0x7fff, 0x7fff };
+VECT_VAR_DECL(expected_round,int,32,4) [] = { 0x7fffffff, 0x7fffffff,
+					      0x7fffffff, 0x7fffffff };
+
+#define INSN vqrdmulh
+#define TEST_MSG "VQRDMULH_N"
+
+#define FNNAME1(NAME) void exec_ ## NAME ## _n (void)
+#define FNNAME(NAME) FNNAME1(NAME)
+
+FNNAME (INSN)
+{
+  int i;
+
+  /* vector_res = vqrdmulh_n(vector,val), then store the result.  */
+#define TEST_VQRDMULH_N2(INSN, Q, T1, T2, W, N, L, EXPECTED_CUMULATIVE_SAT, CMT) \
+  Set_Neon_Cumulative_Sat(0, VECT_VAR(vector_res, T1, W, N));		\
+  VECT_VAR(vector_res, T1, W, N) =					\
+    INSN##Q##_n_##T2##W(VECT_VAR(vector, T1, W, N),			\
+			L);						\
+  vst1##Q##_##T2##W(VECT_VAR(result, T1, W, N),				\
+		    VECT_VAR(vector_res, T1, W, N));			\
+  CHECK_CUMULATIVE_SAT(TEST_MSG, T1, W, N, EXPECTED_CUMULATIVE_SAT, CMT)
+
+  /* Two auxliary macros are necessary to expand INSN */
+#define TEST_VQRDMULH_N1(INSN, Q, T1, T2, W, N, L, EXPECTED_CUMULATIVE_SAT, CMT) \
+  TEST_VQRDMULH_N2(INSN, Q, T1, T2, W, N, L, EXPECTED_CUMULATIVE_SAT, CMT)
+
+#define TEST_VQRDMULH_N(Q, T1, T2, W, N, L, EXPECTED_CUMULATIVE_SAT, CMT) \
+  TEST_VQRDMULH_N1(INSN, Q, T1, T2, W, N, L, EXPECTED_CUMULATIVE_SAT, CMT)
+
+
+  DECL_VARIABLE(vector, int, 16, 4);
+  DECL_VARIABLE(vector, int, 32, 2);
+  DECL_VARIABLE(vector, int, 16, 8);
+  DECL_VARIABLE(vector, int, 32, 4);
+
+  DECL_VARIABLE(vector_res, int, 16, 4);
+  DECL_VARIABLE(vector_res, int, 32, 2);
+  DECL_VARIABLE(vector_res, int, 16, 8);
+  DECL_VARIABLE(vector_res, int, 32, 4);
+
+  clean_results ();
+
+  VLOAD(vector, buffer, , int, s, 16, 4);
+  VLOAD(vector, buffer, , int, s, 32, 2);
+  VLOAD(vector, buffer, q, int, s, 16, 8);
+  VLOAD(vector, buffer, q, int, s, 32, 4);
+
+  /* Choose multiplier arbitrarily.  */
+#define CMT ""
+  TEST_VQRDMULH_N(, int, s, 16, 4, 0x2233, expected_cumulative_sat, CMT);
+  TEST_VQRDMULH_N(, int, s, 32, 2, 0x12345678, expected_cumulative_sat, CMT);
+  TEST_VQRDMULH_N(q, int, s, 16, 8, 0xCD12, expected_cumulative_sat, CMT);
+  TEST_VQRDMULH_N(q, int, s, 32, 4, 0xFA23456, expected_cumulative_sat, CMT);
+
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected, CMT);
+
+  /* Now use input values such that the multiplication causes
+     saturation.  */
+#define TEST_MSG_MUL " (check mul cumulative saturation)"
+  VDUP(vector, , int, s, 16, 4, 0x8000);
+  VDUP(vector, , int, s, 32, 2, 0x80000000);
+  VDUP(vector, q, int, s, 16, 8, 0x8000);
+  VDUP(vector, q, int, s, 32, 4, 0x80000000);
+
+  TEST_VQRDMULH_N(, int, s, 16, 4, 0x8000, expected_cumulative_sat_mul,
+		  TEST_MSG_MUL);
+  TEST_VQRDMULH_N(, int, s, 32, 2, 0x80000000, expected_cumulative_sat_mul,
+		  TEST_MSG_MUL);
+  TEST_VQRDMULH_N(q, int, s, 16, 8, 0x8000, expected_cumulative_sat_mul,
+		  TEST_MSG_MUL);
+  TEST_VQRDMULH_N(q, int, s, 32, 4, 0x80000000, expected_cumulative_sat_mul,
+		  TEST_MSG_MUL);
+
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_mul, TEST_MSG_MUL);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_mul, TEST_MSG_MUL);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_mul, TEST_MSG_MUL);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_mul, TEST_MSG_MUL);
+
+  /* Use input values where rounding produces a result equal to the
+     saturation value, but does not set the saturation flag.  */
+#define TEST_MSG_ROUND " (check rounding)"
+  VDUP(vector, , int, s, 16, 4, 0x8000);
+  VDUP(vector, , int, s, 32, 2, 0x80000000);
+  VDUP(vector, q, int, s, 16, 8, 0x8000);
+  VDUP(vector, q, int, s, 32, 4, 0x80000000);
+
+  TEST_VQRDMULH_N(, int, s, 16, 4, 0x8001, expected_cumulative_sat_round,
+		  TEST_MSG_ROUND);
+  TEST_VQRDMULH_N(, int, s, 32, 2, 0x80000001, expected_cumulative_sat_round,
+		  TEST_MSG_ROUND);
+  TEST_VQRDMULH_N(q, int, s, 16, 8, 0x8001, expected_cumulative_sat_round,
+		  TEST_MSG_ROUND);
+  TEST_VQRDMULH_N(q, int, s, 32, 4, 0x80000001, expected_cumulative_sat_round,
+		  TEST_MSG_ROUND);
+
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_round, TEST_MSG_ROUND);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_round, TEST_MSG_ROUND);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_round, TEST_MSG_ROUND);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_round, TEST_MSG_ROUND);
+}
+
+int main (void)
+{
+  exec_vqrdmulh_n ();
+  return 0;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vqrshl.c
@@ -0,0 +1,1090 @@
+#include <arm_neon.h>
+#include "arm-neon-ref.h"
+#include "compute-ref-data.h"
+
+/* Expected values of cumulative_saturation flag with input=0.  */
+int VECT_VAR(expected_cumulative_sat_0,int,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat_0,int,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat_0,int,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat_0,int,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat_0,uint,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat_0,uint,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat_0,uint,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat_0,uint,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat_0,int,8,16) = 0;
+int VECT_VAR(expected_cumulative_sat_0,int,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_0,int,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat_0,int,64,2) = 0;
+int VECT_VAR(expected_cumulative_sat_0,uint,8,16) = 0;
+int VECT_VAR(expected_cumulative_sat_0,uint,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_0,uint,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat_0,uint,64,2) = 0;
+
+/* Expected results with input=0.  */
+VECT_VAR_DECL(expected_0,int,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,int,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,int,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,int,64,1) [] = { 0x0 };
+VECT_VAR_DECL(expected_0,uint,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					  0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,uint,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,uint,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,uint,64,1) [] = { 0x0 };
+VECT_VAR_DECL(expected_0,int,8,16) [] = { 0x0, 0x0, 0x0, 0x0,
+					  0x0, 0x0, 0x0, 0x0,
+					  0x0, 0x0, 0x0, 0x0,
+					  0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,int,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					  0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,int,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,int,64,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,uint,8,16) [] = { 0x0, 0x0, 0x0, 0x0,
+					   0x0, 0x0, 0x0, 0x0,
+					   0x0, 0x0, 0x0, 0x0,
+					   0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,uint,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					   0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,uint,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,uint,64,2) [] = { 0x0, 0x0 };
+
+/* Expected values of cumulative_saturation flag with input=0 and
+   negative shift amount.  */
+int VECT_VAR(expected_cumulative_sat_0_neg,int,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,int,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,int,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,int,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,uint,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,uint,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,uint,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,uint,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,int,8,16) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,int,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,int,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,int,64,2) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,uint,8,16) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,uint,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,uint,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,uint,64,2) = 0;
+
+/* Expected results with input=0 and negative shift amount.  */
+VECT_VAR_DECL(expected_0_neg,int,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					     0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_neg,int,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_neg,int,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_neg,int,64,1) [] = { 0x0 };
+VECT_VAR_DECL(expected_0_neg,uint,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					      0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_neg,uint,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_neg,uint,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_neg,uint,64,1) [] = { 0x0 };
+VECT_VAR_DECL(expected_0_neg,int,8,16) [] = { 0x0, 0x0, 0x0, 0x0,
+					      0x0, 0x0, 0x0, 0x0,
+					      0x0, 0x0, 0x0, 0x0,
+					      0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_neg,int,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					      0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_neg,int,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_neg,int,64,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_neg,uint,8,16) [] = { 0x0, 0x0, 0x0, 0x0,
+					       0x0, 0x0, 0x0, 0x0,
+					       0x0, 0x0, 0x0, 0x0,
+					       0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_neg,uint,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					       0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_neg,uint,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_neg,uint,64,2) [] = { 0x0, 0x0 };
+
+/* Expected values of cumulative_saturation flag.  */
+int VECT_VAR(expected_cumulative_sat,int,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat,int,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat,int,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat,int,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat,uint,8,8) = 1;
+int VECT_VAR(expected_cumulative_sat,uint,16,4) = 1;
+int VECT_VAR(expected_cumulative_sat,uint,32,2) = 1;
+int VECT_VAR(expected_cumulative_sat,uint,64,1) = 1;
+int VECT_VAR(expected_cumulative_sat,int,8,16) = 1;
+int VECT_VAR(expected_cumulative_sat,int,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat,int,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat,int,64,2) = 1;
+int VECT_VAR(expected_cumulative_sat,uint,8,16) = 1;
+int VECT_VAR(expected_cumulative_sat,uint,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat,uint,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat,uint,64,2) = 1;
+
+/* Expected results.  */
+VECT_VAR_DECL(expected,int,8,8) [] = { 0xe0, 0xe2, 0xe4, 0xe6,
+				       0xe8, 0xea, 0xec, 0xee };
+VECT_VAR_DECL(expected,int,16,4) [] = { 0xff80, 0xff88, 0xff90, 0xff98 };
+VECT_VAR_DECL(expected,int,32,2) [] = { 0xfffff000, 0xfffff100 };
+VECT_VAR_DECL(expected,int,64,1) [] = { 0xffffffffffffff80 };
+VECT_VAR_DECL(expected,uint,8,8) [] = { 0xff, 0xff, 0xff, 0xff,
+					0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected,uint,16,4) [] = { 0xffff, 0xffff, 0xffff, 0xffff };
+VECT_VAR_DECL(expected,uint,32,2) [] = { 0xffffffff, 0xffffffff };
+VECT_VAR_DECL(expected,uint,64,1) [] = { 0xffffffffffffffff };
+VECT_VAR_DECL(expected,int,8,16) [] = { 0x80, 0x80, 0x80, 0x80,
+					0x80, 0x80, 0x80, 0x80,
+					0x80, 0x80, 0x80, 0x80,
+					0x80, 0x80, 0x80, 0x80 };
+VECT_VAR_DECL(expected,int,16,8) [] = { 0x8000, 0x8000, 0x8000, 0x8000,
+					0x8000, 0x8000, 0x8000, 0x8000 };
+VECT_VAR_DECL(expected,int,32,4) [] = { 0x80000000, 0x80000000,
+					0x80000000, 0x80000000 };
+VECT_VAR_DECL(expected,int,64,2) [] = { 0x8000000000000000, 0x8000000000000000 };
+VECT_VAR_DECL(expected,uint,8,16) [] = { 0xff, 0xff, 0xff, 0xff,
+					 0xff, 0xff, 0xff, 0xff,
+					 0xff, 0xff, 0xff, 0xff,
+					 0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected,uint,16,8) [] = { 0xffff, 0xffff, 0xffff, 0xffff,
+					 0xffff, 0xffff, 0xffff, 0xffff };
+VECT_VAR_DECL(expected,uint,32,4) [] = { 0xffffffff, 0xffffffff,
+					 0xffffffff, 0xffffffff };
+VECT_VAR_DECL(expected,uint,64,2) [] = { 0xffffffffffffffff,
+					 0xffffffffffffffff };
+
+/* Expected values of cumulative_saturation flag with negative shift
+   amount.  */
+int VECT_VAR(expected_cumulative_sat_neg,int,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,int,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,int,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,int,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,uint,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,uint,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,uint,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,uint,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,int,8,16) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,int,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,int,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,int,64,2) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,uint,8,16) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,uint,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,uint,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,uint,64,2) = 0;
+
+/* Expected results with negative shift amount.  */
+VECT_VAR_DECL(expected_neg,int,8,8) [] = { 0xfc, 0xfc, 0xfd, 0xfd,
+					   0xfd, 0xfd, 0xfe, 0xfe };
+VECT_VAR_DECL(expected_neg,int,16,4) [] = { 0xfffc, 0xfffc, 0xfffd, 0xfffd };
+VECT_VAR_DECL(expected_neg,int,32,2) [] = { 0xfffffffe, 0xfffffffe };
+VECT_VAR_DECL(expected_neg,int,64,1) [] = { 0xffffffffffffffff };
+VECT_VAR_DECL(expected_neg,uint,8,8) [] = { 0x3c, 0x3c, 0x3d, 0x3d,
+					    0x3d, 0x3d, 0x3e, 0x3e };
+VECT_VAR_DECL(expected_neg,uint,16,4) [] = { 0x3ffc, 0x3ffc, 0x3ffd, 0x3ffd };
+VECT_VAR_DECL(expected_neg,uint,32,2) [] = { 0x1ffffffe, 0x1ffffffe };
+VECT_VAR_DECL(expected_neg,uint,64,1) [] = { 0xfffffffffffffff };
+VECT_VAR_DECL(expected_neg,int,8,16) [] = { 0x0, 0x0, 0x0, 0x0,
+					    0x0, 0x0, 0x0, 0x0,
+					    0x0, 0x0, 0x0, 0x0,
+					    0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_neg,int,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					    0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_neg,int,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_neg,int,64,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_neg,uint,8,16) [] = { 0x2, 0x2, 0x2, 0x2,
+					     0x2, 0x2, 0x2, 0x2,
+					     0x2, 0x2, 0x2, 0x2,
+					     0x2, 0x2, 0x2, 0x2 };
+VECT_VAR_DECL(expected_neg,uint,16,8) [] = { 0x20, 0x20, 0x20, 0x20,
+					     0x20, 0x20, 0x20, 0x20 };
+VECT_VAR_DECL(expected_neg,uint,32,4) [] = { 0x80000, 0x80000,
+					     0x80000, 0x80000 };
+VECT_VAR_DECL(expected_neg,uint,64,2) [] = { 0x100000000000, 0x100000000000 };
+
+/* Expected values of cumulative_saturation flag with input=max and
+   shift by -1.  */
+int VECT_VAR(expected_cumulative_sat_minus1,int,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat_minus1,int,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat_minus1,int,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat_minus1,int,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat_minus1,uint,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat_minus1,uint,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat_minus1,uint,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat_minus1,uint,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat_minus1,int,8,16) = 0;
+int VECT_VAR(expected_cumulative_sat_minus1,int,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_minus1,int,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat_minus1,int,64,2) = 0;
+int VECT_VAR(expected_cumulative_sat_minus1,uint,8,16) = 0;
+int VECT_VAR(expected_cumulative_sat_minus1,uint,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_minus1,uint,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat_minus1,uint,64,2) = 0;
+
+/* Expected results with input=max and shift by -1.  */
+VECT_VAR_DECL(expected_minus1,int,8,8) [] = { 0x40, 0x40, 0x40, 0x40,
+					      0x40, 0x40, 0x40, 0x40 };
+VECT_VAR_DECL(expected_minus1,int,16,4) [] = { 0x4000, 0x4000, 0x4000, 0x4000 };
+VECT_VAR_DECL(expected_minus1,int,32,2) [] = { 0x40000000, 0x40000000 };
+VECT_VAR_DECL(expected_minus1,int,64,1) [] = { 0x4000000000000000 };
+VECT_VAR_DECL(expected_minus1,uint,8,8) [] = { 0x80, 0x80, 0x80, 0x80,
+					       0x80, 0x80, 0x80, 0x80 };
+VECT_VAR_DECL(expected_minus1,uint,16,4) [] = { 0x8000, 0x8000, 0x8000, 0x8000 };
+VECT_VAR_DECL(expected_minus1,uint,32,2) [] = { 0x80000000, 0x80000000 };
+VECT_VAR_DECL(expected_minus1,uint,64,1) [] = { 0x8000000000000000 };
+VECT_VAR_DECL(expected_minus1,int,8,16) [] = { 0x40, 0x40, 0x40, 0x40,
+					       0x40, 0x40, 0x40, 0x40,
+					       0x40, 0x40, 0x40, 0x40,
+					       0x40, 0x40, 0x40, 0x40 };
+VECT_VAR_DECL(expected_minus1,int,16,8) [] = { 0x4000, 0x4000, 0x4000, 0x4000,
+					       0x4000, 0x4000, 0x4000, 0x4000 };
+VECT_VAR_DECL(expected_minus1,int,32,4) [] = { 0x40000000, 0x40000000,
+					       0x40000000, 0x40000000 };
+VECT_VAR_DECL(expected_minus1,int,64,2) [] = { 0x4000000000000000,
+					       0x4000000000000000 };
+VECT_VAR_DECL(expected_minus1,uint,8,16) [] = { 0x80, 0x80, 0x80, 0x80,
+						0x80, 0x80, 0x80, 0x80,
+						0x80, 0x80, 0x80, 0x80,
+						0x80, 0x80, 0x80, 0x80 };
+VECT_VAR_DECL(expected_minus1,uint,16,8) [] = { 0x8000, 0x8000, 0x8000, 0x8000,
+						0x8000, 0x8000, 0x8000, 0x8000 };
+VECT_VAR_DECL(expected_minus1,uint,32,4) [] = { 0x80000000, 0x80000000,
+						0x80000000, 0x80000000 };
+VECT_VAR_DECL(expected_minus1,uint,64,2) [] = { 0x8000000000000000,
+						0x8000000000000000 };
+
+/* Expected values of cumulative_saturation flag with input=max and
+   shift by -3.  */
+int VECT_VAR(expected_cumulative_sat_minus3,int,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat_minus3,int,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat_minus3,int,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat_minus3,int,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat_minus3,uint,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat_minus3,uint,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat_minus3,uint,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat_minus3,uint,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat_minus3,int,8,16) = 0;
+int VECT_VAR(expected_cumulative_sat_minus3,int,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_minus3,int,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat_minus3,int,64,2) = 0;
+int VECT_VAR(expected_cumulative_sat_minus3,uint,8,16) = 0;
+int VECT_VAR(expected_cumulative_sat_minus3,uint,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_minus3,uint,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat_minus3,uint,64,2) = 0;
+
+/* Expected results with input=max and shift by -3.  */
+VECT_VAR_DECL(expected_minus3,int,8,8) [] = { 0x10, 0x10, 0x10, 0x10,
+					      0x10, 0x10, 0x10, 0x10 };
+VECT_VAR_DECL(expected_minus3,int,16,4) [] = { 0x1000, 0x1000, 0x1000, 0x1000 };
+VECT_VAR_DECL(expected_minus3,int,32,2) [] = { 0x10000000, 0x10000000 };
+VECT_VAR_DECL(expected_minus3,int,64,1) [] = { 0x1000000000000000 };
+VECT_VAR_DECL(expected_minus3,uint,8,8) [] = { 0x20, 0x20, 0x20, 0x20,
+					       0x20, 0x20, 0x20, 0x20 };
+VECT_VAR_DECL(expected_minus3,uint,16,4) [] = { 0x2000, 0x2000, 0x2000, 0x2000 };
+VECT_VAR_DECL(expected_minus3,uint,32,2) [] = { 0x20000000, 0x20000000 };
+VECT_VAR_DECL(expected_minus3,uint,64,1) [] = { 0x2000000000000000 };
+VECT_VAR_DECL(expected_minus3,int,8,16) [] = { 0x10, 0x10, 0x10, 0x10,
+					       0x10, 0x10, 0x10, 0x10,
+					       0x10, 0x10, 0x10, 0x10,
+					       0x10, 0x10, 0x10, 0x10 };
+VECT_VAR_DECL(expected_minus3,int,16,8) [] = { 0x1000, 0x1000, 0x1000, 0x1000,
+					       0x1000, 0x1000, 0x1000, 0x1000 };
+VECT_VAR_DECL(expected_minus3,int,32,4) [] = { 0x10000000, 0x10000000,
+					       0x10000000, 0x10000000 };
+VECT_VAR_DECL(expected_minus3,int,64,2) [] = { 0x1000000000000000,
+					       0x1000000000000000 };
+VECT_VAR_DECL(expected_minus3,uint,8,16) [] = { 0x20, 0x20, 0x20, 0x20,
+						0x20, 0x20, 0x20, 0x20,
+						0x20, 0x20, 0x20, 0x20,
+						0x20, 0x20, 0x20, 0x20 };
+VECT_VAR_DECL(expected_minus3,uint,16,8) [] = { 0x2000, 0x2000, 0x2000, 0x2000,
+						0x2000, 0x2000, 0x2000, 0x2000 };
+VECT_VAR_DECL(expected_minus3,uint,32,4) [] = { 0x20000000, 0x20000000,
+						0x20000000, 0x20000000 };
+VECT_VAR_DECL(expected_minus3,uint,64,2) [] = { 0x2000000000000000,
+						0x2000000000000000 };
+
+/* Expected values of cumulative_saturation flag with input=max and
+   large shift amount.  */
+int VECT_VAR(expected_cumulative_sat_large_sh,int,8,8) = 1;
+int VECT_VAR(expected_cumulative_sat_large_sh,int,16,4) = 1;
+int VECT_VAR(expected_cumulative_sat_large_sh,int,32,2) = 1;
+int VECT_VAR(expected_cumulative_sat_large_sh,int,64,1) = 1;
+int VECT_VAR(expected_cumulative_sat_large_sh,uint,8,8) = 1;
+int VECT_VAR(expected_cumulative_sat_large_sh,uint,16,4) = 1;
+int VECT_VAR(expected_cumulative_sat_large_sh,uint,32,2) = 1;
+int VECT_VAR(expected_cumulative_sat_large_sh,uint,64,1) = 1;
+int VECT_VAR(expected_cumulative_sat_large_sh,int,8,16) = 1;
+int VECT_VAR(expected_cumulative_sat_large_sh,int,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat_large_sh,int,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat_large_sh,int,64,2) = 1;
+int VECT_VAR(expected_cumulative_sat_large_sh,uint,8,16) = 1;
+int VECT_VAR(expected_cumulative_sat_large_sh,uint,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat_large_sh,uint,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat_large_sh,uint,64,2) = 1;
+
+/* Expected results with input=max and large shift amount.  */
+VECT_VAR_DECL(expected_large_sh,int,8,8) [] = { 0x7f, 0x7f, 0x7f, 0x7f,
+						0x7f, 0x7f, 0x7f, 0x7f };
+VECT_VAR_DECL(expected_large_sh,int,16,4) [] = { 0x7fff, 0x7fff,
+						 0x7fff, 0x7fff };
+VECT_VAR_DECL(expected_large_sh,int,32,2) [] = { 0x7fffffff, 0x7fffffff };
+VECT_VAR_DECL(expected_large_sh,int,64,1) [] = { 0x7fffffffffffffff };
+VECT_VAR_DECL(expected_large_sh,uint,8,8) [] = { 0xff, 0xff, 0xff, 0xff,
+						 0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_large_sh,uint,16,4) [] = { 0xffff, 0xffff,
+						  0xffff, 0xffff };
+VECT_VAR_DECL(expected_large_sh,uint,32,2) [] = { 0xffffffff, 0xffffffff };
+VECT_VAR_DECL(expected_large_sh,uint,64,1) [] = { 0xffffffffffffffff };
+VECT_VAR_DECL(expected_large_sh,int,8,16) [] = { 0x7f, 0x7f, 0x7f, 0x7f,
+						 0x7f, 0x7f, 0x7f, 0x7f,
+						 0x7f, 0x7f, 0x7f, 0x7f,
+						 0x7f, 0x7f, 0x7f, 0x7f };
+VECT_VAR_DECL(expected_large_sh,int,16,8) [] = { 0x7fff, 0x7fff,
+						 0x7fff, 0x7fff,
+						 0x7fff, 0x7fff,
+						 0x7fff, 0x7fff };
+VECT_VAR_DECL(expected_large_sh,int,32,4) [] = { 0x7fffffff, 0x7fffffff,
+						 0x7fffffff, 0x7fffffff };
+VECT_VAR_DECL(expected_large_sh,int,64,2) [] = { 0x7fffffffffffffff,
+						 0x7fffffffffffffff };
+VECT_VAR_DECL(expected_large_sh,uint,8,16) [] = { 0xff, 0xff, 0xff, 0xff,
+						  0xff, 0xff, 0xff, 0xff,
+						  0xff, 0xff, 0xff, 0xff,
+						  0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_large_sh,uint,16,8) [] = { 0xffff, 0xffff,
+						  0xffff, 0xffff,
+						  0xffff, 0xffff,
+						  0xffff, 0xffff };
+VECT_VAR_DECL(expected_large_sh,uint,32,4) [] = { 0xffffffff, 0xffffffff,
+						  0xffffffff, 0xffffffff };
+VECT_VAR_DECL(expected_large_sh,uint,64,2) [] = { 0xffffffffffffffff,
+						  0xffffffffffffffff };
+
+/* Expected values of cumulative_saturation flag with negative input and
+   large shift amount.  */
+int VECT_VAR(expected_cumulative_sat_neg_large_sh,int,8,8) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large_sh,int,16,4) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large_sh,int,32,2) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large_sh,int,64,1) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large_sh,uint,8,8) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large_sh,uint,16,4) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large_sh,uint,32,2) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large_sh,uint,64,1) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large_sh,int,8,16) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large_sh,int,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large_sh,int,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large_sh,int,64,2) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large_sh,uint,8,16) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large_sh,uint,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large_sh,uint,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large_sh,uint,64,2) = 1;
+
+/* Expected results with negative input and large shift amount.  */
+VECT_VAR_DECL(expected_neg_large_sh,int,8,8) [] = { 0x80, 0x80, 0x80, 0x80,
+						    0x80, 0x80, 0x80, 0x80 };
+VECT_VAR_DECL(expected_neg_large_sh,int,16,4) [] = { 0x8000, 0x8000,
+						     0x8000, 0x8000 };
+VECT_VAR_DECL(expected_neg_large_sh,int,32,2) [] = { 0x80000000, 0x80000000 };
+VECT_VAR_DECL(expected_neg_large_sh,int,64,1) [] = { 0x8000000000000000 };
+VECT_VAR_DECL(expected_neg_large_sh,uint,8,8) [] = { 0xff, 0xff, 0xff, 0xff,
+						     0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_neg_large_sh,uint,16,4) [] = { 0xffff, 0xffff,
+						      0xffff, 0xffff };
+VECT_VAR_DECL(expected_neg_large_sh,uint,32,2) [] = { 0xffffffff,
+						      0xffffffff };
+VECT_VAR_DECL(expected_neg_large_sh,uint,64,1) [] = { 0xffffffffffffffff };
+VECT_VAR_DECL(expected_neg_large_sh,int,8,16) [] = { 0x80, 0x80, 0x80, 0x80,
+						     0x80, 0x80, 0x80, 0x80,
+						     0x80, 0x80, 0x80, 0x80,
+						     0x80, 0x80, 0x80, 0x80 };
+VECT_VAR_DECL(expected_neg_large_sh,int,16,8) [] = { 0x8000, 0x8000,
+						     0x8000, 0x8000,
+						     0x8000, 0x8000,
+						     0x8000, 0x8000 };
+VECT_VAR_DECL(expected_neg_large_sh,int,32,4) [] = { 0x80000000, 0x80000000,
+						     0x80000000, 0x80000000 };
+VECT_VAR_DECL(expected_neg_large_sh,int,64,2) [] = { 0x8000000000000000,
+						     0x8000000000000000 };
+VECT_VAR_DECL(expected_neg_large_sh,uint,8,16) [] = { 0xff, 0xff, 0xff, 0xff,
+						      0xff, 0xff, 0xff, 0xff,
+						      0xff, 0xff, 0xff, 0xff,
+						      0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_neg_large_sh,uint,16,8) [] = { 0xffff, 0xffff,
+						      0xffff, 0xffff,
+						      0xffff, 0xffff,
+						      0xffff, 0xffff };
+VECT_VAR_DECL(expected_neg_large_sh,uint,32,4) [] = { 0xffffffff,
+						      0xffffffff,
+						      0xffffffff,
+						      0xffffffff };
+VECT_VAR_DECL(expected_neg_large_sh,uint,64,2) [] = { 0xffffffffffffffff,
+						      0xffffffffffffffff };
+
+/* Expected values of cumulative_saturation flag with max/min input and
+   large negative shift amount.  */
+int VECT_VAR(expected_cumulative_sat_large_neg_sh,int,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat_large_neg_sh,int,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat_large_neg_sh,int,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat_large_neg_sh,int,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat_large_neg_sh,uint,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat_large_neg_sh,uint,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat_large_neg_sh,uint,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat_large_neg_sh,uint,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat_large_neg_sh,int,8,16) = 0;
+int VECT_VAR(expected_cumulative_sat_large_neg_sh,int,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_large_neg_sh,int,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat_large_neg_sh,int,64,2) = 0;
+int VECT_VAR(expected_cumulative_sat_large_neg_sh,uint,8,16) = 0;
+int VECT_VAR(expected_cumulative_sat_large_neg_sh,uint,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_large_neg_sh,uint,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat_large_neg_sh,uint,64,2) = 0;
+
+/* Expected results with max/min input and large negative shift amount.  */
+VECT_VAR_DECL(expected_large_neg_sh,int,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+						    0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_large_neg_sh,int,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_large_neg_sh,int,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_large_neg_sh,int,64,1) [] = { 0x0 };
+VECT_VAR_DECL(expected_large_neg_sh,uint,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+						     0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_large_neg_sh,uint,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_large_neg_sh,uint,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_large_neg_sh,uint,64,1) [] = { 0x0 };
+VECT_VAR_DECL(expected_large_neg_sh,int,8,16) [] = { 0x0, 0x0, 0x0, 0x0,
+						     0x0, 0x0, 0x0, 0x0,
+						     0x0, 0x0, 0x0, 0x0,
+						     0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_large_neg_sh,int,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+						     0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_large_neg_sh,int,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_large_neg_sh,int,64,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_large_neg_sh,uint,8,16) [] = { 0x0, 0x0, 0x0, 0x0,
+						      0x0, 0x0, 0x0, 0x0,
+						      0x0, 0x0, 0x0, 0x0,
+						      0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_large_neg_sh,uint,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+						      0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_large_neg_sh,uint,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_large_neg_sh,uint,64,2) [] = { 0x0, 0x0 };
+
+/* Expected values of cumulative_saturation flag with input=0 and
+   large negative shift amount.  */
+int VECT_VAR(expected_cumulative_sat_0_large_neg_sh,int,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat_0_large_neg_sh,int,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat_0_large_neg_sh,int,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat_0_large_neg_sh,int,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat_0_large_neg_sh,uint,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat_0_large_neg_sh,uint,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat_0_large_neg_sh,uint,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat_0_large_neg_sh,uint,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat_0_large_neg_sh,int,8,16) = 0;
+int VECT_VAR(expected_cumulative_sat_0_large_neg_sh,int,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_0_large_neg_sh,int,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat_0_large_neg_sh,int,64,2) = 0;
+int VECT_VAR(expected_cumulative_sat_0_large_neg_sh,uint,8,16) = 0;
+int VECT_VAR(expected_cumulative_sat_0_large_neg_sh,uint,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_0_large_neg_sh,uint,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat_0_large_neg_sh,uint,64,2) = 0;
+
+/* Expected results with input=0 and large negative shift amount.  */
+VECT_VAR_DECL(expected_0_large_neg_sh,int,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+						      0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_large_neg_sh,int,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_large_neg_sh,int,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_large_neg_sh,int,64,1) [] = { 0x0 };
+VECT_VAR_DECL(expected_0_large_neg_sh,uint,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+						       0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_large_neg_sh,uint,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_large_neg_sh,uint,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_large_neg_sh,uint,64,1) [] = { 0x0 };
+VECT_VAR_DECL(expected_0_large_neg_sh,int,8,16) [] = { 0x0, 0x0, 0x0, 0x0,
+						       0x0, 0x0, 0x0, 0x0,
+						       0x0, 0x0, 0x0, 0x0,
+						       0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_large_neg_sh,int,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+						       0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_large_neg_sh,int,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_large_neg_sh,int,64,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_large_neg_sh,uint,8,16) [] = { 0x0, 0x0, 0x0, 0x0,
+							0x0, 0x0, 0x0, 0x0,
+							0x0, 0x0, 0x0, 0x0,
+							0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_large_neg_sh,uint,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+							0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_large_neg_sh,uint,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_large_neg_sh,uint,64,2) [] = { 0x0, 0x0 };
+
+#define INSN vqrshl
+#define TEST_MSG "VQRSHL/VQRSHLQ"
+
+#define FNNAME1(NAME) void exec_ ## NAME (void)
+#define FNNAME(NAME) FNNAME1(NAME)
+
+FNNAME (INSN)
+{
+  /* Basic test: v3=vqrshl(v1,v2), then store the result.  */
+#define TEST_VQRSHL2(INSN, T3, Q, T1, T2, W, N, EXPECTED_CUMULATIVE_SAT, CMT) \
+  Set_Neon_Cumulative_Sat(0, VECT_VAR(vector_res, T1, W, N));		\
+  VECT_VAR(vector_res, T1, W, N) =					\
+    INSN##Q##_##T2##W(VECT_VAR(vector, T1, W, N),			\
+		      VECT_VAR(vector_shift, T3, W, N));		\
+  vst1##Q##_##T2##W(VECT_VAR(result, T1, W, N),				\
+		    VECT_VAR(vector_res, T1, W, N));			\
+  CHECK_CUMULATIVE_SAT(TEST_MSG, T1, W, N, EXPECTED_CUMULATIVE_SAT, CMT)
+
+  /* Two auxliary macros are necessary to expand INSN */
+#define TEST_VQRSHL1(INSN, T3, Q, T1, T2, W, N, EXPECTED_CUMULATIVE_SAT, CMT) \
+  TEST_VQRSHL2(INSN, T3, Q, T1, T2, W, N, EXPECTED_CUMULATIVE_SAT, CMT)
+
+#define TEST_VQRSHL(T3, Q, T1, T2, W, N, EXPECTED_CUMULATIVE_SAT, CMT)	\
+  TEST_VQRSHL1(INSN, T3, Q, T1, T2, W, N, EXPECTED_CUMULATIVE_SAT, CMT)
+
+  DECL_VARIABLE_ALL_VARIANTS(vector);
+  DECL_VARIABLE_ALL_VARIANTS(vector_res);
+
+  DECL_VARIABLE_SIGNED_VARIANTS(vector_shift);
+
+  clean_results ();
+
+  /* Fill input vector with 0, to check saturation on limits.  */
+  VDUP(vector, , int, s, 8, 8, 0);
+  VDUP(vector, , int, s, 16, 4, 0);
+  VDUP(vector, , int, s, 32, 2, 0);
+  VDUP(vector, , int, s, 64, 1, 0);
+  VDUP(vector, , uint, u, 8, 8, 0);
+  VDUP(vector, , uint, u, 16, 4, 0);
+  VDUP(vector, , uint, u, 32, 2, 0);
+  VDUP(vector, , uint, u, 64, 1, 0);
+  VDUP(vector, q, int, s, 8, 16, 0);
+  VDUP(vector, q, int, s, 16, 8, 0);
+  VDUP(vector, q, int, s, 32, 4, 0);
+  VDUP(vector, q, int, s, 64, 2, 0);
+  VDUP(vector, q, uint, u, 8, 16, 0);
+  VDUP(vector, q, uint, u, 16, 8, 0);
+  VDUP(vector, q, uint, u, 32, 4, 0);
+  VDUP(vector, q, uint, u, 64, 2, 0);
+
+  /* Choose init value arbitrarily, will be used as shift amount */
+  /* Use values equal to or one-less-than the type width to check
+     behaviour on limits.  */
+  VDUP(vector_shift, , int, s, 8, 8, 7);
+  VDUP(vector_shift, , int, s, 16, 4, 15);
+  VDUP(vector_shift, , int, s, 32, 2, 31);
+  VDUP(vector_shift, , int, s, 64, 1, 63);
+  VDUP(vector_shift, q, int, s, 8, 16, 8);
+  VDUP(vector_shift, q, int, s, 16, 8, 16);
+  VDUP(vector_shift, q, int, s, 32, 4, 32);
+  VDUP(vector_shift, q, int, s, 64, 2, 64);
+
+#define CMT " (with input = 0)"
+  TEST_VQRSHL(int, , int, s, 8, 8, expected_cumulative_sat_0, CMT);
+  TEST_VQRSHL(int, , int, s, 16, 4, expected_cumulative_sat_0, CMT);
+  TEST_VQRSHL(int, , int, s, 32, 2, expected_cumulative_sat_0, CMT);
+  TEST_VQRSHL(int, , int, s, 64, 1, expected_cumulative_sat_0, CMT);
+  TEST_VQRSHL(int, , uint, u, 8, 8, expected_cumulative_sat_0, CMT);
+  TEST_VQRSHL(int, , uint, u, 16, 4, expected_cumulative_sat_0, CMT);
+  TEST_VQRSHL(int, , uint, u, 32, 2, expected_cumulative_sat_0, CMT);
+  TEST_VQRSHL(int, , uint, u, 64, 1, expected_cumulative_sat_0, CMT);
+  TEST_VQRSHL(int, q, int, s, 8, 16, expected_cumulative_sat_0, CMT);
+  TEST_VQRSHL(int, q, int, s, 16, 8, expected_cumulative_sat_0, CMT);
+  TEST_VQRSHL(int, q, int, s, 32, 4, expected_cumulative_sat_0, CMT);
+  TEST_VQRSHL(int, q, int, s, 64, 2, expected_cumulative_sat_0, CMT);
+  TEST_VQRSHL(int, q, uint, u, 8, 16, expected_cumulative_sat_0, CMT);
+  TEST_VQRSHL(int, q, uint, u, 16, 8, expected_cumulative_sat_0, CMT);
+  TEST_VQRSHL(int, q, uint, u, 32, 4, expected_cumulative_sat_0, CMT);
+  TEST_VQRSHL(int, q, uint, u, 64, 2, expected_cumulative_sat_0, CMT);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_0, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_0, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_0, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_0, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_0, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_0, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_0, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_0, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_0, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_0, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_0, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_0, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_0, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_0, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_0, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_0, CMT);
+
+
+  /* Use negative shift amounts.  */
+  VDUP(vector_shift, , int, s, 8, 8, -1);
+  VDUP(vector_shift, , int, s, 16, 4, -2);
+  VDUP(vector_shift, , int, s, 32, 2, -3);
+  VDUP(vector_shift, , int, s, 64, 1, -4);
+  VDUP(vector_shift, q, int, s, 8, 16, -7);
+  VDUP(vector_shift, q, int, s, 16, 8, -11);
+  VDUP(vector_shift, q, int, s, 32, 4, -13);
+  VDUP(vector_shift, q, int, s, 64, 2, -20);
+
+#undef CMT
+#define CMT " (input 0 and negative shift amount)"
+  TEST_VQRSHL(int, , int, s, 8, 8, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQRSHL(int, , int, s, 16, 4, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQRSHL(int, , int, s, 32, 2, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQRSHL(int, , int, s, 64, 1, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQRSHL(int, , uint, u, 8, 8, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQRSHL(int, , uint, u, 16, 4, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQRSHL(int, , uint, u, 32, 2, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQRSHL(int, , uint, u, 64, 1, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQRSHL(int, q, int, s, 8, 16, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQRSHL(int, q, int, s, 16, 8, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQRSHL(int, q, int, s, 32, 4, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQRSHL(int, q, int, s, 64, 2, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQRSHL(int, q, uint, u, 8, 16, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQRSHL(int, q, uint, u, 16, 8, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQRSHL(int, q, uint, u, 32, 4, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQRSHL(int, q, uint, u, 64, 2, expected_cumulative_sat_0_neg, CMT);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_0_neg, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_0_neg, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_0_neg, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_0_neg, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_0_neg, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_0_neg, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_0_neg, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_0_neg, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_0_neg, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_0_neg, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_0_neg, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_0_neg, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_0_neg, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_0_neg, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_0_neg, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_0_neg, CMT);
+
+
+  /* Test again, with predefined input values.  */
+  TEST_MACRO_ALL_VARIANTS_2_5(VLOAD, vector, buffer);
+
+  /* Choose init value arbitrarily, will be used as shift amount.  */
+  VDUP(vector_shift, , int, s, 8, 8, 1);
+  VDUP(vector_shift, , int, s, 16, 4, 3);
+  VDUP(vector_shift, , int, s, 32, 2, 8);
+  VDUP(vector_shift, , int, s, 64, 1, 3);
+  VDUP(vector_shift, q, int, s, 8, 16, 10);
+  VDUP(vector_shift, q, int, s, 16, 8, 12);
+  VDUP(vector_shift, q, int, s, 32, 4, 31);
+  VDUP(vector_shift, q, int, s, 64, 2, 63);
+
+#undef CMT
+#define CMT ""
+  TEST_VQRSHL(int, , int, s, 8, 8, expected_cumulative_sat, CMT);
+  TEST_VQRSHL(int, , int, s, 16, 4, expected_cumulative_sat, CMT);
+  TEST_VQRSHL(int, , int, s, 32, 2, expected_cumulative_sat, CMT);
+  TEST_VQRSHL(int, , int, s, 64, 1, expected_cumulative_sat, CMT);
+  TEST_VQRSHL(int, , uint, u, 8, 8, expected_cumulative_sat, CMT);
+  TEST_VQRSHL(int, , uint, u, 16, 4, expected_cumulative_sat, CMT);
+  TEST_VQRSHL(int, , uint, u, 32, 2, expected_cumulative_sat, CMT);
+  TEST_VQRSHL(int, , uint, u, 64, 1, expected_cumulative_sat, CMT);
+  TEST_VQRSHL(int, q, int, s, 8, 16, expected_cumulative_sat, CMT);
+  TEST_VQRSHL(int, q, int, s, 16, 8, expected_cumulative_sat, CMT);
+  TEST_VQRSHL(int, q, int, s, 32, 4, expected_cumulative_sat, CMT);
+  TEST_VQRSHL(int, q, int, s, 64, 2, expected_cumulative_sat, CMT);
+  TEST_VQRSHL(int, q, uint, u, 8, 16, expected_cumulative_sat, CMT);
+  TEST_VQRSHL(int, q, uint, u, 16, 8, expected_cumulative_sat, CMT);
+  TEST_VQRSHL(int, q, uint, u, 32, 4, expected_cumulative_sat, CMT);
+  TEST_VQRSHL(int, q, uint, u, 64, 2, expected_cumulative_sat, CMT);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected, CMT);
+
+
+  /* Use negative shift amounts.  */
+  VDUP(vector_shift, , int, s, 8, 8, -2);
+  VDUP(vector_shift, , int, s, 16, 4, -2);
+  VDUP(vector_shift, , int, s, 32, 2, -3);
+  VDUP(vector_shift, , int, s, 64, 1, -4);
+  VDUP(vector_shift, q, int, s, 8, 16, -7);
+  VDUP(vector_shift, q, int, s, 16, 8, -11);
+  VDUP(vector_shift, q, int, s, 32, 4, -13);
+  VDUP(vector_shift, q, int, s, 64, 2, -20);
+
+#undef CMT
+#define CMT " (negative shift amount)"
+  TEST_VQRSHL(int, , int, s, 8, 8, expected_cumulative_sat_neg, CMT);
+  TEST_VQRSHL(int, , int, s, 16, 4, expected_cumulative_sat_neg, CMT);
+  TEST_VQRSHL(int, , int, s, 32, 2, expected_cumulative_sat_neg, CMT);
+  TEST_VQRSHL(int, , int, s, 64, 1, expected_cumulative_sat_neg, CMT);
+  TEST_VQRSHL(int, , uint, u, 8, 8, expected_cumulative_sat_neg, CMT);
+  TEST_VQRSHL(int, , uint, u, 16, 4, expected_cumulative_sat_neg, CMT);
+  TEST_VQRSHL(int, , uint, u, 32, 2, expected_cumulative_sat_neg, CMT);
+  TEST_VQRSHL(int, , uint, u, 64, 1, expected_cumulative_sat_neg, CMT);
+  TEST_VQRSHL(int, q, int, s, 8, 16, expected_cumulative_sat_neg, CMT);
+  TEST_VQRSHL(int, q, int, s, 16, 8, expected_cumulative_sat_neg, CMT);
+  TEST_VQRSHL(int, q, int, s, 32, 4, expected_cumulative_sat_neg, CMT);
+  TEST_VQRSHL(int, q, int, s, 64, 2, expected_cumulative_sat_neg, CMT);
+  TEST_VQRSHL(int, q, uint, u, 8, 16, expected_cumulative_sat_neg, CMT);
+  TEST_VQRSHL(int, q, uint, u, 16, 8, expected_cumulative_sat_neg, CMT);
+  TEST_VQRSHL(int, q, uint, u, 32, 4, expected_cumulative_sat_neg, CMT);
+  TEST_VQRSHL(int, q, uint, u, 64, 2, expected_cumulative_sat_neg, CMT);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_neg, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_neg, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_neg, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_neg, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_neg, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_neg, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_neg, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_neg, CMT);
+
+
+  /* Fill input vector with max value, to check saturation on
+     limits.  */
+  VDUP(vector, , int, s, 8, 8, 0x7F);
+  VDUP(vector, , int, s, 16, 4, 0x7FFF);
+  VDUP(vector, , int, s, 32, 2, 0x7FFFFFFF);
+  VDUP(vector, , int, s, 64, 1, 0x7FFFFFFFFFFFFFFFLL);
+  VDUP(vector, , uint, u, 8, 8, 0xFF);
+  VDUP(vector, , uint, u, 16, 4, 0xFFFF);
+  VDUP(vector, , uint, u, 32, 2, 0xFFFFFFFF);
+  VDUP(vector, , uint, u, 64, 1, 0xFFFFFFFFFFFFFFFFULL);
+  VDUP(vector, q, int, s, 8, 16, 0x7F);
+  VDUP(vector, q, int, s, 16, 8, 0x7FFF);
+  VDUP(vector, q, int, s, 32, 4, 0x7FFFFFFF);
+  VDUP(vector, q, int, s, 64, 2, 0x7FFFFFFFFFFFFFFFLL);
+  VDUP(vector, q, uint, u, 8, 16, 0xFF);
+  VDUP(vector, q, uint, u, 16, 8, 0xFFFF);
+  VDUP(vector, q, uint, u, 32, 4, 0xFFFFFFFF);
+  VDUP(vector, q, uint, u, 64, 2, 0xFFFFFFFFFFFFFFFFULL);
+
+  /* Use -1 shift amount to check cumulative saturation with
+     round_const.  */
+  VDUP(vector_shift, , int, s, 8, 8, -1);
+  VDUP(vector_shift, , int, s, 16, 4, -1);
+  VDUP(vector_shift, , int, s, 32, 2, -1);
+  VDUP(vector_shift, , int, s, 64, 1, -1);
+  VDUP(vector_shift, q, int, s, 8, 16, -1);
+  VDUP(vector_shift, q, int, s, 16, 8, -1);
+  VDUP(vector_shift, q, int, s, 32, 4, -1);
+  VDUP(vector_shift, q, int, s, 64, 2, -1);
+
+#undef CMT
+#define CMT " (checking cumulative saturation: shift by -1)"
+  TEST_VQRSHL(int, , int, s, 8, 8, expected_cumulative_sat_minus1, CMT);
+  TEST_VQRSHL(int, , int, s, 16, 4, expected_cumulative_sat_minus1, CMT);
+  TEST_VQRSHL(int, , int, s, 32, 2, expected_cumulative_sat_minus1, CMT);
+  TEST_VQRSHL(int, , int, s, 64, 1, expected_cumulative_sat_minus1, CMT);
+  TEST_VQRSHL(int, , uint, u, 8, 8, expected_cumulative_sat_minus1, CMT);
+  TEST_VQRSHL(int, , uint, u, 16, 4, expected_cumulative_sat_minus1, CMT);
+  TEST_VQRSHL(int, , uint, u, 32, 2, expected_cumulative_sat_minus1, CMT);
+  TEST_VQRSHL(int, , uint, u, 64, 1, expected_cumulative_sat_minus1, CMT);
+  TEST_VQRSHL(int, q, int, s, 8, 16, expected_cumulative_sat_minus1, CMT);
+  TEST_VQRSHL(int, q, int, s, 16, 8, expected_cumulative_sat_minus1, CMT);
+  TEST_VQRSHL(int, q, int, s, 32, 4, expected_cumulative_sat_minus1, CMT);
+  TEST_VQRSHL(int, q, int, s, 64, 2, expected_cumulative_sat_minus1, CMT);
+  TEST_VQRSHL(int, q, uint, u, 8, 16, expected_cumulative_sat_minus1, CMT);
+  TEST_VQRSHL(int, q, uint, u, 16, 8, expected_cumulative_sat_minus1, CMT);
+  TEST_VQRSHL(int, q, uint, u, 32, 4, expected_cumulative_sat_minus1, CMT);
+  TEST_VQRSHL(int, q, uint, u, 64, 2, expected_cumulative_sat_minus1, CMT);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_minus1, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_minus1, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_minus1, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_minus1, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_minus1, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_minus1, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_minus1, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_minus1, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_minus1, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_minus1, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_minus1, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_minus1, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_minus1, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_minus1, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_minus1, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_minus1, CMT);
+
+
+  /* Use -3 shift amount to check cumulative saturation with
+     round_const. */
+  VDUP(vector_shift, , int, s, 8, 8, -3);
+  VDUP(vector_shift, , int, s, 16, 4, -3);
+  VDUP(vector_shift, , int, s, 32, 2, -3);
+  VDUP(vector_shift, , int, s, 64, 1, -3);
+  VDUP(vector_shift, q, int, s, 8, 16, -3);
+  VDUP(vector_shift, q, int, s, 16, 8, -3);
+  VDUP(vector_shift, q, int, s, 32, 4, -3);
+  VDUP(vector_shift, q, int, s, 64, 2, -3);
+
+#undef CMT
+#define CMT " (checking cumulative saturation: shift by -3)"
+  TEST_VQRSHL(int, , int, s, 8, 8, expected_cumulative_sat_minus3, CMT);
+  TEST_VQRSHL(int, , int, s, 16, 4, expected_cumulative_sat_minus3, CMT);
+  TEST_VQRSHL(int, , int, s, 32, 2, expected_cumulative_sat_minus3, CMT);
+  TEST_VQRSHL(int, , int, s, 64, 1, expected_cumulative_sat_minus3, CMT);
+  TEST_VQRSHL(int, , uint, u, 8, 8, expected_cumulative_sat_minus3, CMT);
+  TEST_VQRSHL(int, , uint, u, 16, 4, expected_cumulative_sat_minus3, CMT);
+  TEST_VQRSHL(int, , uint, u, 32, 2, expected_cumulative_sat_minus3, CMT);
+  TEST_VQRSHL(int, , uint, u, 64, 1, expected_cumulative_sat_minus3, CMT);
+  TEST_VQRSHL(int, q, int, s, 8, 16, expected_cumulative_sat_minus3, CMT);
+  TEST_VQRSHL(int, q, int, s, 16, 8, expected_cumulative_sat_minus3, CMT);
+  TEST_VQRSHL(int, q, int, s, 32, 4, expected_cumulative_sat_minus3, CMT);
+  TEST_VQRSHL(int, q, int, s, 64, 2, expected_cumulative_sat_minus3, CMT);
+  TEST_VQRSHL(int, q, uint, u, 8, 16, expected_cumulative_sat_minus3, CMT);
+  TEST_VQRSHL(int, q, uint, u, 16, 8, expected_cumulative_sat_minus3, CMT);
+  TEST_VQRSHL(int, q, uint, u, 32, 4, expected_cumulative_sat_minus3, CMT);
+  TEST_VQRSHL(int, q, uint, u, 64, 2, expected_cumulative_sat_minus3, CMT);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_minus3, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_minus3, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_minus3, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_minus3, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_minus3, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_minus3, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_minus3, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_minus3, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_minus3, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_minus3, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_minus3, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_minus3, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_minus3, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_minus3, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_minus3, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_minus3, CMT);
+
+
+  /* Use large shift amount.  */
+  VDUP(vector_shift, , int, s, 8, 8, 10);
+  VDUP(vector_shift, , int, s, 16, 4, 20);
+  VDUP(vector_shift, , int, s, 32, 2, 40);
+  VDUP(vector_shift, , int, s, 64, 1, 70);
+  VDUP(vector_shift, q, int, s, 8, 16, 10);
+  VDUP(vector_shift, q, int, s, 16, 8, 20);
+  VDUP(vector_shift, q, int, s, 32, 4, 40);
+  VDUP(vector_shift, q, int, s, 64, 2, 70);
+
+#undef CMT
+#define CMT " (checking cumulative saturation: large shift amount)"
+  TEST_VQRSHL(int, , int, s, 8, 8, expected_cumulative_sat_large_sh, CMT);
+  TEST_VQRSHL(int, , int, s, 16, 4, expected_cumulative_sat_large_sh, CMT);
+  TEST_VQRSHL(int, , int, s, 32, 2, expected_cumulative_sat_large_sh, CMT);
+  TEST_VQRSHL(int, , int, s, 64, 1, expected_cumulative_sat_large_sh, CMT);
+  TEST_VQRSHL(int, , uint, u, 8, 8, expected_cumulative_sat_large_sh, CMT);
+  TEST_VQRSHL(int, , uint, u, 16, 4, expected_cumulative_sat_large_sh, CMT);
+  TEST_VQRSHL(int, , uint, u, 32, 2, expected_cumulative_sat_large_sh, CMT);
+  TEST_VQRSHL(int, , uint, u, 64, 1, expected_cumulative_sat_large_sh, CMT);
+  TEST_VQRSHL(int, q, int, s, 8, 16, expected_cumulative_sat_large_sh, CMT);
+  TEST_VQRSHL(int, q, int, s, 16, 8, expected_cumulative_sat_large_sh, CMT);
+  TEST_VQRSHL(int, q, int, s, 32, 4, expected_cumulative_sat_large_sh, CMT);
+  TEST_VQRSHL(int, q, int, s, 64, 2, expected_cumulative_sat_large_sh, CMT);
+  TEST_VQRSHL(int, q, uint, u, 8, 16, expected_cumulative_sat_large_sh, CMT);
+  TEST_VQRSHL(int, q, uint, u, 16, 8, expected_cumulative_sat_large_sh, CMT);
+  TEST_VQRSHL(int, q, uint, u, 32, 4, expected_cumulative_sat_large_sh, CMT);
+  TEST_VQRSHL(int, q, uint, u, 64, 2, expected_cumulative_sat_large_sh, CMT);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_large_sh, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_large_sh, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_large_sh, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_large_sh, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_large_sh, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_large_sh, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_large_sh, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_large_sh, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_large_sh, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_large_sh, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_large_sh, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_large_sh, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_large_sh, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_large_sh, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_large_sh, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_large_sh, CMT);
+
+
+  /* Fill input vector with negative values, to check saturation on
+     limits.  */
+  VDUP(vector, , int, s, 8, 8, 0x80);
+  VDUP(vector, , int, s, 16, 4, 0x8000);
+  VDUP(vector, , int, s, 32, 2, 0x80000000);
+  VDUP(vector, , int, s, 64, 1, 0x8000000000000000LL);
+  VDUP(vector, q, int, s, 8, 16, 0x80);
+  VDUP(vector, q, int, s, 16, 8, 0x8000);
+  VDUP(vector, q, int, s, 32, 4, 0x80000000);
+  VDUP(vector, q, int, s, 64, 2, 0x8000000000000000LL);
+
+  /* Use large shift amount.  */
+  VDUP(vector_shift, , int, s, 8, 8, 10);
+  VDUP(vector_shift, , int, s, 16, 4, 20);
+  VDUP(vector_shift, , int, s, 32, 2, 40);
+  VDUP(vector_shift, , int, s, 64, 1, 70);
+  VDUP(vector_shift, q, int, s, 8, 16, 10);
+  VDUP(vector_shift, q, int, s, 16, 8, 20);
+  VDUP(vector_shift, q, int, s, 32, 4, 40);
+  VDUP(vector_shift, q, int, s, 64, 2, 70);
+
+#undef CMT
+#define CMT " (checking cumulative saturation: large shift amount with negative input)"
+  TEST_VQRSHL(int, , int, s, 8, 8, expected_cumulative_sat_neg_large_sh, CMT);
+  TEST_VQRSHL(int, , int, s, 16, 4, expected_cumulative_sat_neg_large_sh, CMT);
+  TEST_VQRSHL(int, , int, s, 32, 2, expected_cumulative_sat_neg_large_sh, CMT);
+  TEST_VQRSHL(int, , int, s, 64, 1, expected_cumulative_sat_neg_large_sh, CMT);
+  TEST_VQRSHL(int, , uint, u, 8, 8, expected_cumulative_sat_neg_large_sh, CMT);
+  TEST_VQRSHL(int, , uint, u, 16, 4, expected_cumulative_sat_neg_large_sh, CMT);
+  TEST_VQRSHL(int, , uint, u, 32, 2, expected_cumulative_sat_neg_large_sh, CMT);
+  TEST_VQRSHL(int, , uint, u, 64, 1, expected_cumulative_sat_neg_large_sh, CMT);
+  TEST_VQRSHL(int, q, int, s, 8, 16, expected_cumulative_sat_neg_large_sh, CMT);
+  TEST_VQRSHL(int, q, int, s, 16, 8, expected_cumulative_sat_neg_large_sh, CMT);
+  TEST_VQRSHL(int, q, int, s, 32, 4, expected_cumulative_sat_neg_large_sh, CMT);
+  TEST_VQRSHL(int, q, int, s, 64, 2, expected_cumulative_sat_neg_large_sh, CMT);
+  TEST_VQRSHL(int, q, uint, u, 8, 16, expected_cumulative_sat_neg_large_sh, CMT);
+  TEST_VQRSHL(int, q, uint, u, 16, 8, expected_cumulative_sat_neg_large_sh, CMT);
+  TEST_VQRSHL(int, q, uint, u, 32, 4, expected_cumulative_sat_neg_large_sh, CMT);
+  TEST_VQRSHL(int, q, uint, u, 64, 2, expected_cumulative_sat_neg_large_sh, CMT);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_neg_large_sh, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_neg_large_sh, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_neg_large_sh, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_neg_large_sh, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_neg_large_sh, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_neg_large_sh, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_neg_large_sh, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_neg_large_sh, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_neg_large_sh, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_neg_large_sh, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_neg_large_sh, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_neg_large_sh, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_neg_large_sh, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_neg_large_sh, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_neg_large_sh, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_neg_large_sh, CMT);
+
+
+  /* Fill input vector with negative and positive values, to check
+   * saturation on limits */
+  VDUP(vector, , int, s, 8, 8, 0x7F);
+  VDUP(vector, , int, s, 16, 4, 0x7FFF);
+  VDUP(vector, , int, s, 32, 2, 0x7FFFFFFF);
+  VDUP(vector, , int, s, 64, 1, 0x7FFFFFFFFFFFFFFFLL);
+  VDUP(vector, q, int, s, 8, 16, 0x80);
+  VDUP(vector, q, int, s, 16, 8, 0x8000);
+  VDUP(vector, q, int, s, 32, 4, 0x80000000);
+  VDUP(vector, q, int, s, 64, 2, 0x8000000000000000LL);
+
+  /* Use large negative shift amount  */
+  VDUP(vector_shift, , int, s, 8, 8, -10);
+  VDUP(vector_shift, , int, s, 16, 4, -20);
+  VDUP(vector_shift, , int, s, 32, 2, -40);
+  VDUP(vector_shift, , int, s, 64, 1, -70);
+  VDUP(vector_shift, q, int, s, 8, 16, -10);
+  VDUP(vector_shift, q, int, s, 16, 8, -20);
+  VDUP(vector_shift, q, int, s, 32, 4, -40);
+  VDUP(vector_shift, q, int, s, 64, 2, -70);
+
+#undef CMT
+#define CMT " (checking cumulative saturation: large negative shift amount)"
+  TEST_VQRSHL(int, , int, s, 8, 8, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, , int, s, 16, 4, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, , int, s, 32, 2, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, , int, s, 64, 1, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, , uint, u, 8, 8, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, , uint, u, 16, 4, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, , uint, u, 32, 2, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, , uint, u, 64, 1, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, q, int, s, 8, 16, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, q, int, s, 16, 8, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, q, int, s, 32, 4, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, q, int, s, 64, 2, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, q, uint, u, 8, 16, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, q, uint, u, 16, 8, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, q, uint, u, 32, 4, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, q, uint, u, 64, 2, expected_cumulative_sat_large_neg_sh, CMT);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_large_neg_sh, CMT);
+
+
+  /* Fill input vector with 0, to check saturation in case of large
+   * shift amount */
+  VDUP(vector, , int, s, 8, 8, 0);
+  VDUP(vector, , int, s, 16, 4, 0);
+  VDUP(vector, , int, s, 32, 2, 0);
+  VDUP(vector, , int, s, 64, 1, 0);
+  VDUP(vector, q, int, s, 8, 16, 0);
+  VDUP(vector, q, int, s, 16, 8, 0);
+  VDUP(vector, q, int, s, 32, 4, 0);
+  VDUP(vector, q, int, s, 64, 2, 0);
+
+  /* Use large shift amount  */
+  VDUP(vector_shift, , int, s, 8, 8, -10);
+  VDUP(vector_shift, , int, s, 16, 4, -20);
+  VDUP(vector_shift, , int, s, 32, 2, -40);
+  VDUP(vector_shift, , int, s, 64, 1, -70);
+  VDUP(vector_shift, q, int, s, 8, 16, -10);
+  VDUP(vector_shift, q, int, s, 16, 8, -20);
+  VDUP(vector_shift, q, int, s, 32, 4, -40);
+  VDUP(vector_shift, q, int, s, 64, 2, -70);
+
+#undef CMT
+#define CMT " (checking cumulative saturation: large negative shift amount with 0 input)"
+  TEST_VQRSHL(int, , int, s, 8, 8, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, , int, s, 16, 4, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, , int, s, 32, 2, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, , int, s, 64, 1, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, , uint, u, 8, 8, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, , uint, u, 16, 4, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, , uint, u, 32, 2, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, , uint, u, 64, 1, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, q, int, s, 8, 16, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, q, int, s, 16, 8, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, q, int, s, 32, 4, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, q, int, s, 64, 2, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, q, uint, u, 8, 16, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, q, uint, u, 16, 8, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, q, uint, u, 32, 4, expected_cumulative_sat_large_neg_sh, CMT);
+  TEST_VQRSHL(int, q, uint, u, 64, 2, expected_cumulative_sat_large_neg_sh, CMT);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_large_neg_sh, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_large_neg_sh, CMT);
+}
+
+int main (void)
+{
+  exec_vqrshl ();
+  return 0;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vqrshrn_n.c
@@ -0,0 +1,174 @@
+#include <arm_neon.h>
+#include "arm-neon-ref.h"
+#include "compute-ref-data.h"
+
+/* Expected values of cumulative_saturation flag.  */
+int VECT_VAR(expected_cumulative_sat,int,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat,int,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat,int,64,2) = 0;
+int VECT_VAR(expected_cumulative_sat,uint,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat,uint,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat,uint,64,2) = 1;
+
+/* Expected results.  */
+VECT_VAR_DECL(expected,int,8,8) [] = { 0xf8, 0xf9, 0xf9, 0xfa,
+				       0xfa, 0xfb, 0xfb, 0xfc };
+VECT_VAR_DECL(expected,int,16,4) [] = { 0xfff8, 0xfff9, 0xfff9, 0xfffa };
+VECT_VAR_DECL(expected,int,32,2) [] = { 0xfffffffc, 0xfffffffc };
+VECT_VAR_DECL(expected,uint,8,8) [] = { 0xff, 0xff, 0xff, 0xff,
+					0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected,uint,16,4) [] = { 0xffff, 0xffff, 0xffff, 0xffff };
+VECT_VAR_DECL(expected,uint,32,2) [] = { 0xffffffff, 0xffffffff };
+
+/* Expected values of cumulative_saturation flag with shift by 3.  */
+int VECT_VAR(expected_cumulative_sat_sh3,int,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat_sh3,int,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat_sh3,int,64,2) = 1;
+int VECT_VAR(expected_cumulative_sat_sh3,uint,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat_sh3,uint,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat_sh3,uint,64,2) = 1;
+
+/* Expected results with shift by 3.  */
+VECT_VAR_DECL(expected_sh3,int,8,8) [] = { 0x7f, 0x7f, 0x7f, 0x7f,
+					   0x7f, 0x7f, 0x7f, 0x7f };
+VECT_VAR_DECL(expected_sh3,int,16,4) [] = { 0x7fff, 0x7fff, 0x7fff, 0x7fff };
+VECT_VAR_DECL(expected_sh3,int,32,2) [] = { 0x7fffffff, 0x7fffffff };
+VECT_VAR_DECL(expected_sh3,uint,8,8) [] = { 0xff, 0xff, 0xff, 0xff,
+					    0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_sh3,uint,16,4) [] = { 0xffff, 0xffff, 0xffff, 0xffff };
+VECT_VAR_DECL(expected_sh3,uint,32,2) [] = { 0xffffffff, 0xffffffff };
+
+/* Expected values of cumulative_saturation flag with shift by max
+   amount.  */
+int VECT_VAR(expected_cumulative_sat_shmax,int,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat_shmax,int,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat_shmax,int,64,2) = 1;
+int VECT_VAR(expected_cumulative_sat_shmax,uint,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat_shmax,uint,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat_shmax,uint,64,2) = 1;
+
+/* Expected results with shift by max amount.  */
+VECT_VAR_DECL(expected_shmax,int,8,8) [] = { 0x7f, 0x7f, 0x7f, 0x7f,
+					     0x7f, 0x7f, 0x7f, 0x7f };
+VECT_VAR_DECL(expected_shmax,int,16,4) [] = { 0x7fff, 0x7fff, 0x7fff, 0x7fff };
+VECT_VAR_DECL(expected_shmax,int,32,2) [] = { 0x7fffffff, 0x7fffffff };
+VECT_VAR_DECL(expected_shmax,uint,8,8) [] = { 0xff, 0xff, 0xff, 0xff,
+					      0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_shmax,uint,16,4) [] = { 0xffff, 0xffff, 0xffff, 0xffff };
+VECT_VAR_DECL(expected_shmax,uint,32,2) [] = { 0xffffffff, 0xffffffff };
+
+#define INSN vqrshrn_n
+#define TEST_MSG "VQRSHRN_N"
+
+#define FNNAME1(NAME) void exec_ ## NAME (void)
+#define FNNAME(NAME) FNNAME1(NAME)
+
+FNNAME (INSN)
+{
+  /* Basic test: y=vqrshrn_n(x,v), then store the result.  */
+#define TEST_VQRSHRN_N2(INSN, T1, T2, W, W2, N, V, EXPECTED_CUMULATIVE_SAT, CMT) \
+  Set_Neon_Cumulative_Sat(0, VECT_VAR(vector_res, T1, W2, N));		\
+  VECT_VAR(vector_res, T1, W2, N) =					\
+    INSN##_##T2##W(VECT_VAR(vector, T1, W, N),				\
+		   V);							\
+  vst1_##T2##W2(VECT_VAR(result, T1, W2, N),				\
+		VECT_VAR(vector_res, T1, W2, N));			\
+  CHECK_CUMULATIVE_SAT(TEST_MSG, T1, W, N, EXPECTED_CUMULATIVE_SAT, CMT)
+
+  /* Two auxliary macros are necessary to expand INSN */
+#define TEST_VQRSHRN_N1(INSN, T1, T2, W, W2, N, V, EXPECTED_CUMULATIVE_SAT, CMT) \
+  TEST_VQRSHRN_N2(INSN, T1, T2, W, W2, N, V, EXPECTED_CUMULATIVE_SAT, CMT)
+
+#define TEST_VQRSHRN_N(T1, T2, W, W2, N, V, EXPECTED_CUMULATIVE_SAT, CMT) \
+  TEST_VQRSHRN_N1(INSN, T1, T2, W, W2, N, V, EXPECTED_CUMULATIVE_SAT, CMT)
+
+
+  /* vector is twice as large as vector_res.  */
+  DECL_VARIABLE(vector, int, 16, 8);
+  DECL_VARIABLE(vector, int, 32, 4);
+  DECL_VARIABLE(vector, int, 64, 2);
+  DECL_VARIABLE(vector, uint, 16, 8);
+  DECL_VARIABLE(vector, uint, 32, 4);
+  DECL_VARIABLE(vector, uint, 64, 2);
+
+  DECL_VARIABLE(vector_res, int, 8, 8);
+  DECL_VARIABLE(vector_res, int, 16, 4);
+  DECL_VARIABLE(vector_res, int, 32, 2);
+  DECL_VARIABLE(vector_res, uint, 8, 8);
+  DECL_VARIABLE(vector_res, uint, 16, 4);
+  DECL_VARIABLE(vector_res, uint, 32, 2);
+
+  clean_results ();
+
+  VLOAD(vector, buffer, q, int, s, 16, 8);
+  VLOAD(vector, buffer, q, int, s, 32, 4);
+  VLOAD(vector, buffer, q, int, s, 64, 2);
+  VLOAD(vector, buffer, q, uint, u, 16, 8);
+  VLOAD(vector, buffer, q, uint, u, 32, 4);
+  VLOAD(vector, buffer, q, uint, u, 64, 2);
+
+  /* Choose shift amount arbitrarily.  */
+#define CMT ""
+  TEST_VQRSHRN_N(int, s, 16, 8, 8, 1, expected_cumulative_sat, CMT);
+  TEST_VQRSHRN_N(int, s, 32, 16, 4, 1, expected_cumulative_sat, CMT);
+  TEST_VQRSHRN_N(int, s, 64, 32, 2, 2, expected_cumulative_sat, CMT);
+  TEST_VQRSHRN_N(uint, u, 16, 8, 8, 2, expected_cumulative_sat, CMT);
+  TEST_VQRSHRN_N(uint, u, 32, 16, 4, 3, expected_cumulative_sat, CMT);
+  TEST_VQRSHRN_N(uint, u, 64, 32, 2, 3, expected_cumulative_sat, CMT);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected, CMT);
+
+
+  /* Another set of tests, shifting max value by 3.  */
+  VDUP(vector, q, int, s, 16, 8, 0x7FFF);
+  VDUP(vector, q, int, s, 32, 4, 0x7FFFFFFF);
+  VDUP(vector, q, int, s, 64, 2, 0x7FFFFFFFFFFFFFFFLL);
+  VDUP(vector, q, uint, u, 16, 8, 0xFFFF);
+  VDUP(vector, q, uint, u, 32, 4, 0xFFFFFFFF);
+  VDUP(vector, q, uint, u, 64, 2, 0xFFFFFFFFFFFFFFFFULL);
+
+#undef CMT
+#define CMT " (check saturation: shift by 3)"
+  TEST_VQRSHRN_N(int, s, 16, 8, 8, 3, expected_cumulative_sat_sh3, CMT);
+  TEST_VQRSHRN_N(int, s, 32, 16, 4, 3, expected_cumulative_sat_sh3, CMT);
+  TEST_VQRSHRN_N(int, s, 64, 32, 2, 3, expected_cumulative_sat_sh3, CMT);
+  TEST_VQRSHRN_N(uint, u, 16, 8, 8, 3, expected_cumulative_sat_sh3, CMT);
+  TEST_VQRSHRN_N(uint, u, 32, 16, 4, 3, expected_cumulative_sat_sh3, CMT);
+  TEST_VQRSHRN_N(uint, u, 64, 32, 2, 3, expected_cumulative_sat_sh3, CMT);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_sh3, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_sh3, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_sh3, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_sh3, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_sh3, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_sh3, CMT);
+
+
+  /* Shift by max amount.  */
+#undef CMT
+#define CMT " (check saturation: shift by max)"
+  TEST_VQRSHRN_N(int, s, 16, 8, 8, 8, expected_cumulative_sat_shmax, CMT);
+  TEST_VQRSHRN_N(int, s, 32, 16, 4, 16, expected_cumulative_sat_shmax, CMT);
+  TEST_VQRSHRN_N(int, s, 64, 32, 2, 32, expected_cumulative_sat_shmax, CMT);
+  TEST_VQRSHRN_N(uint, u, 16, 8, 8, 8, expected_cumulative_sat_shmax, CMT);
+  TEST_VQRSHRN_N(uint, u, 32, 16, 4, 16, expected_cumulative_sat_shmax, CMT);
+  TEST_VQRSHRN_N(uint, u, 64, 32, 2, 32, expected_cumulative_sat_shmax, CMT);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_shmax, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_shmax, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_shmax, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_shmax, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_shmax, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_shmax, CMT);
+}
+
+int main (void)
+{
+  exec_vqrshrn_n ();
+  return 0;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vqrshrun_n.c
@@ -0,0 +1,189 @@
+#include <arm_neon.h>
+#include "arm-neon-ref.h"
+#include "compute-ref-data.h"
+
+/* Expected values of cumulative_saturation flag with negative unput.  */
+int VECT_VAR(expected_cumulative_sat_neg,int,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,int,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,int,64,2) = 1;
+
+/* Expected results with negative input.  */
+VECT_VAR_DECL(expected_neg,uint,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					    0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_neg,uint,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_neg,uint,32,2) [] = { 0x0, 0x0 };
+
+/* Expected values of cumulative_saturation flag with max input value
+   shifted by 1.  */
+int VECT_VAR(expected_cumulative_sat_max_sh1,int,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat_max_sh1,int,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat_max_sh1,int,64,2) = 1;
+
+/* Expected results with max input value shifted by 1.  */
+VECT_VAR_DECL(expected_max_sh1,uint,8,8) [] = { 0xff, 0xff, 0xff, 0xff,
+						0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_max_sh1,uint,16,4) [] = { 0xffff, 0xffff,
+						 0xffff, 0xffff };
+VECT_VAR_DECL(expected_max_sh1,uint,32,2) [] = { 0xffffffff, 0xffffffff };
+VECT_VAR_DECL(expected_max_sh1,uint,64,1) [] = { 0x3333333333333333 };
+
+/* Expected values of cumulative_saturation flag with max input value
+   shifted by max amount.  */
+int VECT_VAR(expected_cumulative_sat_max_shmax,int,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_max_shmax,int,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat_max_shmax,int,64,2) = 0;
+
+/* Expected results with max input value shifted by max amount.  */
+VECT_VAR_DECL(expected_max_shmax,uint,8,8) [] = { 0x80, 0x80, 0x80, 0x80,
+						  0x80, 0x80, 0x80, 0x80 };
+VECT_VAR_DECL(expected_max_shmax,uint,16,4) [] = { 0x8000, 0x8000,
+						   0x8000, 0x8000 };
+VECT_VAR_DECL(expected_max_shmax,uint,32,2) [] = { 0x80000000, 0x80000000 };
+
+/* Expected values of cumulative_saturation flag with min input value
+   shifted by max amount.  */
+int VECT_VAR(expected_cumulative_sat_min_shmax,int,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat_min_shmax,int,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat_min_shmax,int,64,2) = 1;
+
+/* Expected results with min input value shifted by max amount.  */
+VECT_VAR_DECL(expected_min_shmax,uint,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+						  0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_min_shmax,uint,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_min_shmax,uint,32,2) [] = { 0x0, 0x0 };
+
+/* Expected values of cumulative_saturation flag with inputs in usual
+   range.  */
+int VECT_VAR(expected_cumulative_sat,int,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat,int,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat,int,64,2) = 0;
+
+/* Expected results with inputs in usual range.  */
+VECT_VAR_DECL(expected,uint,8,8) [] = { 0x49, 0x49, 0x49, 0x49,
+					0x49, 0x49, 0x49, 0x49 };
+VECT_VAR_DECL(expected,uint,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected,uint,32,2) [] = { 0xdeadbf, 0xdeadbf };
+
+#define INSN vqrshrun_n
+#define TEST_MSG "VQRSHRUN_N"
+
+#define FNNAME1(NAME) void exec_ ## NAME (void)
+#define FNNAME(NAME) FNNAME1(NAME)
+
+FNNAME (INSN)
+{
+  /* Basic test: y=vqrshrun_n(x,v), then store the result.  */
+#define TEST_VQRSHRUN_N2(INSN, T1, T2, W, W2, N, V, EXPECTED_CUMULATIVE_SAT, CMT) \
+  Set_Neon_Cumulative_Sat(0, VECT_VAR(vector_res, uint, W2, N));	\
+  VECT_VAR(vector_res, uint, W2, N) =					\
+    INSN##_##T2##W(VECT_VAR(vector, T1, W, N),				\
+		   V);							\
+  vst1_u##W2(VECT_VAR(result, uint, W2, N),				\
+	     VECT_VAR(vector_res, uint, W2, N));			\
+  CHECK_CUMULATIVE_SAT(TEST_MSG, T1, W, N, EXPECTED_CUMULATIVE_SAT, CMT)
+
+  /* Two auxliary macros are necessary to expand INSN */
+#define TEST_VQRSHRUN_N1(INSN, T1, T2, W, W2, N, V, EXPECTED_CUMULATIVE_SAT, CMT) \
+  TEST_VQRSHRUN_N2(INSN, T1, T2, W, W2, N, V, EXPECTED_CUMULATIVE_SAT, CMT)
+
+#define TEST_VQRSHRUN_N(T1, T2, W, W2, N, V, EXPECTED_CUMULATIVE_SAT, CMT) \
+  TEST_VQRSHRUN_N1(INSN, T1, T2, W, W2, N, V, EXPECTED_CUMULATIVE_SAT, CMT)
+
+
+  /* vector is twice as large as vector_res.  */
+  DECL_VARIABLE(vector, int, 16, 8);
+  DECL_VARIABLE(vector, int, 32, 4);
+  DECL_VARIABLE(vector, int, 64, 2);
+
+  DECL_VARIABLE(vector_res, uint, 8, 8);
+  DECL_VARIABLE(vector_res, uint, 16, 4);
+  DECL_VARIABLE(vector_res, uint, 32, 2);
+
+  clean_results ();
+
+  /* Fill input vector with negative values, to check saturation on
+     limits.  */
+  VDUP(vector, q, int, s, 16, 8, -2);
+  VDUP(vector, q, int, s, 32, 4, -3);
+  VDUP(vector, q, int, s, 64, 2, -4);
+
+  /* Choose shift amount arbitrarily.   */
+#define CMT " (negative input)"
+  TEST_VQRSHRUN_N(int, s, 16, 8, 8, 3, expected_cumulative_sat_neg, CMT);
+  TEST_VQRSHRUN_N(int, s, 32, 16, 4, 4, expected_cumulative_sat_neg, CMT);
+  TEST_VQRSHRUN_N(int, s, 64, 32, 2, 2, expected_cumulative_sat_neg, CMT);
+
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_neg, CMT);
+
+
+  /* Fill input vector with max value, to check saturation on
+     limits.  */
+  VDUP(vector, q, int, s, 16, 8, 0x7FFF);
+  VDUP(vector, q, int, s, 32, 4, 0x7FFFFFFF);
+  VDUP(vector, q, int, s, 64, 2, 0x7FFFFFFFFFFFFFFFLL);
+
+  /* shift by 1.  */
+#undef CMT
+#define CMT " (check cumulative saturation: shift by 1)"
+  TEST_VQRSHRUN_N(int, s, 16, 8, 8, 1, expected_cumulative_sat_max_sh1, CMT);
+  TEST_VQRSHRUN_N(int, s, 32, 16, 4, 1, expected_cumulative_sat_max_sh1, CMT);
+  TEST_VQRSHRUN_N(int, s, 64, 32, 2, 1, expected_cumulative_sat_max_sh1, CMT);
+
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_max_sh1, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_max_sh1, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_max_sh1, CMT);
+
+
+  /* shift by max.  */
+#undef CMT
+#define CMT " (check cumulative saturation: shift by max, positive input)"
+  TEST_VQRSHRUN_N(int, s, 16, 8, 8, 8, expected_cumulative_sat_max_shmax, CMT);
+  TEST_VQRSHRUN_N(int, s, 32, 16, 4, 16, expected_cumulative_sat_max_shmax, CMT);
+  TEST_VQRSHRUN_N(int, s, 64, 32, 2, 32, expected_cumulative_sat_max_shmax, CMT);
+
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_max_shmax, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_max_shmax, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_max_shmax, CMT);
+
+
+  /* Fill input vector with min value, to check saturation on limits.  */
+  VDUP(vector, q, int, s, 16, 8, 0x8000);
+  VDUP(vector, q, int, s, 32, 4, 0x80000000);
+  VDUP(vector, q, int, s, 64, 2, 0x8000000000000000LL);
+
+  /* shift by max  */
+#undef CMT
+#define CMT " (check cumulative saturation: shift by max, negative input)"
+  TEST_VQRSHRUN_N(int, s, 16, 8, 8, 8, expected_cumulative_sat_min_shmax, CMT);
+  TEST_VQRSHRUN_N(int, s, 32, 16, 4, 16, expected_cumulative_sat_min_shmax, CMT);
+  TEST_VQRSHRUN_N(int, s, 64, 32, 2, 32, expected_cumulative_sat_min_shmax, CMT);
+
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_min_shmax, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_min_shmax, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_min_shmax, CMT);
+
+
+  /* Fill input vector with positive values, to check normal case.  */
+  VDUP(vector, q, int, s, 16, 8, 0x1234);
+  VDUP(vector, q, int, s, 32, 4, 0x87654321);
+  VDUP(vector, q, int, s, 64, 2, 0xDEADBEEF);
+
+  /* shift arbitrary amount.  */
+#undef CMT
+#define CMT ""
+  TEST_VQRSHRUN_N(int, s, 16, 8, 8, 6, expected_cumulative_sat, CMT);
+  TEST_VQRSHRUN_N(int, s, 32, 16, 4, 7, expected_cumulative_sat, CMT);
+  TEST_VQRSHRUN_N(int, s, 64, 32, 2, 8, expected_cumulative_sat, CMT);
+
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected, CMT);
+}
+
+int main (void)
+{
+  exec_vqrshrun_n ();
+  return 0;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vqshl.c
@@ -0,0 +1,829 @@
+#include <arm_neon.h>
+#include "arm-neon-ref.h"
+#include "compute-ref-data.h"
+
+/* Expected values of cumulative_saturation flag with input=0.  */
+int VECT_VAR(expected_cumulative_sat_0,int,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat_0,int,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat_0,int,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat_0,int,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat_0,uint,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat_0,uint,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat_0,uint,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat_0,uint,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat_0,int,8,16) = 0;
+int VECT_VAR(expected_cumulative_sat_0,int,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_0,int,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat_0,int,64,2) = 0;
+int VECT_VAR(expected_cumulative_sat_0,uint,8,16) = 0;
+int VECT_VAR(expected_cumulative_sat_0,uint,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_0,uint,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat_0,uint,64,2) = 0;
+
+/* Expected results with input=0.  */
+VECT_VAR_DECL(expected_0,int,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,int,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,int,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,int,64,1) [] = { 0x0 };
+VECT_VAR_DECL(expected_0,uint,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					  0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,uint,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,uint,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,uint,64,1) [] = { 0x0 };
+VECT_VAR_DECL(expected_0,int,8,16) [] = { 0x0, 0x0, 0x0, 0x0,
+					  0x0, 0x0, 0x0, 0x0,
+					  0x0, 0x0, 0x0, 0x0,
+					  0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,int,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					  0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,int,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,int,64,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,uint,8,16) [] = { 0x0, 0x0, 0x0, 0x0,
+					   0x0, 0x0, 0x0, 0x0,
+					   0x0, 0x0, 0x0, 0x0,
+					   0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,uint,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					   0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,uint,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,uint,64,2) [] = { 0x0, 0x0 };
+
+/* Expected values of cumulative_saturation flag with input=0 and
+   negative shift amount.  */
+int VECT_VAR(expected_cumulative_sat_0_neg,int,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,int,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,int,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,int,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,uint,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,uint,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,uint,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,uint,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,int,8,16) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,int,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,int,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,int,64,2) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,uint,8,16) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,uint,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,uint,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat_0_neg,uint,64,2) = 0;
+
+/* Expected results with input=0 and negative shift amount.  */
+VECT_VAR_DECL(expected_0_neg,int,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					     0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_neg,int,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_neg,int,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_neg,int,64,1) [] = { 0x0 };
+VECT_VAR_DECL(expected_0_neg,uint,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					      0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_neg,uint,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_neg,uint,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_neg,uint,64,1) [] = { 0x0 };
+VECT_VAR_DECL(expected_0_neg,int,8,16) [] = { 0x0, 0x0, 0x0, 0x0,
+					      0x0, 0x0, 0x0, 0x0,
+					      0x0, 0x0, 0x0, 0x0,
+					      0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_neg,int,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					      0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_neg,int,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_neg,int,64,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_neg,uint,8,16) [] = { 0x0, 0x0, 0x0, 0x0,
+					       0x0, 0x0, 0x0, 0x0,
+					       0x0, 0x0, 0x0, 0x0,
+					       0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_neg,uint,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					       0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_neg,uint,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_neg,uint,64,2) [] = { 0x0, 0x0 };
+
+/* Expected values of cumulative_saturation flag.  */
+int VECT_VAR(expected_cumulative_sat,int,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat,int,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat,int,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat,int,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat,uint,8,8) = 1;
+int VECT_VAR(expected_cumulative_sat,uint,16,4) = 1;
+int VECT_VAR(expected_cumulative_sat,uint,32,2) = 1;
+int VECT_VAR(expected_cumulative_sat,uint,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat,int,8,16) = 1;
+int VECT_VAR(expected_cumulative_sat,int,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat,int,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat,int,64,2) = 1;
+int VECT_VAR(expected_cumulative_sat,uint,8,16) = 1;
+int VECT_VAR(expected_cumulative_sat,uint,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat,uint,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat,uint,64,2) = 1;
+
+/* Expected results.  */
+VECT_VAR_DECL(expected,int,8,8) [] = { 0xe0, 0xe2, 0xe4, 0xe6,
+				       0xe8, 0xea, 0xec, 0xee };
+VECT_VAR_DECL(expected,int,16,4) [] = { 0xff80, 0xff88, 0xff90, 0xff98 };
+VECT_VAR_DECL(expected,int,32,2) [] = { 0xfffff000, 0xfffff100 };
+VECT_VAR_DECL(expected,int,64,1) [] = { 0xfffffffffffffffe };
+VECT_VAR_DECL(expected,uint,8,8) [] = { 0xff, 0xff, 0xff, 0xff,
+					0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected,uint,16,4) [] = { 0xffff, 0xffff, 0xffff, 0xffff };
+VECT_VAR_DECL(expected,uint,32,2) [] = { 0xffffffff, 0xffffffff };
+VECT_VAR_DECL(expected,uint,64,1) [] = { 0x1ffffffffffffffe };
+VECT_VAR_DECL(expected,int,8,16) [] = { 0x80, 0x80, 0x80, 0x80,
+					0x80, 0x80, 0x80, 0x80,
+					0x80, 0x80, 0x80, 0x80,
+					0x80, 0x80, 0x80, 0x80 };
+VECT_VAR_DECL(expected,int,16,8) [] = { 0x8000, 0x8000, 0x8000, 0x8000,
+					0x8000, 0x8000, 0x8000, 0x8000 };
+VECT_VAR_DECL(expected,int,32,4) [] = { 0x80000000, 0x80000000,
+					0x80000000, 0x80000000 };
+VECT_VAR_DECL(expected,int,64,2) [] = { 0x8000000000000000,
+					0x8000000000000000 };
+VECT_VAR_DECL(expected,uint,8,16) [] = { 0xff, 0xff, 0xff, 0xff,
+					 0xff, 0xff, 0xff, 0xff,
+					 0xff, 0xff, 0xff, 0xff,
+					 0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected,uint,16,8) [] = { 0xffff, 0xffff, 0xffff, 0xffff,
+					 0xffff, 0xffff, 0xffff, 0xffff };
+VECT_VAR_DECL(expected,uint,32,4) [] = { 0xffffffff, 0xffffffff,
+					 0xffffffff, 0xffffffff };
+VECT_VAR_DECL(expected,uint,64,2) [] = { 0xffffffffffffffff,
+					 0xffffffffffffffff };
+
+/* Expected values of cumulative_sat_saturation flag with negative shift
+   amount.  */
+int VECT_VAR(expected_cumulative_sat_neg,int,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,int,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,int,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,int,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,uint,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,uint,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,uint,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,uint,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,int,8,16) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,int,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,int,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,int,64,2) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,uint,8,16) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,uint,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,uint,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat_neg,uint,64,2) = 0;
+
+/* Expected results with negative shift amount.  */
+VECT_VAR_DECL(expected_neg,int,8,8) [] = { 0xf8, 0xf8, 0xf9, 0xf9,
+					   0xfa, 0xfa, 0xfb, 0xfb };
+VECT_VAR_DECL(expected_neg,int,16,4) [] = { 0xfffc, 0xfffc, 0xfffc, 0xfffc };
+VECT_VAR_DECL(expected_neg,int,32,2) [] = { 0xfffffffe, 0xfffffffe };
+VECT_VAR_DECL(expected_neg,int,64,1) [] = { 0xffffffffffffffff };
+VECT_VAR_DECL(expected_neg,uint,8,8) [] = { 0x78, 0x78, 0x79, 0x79,
+					    0x7a, 0x7a, 0x7b, 0x7b };
+VECT_VAR_DECL(expected_neg,uint,16,4) [] = { 0x3ffc, 0x3ffc, 0x3ffc, 0x3ffc };
+VECT_VAR_DECL(expected_neg,uint,32,2) [] = { 0x1ffffffe, 0x1ffffffe };
+VECT_VAR_DECL(expected_neg,uint,64,1) [] = { 0xfffffffffffffff };
+VECT_VAR_DECL(expected_neg,int,8,16) [] = { 0xff, 0xff, 0xff, 0xff,
+					    0xff, 0xff, 0xff, 0xff,
+					    0xff, 0xff, 0xff, 0xff,
+					    0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_neg,int,16,8) [] = { 0xffff, 0xffff, 0xffff, 0xffff,
+					    0xffff, 0xffff, 0xffff, 0xffff };
+VECT_VAR_DECL(expected_neg,int,32,4) [] = { 0xffffffff, 0xffffffff,
+					    0xffffffff, 0xffffffff };
+VECT_VAR_DECL(expected_neg,int,64,2) [] = { 0xffffffffffffffff,
+					    0xffffffffffffffff };
+VECT_VAR_DECL(expected_neg,uint,8,16) [] = { 0x1, 0x1, 0x1, 0x1,
+					     0x1, 0x1, 0x1, 0x1,
+					     0x1, 0x1, 0x1, 0x1,
+					     0x1, 0x1, 0x1, 0x1 };
+VECT_VAR_DECL(expected_neg,uint,16,8) [] = { 0x1f, 0x1f, 0x1f, 0x1f,
+					     0x1f, 0x1f, 0x1f, 0x1f };
+VECT_VAR_DECL(expected_neg,uint,32,4) [] = { 0x7ffff, 0x7ffff,
+					     0x7ffff, 0x7ffff };
+VECT_VAR_DECL(expected_neg,uint,64,2) [] = { 0xfffffffffff, 0xfffffffffff };
+
+/* Expected values of cumulative_sat_saturation flag with negative
+   input and large shift amount.  */
+int VECT_VAR(expected_cumulative_sat_neg_large,int,8,8) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large,int,16,4) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large,int,32,2) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large,int,64,1) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large,uint,8,8) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large,uint,16,4) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large,uint,32,2) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large,uint,64,1) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large,int,8,16) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large,int,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large,int,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large,int,64,2) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large,uint,8,16) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large,uint,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large,uint,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat_neg_large,uint,64,2) = 1;
+
+/* Expected results with negative input and large shift amount.  */
+VECT_VAR_DECL(expected_neg_large,int,8,8) [] = { 0x80, 0x80, 0x80, 0x80,
+						 0x80, 0x80, 0x80, 0x80 };
+VECT_VAR_DECL(expected_neg_large,int,16,4) [] = { 0x8000, 0x8000,
+						  0x8000, 0x8000 };
+VECT_VAR_DECL(expected_neg_large,int,32,2) [] = { 0x80000000, 0x80000000 };
+VECT_VAR_DECL(expected_neg_large,int,64,1) [] = { 0x8000000000000000 };
+VECT_VAR_DECL(expected_neg_large,uint,8,8) [] = { 0xff, 0xff, 0xff, 0xff,
+						  0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_neg_large,uint,16,4) [] = { 0xffff, 0xffff,
+						   0xffff, 0xffff };
+VECT_VAR_DECL(expected_neg_large,uint,32,2) [] = { 0xffffffff, 0xffffffff };
+VECT_VAR_DECL(expected_neg_large,uint,64,1) [] = { 0xffffffffffffffff };
+VECT_VAR_DECL(expected_neg_large,int,8,16) [] = { 0x80, 0x80, 0x80, 0x80,
+						  0x80, 0x80, 0x80, 0x80,
+						  0x80, 0x80, 0x80, 0x80,
+						  0x80, 0x80, 0x80, 0x80 };
+VECT_VAR_DECL(expected_neg_large,int,16,8) [] = { 0x8000, 0x8000,
+						  0x8000, 0x8000,
+						  0x8000, 0x8000,
+						  0x8000, 0x8000 };
+VECT_VAR_DECL(expected_neg_large,int,32,4) [] = { 0x80000000, 0x80000000,
+						  0x80000000, 0x80000000 };
+VECT_VAR_DECL(expected_neg_large,int,64,2) [] = { 0x8000000000000000,
+						  0x8000000000000000 };
+VECT_VAR_DECL(expected_neg_large,uint,8,16) [] = { 0xff, 0xff, 0xff, 0xff,
+						   0xff, 0xff, 0xff, 0xff,
+						   0xff, 0xff, 0xff, 0xff,
+						   0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_neg_large,uint,16,8) [] = { 0xffff, 0xffff,
+						   0xffff, 0xffff,
+						   0xffff, 0xffff,
+						   0xffff, 0xffff };
+VECT_VAR_DECL(expected_neg_large,uint,32,4) [] = { 0xffffffff, 0xffffffff,
+						   0xffffffff, 0xffffffff };
+VECT_VAR_DECL(expected_neg_large,uint,64,2) [] = { 0xffffffffffffffff,
+						   0xffffffffffffffff };
+
+/* Expected values of cumulative_sat_saturation flag with max input
+   and shift by -1.  */
+int VECT_VAR(expected_cumulative_sat_max_minus1,int,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat_max_minus1,int,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat_max_minus1,int,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat_max_minus1,int,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat_max_minus1,uint,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat_max_minus1,uint,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat_max_minus1,uint,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat_max_minus1,uint,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat_max_minus1,int,8,16) = 0;
+int VECT_VAR(expected_cumulative_sat_max_minus1,int,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_max_minus1,int,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat_max_minus1,int,64,2) = 0;
+int VECT_VAR(expected_cumulative_sat_max_minus1,uint,8,16) = 0;
+int VECT_VAR(expected_cumulative_sat_max_minus1,uint,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_max_minus1,uint,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat_max_minus1,uint,64,2) = 0;
+
+/* Expected results with max input and shift by -1.  */
+VECT_VAR_DECL(expected_max_minus1,int,8,8) [] = { 0x3f, 0x3f, 0x3f, 0x3f,
+						  0x3f, 0x3f, 0x3f, 0x3f };
+VECT_VAR_DECL(expected_max_minus1,int,16,4) [] = { 0x3fff, 0x3fff,
+						   0x3fff, 0x3fff };
+VECT_VAR_DECL(expected_max_minus1,int,32,2) [] = { 0x3fffffff, 0x3fffffff };
+VECT_VAR_DECL(expected_max_minus1,int,64,1) [] = { 0x3fffffffffffffff };
+VECT_VAR_DECL(expected_max_minus1,uint,8,8) [] = { 0x7f, 0x7f, 0x7f, 0x7f,
+						   0x7f, 0x7f, 0x7f, 0x7f };
+VECT_VAR_DECL(expected_max_minus1,uint,16,4) [] = { 0x7fff, 0x7fff,
+						    0x7fff, 0x7fff };
+VECT_VAR_DECL(expected_max_minus1,uint,32,2) [] = { 0x7fffffff, 0x7fffffff };
+VECT_VAR_DECL(expected_max_minus1,uint,64,1) [] = { 0x7fffffffffffffff };
+VECT_VAR_DECL(expected_max_minus1,int,8,16) [] = { 0x3f, 0x3f, 0x3f, 0x3f,
+						   0x3f, 0x3f, 0x3f, 0x3f,
+						   0x3f, 0x3f, 0x3f, 0x3f,
+						   0x3f, 0x3f, 0x3f, 0x3f };
+VECT_VAR_DECL(expected_max_minus1,int,16,8) [] = { 0x3fff, 0x3fff,
+						   0x3fff, 0x3fff,
+						   0x3fff, 0x3fff,
+						   0x3fff, 0x3fff };
+VECT_VAR_DECL(expected_max_minus1,int,32,4) [] = { 0x3fffffff, 0x3fffffff,
+						   0x3fffffff, 0x3fffffff };
+VECT_VAR_DECL(expected_max_minus1,int,64,2) [] = { 0x3fffffffffffffff,
+						   0x3fffffffffffffff };
+VECT_VAR_DECL(expected_max_minus1,uint,8,16) [] = { 0x7f, 0x7f, 0x7f, 0x7f,
+						    0x7f, 0x7f, 0x7f, 0x7f,
+						    0x7f, 0x7f, 0x7f, 0x7f,
+						    0x7f, 0x7f, 0x7f, 0x7f };
+VECT_VAR_DECL(expected_max_minus1,uint,16,8) [] = { 0x7fff, 0x7fff,
+						    0x7fff, 0x7fff,
+						    0x7fff, 0x7fff,
+						    0x7fff, 0x7fff };
+VECT_VAR_DECL(expected_max_minus1,uint,32,4) [] = { 0x7fffffff, 0x7fffffff,
+						    0x7fffffff, 0x7fffffff };
+VECT_VAR_DECL(expected_max_minus1,uint,64,2) [] = { 0x7fffffffffffffff,
+						    0x7fffffffffffffff };
+
+/* Expected values of cumulative_sat_saturation flag with max input
+   and large shift amount.  */
+int VECT_VAR(expected_cumulative_sat_max_large,int,8,8) = 1;
+int VECT_VAR(expected_cumulative_sat_max_large,int,16,4) = 1;
+int VECT_VAR(expected_cumulative_sat_max_large,int,32,2) = 1;
+int VECT_VAR(expected_cumulative_sat_max_large,int,64,1) = 1;
+int VECT_VAR(expected_cumulative_sat_max_large,uint,8,8) = 1;
+int VECT_VAR(expected_cumulative_sat_max_large,uint,16,4) = 1;
+int VECT_VAR(expected_cumulative_sat_max_large,uint,32,2) = 1;
+int VECT_VAR(expected_cumulative_sat_max_large,uint,64,1) = 1;
+int VECT_VAR(expected_cumulative_sat_max_large,int,8,16) = 1;
+int VECT_VAR(expected_cumulative_sat_max_large,int,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat_max_large,int,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat_max_large,int,64,2) = 1;
+int VECT_VAR(expected_cumulative_sat_max_large,uint,8,16) = 1;
+int VECT_VAR(expected_cumulative_sat_max_large,uint,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat_max_large,uint,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat_max_large,uint,64,2) = 1;
+
+/* Expected results with max input and large shift amount.  */
+VECT_VAR_DECL(expected_max_large,int,8,8) [] = { 0x7f, 0x7f, 0x7f, 0x7f,
+					       0x7f, 0x7f, 0x7f, 0x7f };
+VECT_VAR_DECL(expected_max_large,int,16,4) [] = { 0x7fff, 0x7fff,
+						0x7fff, 0x7fff };
+VECT_VAR_DECL(expected_max_large,int,32,2) [] = { 0x7fffffff, 0x7fffffff };
+VECT_VAR_DECL(expected_max_large,int,64,1) [] = { 0x7fffffffffffffff };
+VECT_VAR_DECL(expected_max_large,uint,8,8) [] = { 0xff, 0xff, 0xff, 0xff,
+						0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_max_large,uint,16,4) [] = { 0xffff, 0xffff,
+						 0xffff, 0xffff };
+VECT_VAR_DECL(expected_max_large,uint,32,2) [] = { 0xffffffff, 0xffffffff };
+VECT_VAR_DECL(expected_max_large,uint,64,1) [] = { 0xffffffffffffffff };
+VECT_VAR_DECL(expected_max_large,int,8,16) [] = { 0x7f, 0x7f, 0x7f, 0x7f,
+						0x7f, 0x7f, 0x7f, 0x7f,
+						0x7f, 0x7f, 0x7f, 0x7f,
+						0x7f, 0x7f, 0x7f, 0x7f };
+VECT_VAR_DECL(expected_max_large,int,16,8) [] = { 0x7fff, 0x7fff,
+						0x7fff, 0x7fff,
+						0x7fff, 0x7fff,
+						0x7fff, 0x7fff };
+VECT_VAR_DECL(expected_max_large,int,32,4) [] = { 0x7fffffff, 0x7fffffff,
+						0x7fffffff, 0x7fffffff };
+VECT_VAR_DECL(expected_max_large,int,64,2) [] = { 0x7fffffffffffffff,
+						0x7fffffffffffffff };
+VECT_VAR_DECL(expected_max_large,uint,8,16) [] = { 0xff, 0xff, 0xff, 0xff,
+						 0xff, 0xff, 0xff, 0xff,
+						 0xff, 0xff, 0xff, 0xff,
+						 0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_max_large,uint,16,8) [] = { 0xffff, 0xffff,
+						 0xffff, 0xffff,
+						 0xffff, 0xffff,
+						 0xffff, 0xffff };
+VECT_VAR_DECL(expected_max_large,uint,32,4) [] = { 0xffffffff, 0xffffffff,
+						 0xffffffff, 0xffffffff };
+VECT_VAR_DECL(expected_max_large,uint,64,2) [] = { 0xffffffffffffffff,
+						 0xffffffffffffffff };
+
+/* Expected values of cumulative_sat_saturation flag with saturation
+   on 64-bits values.  */
+int VECT_VAR(expected_cumulative_sat_64,int,64,1) = 1;
+int VECT_VAR(expected_cumulative_sat_64,int,64,2) = 1;
+
+/* Expected results with saturation on 64-bits values..  */
+VECT_VAR_DECL(expected_64,int,64,1) [] = { 0x8000000000000000 };
+VECT_VAR_DECL(expected_64,int,64,2) [] = { 0x7fffffffffffffff,
+					   0x7fffffffffffffff };
+
+#define INSN vqshl
+#define TEST_MSG "VQSHL/VQSHLQ"
+
+#define FNNAME1(NAME) void exec_ ## NAME (void)
+#define FNNAME(NAME) FNNAME1(NAME)
+
+FNNAME (INSN)
+{
+  /* Basic test: v3=vqshl(v1,v2), then store the result.  */
+#define TEST_VQSHL2(INSN, T3, Q, T1, T2, W, N, EXPECTED_CUMULATIVE_SAT, CMT) \
+  Set_Neon_Cumulative_Sat(0, VECT_VAR(vector_res, T1, W, N));		\
+  VECT_VAR(vector_res, T1, W, N) =					\
+    INSN##Q##_##T2##W(VECT_VAR(vector, T1, W, N),			\
+		      VECT_VAR(vector_shift, T3, W, N));		\
+  vst1##Q##_##T2##W(VECT_VAR(result, T1, W, N),				\
+		    VECT_VAR(vector_res, T1, W, N));			\
+  CHECK_CUMULATIVE_SAT(TEST_MSG, T1, W, N, EXPECTED_CUMULATIVE_SAT, CMT)
+
+  /* Two auxliary macros are necessary to expand INSN */
+#define TEST_VQSHL1(INSN, T3, Q, T1, T2, W, N, EXPECTED_CUMULATIVE_SAT, CMT) \
+  TEST_VQSHL2(INSN, T3, Q, T1, T2, W, N, EXPECTED_CUMULATIVE_SAT, CMT)
+
+#define TEST_VQSHL(T3, Q, T1, T2, W, N, EXPECTED_CUMULATIVE_SAT, CMT)	\
+  TEST_VQSHL1(INSN, T3, Q, T1, T2, W, N, EXPECTED_CUMULATIVE_SAT, CMT)
+
+
+  DECL_VARIABLE_ALL_VARIANTS(vector);
+  DECL_VARIABLE_ALL_VARIANTS(vector_res);
+
+  DECL_VARIABLE_SIGNED_VARIANTS(vector_shift);
+
+  clean_results ();
+
+  /* Fill input vector with 0, to check saturation on limits.  */
+  VDUP(vector, , int, s, 8, 8, 0);
+  VDUP(vector, , int, s, 16, 4, 0);
+  VDUP(vector, , int, s, 32, 2, 0);
+  VDUP(vector, , int, s, 64, 1, 0);
+  VDUP(vector, , uint, u, 8, 8, 0);
+  VDUP(vector, , uint, u, 16, 4, 0);
+  VDUP(vector, , uint, u, 32, 2, 0);
+  VDUP(vector, , uint, u, 64, 1, 0);
+  VDUP(vector, q, int, s, 8, 16, 0);
+  VDUP(vector, q, int, s, 16, 8, 0);
+  VDUP(vector, q, int, s, 32, 4, 0);
+  VDUP(vector, q, int, s, 64, 2, 0);
+  VDUP(vector, q, uint, u, 8, 16, 0);
+  VDUP(vector, q, uint, u, 16, 8, 0);
+  VDUP(vector, q, uint, u, 32, 4, 0);
+  VDUP(vector, q, uint, u, 64, 2, 0);
+
+  /* Choose init value arbitrarily, will be used as shift amount */
+  /* Use values equal or one-less-than the type width to check
+     behaviour on limits.  */
+
+  /* 64-bits vectors first.  */
+  /* Shift 8-bits lanes by 7...  */
+  VDUP(vector_shift, , int, s, 8, 8, 7);
+  /* ... except: lane 0 (by 6), lane 1 (by 8) and lane 2 (by 9).  */
+  VSET_LANE(vector_shift, , int, s, 8, 8, 0, 6);
+  VSET_LANE(vector_shift, , int, s, 8, 8, 1, 8);
+  VSET_LANE(vector_shift, , int, s, 8, 8, 2, 9);
+
+  /* Shift 16-bits lanes by 15... */
+  VDUP(vector_shift, , int, s, 16, 4, 15);
+  /* ... except: lane 0 (by 14), lane 1 (by 16), and lane 2 (by 17).  */
+  VSET_LANE(vector_shift, , int, s, 16, 4, 0, 14);
+  VSET_LANE(vector_shift, , int, s, 16, 4, 1, 16);
+  VSET_LANE(vector_shift, , int, s, 16, 4, 2, 17);
+
+  /* Shift 32-bits lanes by 31... */
+  VDUP(vector_shift, , int, s, 32, 2, 31);
+  /* ... except lane 1 (by 30).  */
+  VSET_LANE(vector_shift, , int, s, 32, 2, 1, 30);
+
+  /* Shift 64 bits lane by 63.  */
+  VDUP(vector_shift, , int, s, 64, 1, 63);
+
+  /* 128-bits vectors.  */
+  /* Shift 8-bits lanes by 8.  */
+  VDUP(vector_shift, q, int, s, 8, 16, 8);
+  /* Shift 16-bits lanes by 16.  */
+  VDUP(vector_shift, q, int, s, 16, 8, 16);
+  /* Shift 32-bits lanes by 32...  */
+  VDUP(vector_shift, q, int, s, 32, 4, 32);
+  /* ... except lane 1 (by 33).  */
+  VSET_LANE(vector_shift, q, int, s, 32, 4, 1, 33);
+
+  /* Shift 64-bits lanes by 64... */
+  VDUP(vector_shift, q, int, s, 64, 2, 64);
+  /* ... except lane 1 (by 62).  */
+  VSET_LANE(vector_shift, q, int, s, 64, 2, 1, 62);
+
+#define CMT " (with input = 0)"
+  TEST_VQSHL(int, , int, s, 8, 8, expected_cumulative_sat_0, CMT);
+  TEST_VQSHL(int, , int, s, 16, 4, expected_cumulative_sat_0, CMT);
+  TEST_VQSHL(int, , int, s, 32, 2, expected_cumulative_sat_0, CMT);
+  TEST_VQSHL(int, , int, s, 64, 1, expected_cumulative_sat_0, CMT);
+  TEST_VQSHL(int, , uint, u, 8, 8, expected_cumulative_sat_0, CMT);
+  TEST_VQSHL(int, , uint, u, 16, 4, expected_cumulative_sat_0, CMT);
+  TEST_VQSHL(int, , uint, u, 32, 2, expected_cumulative_sat_0, CMT);
+  TEST_VQSHL(int, , uint, u, 64, 1, expected_cumulative_sat_0, CMT);
+  TEST_VQSHL(int, q, int, s, 8, 16, expected_cumulative_sat_0, CMT);
+  TEST_VQSHL(int, q, int, s, 16, 8, expected_cumulative_sat_0, CMT);
+  TEST_VQSHL(int, q, int, s, 32, 4, expected_cumulative_sat_0, CMT);
+  TEST_VQSHL(int, q, int, s, 64, 2, expected_cumulative_sat_0, CMT);
+  TEST_VQSHL(int, q, uint, u, 8, 16, expected_cumulative_sat_0, CMT);
+  TEST_VQSHL(int, q, uint, u, 16, 8, expected_cumulative_sat_0, CMT);
+  TEST_VQSHL(int, q, uint, u, 32, 4, expected_cumulative_sat_0, CMT);
+  TEST_VQSHL(int, q, uint, u, 64, 2, expected_cumulative_sat_0, CMT);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_0, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_0, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_0, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_0, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_0, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_0, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_0, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_0, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_0, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_0, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_0, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_0, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_0, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_0, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_0, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_0, CMT);
+
+
+  /* Use negative shift amounts */
+  VDUP(vector_shift, , int, s, 8, 8, -1);
+  VDUP(vector_shift, , int, s, 16, 4, -2);
+  VDUP(vector_shift, , int, s, 32, 2, -3);
+  VDUP(vector_shift, , int, s, 64, 1, -4);
+  VDUP(vector_shift, q, int, s, 8, 16, -7);
+  VDUP(vector_shift, q, int, s, 16, 8, -11);
+  VDUP(vector_shift, q, int, s, 32, 4, -13);
+  VDUP(vector_shift, q, int, s, 64, 2, -20);
+
+#undef CMT
+#define CMT " (input 0 and negative shift amount)"
+  TEST_VQSHL(int, , int, s, 8, 8, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQSHL(int, , int, s, 16, 4, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQSHL(int, , int, s, 32, 2, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQSHL(int, , int, s, 64, 1, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQSHL(int, , uint, u, 8, 8, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQSHL(int, , uint, u, 16, 4, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQSHL(int, , uint, u, 32, 2, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQSHL(int, , uint, u, 64, 1, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQSHL(int, q, int, s, 8, 16, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQSHL(int, q, int, s, 16, 8, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQSHL(int, q, int, s, 32, 4, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQSHL(int, q, int, s, 64, 2, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQSHL(int, q, uint, u, 8, 16, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQSHL(int, q, uint, u, 16, 8, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQSHL(int, q, uint, u, 32, 4, expected_cumulative_sat_0_neg, CMT);
+  TEST_VQSHL(int, q, uint, u, 64, 2, expected_cumulative_sat_0_neg, CMT);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_0_neg, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_0_neg, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_0_neg, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_0_neg, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_0_neg, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_0_neg, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_0_neg, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_0_neg, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_0_neg, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_0_neg, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_0_neg, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_0_neg, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_0_neg, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_0_neg, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_0_neg, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_0_neg, CMT);
+
+  /* Test again, with predefined input values.  */
+  TEST_MACRO_ALL_VARIANTS_2_5(VLOAD, vector, buffer);
+
+  /* Choose init value arbitrarily, will be used as shift amount.  */
+  VDUP(vector_shift, , int, s, 8, 8, 1);
+  VDUP(vector_shift, , int, s, 16, 4, 3);
+  VDUP(vector_shift, , int, s, 32, 2, 8);
+  VDUP(vector_shift, , int, s, 64, 1, -3);
+  VDUP(vector_shift, q, int, s, 8, 16, 10);
+  VDUP(vector_shift, q, int, s, 16, 8, 12);
+  VDUP(vector_shift, q, int, s, 32, 4, 32);
+  VDUP(vector_shift, q, int, s, 64, 2, 63);
+
+#undef CMT
+#define CMT ""
+  TEST_VQSHL(int, , int, s, 8, 8, expected_cumulative_sat, CMT);
+  TEST_VQSHL(int, , int, s, 16, 4, expected_cumulative_sat, CMT);
+  TEST_VQSHL(int, , int, s, 32, 2, expected_cumulative_sat, CMT);
+  TEST_VQSHL(int, , int, s, 64, 1, expected_cumulative_sat, CMT);
+  TEST_VQSHL(int, , uint, u, 8, 8, expected_cumulative_sat, CMT);
+  TEST_VQSHL(int, , uint, u, 16, 4, expected_cumulative_sat, CMT);
+  TEST_VQSHL(int, , uint, u, 32, 2, expected_cumulative_sat, CMT);
+  TEST_VQSHL(int, , uint, u, 64, 1, expected_cumulative_sat, CMT);
+  TEST_VQSHL(int, q, int, s, 8, 16, expected_cumulative_sat, CMT);
+  TEST_VQSHL(int, q, int, s, 16, 8, expected_cumulative_sat, CMT);
+  TEST_VQSHL(int, q, int, s, 32, 4, expected_cumulative_sat, CMT);
+  TEST_VQSHL(int, q, int, s, 64, 2, expected_cumulative_sat, CMT);
+  TEST_VQSHL(int, q, uint, u, 8, 16, expected_cumulative_sat, CMT);
+  TEST_VQSHL(int, q, uint, u, 16, 8, expected_cumulative_sat, CMT);
+  TEST_VQSHL(int, q, uint, u, 32, 4, expected_cumulative_sat, CMT);
+  TEST_VQSHL(int, q, uint, u, 64, 2, expected_cumulative_sat, CMT);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected, CMT);
+
+
+  /* Use negative shift amounts */
+  VDUP(vector_shift, , int, s, 8, 8, -1);
+  VDUP(vector_shift, , int, s, 16, 4, -2);
+  VDUP(vector_shift, , int, s, 32, 2, -3);
+  VDUP(vector_shift, , int, s, 64, 1, -4);
+  VDUP(vector_shift, q, int, s, 8, 16, -7);
+  VDUP(vector_shift, q, int, s, 16, 8, -11);
+  VDUP(vector_shift, q, int, s, 32, 4, -13);
+  VDUP(vector_shift, q, int, s, 64, 2, -20);
+
+#undef CMT
+#define CMT " (negative shift amount)"
+  TEST_VQSHL(int, , int, s, 8, 8, expected_cumulative_sat_neg, CMT);
+  TEST_VQSHL(int, , int, s, 16, 4, expected_cumulative_sat_neg, CMT);
+  TEST_VQSHL(int, , int, s, 32, 2, expected_cumulative_sat_neg, CMT);
+  TEST_VQSHL(int, , int, s, 64, 1, expected_cumulative_sat_neg, CMT);
+  TEST_VQSHL(int, , uint, u, 8, 8, expected_cumulative_sat_neg, CMT);
+  TEST_VQSHL(int, , uint, u, 16, 4, expected_cumulative_sat_neg, CMT);
+  TEST_VQSHL(int, , uint, u, 32, 2, expected_cumulative_sat_neg, CMT);
+  TEST_VQSHL(int, , uint, u, 64, 1, expected_cumulative_sat_neg, CMT);
+  TEST_VQSHL(int, q, int, s, 8, 16, expected_cumulative_sat_neg, CMT);
+  TEST_VQSHL(int, q, int, s, 16, 8, expected_cumulative_sat_neg, CMT);
+  TEST_VQSHL(int, q, int, s, 32, 4, expected_cumulative_sat_neg, CMT);
+  TEST_VQSHL(int, q, int, s, 64, 2, expected_cumulative_sat_neg, CMT);
+  TEST_VQSHL(int, q, uint, u, 8, 16, expected_cumulative_sat_neg, CMT);
+  TEST_VQSHL(int, q, uint, u, 16, 8, expected_cumulative_sat_neg, CMT);
+  TEST_VQSHL(int, q, uint, u, 32, 4, expected_cumulative_sat_neg, CMT);
+  TEST_VQSHL(int, q, uint, u, 64, 2, expected_cumulative_sat_neg, CMT);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_neg, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_neg, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_neg, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_neg, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_neg, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_neg, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_neg, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_neg, CMT);
+
+
+  /* Use large shift amounts.  */
+  VDUP(vector_shift, , int, s, 8, 8, 8);
+  VDUP(vector_shift, , int, s, 16, 4, 16);
+  VDUP(vector_shift, , int, s, 32, 2, 32);
+  VDUP(vector_shift, , int, s, 64, 1, 64);
+  VDUP(vector_shift, q, int, s, 8, 16, 8);
+  VDUP(vector_shift, q, int, s, 16, 8, 16);
+  VDUP(vector_shift, q, int, s, 32, 4, 32);
+  VDUP(vector_shift, q, int, s, 64, 2, 64);
+
+#undef CMT
+#define CMT " (large shift amount, negative input)"
+  TEST_VQSHL(int, , int, s, 8, 8, expected_cumulative_sat_neg_large, CMT);
+  TEST_VQSHL(int, , int, s, 16, 4, expected_cumulative_sat_neg_large, CMT);
+  TEST_VQSHL(int, , int, s, 32, 2, expected_cumulative_sat_neg_large, CMT);
+  TEST_VQSHL(int, , int, s, 64, 1, expected_cumulative_sat_neg_large, CMT);
+  TEST_VQSHL(int, , uint, u, 8, 8, expected_cumulative_sat_neg_large, CMT);
+  TEST_VQSHL(int, , uint, u, 16, 4, expected_cumulative_sat_neg_large, CMT);
+  TEST_VQSHL(int, , uint, u, 32, 2, expected_cumulative_sat_neg_large, CMT);
+  TEST_VQSHL(int, , uint, u, 64, 1, expected_cumulative_sat_neg_large, CMT);
+  TEST_VQSHL(int, q, int, s, 8, 16, expected_cumulative_sat_neg_large, CMT);
+  TEST_VQSHL(int, q, int, s, 16, 8, expected_cumulative_sat_neg_large, CMT);
+  TEST_VQSHL(int, q, int, s, 32, 4, expected_cumulative_sat_neg_large, CMT);
+  TEST_VQSHL(int, q, int, s, 64, 2, expected_cumulative_sat_neg_large, CMT);
+  TEST_VQSHL(int, q, uint, u, 8, 16, expected_cumulative_sat_neg_large, CMT);
+  TEST_VQSHL(int, q, uint, u, 16, 8, expected_cumulative_sat_neg_large, CMT);
+  TEST_VQSHL(int, q, uint, u, 32, 4, expected_cumulative_sat_neg_large, CMT);
+  TEST_VQSHL(int, q, uint, u, 64, 2, expected_cumulative_sat_neg_large, CMT);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_neg_large, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_neg_large, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_neg_large, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_neg_large, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_neg_large, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_neg_large, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_neg_large, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_neg_large, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_neg_large, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_neg_large, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_neg_large, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_neg_large, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_neg_large, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_neg_large, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_neg_large, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_neg_large, CMT);
+
+
+  /* Fill input vector with max value, to check saturation on limits */
+  VDUP(vector, , int, s, 8, 8, 0x7F);
+  VDUP(vector, , int, s, 16, 4, 0x7FFF);
+  VDUP(vector, , int, s, 32, 2, 0x7FFFFFFF);
+  VDUP(vector, , int, s, 64, 1, 0x7FFFFFFFFFFFFFFFLL);
+  VDUP(vector, , uint, u, 8, 8, 0xFF);
+  VDUP(vector, , uint, u, 16, 4, 0xFFFF);
+  VDUP(vector, , uint, u, 32, 2, 0xFFFFFFFF);
+  VDUP(vector, , uint, u, 64, 1, 0xFFFFFFFFFFFFFFFFULL);
+  VDUP(vector, q, int, s, 8, 16, 0x7F);
+  VDUP(vector, q, int, s, 16, 8, 0x7FFF);
+  VDUP(vector, q, int, s, 32, 4, 0x7FFFFFFF);
+  VDUP(vector, q, int, s, 64, 2, 0x7FFFFFFFFFFFFFFFLL);
+  VDUP(vector, q, uint, u, 8, 16, 0xFF);
+  VDUP(vector, q, uint, u, 16, 8, 0xFFFF);
+  VDUP(vector, q, uint, u, 32, 4, 0xFFFFFFFF);
+  VDUP(vector, q, uint, u, 64, 2, 0xFFFFFFFFFFFFFFFFULL);
+
+  /* Shift by -1 */
+  VDUP(vector_shift, , int, s, 8, 8, -1);
+  VDUP(vector_shift, , int, s, 16, 4, -1);
+  VDUP(vector_shift, , int, s, 32, 2, -1);
+  VDUP(vector_shift, , int, s, 64, 1, -1);
+  VDUP(vector_shift, q, int, s, 8, 16, -1);
+  VDUP(vector_shift, q, int, s, 16, 8, -1);
+  VDUP(vector_shift, q, int, s, 32, 4, -1);
+  VDUP(vector_shift, q, int, s, 64, 2, -1);
+
+#undef CMT
+#define CMT " (max input, shift by -1)"
+  TEST_VQSHL(int, , int, s, 8, 8, expected_cumulative_sat_max_minus1, CMT);
+  TEST_VQSHL(int, , int, s, 16, 4, expected_cumulative_sat_max_minus1, CMT);
+  TEST_VQSHL(int, , int, s, 32, 2, expected_cumulative_sat_max_minus1, CMT);
+  TEST_VQSHL(int, , int, s, 64, 1, expected_cumulative_sat_max_minus1, CMT);
+  TEST_VQSHL(int, , uint, u, 8, 8, expected_cumulative_sat_max_minus1, CMT);
+  TEST_VQSHL(int, , uint, u, 16, 4, expected_cumulative_sat_max_minus1, CMT);
+  TEST_VQSHL(int, , uint, u, 32, 2, expected_cumulative_sat_max_minus1, CMT);
+  TEST_VQSHL(int, , uint, u, 64, 1, expected_cumulative_sat_max_minus1, CMT);
+  TEST_VQSHL(int, q, int, s, 8, 16, expected_cumulative_sat_max_minus1, CMT);
+  TEST_VQSHL(int, q, int, s, 16, 8, expected_cumulative_sat_max_minus1, CMT);
+  TEST_VQSHL(int, q, int, s, 32, 4, expected_cumulative_sat_max_minus1, CMT);
+  TEST_VQSHL(int, q, int, s, 64, 2, expected_cumulative_sat_max_minus1, CMT);
+  TEST_VQSHL(int, q, uint, u, 8, 16, expected_cumulative_sat_max_minus1, CMT);
+  TEST_VQSHL(int, q, uint, u, 16, 8, expected_cumulative_sat_max_minus1, CMT);
+  TEST_VQSHL(int, q, uint, u, 32, 4, expected_cumulative_sat_max_minus1, CMT);
+  TEST_VQSHL(int, q, uint, u, 64, 2, expected_cumulative_sat_max_minus1, CMT);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_max_minus1, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_max_minus1, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_max_minus1, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_max_minus1, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_max_minus1, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_max_minus1, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_max_minus1, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_max_minus1, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_max_minus1, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_max_minus1, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_max_minus1, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_max_minus1, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_max_minus1, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_max_minus1, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_max_minus1, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_max_minus1, CMT);
+
+
+  /* Use large shift amounts */
+  VDUP(vector_shift, , int, s, 8, 8, 8);
+  VDUP(vector_shift, , int, s, 16, 4, 16);
+  VDUP(vector_shift, , int, s, 32, 2, 32);
+  VDUP(vector_shift, , int, s, 64, 1, 64);
+  VDUP(vector_shift, q, int, s, 8, 16, 8);
+  VDUP(vector_shift, q, int, s, 16, 8, 16);
+  VDUP(vector_shift, q, int, s, 32, 4, 32);
+  VDUP(vector_shift, q, int, s, 64, 2, 64);
+
+#undef CMT
+#define CMT " (max input, large shift amount)"
+  TEST_VQSHL(int, , int, s, 8, 8, expected_cumulative_sat_max_large, CMT);
+  TEST_VQSHL(int, , int, s, 16, 4, expected_cumulative_sat_max_large, CMT);
+  TEST_VQSHL(int, , int, s, 32, 2, expected_cumulative_sat_max_large, CMT);
+  TEST_VQSHL(int, , int, s, 64, 1, expected_cumulative_sat_max_large, CMT);
+  TEST_VQSHL(int, , uint, u, 8, 8, expected_cumulative_sat_max_large, CMT);
+  TEST_VQSHL(int, , uint, u, 16, 4, expected_cumulative_sat_max_large, CMT);
+  TEST_VQSHL(int, , uint, u, 32, 2, expected_cumulative_sat_max_large, CMT);
+  TEST_VQSHL(int, , uint, u, 64, 1, expected_cumulative_sat_max_large, CMT);
+  TEST_VQSHL(int, q, int, s, 8, 16, expected_cumulative_sat_max_large, CMT);
+  TEST_VQSHL(int, q, int, s, 16, 8, expected_cumulative_sat_max_large, CMT);
+  TEST_VQSHL(int, q, int, s, 32, 4, expected_cumulative_sat_max_large, CMT);
+  TEST_VQSHL(int, q, int, s, 64, 2, expected_cumulative_sat_max_large, CMT);
+  TEST_VQSHL(int, q, uint, u, 8, 16, expected_cumulative_sat_max_large, CMT);
+  TEST_VQSHL(int, q, uint, u, 16, 8, expected_cumulative_sat_max_large, CMT);
+  TEST_VQSHL(int, q, uint, u, 32, 4, expected_cumulative_sat_max_large, CMT);
+  TEST_VQSHL(int, q, uint, u, 64, 2, expected_cumulative_sat_max_large, CMT);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_max_large, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_max_large, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_max_large, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_max_large, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_max_large, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_max_large, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_max_large, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_max_large, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_max_large, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_max_large, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_max_large, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_max_large, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_max_large, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_max_large, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_max_large, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_max_large, CMT);
+
+
+  /* Check 64 bits saturation.  */
+  VDUP(vector, , int, s, 64, 1, -10);
+  VDUP(vector_shift, , int, s, 64, 1, 64);
+  VDUP(vector, q, int, s, 64, 2, 10);
+  VDUP(vector_shift, q, int, s, 64, 2, 64);
+
+#undef CMT
+#define CMT " (check saturation on 64 bits)"
+  TEST_VQSHL(int, , int, s, 64, 1, expected_cumulative_sat_64, CMT);
+  TEST_VQSHL(int, q, int, s, 64, 2, expected_cumulative_sat_64, CMT);
+
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_64, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_64, CMT);
+}
+
+int main (void)
+{
+  exec_vqshl ();
+  return 0;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vqshl_n.c
@@ -0,0 +1,234 @@
+#include <arm_neon.h>
+#include "arm-neon-ref.h"
+#include "compute-ref-data.h"
+
+/* Expected values of cumulative_saturation flag.  */
+int VECT_VAR(expected_cumulative_sat,int,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat,int,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat,int,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat,int,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat,uint,8,8) = 1;
+int VECT_VAR(expected_cumulative_sat,uint,16,4) = 1;
+int VECT_VAR(expected_cumulative_sat,uint,32,2) = 1;
+int VECT_VAR(expected_cumulative_sat,uint,64,1) = 1;
+int VECT_VAR(expected_cumulative_sat,int,8,16) = 0;
+int VECT_VAR(expected_cumulative_sat,int,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat,int,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat,int,64,2) = 0;
+int VECT_VAR(expected_cumulative_sat,uint,8,16) = 1;
+int VECT_VAR(expected_cumulative_sat,uint,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat,uint,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat,uint,64,2) = 1;
+
+/* Expected results.  */
+VECT_VAR_DECL(expected,int,8,8) [] = { 0xc0, 0xc4, 0xc8, 0xcc,
+				       0xd0, 0xd4, 0xd8, 0xdc };
+VECT_VAR_DECL(expected,int,16,4) [] = { 0xffe0, 0xffe2, 0xffe4, 0xffe6 };
+VECT_VAR_DECL(expected,int,32,2) [] = { 0xffffffe0, 0xffffffe2 };
+VECT_VAR_DECL(expected,int,64,1) [] = { 0xffffffffffffffc0 };
+VECT_VAR_DECL(expected,uint,8,8) [] = { 0xff, 0xff, 0xff, 0xff,
+					0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected,uint,16,4) [] = { 0xffff, 0xffff, 0xffff, 0xffff };
+VECT_VAR_DECL(expected,uint,32,2) [] = { 0xffffffff, 0xffffffff };
+VECT_VAR_DECL(expected,uint,64,1) [] = { 0xffffffffffffffff };
+VECT_VAR_DECL(expected,int,8,16) [] = { 0xc0, 0xc4, 0xc8, 0xcc,
+					0xd0, 0xd4, 0xd8, 0xdc,
+					0xe0, 0xe4, 0xe8, 0xec,
+					0xf0, 0xf4, 0xf8, 0xfc };
+VECT_VAR_DECL(expected,int,16,8) [] = { 0xffe0, 0xffe2, 0xffe4, 0xffe6,
+					0xffe8, 0xffea, 0xffec, 0xffee };
+VECT_VAR_DECL(expected,int,32,4) [] = { 0xffffffe0, 0xffffffe2,
+					0xffffffe4, 0xffffffe6 };
+VECT_VAR_DECL(expected,int,64,2) [] = { 0xffffffffffffffc0, 0xffffffffffffffc4 };
+VECT_VAR_DECL(expected,uint,8,16) [] = { 0xff, 0xff, 0xff, 0xff,
+					 0xff, 0xff, 0xff, 0xff,
+					 0xff, 0xff, 0xff, 0xff,
+					 0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected,uint,16,8) [] = { 0xffff, 0xffff, 0xffff, 0xffff,
+					 0xffff, 0xffff, 0xffff, 0xffff };
+VECT_VAR_DECL(expected,uint,32,4) [] = { 0xffffffff, 0xffffffff,
+					 0xffffffff, 0xffffffff };
+VECT_VAR_DECL(expected,uint,64,2) [] = { 0xffffffffffffffff,
+					 0xffffffffffffffff };
+
+/* Expected values of cumulative_saturation flag with max positive input.  */
+int VECT_VAR(expected_cumulative_sat_max,int,8,8) = 1;
+int VECT_VAR(expected_cumulative_sat_max,int,16,4) = 1;
+int VECT_VAR(expected_cumulative_sat_max,int,32,2) = 1;
+int VECT_VAR(expected_cumulative_sat_max,int,64,1) = 1;
+int VECT_VAR(expected_cumulative_sat_max,uint,8,8) = 1;
+int VECT_VAR(expected_cumulative_sat_max,uint,16,4) = 1;
+int VECT_VAR(expected_cumulative_sat_max,uint,32,2) = 1;
+int VECT_VAR(expected_cumulative_sat_max,uint,64,1) = 1;
+int VECT_VAR(expected_cumulative_sat_max,int,8,16) = 1;
+int VECT_VAR(expected_cumulative_sat_max,int,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat_max,int,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat_max,int,64,2) = 1;
+int VECT_VAR(expected_cumulative_sat_max,uint,8,16) = 1;
+int VECT_VAR(expected_cumulative_sat_max,uint,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat_max,uint,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat_max,uint,64,2) = 1;
+
+/* Expected results with max positive input.  */
+VECT_VAR_DECL(expected_max,int,8,8) [] = { 0x7f, 0x7f, 0x7f, 0x7f,
+					   0x7f, 0x7f, 0x7f, 0x7f };
+VECT_VAR_DECL(expected_max,int,16,4) [] = { 0x7fff, 0x7fff, 0x7fff, 0x7fff };
+VECT_VAR_DECL(expected_max,int,32,2) [] = { 0x7fffffff, 0x7fffffff };
+VECT_VAR_DECL(expected_max,int,64,1) [] = { 0x7fffffffffffffff };
+VECT_VAR_DECL(expected_max,uint,8,8) [] = { 0xff, 0xff, 0xff, 0xff,
+					    0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_max,uint,16,4) [] = { 0xffff, 0xffff, 0xffff, 0xffff };
+VECT_VAR_DECL(expected_max,uint,32,2) [] = { 0xffffffff, 0xffffffff };
+VECT_VAR_DECL(expected_max,uint,64,1) [] = { 0xffffffffffffffff };
+VECT_VAR_DECL(expected_max,int,8,16) [] = { 0x7f, 0x7f, 0x7f, 0x7f,
+					    0x7f, 0x7f, 0x7f, 0x7f,
+					    0x7f, 0x7f, 0x7f, 0x7f,
+					    0x7f, 0x7f, 0x7f, 0x7f };
+VECT_VAR_DECL(expected_max,int,16,8) [] = { 0x7fff, 0x7fff, 0x7fff, 0x7fff,
+					    0x7fff, 0x7fff, 0x7fff, 0x7fff };
+VECT_VAR_DECL(expected_max,int,32,4) [] = { 0x7fffffff, 0x7fffffff,
+					    0x7fffffff, 0x7fffffff };
+VECT_VAR_DECL(expected_max,int,64,2) [] = { 0x7fffffffffffffff,
+					    0x7fffffffffffffff };
+VECT_VAR_DECL(expected_max,uint,8,16) [] = { 0xff, 0xff, 0xff, 0xff,
+					     0xff, 0xff, 0xff, 0xff,
+					     0xff, 0xff, 0xff, 0xff,
+					     0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_max,uint,16,8) [] = { 0xffff, 0xffff, 0xffff, 0xffff,
+					     0xffff, 0xffff, 0xffff, 0xffff };
+VECT_VAR_DECL(expected_max,uint,32,4) [] = { 0xffffffff, 0xffffffff,
+					     0xffffffff, 0xffffffff };
+VECT_VAR_DECL(expected_max,uint,64,2) [] = { 0xffffffffffffffff,
+					     0xffffffffffffffff };
+
+#define INSN vqshl
+#define TEST_MSG "VQSHL_N/VQSHLQ_N"
+
+#define FNNAME1(NAME) void exec_ ## NAME ##_n (void)
+#define FNNAME(NAME) FNNAME1(NAME)
+
+FNNAME (INSN)
+{
+  /* Basic test: v2=vqshl_n(v1,v), then store the result.  */
+#define TEST_VQSHL_N2(INSN, Q, T1, T2, W, N, V, EXPECTED_CUMULATIVE_SAT, CMT) \
+  Set_Neon_Cumulative_Sat(0, VECT_VAR(vector_res, T1, W, N));		\
+  VECT_VAR(vector_res, T1, W, N) =					\
+    INSN##Q##_n_##T2##W(VECT_VAR(vector, T1, W, N),			\
+			V);						\
+  vst1##Q##_##T2##W(VECT_VAR(result, T1, W, N),				\
+		    VECT_VAR(vector_res, T1, W, N));			\
+  CHECK_CUMULATIVE_SAT(TEST_MSG, T1, W, N, EXPECTED_CUMULATIVE_SAT, CMT)
+
+  /* Two auxliary macros are necessary to expand INSN */
+#define TEST_VQSHL_N1(INSN, T3, Q, T1, T2, W, N, EXPECTED_CUMULATIVE_SAT, CMT) \
+  TEST_VQSHL_N2(INSN, T3, Q, T1, T2, W, N, EXPECTED_CUMULATIVE_SAT, CMT)
+
+#define TEST_VQSHL_N(T3, Q, T1, T2, W, N, EXPECTED_CUMULATIVE_SAT, CMT)	\
+  TEST_VQSHL_N1(INSN, T3, Q, T1, T2, W, N, EXPECTED_CUMULATIVE_SAT, CMT)
+
+  DECL_VARIABLE_ALL_VARIANTS(vector);
+  DECL_VARIABLE_ALL_VARIANTS(vector_res);
+
+  clean_results ();
+
+  TEST_MACRO_ALL_VARIANTS_2_5(VLOAD, vector, buffer);
+
+  /* Choose shift amount arbitrarily.  */
+#define CMT ""
+  TEST_VQSHL_N(, int, s, 8, 8, 2, expected_cumulative_sat, CMT);
+  TEST_VQSHL_N(, int, s, 16, 4, 1, expected_cumulative_sat, CMT);
+  TEST_VQSHL_N(, int, s, 32, 2, 1, expected_cumulative_sat, CMT);
+  TEST_VQSHL_N(, int, s, 64, 1, 2, expected_cumulative_sat, CMT);
+  TEST_VQSHL_N(, uint, u, 8, 8, 3, expected_cumulative_sat, CMT);
+  TEST_VQSHL_N(, uint, u, 16, 4, 2, expected_cumulative_sat, CMT);
+  TEST_VQSHL_N(, uint, u, 32, 2, 3, expected_cumulative_sat, CMT);
+  TEST_VQSHL_N(, uint, u, 64, 1, 3, expected_cumulative_sat, CMT);
+
+  TEST_VQSHL_N(q, int, s, 8, 16, 2, expected_cumulative_sat, CMT);
+  TEST_VQSHL_N(q, int, s, 16, 8, 1, expected_cumulative_sat, CMT);
+  TEST_VQSHL_N(q, int, s, 32, 4, 1, expected_cumulative_sat, CMT);
+  TEST_VQSHL_N(q, int, s, 64, 2, 2, expected_cumulative_sat, CMT);
+  TEST_VQSHL_N(q, uint, u, 8, 16, 3, expected_cumulative_sat, CMT);
+  TEST_VQSHL_N(q, uint, u, 16, 8, 2, expected_cumulative_sat, CMT);
+  TEST_VQSHL_N(q, uint, u, 32, 4, 3, expected_cumulative_sat, CMT);
+  TEST_VQSHL_N(q, uint, u, 64, 2, 3, expected_cumulative_sat, CMT);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected, CMT);
+
+
+  /* Fill input vector with max value, to check saturation on limits.  */
+  VDUP(vector, , int, s, 8, 8, 0x7F);
+  VDUP(vector, , int, s, 16, 4, 0x7FFF);
+  VDUP(vector, , int, s, 32, 2, 0x7FFFFFFF);
+  VDUP(vector, , int, s, 64, 1, 0x7FFFFFFFFFFFFFFFLL);
+  VDUP(vector, , uint, u, 8, 8, 0xFF);
+  VDUP(vector, , uint, u, 16, 4, 0xFFFF);
+  VDUP(vector, , uint, u, 32, 2, 0xFFFFFFFF);
+  VDUP(vector, , uint, u, 64, 1, 0xFFFFFFFFFFFFFFFFULL);
+  VDUP(vector, q, int, s, 8, 16, 0x7F);
+  VDUP(vector, q, int, s, 16, 8, 0x7FFF);
+  VDUP(vector, q, int, s, 32, 4, 0x7FFFFFFF);
+  VDUP(vector, q, int, s, 64, 2, 0x7FFFFFFFFFFFFFFFLL);
+  VDUP(vector, q, uint, u, 8, 16, 0xFF);
+  VDUP(vector, q, uint, u, 16, 8, 0xFFFF);
+  VDUP(vector, q, uint, u, 32, 4, 0xFFFFFFFF);
+  VDUP(vector, q, uint, u, 64, 2, 0xFFFFFFFFFFFFFFFFULL);
+
+#undef CMT
+#define CMT " (with max input)"
+  TEST_VQSHL_N(, int, s, 8, 8, 2, expected_cumulative_sat_max, CMT);
+  TEST_VQSHL_N(, int, s, 16, 4, 1, expected_cumulative_sat_max, CMT);
+  TEST_VQSHL_N(, int, s, 32, 2, 1, expected_cumulative_sat_max, CMT);
+  TEST_VQSHL_N(, int, s, 64, 1, 2, expected_cumulative_sat_max, CMT);
+  TEST_VQSHL_N(, uint, u, 8, 8, 3, expected_cumulative_sat_max, CMT);
+  TEST_VQSHL_N(, uint, u, 16, 4, 2, expected_cumulative_sat_max, CMT);
+  TEST_VQSHL_N(, uint, u, 32, 2, 3, expected_cumulative_sat_max, CMT);
+  TEST_VQSHL_N(, uint, u, 64, 1, 3, expected_cumulative_sat_max, CMT);
+
+  TEST_VQSHL_N(q, int, s, 8, 16, 2, expected_cumulative_sat_max, CMT);
+  TEST_VQSHL_N(q, int, s, 16, 8, 1, expected_cumulative_sat_max, CMT);
+  TEST_VQSHL_N(q, int, s, 32, 4, 1, expected_cumulative_sat_max, CMT);
+  TEST_VQSHL_N(q, int, s, 64, 2, 2, expected_cumulative_sat_max, CMT);
+  TEST_VQSHL_N(q, uint, u, 8, 16, 3, expected_cumulative_sat_max, CMT);
+  TEST_VQSHL_N(q, uint, u, 16, 8, 2, expected_cumulative_sat_max, CMT);
+  TEST_VQSHL_N(q, uint, u, 32, 4, 3, expected_cumulative_sat_max, CMT);
+  TEST_VQSHL_N(q, uint, u, 64, 2, 3, expected_cumulative_sat_max, CMT);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_max, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_max, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_max, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_max, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_max, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_max, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_max, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_max, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_max, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_max, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_max, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_max, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_max, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_max, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_max, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_max, CMT);
+}
+
+int main (void)
+{
+  exec_vqshl_n ();
+  return 0;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vqshlu_n.c
@@ -0,0 +1,263 @@
+#include <arm_neon.h>
+#include "arm-neon-ref.h"
+#include "compute-ref-data.h"
+
+/* Expected values of cumulative_saturation flag with negative
+   input.  */
+int VECT_VAR(expected_cumulative_sat_neg,int,8,8) = 1;
+int VECT_VAR(expected_cumulative_sat_neg,int,16,4) = 1;
+int VECT_VAR(expected_cumulative_sat_neg,int,32,2) = 1;
+int VECT_VAR(expected_cumulative_sat_neg,int,64,1) = 1;
+int VECT_VAR(expected_cumulative_sat_neg,int,8,16) = 1;
+int VECT_VAR(expected_cumulative_sat_neg,int,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat_neg,int,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat_neg,int,64,2) = 1;
+
+/* Expected results with negative input.  */
+VECT_VAR_DECL(expected_neg,uint,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					    0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_neg,uint,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_neg,uint,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_neg,uint,64,1) [] = { 0x0 };
+VECT_VAR_DECL(expected_neg,uint,8,16) [] = { 0x0, 0x0, 0x0, 0x0,
+					     0x0, 0x0, 0x0, 0x0,
+					     0x0, 0x0, 0x0, 0x0,
+					     0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_neg,uint,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					     0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_neg,uint,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_neg,uint,64,2) [] = { 0x0, 0x0 };
+
+/* Expected values of cumulative_saturation flag with shift by 1.  */
+int VECT_VAR(expected_cumulative_sat_sh1,int,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat_sh1,int,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat_sh1,int,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat_sh1,int,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat_sh1,int,8,16) = 0;
+int VECT_VAR(expected_cumulative_sat_sh1,int,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_sh1,int,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat_sh1,int,64,2) = 0;
+
+/* Expected results with shift by 1.  */
+VECT_VAR_DECL(expected_sh1,uint,8,8) [] = { 0xfe, 0xfe, 0xfe, 0xfe,
+					    0xfe, 0xfe, 0xfe, 0xfe };
+VECT_VAR_DECL(expected_sh1,uint,16,4) [] = { 0xfffe, 0xfffe, 0xfffe, 0xfffe };
+VECT_VAR_DECL(expected_sh1,uint,32,2) [] = { 0xfffffffe, 0xfffffffe };
+VECT_VAR_DECL(expected_sh1,uint,64,1) [] = { 0xfffffffffffffffe };
+VECT_VAR_DECL(expected_sh1,uint,8,16) [] = { 0xfe, 0xfe, 0xfe, 0xfe,
+					     0xfe, 0xfe, 0xfe, 0xfe,
+					     0xfe, 0xfe, 0xfe, 0xfe,
+					     0xfe, 0xfe, 0xfe, 0xfe };
+VECT_VAR_DECL(expected_sh1,uint,16,8) [] = { 0xfffe, 0xfffe, 0xfffe, 0xfffe,
+					     0xfffe, 0xfffe, 0xfffe, 0xfffe };
+VECT_VAR_DECL(expected_sh1,uint,32,4) [] = { 0xfffffffe, 0xfffffffe,
+					     0xfffffffe, 0xfffffffe };
+VECT_VAR_DECL(expected_sh1,uint,64,2) [] = { 0xfffffffffffffffe,
+					     0xfffffffffffffffe };
+
+/* Expected values of cumulative_saturation flag with shift by 2.  */
+int VECT_VAR(expected_cumulative_sat_sh2,int,8,8) = 1;
+int VECT_VAR(expected_cumulative_sat_sh2,int,16,4) = 1;
+int VECT_VAR(expected_cumulative_sat_sh2,int,32,2) = 1;
+int VECT_VAR(expected_cumulative_sat_sh2,int,64,1) = 1;
+int VECT_VAR(expected_cumulative_sat_sh2,int,8,16) = 1;
+int VECT_VAR(expected_cumulative_sat_sh2,int,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat_sh2,int,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat_sh2,int,64,2) = 1;
+
+/* Expected results with shift by 2.  */
+VECT_VAR_DECL(expected_sh2,uint,8,8) [] = { 0xff, 0xff, 0xff, 0xff,
+					    0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_sh2,uint,16,4) [] = { 0xffff, 0xffff, 0xffff, 0xffff };
+VECT_VAR_DECL(expected_sh2,uint,32,2) [] = { 0xffffffff, 0xffffffff };
+VECT_VAR_DECL(expected_sh2,uint,64,1) [] = { 0xffffffffffffffff };
+VECT_VAR_DECL(expected_sh2,uint,8,16) [] = { 0xff, 0xff, 0xff, 0xff,
+					     0xff, 0xff, 0xff, 0xff,
+					     0xff, 0xff, 0xff, 0xff,
+					     0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_sh2,uint,16,8) [] = { 0xffff, 0xffff, 0xffff, 0xffff,
+					     0xffff, 0xffff, 0xffff, 0xffff };
+VECT_VAR_DECL(expected_sh2,uint,32,4) [] = { 0xffffffff, 0xffffffff,
+					     0xffffffff, 0xffffffff };
+VECT_VAR_DECL(expected_sh2,uint,64,2) [] = { 0xffffffffffffffff,
+					     0xffffffffffffffff };
+
+/* Expected values of cumulative_saturation flag.  */
+int VECT_VAR(expected_cumulative_sat,int,8,8) = 0;
+int VECT_VAR(expected_cumulative_sat,int,16,4) = 0;
+int VECT_VAR(expected_cumulative_sat,int,32,2) = 0;
+int VECT_VAR(expected_cumulative_sat,int,64,1) = 0;
+int VECT_VAR(expected_cumulative_sat,int,8,16) = 0;
+int VECT_VAR(expected_cumulative_sat,int,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat,int,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat,int,64,2) = 0;
+
+/* Expected results.  */
+VECT_VAR_DECL(expected,uint,8,8) [] = { 0x2, 0x2, 0x2, 0x2, 0x2, 0x2, 0x2, 0x2 };
+VECT_VAR_DECL(expected,uint,16,4) [] = { 0x8, 0x8, 0x8, 0x8 };
+VECT_VAR_DECL(expected,uint,32,2) [] = { 0x18, 0x18 };
+VECT_VAR_DECL(expected,uint,64,1) [] = { 0x40 };
+VECT_VAR_DECL(expected,uint,8,16) [] = { 0xa0, 0xa0, 0xa0, 0xa0,
+					 0xa0, 0xa0, 0xa0, 0xa0,
+					 0xa0, 0xa0, 0xa0, 0xa0,
+					 0xa0, 0xa0, 0xa0, 0xa0 };
+VECT_VAR_DECL(expected,uint,16,8) [] = { 0x180, 0x180, 0x180, 0x180,
+					 0x180, 0x180, 0x180, 0x180 };
+VECT_VAR_DECL(expected,uint,32,4) [] = { 0x380, 0x380, 0x380, 0x380 };
+VECT_VAR_DECL(expected,uint,64,2) [] = { 0x800, 0x800 };
+
+
+#define INSN vqshlu
+#define TEST_MSG "VQSHLU_N/VQSHLUQ_N"
+
+#define FNNAME1(NAME) void exec_ ## NAME ## _n(void)
+#define FNNAME(NAME) FNNAME1(NAME)
+
+FNNAME (INSN)
+{
+  /* Basic test: v2=vqshlu_n(v1,v), then store the result.  */
+#define TEST_VQSHLU_N2(INSN, Q, T1, T2, T3, T4, W, N, V, EXPECTED_CUMULATIVE_SAT, CMT) \
+  Set_Neon_Cumulative_Sat(0, VECT_VAR(vector_res, T3, W, N));		\
+  VECT_VAR(vector_res, T3, W, N) =					\
+    INSN##Q##_n_##T2##W(VECT_VAR(vector, T1, W, N),			\
+			V);						\
+  vst1##Q##_##T4##W(VECT_VAR(result, T3, W, N),				\
+		    VECT_VAR(vector_res, T3, W, N));			\
+  CHECK_CUMULATIVE_SAT(TEST_MSG, T1, W, N, EXPECTED_CUMULATIVE_SAT, CMT)
+
+  /* Two auxliary macros are necessary to expand INSN */
+#define TEST_VQSHLU_N1(INSN, Q, T1, T2, T3, T4, W, N, V, EXPECTED_CUMULATIVE_SAT, CMT) \
+  TEST_VQSHLU_N2(INSN, Q, T1, T2, T3, T4, W, N, V, EXPECTED_CUMULATIVE_SAT, CMT)
+
+#define TEST_VQSHLU_N(Q, T1, T2, T3, T4, W, N, V, EXPECTED_CUMULATIVE_SAT, CMT) \
+  TEST_VQSHLU_N1(INSN, Q, T1, T2, T3, T4, W, N, V, EXPECTED_CUMULATIVE_SAT, CMT)
+
+
+  DECL_VARIABLE_ALL_VARIANTS(vector);
+  DECL_VARIABLE_ALL_VARIANTS(vector_res);
+
+  clean_results ();
+
+  /* Fill input vector with negative values, to check saturation on
+     limits.  */
+  VDUP(vector, , int, s, 8, 8, -1);
+  VDUP(vector, , int, s, 16, 4, -2);
+  VDUP(vector, , int, s, 32, 2, -3);
+  VDUP(vector, , int, s, 64, 1, -4);
+  VDUP(vector, q, int, s, 8, 16, -1);
+  VDUP(vector, q, int, s, 16, 8, -2);
+  VDUP(vector, q, int, s, 32, 4, -3);
+  VDUP(vector, q, int, s, 64, 2, -4);
+
+  /* Choose shift amount arbitrarily.  */
+#define CMT " (negative input)"
+  TEST_VQSHLU_N(, int, s, uint, u, 8, 8, 2, expected_cumulative_sat_neg, CMT);
+  TEST_VQSHLU_N(, int, s, uint, u, 16, 4, 1, expected_cumulative_sat_neg, CMT);
+  TEST_VQSHLU_N(, int, s, uint, u, 32, 2, 1, expected_cumulative_sat_neg, CMT);
+  TEST_VQSHLU_N(, int, s, uint, u, 64, 1, 2, expected_cumulative_sat_neg, CMT);
+  TEST_VQSHLU_N(q, int, s, uint, u, 8, 16, 2, expected_cumulative_sat_neg, CMT);
+  TEST_VQSHLU_N(q, int, s, uint, u, 16, 8, 1, expected_cumulative_sat_neg, CMT);
+  TEST_VQSHLU_N(q, int, s, uint, u, 32, 4, 1, expected_cumulative_sat_neg, CMT);
+  TEST_VQSHLU_N(q, int, s, uint, u, 64, 2, 2, expected_cumulative_sat_neg, CMT);
+
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_neg, CMT);
+
+  
+  /* Fill input vector with max value, to check saturation on
+     limits.  */
+  VDUP(vector, , int, s, 8, 8, 0x7F);
+  VDUP(vector, , int, s, 16, 4, 0x7FFF);
+  VDUP(vector, , int, s, 32, 2, 0x7FFFFFFF);
+  VDUP(vector, , int, s, 64, 1, 0x7FFFFFFFFFFFFFFFLL);
+  VDUP(vector, q, int, s, 8, 16, 0x7F);
+  VDUP(vector, q, int, s, 16, 8, 0x7FFF);
+  VDUP(vector, q, int, s, 32, 4, 0x7FFFFFFF);
+  VDUP(vector, q, int, s, 64, 2, 0x7FFFFFFFFFFFFFFFULL);
+
+  /* shift by 1.  */
+#undef CMT
+#define CMT " (shift by 1)"
+  TEST_VQSHLU_N(, int, s, uint, u, 8, 8, 1, expected_cumulative_sat_sh1, CMT);
+  TEST_VQSHLU_N(, int, s, uint, u, 16, 4, 1, expected_cumulative_sat_sh1, CMT);
+  TEST_VQSHLU_N(, int, s, uint, u, 32, 2, 1, expected_cumulative_sat_sh1, CMT);
+  TEST_VQSHLU_N(, int, s, uint, u, 64, 1, 1, expected_cumulative_sat_sh1, CMT);
+  TEST_VQSHLU_N(q, int, s, uint, u, 8, 16, 1, expected_cumulative_sat_sh1, CMT);
+  TEST_VQSHLU_N(q, int, s, uint, u, 16, 8, 1, expected_cumulative_sat_sh1, CMT);
+  TEST_VQSHLU_N(q, int, s, uint, u, 32, 4, 1, expected_cumulative_sat_sh1, CMT);
+  TEST_VQSHLU_N(q, int, s, uint, u, 64, 2, 1, expected_cumulative_sat_sh1, CMT);
+
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_sh1, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_sh1, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_sh1, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_sh1, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_sh1, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_sh1, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_sh1, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_sh1, CMT);
+
+  /* shift by 2 to force saturation.  */
+#undef CMT
+#define CMT " (shift by 2)"
+  TEST_VQSHLU_N(, int, s, uint, u, 8, 8, 2, expected_cumulative_sat_sh2, CMT);
+  TEST_VQSHLU_N(, int, s, uint, u, 16, 4, 2, expected_cumulative_sat_sh2, CMT);
+  TEST_VQSHLU_N(, int, s, uint, u, 32, 2, 2, expected_cumulative_sat_sh2, CMT);
+  TEST_VQSHLU_N(, int, s, uint, u, 64, 1, 2, expected_cumulative_sat_sh2, CMT);
+  TEST_VQSHLU_N(q, int, s, uint, u, 8, 16, 2, expected_cumulative_sat_sh2, CMT);
+  TEST_VQSHLU_N(q, int, s, uint, u, 16, 8, 2, expected_cumulative_sat_sh2, CMT);
+  TEST_VQSHLU_N(q, int, s, uint, u, 32, 4, 2, expected_cumulative_sat_sh2, CMT);
+  TEST_VQSHLU_N(q, int, s, uint, u, 64, 2, 2, expected_cumulative_sat_sh2, CMT);
+
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_sh2, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_sh2, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_sh2, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_sh2, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_sh2, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_sh2, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_sh2, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_sh2, CMT);
+
+  
+  /* Fill input vector with positive values, to check normal case.  */
+  VDUP(vector, , int, s, 8, 8, 1);
+  VDUP(vector, , int, s, 16, 4, 2);
+  VDUP(vector, , int, s, 32, 2, 3);
+  VDUP(vector, , int, s, 64, 1, 4);
+  VDUP(vector, q, int, s, 8, 16, 5);
+  VDUP(vector, q, int, s, 16, 8, 6);
+  VDUP(vector, q, int, s, 32, 4, 7);
+  VDUP(vector, q, int, s, 64, 2, 8);
+
+  /* Arbitrary shift amount.  */
+#undef CMT
+#define CMT ""
+  TEST_VQSHLU_N(, int, s, uint, u, 8, 8, 1, expected_cumulative_sat, CMT);
+  TEST_VQSHLU_N(, int, s, uint, u, 16, 4, 2, expected_cumulative_sat, CMT);
+  TEST_VQSHLU_N(, int, s, uint, u, 32, 2, 3, expected_cumulative_sat, CMT);
+  TEST_VQSHLU_N(, int, s, uint, u, 64, 1, 4, expected_cumulative_sat, CMT);
+  TEST_VQSHLU_N(q, int, s, uint, u, 8, 16, 5, expected_cumulative_sat, CMT);
+  TEST_VQSHLU_N(q, int, s, uint, u, 16, 8, 6, expected_cumulative_sat, CMT);
+  TEST_VQSHLU_N(q, int, s, uint, u, 32, 4, 7, expected_cumulative_sat, CMT);
+  TEST_VQSHLU_N(q, int, s, uint, u, 64, 2, 8, expected_cumulative_sat, CMT);
+
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected, CMT);
+}
+
+int main (void)
+{
+  exec_vqshlu_n ();
+  return 0;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vqshrn_n.c
@@ -0,0 +1,177 @@
+#include <arm_neon.h>
+#include "arm-neon-ref.h"
+#include "compute-ref-data.h"
+
+/* Expected values of cumulative_saturation flag.  */
+int VECT_VAR(expected_cumulative_sat,int,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat,int,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat,int,64,2) = 0;
+int VECT_VAR(expected_cumulative_sat,uint,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat,uint,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat,uint,64,2) = 1;
+
+/* Expected results.  */
+VECT_VAR_DECL(expected,int,8,8) [] = { 0xf8, 0xf8, 0xf9, 0xf9,
+				       0xfa, 0xfa, 0xfb, 0xfb };
+VECT_VAR_DECL(expected,int,16,4) [] = { 0xfff8, 0xfff8, 0xfff9, 0xfff9 };
+VECT_VAR_DECL(expected,int,32,2) [] = { 0xfffffffc, 0xfffffffc };
+VECT_VAR_DECL(expected,uint,8,8) [] = { 0xff, 0xff, 0xff, 0xff,
+					0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected,uint,16,4) [] = { 0xffff, 0xffff, 0xffff, 0xffff };
+VECT_VAR_DECL(expected,uint,32,2) [] = { 0xffffffff, 0xffffffff };
+
+/* Expected values of cumulative_saturation flag with max input value
+   shifted by 3.  */
+int VECT_VAR(expected_cumulative_sat_max_sh3,int,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat_max_sh3,int,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat_max_sh3,int,64,2) = 1;
+int VECT_VAR(expected_cumulative_sat_max_sh3,uint,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat_max_sh3,uint,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat_max_sh3,uint,64,2) = 1;
+
+/* Expected results with max input value shifted by 3.  */
+VECT_VAR_DECL(expected_max_sh3,int,8,8) [] = { 0x7f, 0x7f, 0x7f, 0x7f,
+					       0x7f, 0x7f, 0x7f, 0x7f };
+VECT_VAR_DECL(expected_max_sh3,int,16,4) [] = { 0x7fff, 0x7fff, 0x7fff, 0x7fff };
+VECT_VAR_DECL(expected_max_sh3,int,32,2) [] = { 0x7fffffff, 0x7fffffff };
+VECT_VAR_DECL(expected_max_sh3,uint,8,8) [] = { 0xff, 0xff, 0xff, 0xff,
+						0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_max_sh3,uint,16,4) [] = { 0xffff, 0xffff,
+						 0xffff, 0xffff };
+VECT_VAR_DECL(expected_max_sh3,uint,32,2) [] = { 0xffffffff, 0xffffffff };
+
+/* Expected values of cumulative_saturation flag with max input value
+   shifted by type size.  */
+int VECT_VAR(expected_cumulative_sat_max_shmax,int,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_max_shmax,int,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat_max_shmax,int,64,2) = 0;
+int VECT_VAR(expected_cumulative_sat_max_shmax,uint,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat_max_shmax,uint,32,4) = 0;
+int VECT_VAR(expected_cumulative_sat_max_shmax,uint,64,2) = 0;
+
+/* Expected results with max input value shifted by type size.  */
+VECT_VAR_DECL(expected_max_shmax,int,8,8) [] = { 0x7f, 0x7f, 0x7f, 0x7f,
+						 0x7f, 0x7f, 0x7f, 0x7f };
+VECT_VAR_DECL(expected_max_shmax,int,16,4) [] = { 0x7fff, 0x7fff,
+						  0x7fff, 0x7fff };
+VECT_VAR_DECL(expected_max_shmax,int,32,2) [] = { 0x7fffffff, 0x7fffffff };
+VECT_VAR_DECL(expected_max_shmax,uint,8,8) [] = { 0xff, 0xff, 0xff, 0xff,
+						  0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_max_shmax,uint,16,4) [] = { 0xffff, 0xffff,
+						   0xffff, 0xffff };
+VECT_VAR_DECL(expected_max_shmax,uint,32,2) [] = { 0xffffffff, 0xffffffff };
+
+#define INSN vqshrn_n
+#define TEST_MSG "VQSHRN_N"
+
+#define FNNAME1(NAME) void exec_ ## NAME (void)
+#define FNNAME(NAME) FNNAME1(NAME)
+
+FNNAME (INSN)
+{
+  /* Basic test: y=vqshrn_n(x,v), then store the result.  */
+#define TEST_VQSHRN_N2(INSN, T1, T2, W, W2, N, V, EXPECTED_CUMULATIVE_SAT, CMT) \
+  Set_Neon_Cumulative_Sat(0, VECT_VAR(vector_res, T1, W2, N));		\
+  VECT_VAR(vector_res, T1, W2, N) =					\
+    INSN##_##T2##W(VECT_VAR(vector, T1, W, N),				\
+		   V);							\
+  vst1_##T2##W2(VECT_VAR(result, T1, W2, N),				\
+		VECT_VAR(vector_res, T1, W2, N));			\
+  CHECK_CUMULATIVE_SAT(TEST_MSG, T1, W, N, EXPECTED_CUMULATIVE_SAT, CMT)
+
+  /* Two auxliary macros are necessary to expand INSN */
+#define TEST_VQSHRN_N1(INSN, T1, T2, W, W2, N, V, EXPECTED_CUMULATIVE_SAT, CMT) \
+  TEST_VQSHRN_N2(INSN, T1, T2, W, W2, N, V, EXPECTED_CUMULATIVE_SAT, CMT)
+
+#define TEST_VQSHRN_N(T1, T2, W, W2, N, V, EXPECTED_CUMULATIVE_SAT, CMT) \
+  TEST_VQSHRN_N1(INSN, T1, T2, W, W2, N, V, EXPECTED_CUMULATIVE_SAT, CMT)
+
+
+  /* vector is twice as large as vector_res.  */
+  DECL_VARIABLE(vector, int, 16, 8);
+  DECL_VARIABLE(vector, int, 32, 4);
+  DECL_VARIABLE(vector, int, 64, 2);
+  DECL_VARIABLE(vector, uint, 16, 8);
+  DECL_VARIABLE(vector, uint, 32, 4);
+  DECL_VARIABLE(vector, uint, 64, 2);
+
+  DECL_VARIABLE(vector_res, int, 8, 8);
+  DECL_VARIABLE(vector_res, int, 16, 4);
+  DECL_VARIABLE(vector_res, int, 32, 2);
+  DECL_VARIABLE(vector_res, uint, 8, 8);
+  DECL_VARIABLE(vector_res, uint, 16, 4);
+  DECL_VARIABLE(vector_res, uint, 32, 2);
+
+  clean_results ();
+
+  VLOAD(vector, buffer, q, int, s, 16, 8);
+  VLOAD(vector, buffer, q, int, s, 32, 4);
+  VLOAD(vector, buffer, q, int, s, 64, 2);
+  VLOAD(vector, buffer, q, uint, u, 16, 8);
+  VLOAD(vector, buffer, q, uint, u, 32, 4);
+  VLOAD(vector, buffer, q, uint, u, 64, 2);
+
+  /* Choose shift amount arbitrarily.  */
+#define CMT ""
+  TEST_VQSHRN_N(int, s, 16, 8, 8, 1, expected_cumulative_sat, CMT);
+  TEST_VQSHRN_N(int, s, 32, 16, 4, 1, expected_cumulative_sat, CMT);
+  TEST_VQSHRN_N(int, s, 64, 32, 2, 2, expected_cumulative_sat, CMT);
+  TEST_VQSHRN_N(uint, u, 16, 8, 8, 2, expected_cumulative_sat, CMT);
+  TEST_VQSHRN_N(uint, u, 32, 16, 4, 3, expected_cumulative_sat, CMT);
+  TEST_VQSHRN_N(uint, u, 64, 32, 2, 3, expected_cumulative_sat, CMT);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected, CMT);
+
+
+  /* Use max possible value as input.  */
+  VDUP(vector, q, int, s, 16, 8, 0x7FFF);
+  VDUP(vector, q, int, s, 32, 4, 0x7FFFFFFF);
+  VDUP(vector, q, int, s, 64, 2, 0x7FFFFFFFFFFFFFFFLL);
+  VDUP(vector, q, uint, u, 16, 8, 0xFFFF);
+  VDUP(vector, q, uint, u, 32, 4, 0xFFFFFFFF);
+  VDUP(vector, q, uint, u, 64, 2, 0xFFFFFFFFFFFFFFFFULL);
+
+#undef CMT
+#define CMT " (check saturation: shift by 3)"
+  TEST_VQSHRN_N(int, s, 16, 8, 8, 3, expected_cumulative_sat_max_sh3, CMT);
+  TEST_VQSHRN_N(int, s, 32, 16, 4, 3, expected_cumulative_sat_max_sh3, CMT);
+  TEST_VQSHRN_N(int, s, 64, 32, 2, 3, expected_cumulative_sat_max_sh3, CMT);
+  TEST_VQSHRN_N(uint, u, 16, 8, 8, 3, expected_cumulative_sat_max_sh3, CMT);
+  TEST_VQSHRN_N(uint, u, 32, 16, 4, 3, expected_cumulative_sat_max_sh3, CMT);
+  TEST_VQSHRN_N(uint, u, 64, 32, 2, 3, expected_cumulative_sat_max_sh3, CMT);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_max_sh3, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_max_sh3, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_max_sh3, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_max_sh3, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_max_sh3, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_max_sh3, CMT);
+
+
+#undef CMT
+#define CMT " (check saturation: shift by max)"
+  TEST_VQSHRN_N(int, s, 16, 8, 8, 8, expected_cumulative_sat_max_shmax, CMT);
+  TEST_VQSHRN_N(int, s, 32, 16, 4, 16, expected_cumulative_sat_max_shmax, CMT);
+  TEST_VQSHRN_N(int, s, 64, 32, 2, 32, expected_cumulative_sat_max_shmax, CMT);
+  TEST_VQSHRN_N(uint, u, 16, 8, 8, 8, expected_cumulative_sat_max_shmax, CMT);
+  TEST_VQSHRN_N(uint, u, 32, 16, 4, 16, expected_cumulative_sat_max_shmax, CMT);
+  TEST_VQSHRN_N(uint, u, 64, 32, 2, 32, expected_cumulative_sat_max_shmax, CMT);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_max_shmax, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_max_shmax, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_max_shmax, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_max_shmax, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_max_shmax, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_max_shmax, CMT);
+}
+
+int main (void)
+{
+  exec_vqshrn_n ();
+  return 0;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vqshrun_n.c
@@ -0,0 +1,133 @@
+#include <arm_neon.h>
+#include "arm-neon-ref.h"
+#include "compute-ref-data.h"
+
+/* Expected values of cumulative_saturation flag with negative input.  */
+int VECT_VAR(expected_cumulative_sat_neg,int,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat_neg,int,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat_neg,int,64,2) = 1;
+
+/* Expected results with negative input.  */
+VECT_VAR_DECL(expected_neg,uint,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					    0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_neg,uint,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_neg,uint,32,2) [] = { 0x0, 0x0 };
+
+/* Expected values of cumulative_saturation flag with max input value
+   shifted by 1.  */
+int VECT_VAR(expected_cumulative_sat_max_sh1,int,16,8) = 1;
+int VECT_VAR(expected_cumulative_sat_max_sh1,int,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat_max_sh1,int,64,2) = 1;
+
+/* Expected results with max input value shifted by 1.  */
+VECT_VAR_DECL(expected_max_sh1,uint,8,8) [] = { 0xff, 0xff, 0xff, 0xff,
+						0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_max_sh1,uint,16,4) [] = { 0xffff, 0xffff,
+						 0xffff, 0xffff };
+VECT_VAR_DECL(expected_max_sh1,uint,32,2) [] = { 0xffffffff, 0xffffffff };
+VECT_VAR_DECL(expected_max_sh1,uint,64,1) [] = { 0x3333333333333333 };
+
+/* Expected values of cumulative_saturation flag.  */
+int VECT_VAR(expected_cumulative_sat,int,16,8) = 0;
+int VECT_VAR(expected_cumulative_sat,int,32,4) = 1;
+int VECT_VAR(expected_cumulative_sat,int,64,2) = 0;
+
+/* Expected results.  */
+VECT_VAR_DECL(expected,uint,8,8) [] = { 0x48, 0x48, 0x48, 0x48,
+					0x48, 0x48, 0x48, 0x48 };
+VECT_VAR_DECL(expected,uint,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected,uint,32,2) [] = { 0xdeadbe, 0xdeadbe };
+
+
+#define INSN vqshrun_n
+#define TEST_MSG "VQSHRUN_N"
+
+#define FNNAME1(NAME) void exec_ ## NAME (void)
+#define FNNAME(NAME) FNNAME1(NAME)
+
+FNNAME (INSN)
+{
+  /* Basic test: y=vqshrun_n(x,v), then store the result.  */
+#define TEST_VQSHRUN_N2(INSN, T1, T2, W, W2, N, V, EXPECTED_CUMULATIVE_SAT, CMT) \
+  Set_Neon_Cumulative_Sat(0, VECT_VAR(vector_res, uint, W2, N));	\
+  VECT_VAR(vector_res, uint, W2, N) =					\
+    INSN##_##T2##W(VECT_VAR(vector, T1, W, N),				\
+		   V);							\
+  vst1_u##W2(VECT_VAR(result, uint, W2, N),				\
+	     VECT_VAR(vector_res, uint, W2, N));			\
+  CHECK_CUMULATIVE_SAT(TEST_MSG, T1, W, N, EXPECTED_CUMULATIVE_SAT, CMT)
+
+  /* Two auxliary macros are necessary to expand INSN */
+#define TEST_VQSHRUN_N1(INSN, T1, T2, W, W2, N, V, EXPECTED_CUMULATIVE_SAT, CMT) \
+  TEST_VQSHRUN_N2(INSN, T1, T2, W, W2, N, V, EXPECTED_CUMULATIVE_SAT, CMT)
+
+#define TEST_VQSHRUN_N(T1, T2, W, W2, N, V, EXPECTED_CUMULATIVE_SAT, CMT) \
+  TEST_VQSHRUN_N1(INSN, T1, T2, W, W2, N, V, EXPECTED_CUMULATIVE_SAT, CMT)
+
+
+  /* vector is twice as large as vector_res.  */
+  DECL_VARIABLE(vector, int, 16, 8);
+  DECL_VARIABLE(vector, int, 32, 4);
+  DECL_VARIABLE(vector, int, 64, 2);
+
+  DECL_VARIABLE(vector_res, uint, 8, 8);
+  DECL_VARIABLE(vector_res, uint, 16, 4);
+  DECL_VARIABLE(vector_res, uint, 32, 2);
+
+  clean_results ();
+
+  /* Fill input vector with negative values, to check saturation on
+     limits.  */
+  VDUP(vector, q, int, s, 16, 8, -2);
+  VDUP(vector, q, int, s, 32, 4, -3);
+  VDUP(vector, q, int, s, 64, 2, -4);
+
+  /* Choose shift amount arbitrarily.  */
+#define CMT " (negative input)"
+  TEST_VQSHRUN_N(int, s, 16, 8, 8, 3, expected_cumulative_sat_neg, CMT);
+  TEST_VQSHRUN_N(int, s, 32, 16, 4, 4, expected_cumulative_sat_neg, CMT);
+  TEST_VQSHRUN_N(int, s, 64, 32, 2, 2, expected_cumulative_sat_neg, CMT);
+
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_neg, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_neg, CMT);
+
+  
+  /* Fill input vector with max value, to check saturation on
+     limits.  */
+  VDUP(vector, q, int, s, 16, 8, 0x7FFF);
+  VDUP(vector, q, int, s, 32, 4, 0x7FFFFFFF);
+  VDUP(vector, q, int, s, 64, 2, 0x7FFFFFFFFFFFFFFFLL);
+
+#undef CMT
+#define CMT " (check cumulative saturation)"
+  TEST_VQSHRUN_N(int, s, 16, 8, 8, 1, expected_cumulative_sat_max_sh1, CMT);
+  TEST_VQSHRUN_N(int, s, 32, 16, 4, 1, expected_cumulative_sat_max_sh1, CMT);
+  TEST_VQSHRUN_N(int, s, 64, 32, 2, 1, expected_cumulative_sat_max_sh1, CMT);
+
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_max_sh1, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_max_sh1, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_max_sh1, CMT);
+
+  
+  /* Fill input vector with positive values, to check normal case.  */
+  VDUP(vector, q, int, s, 16, 8, 0x1234);
+  VDUP(vector, q, int, s, 32, 4, 0x87654321);
+  VDUP(vector, q, int, s, 64, 2, 0xDEADBEEF);
+
+#undef CMT
+#define CMT ""
+  TEST_VQSHRUN_N(int, s, 16, 8, 8, 6, expected_cumulative_sat, CMT);
+  TEST_VQSHRUN_N(int, s, 32, 16, 4, 7, expected_cumulative_sat, CMT);
+  TEST_VQSHRUN_N(int, s, 64, 32, 2, 8, expected_cumulative_sat, CMT);
+
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected, CMT);
+}
+
+int main (void)
+{
+  exec_vqshrun_n ();
+  return 0;
+}
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vqsub.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vqsub.c
@@ -25,10 +25,6 @@ VECT_VAR_DECL(expected,uint,16,4) [] = { 0xff8a, 0xff8b,
 					 0xff8c, 0xff8d };
 VECT_VAR_DECL(expected,uint,32,2) [] = { 0xffffff79, 0xffffff7a };
 VECT_VAR_DECL(expected,uint,64,1) [] = { 0xffffffffffffff68 };
-VECT_VAR_DECL(expected,poly,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,2) [] = { 0x33333333, 0x33333333 };
 VECT_VAR_DECL(expected,int,8,16) [] = { 0xdf, 0xe0, 0xe1, 0xe2,
 					0xe3, 0xe4, 0xe5, 0xe6,
 					0xe7, 0xe8, 0xe9, 0xea,
@@ -49,14 +45,6 @@ VECT_VAR_DECL(expected,uint,32,4) [] = { 0xffffff79, 0xffffff7a,
 					 0xffffff7b, 0xffffff7c };
 VECT_VAR_DECL(expected,uint,64,2) [] = { 0xffffffffffffff68,
 					 0xffffffffffffff69 };
-VECT_VAR_DECL(expected,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,4) [] = { 0x33333333, 0x33333333,
-					   0x33333333, 0x33333333 };
 
 /* Expected values of cumulative saturation flag.  */
 int VECT_VAR(expected_cumulative_sat,int,8,8) = 0;
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vrecpe.c
@@ -0,0 +1,154 @@
+#include <arm_neon.h>
+#include "arm-neon-ref.h"
+#include "compute-ref-data.h"
+#include <math.h>
+
+/* Expected results with positive input.  */
+VECT_VAR_DECL(expected_positive,uint,32,2) [] = { 0xffffffff, 0xffffffff };
+VECT_VAR_DECL(expected_positive,uint,32,4) [] = { 0xbf000000, 0xbf000000,
+						  0xbf000000, 0xbf000000 };
+VECT_VAR_DECL(expected_positive,hfloat,32,2) [] = { 0x3f068000, 0x3f068000 };
+VECT_VAR_DECL(expected_positive,hfloat,32,4) [] = { 0x3c030000, 0x3c030000,
+						    0x3c030000, 0x3c030000 };
+
+/* Expected results with negative input.  */
+VECT_VAR_DECL(expected_negative,uint,32,2) [] = { 0x80000000, 0x80000000 };
+VECT_VAR_DECL(expected_negative,uint,32,4) [] = { 0xee800000, 0xee800000,
+						  0xee800000, 0xee800000 };
+VECT_VAR_DECL(expected_negative,hfloat,32,2) [] = { 0xbdcc8000, 0xbdcc8000 };
+VECT_VAR_DECL(expected_negative,hfloat,32,4) [] = { 0xbc030000, 0xbc030000,
+						    0xbc030000, 0xbc030000 };
+
+/* Expected results with FP special values (NaN, infinity).  */
+VECT_VAR_DECL(expected_fp1,hfloat,32,2) [] = { 0x7fc00000, 0x7fc00000 };
+VECT_VAR_DECL(expected_fp1,hfloat,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+
+/* Expected results with FP special values (zero, large value).  */
+VECT_VAR_DECL(expected_fp2,hfloat,32,2) [] = { 0x7f800000, 0x7f800000 };
+VECT_VAR_DECL(expected_fp2,hfloat,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+
+/* Expected results with FP special values (-0, -infinity).  */
+VECT_VAR_DECL(expected_fp3,hfloat,32,2) [] = { 0xff800000, 0xff800000 };
+VECT_VAR_DECL(expected_fp3,hfloat,32,4) [] = { 0x80000000, 0x80000000,
+					       0x80000000, 0x80000000 };
+
+/* Expected results with FP special large negative value.  */
+VECT_VAR_DECL(expected_fp4,hfloat,32,2) [] = { 0x80000000, 0x80000000 };
+
+#define TEST_MSG "VRECPE/VRECPEQ"
+void exec_vrecpe(void)
+{
+  int i;
+
+  /* Basic test: y=vrecpe(x), then store the result.  */
+#define TEST_VRECPE(Q, T1, T2, W, N)			\
+  VECT_VAR(vector_res, T1, W, N) =			\
+    vrecpe##Q##_##T2##W(VECT_VAR(vector, T1, W, N));	\
+  vst1##Q##_##T2##W(VECT_VAR(result, T1, W, N),		\
+		    VECT_VAR(vector_res, T1, W, N))
+
+  /* No need for 64 bits variants.  */
+  DECL_VARIABLE(vector, uint, 32, 2);
+  DECL_VARIABLE(vector, uint, 32, 4);
+  DECL_VARIABLE(vector, float, 32, 2);
+  DECL_VARIABLE(vector, float, 32, 4);
+
+  DECL_VARIABLE(vector_res, uint, 32, 2);
+  DECL_VARIABLE(vector_res, uint, 32, 4);
+  DECL_VARIABLE(vector_res, float, 32, 2);
+  DECL_VARIABLE(vector_res, float, 32, 4);
+
+  clean_results ();
+
+  /* Choose init value arbitrarily, positive.  */
+  VDUP(vector, , uint, u, 32, 2, 0x12345678);
+  VDUP(vector, , float, f, 32, 2, 1.9f);
+  VDUP(vector, q, uint, u, 32, 4, 0xABCDEF10);
+  VDUP(vector, q, float, f, 32, 4, 125.0f);
+
+  /* Apply the operator.  */
+  TEST_VRECPE(, uint, u, 32, 2);
+  TEST_VRECPE(, float, f, 32, 2);
+  TEST_VRECPE(q, uint, u, 32, 4);
+  TEST_VRECPE(q, float, f, 32, 4);
+
+#define CMT " (positive input)"
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_positive, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_positive, CMT);
+  CHECK_FP(TEST_MSG, float, 32, 2, PRIx32, expected_positive, CMT);
+  CHECK_FP(TEST_MSG, float, 32, 4, PRIx32, expected_positive, CMT);
+
+  /* Choose init value arbitrarily,negative.  */
+  VDUP(vector, , uint, u, 32, 2, 0xFFFFFFFF);
+  VDUP(vector, , float, f, 32, 2, -10.0f);
+  VDUP(vector, q, uint, u, 32, 4, 0x89081234);
+  VDUP(vector, q, float, f, 32, 4, -125.0f);
+
+  /* Apply the operator.  */
+  TEST_VRECPE(, uint, u, 32, 2);
+  TEST_VRECPE(, float, f, 32, 2);
+  TEST_VRECPE(q, uint, u, 32, 4);
+  TEST_VRECPE(q, float, f, 32, 4);
+
+#undef CMT
+#define CMT " (negative input)"
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_negative, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_negative, CMT);
+  CHECK_FP(TEST_MSG, float, 32, 2, PRIx32, expected_negative, CMT);
+  CHECK_FP(TEST_MSG, float, 32, 4, PRIx32, expected_negative, CMT);
+
+  /* Test FP variants with special input values (NaN, infinity).  */
+  VDUP(vector, , float, f, 32, 2, NAN);
+  VDUP(vector, q, float, f, 32, 4, HUGE_VALF);
+
+  /* Apply the operator.  */
+  TEST_VRECPE(, float, f, 32, 2);
+  TEST_VRECPE(q, float, f, 32, 4);
+
+#undef CMT
+#define CMT " FP special (NaN, infinity)"
+  CHECK_FP(TEST_MSG, float, 32, 2, PRIx32, expected_fp1, CMT);
+  CHECK_FP(TEST_MSG, float, 32, 4, PRIx32, expected_fp1, CMT);
+
+  /* Test FP variants with special input values (zero, large value).  */
+  VDUP(vector, , float, f, 32, 2, 0.0f);
+  VDUP(vector, q, float, f, 32, 4, 8.97229e37f /*9.0e37f*/);
+
+  /* Apply the operator.  */
+  TEST_VRECPE(, float, f, 32, 2);
+  TEST_VRECPE(q, float, f, 32, 4);
+
+#undef CMT
+#define CMT " FP special (zero, large value)"
+  CHECK_FP(TEST_MSG, float, 32, 2, PRIx32, expected_fp2, CMT);
+  CHECK_FP(TEST_MSG, float, 32, 4, PRIx32, expected_fp2, CMT);
+
+  /* Test FP variants with special input values (-0, -infinity).  */
+  VDUP(vector, , float, f, 32, 2, -0.0f);
+  VDUP(vector, q, float, f, 32, 4, -HUGE_VALF);
+
+  /* Apply the operator.  */
+  TEST_VRECPE(, float, f, 32, 2);
+  TEST_VRECPE(q, float, f, 32, 4);
+
+#undef CMT
+#define CMT " FP special (-0, -infinity)"
+  CHECK_FP(TEST_MSG, float, 32, 2, PRIx32, expected_fp3, CMT);
+  CHECK_FP(TEST_MSG, float, 32, 4, PRIx32, expected_fp3, CMT);
+
+  /* Test FP variants with special input values (large negative value).  */
+  VDUP(vector, , float, f, 32, 2, -9.0e37f);
+
+  /* Apply the operator.  */
+  TEST_VRECPE(, float, f, 32, 2);
+
+#undef CMT
+#define CMT " FP special (large negative value)"
+  CHECK_FP(TEST_MSG, float, 32, 2, PRIx32, expected_fp4, CMT);
+}
+
+int main (void)
+{
+  exec_vrecpe ();
+  return 0;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vrecps.c
@@ -0,0 +1,117 @@
+#include <arm_neon.h>
+#include "arm-neon-ref.h"
+#include "compute-ref-data.h"
+#include <math.h>
+
+/* Expected results with positive input.  */
+VECT_VAR_DECL(expected,hfloat,32,2) [] = { 0xc2e19eb7, 0xc2e19eb7 };
+VECT_VAR_DECL(expected,hfloat,32,4) [] = { 0xc1db851f, 0xc1db851f,
+					   0xc1db851f, 0xc1db851f };
+
+/* Expected results with FP special values (NaN).  */
+VECT_VAR_DECL(expected_fp1,hfloat,32,2) [] = { 0x7fc00000, 0x7fc00000 };
+VECT_VAR_DECL(expected_fp1,hfloat,32,4) [] = { 0x7fc00000, 0x7fc00000,
+					       0x7fc00000, 0x7fc00000 };
+
+/* Expected results with FP special values (infinity, 0) and normal
+   values.  */
+VECT_VAR_DECL(expected_fp2,hfloat,32,2) [] = { 0xff800000, 0xff800000 };
+VECT_VAR_DECL(expected_fp2,hfloat,32,4) [] = { 0x40000000, 0x40000000,
+					       0x40000000, 0x40000000 };
+
+/* Expected results with FP special values (infinity, 0).  */
+VECT_VAR_DECL(expected_fp3,hfloat,32,2) [] = { 0x40000000, 0x40000000 };
+VECT_VAR_DECL(expected_fp3,hfloat,32,4) [] = { 0x40000000, 0x40000000,
+					       0x40000000, 0x40000000 };
+
+#define TEST_MSG "VRECPS/VRECPSQ"
+void exec_vrecps(void)
+{
+  int i;
+
+  /* Basic test: y=vrecps(x), then store the result.  */
+#define TEST_VRECPS(Q, T1, T2, W, N)			\
+  VECT_VAR(vector_res, T1, W, N) =			\
+    vrecps##Q##_##T2##W(VECT_VAR(vector, T1, W, N),	\
+			VECT_VAR(vector2, T1, W, N));	\
+  vst1##Q##_##T2##W(VECT_VAR(result, T1, W, N),		\
+		    VECT_VAR(vector_res, T1, W, N))
+
+  /* No need for integer variants.  */
+  DECL_VARIABLE(vector, float, 32, 2);
+  DECL_VARIABLE(vector, float, 32, 4);
+
+  DECL_VARIABLE(vector2, float, 32, 2);
+  DECL_VARIABLE(vector2, float, 32, 4);
+
+  DECL_VARIABLE(vector_res, float, 32, 2);
+  DECL_VARIABLE(vector_res, float, 32, 4);
+
+  clean_results ();
+
+  /* Choose init value arbitrarily.  */
+  VDUP(vector, , float, f, 32, 2, 12.9f);
+  VDUP(vector, q, float, f, 32, 4, 9.2f);
+
+  VDUP(vector2, , float, f, 32, 2, 8.9f);
+  VDUP(vector2, q, float, f, 32, 4, 3.2f);
+
+  /* Apply the operator.  */
+  TEST_VRECPS(, float, f, 32, 2);
+  TEST_VRECPS(q, float, f, 32, 4);
+
+#define CMT " (positive input)"
+  CHECK_FP(TEST_MSG, float, 32, 2, PRIx32, expected, CMT);
+  CHECK_FP(TEST_MSG, float, 32, 4, PRIx32, expected, CMT);
+
+
+  /* Test FP variants with special input values (NaN).  */
+  VDUP(vector, , float, f, 32, 2, NAN);
+  VDUP(vector2, q, float, f, 32, 4, NAN);
+
+  /* Apply the operator.  */
+  TEST_VRECPS(, float, f, 32, 2);
+  TEST_VRECPS(q, float, f, 32, 4);
+
+#undef CMT
+#define CMT " FP special (NaN)"
+  CHECK_FP(TEST_MSG, float, 32, 2, PRIx32, expected_fp1, CMT);
+  CHECK_FP(TEST_MSG, float, 32, 4, PRIx32, expected_fp1, CMT);
+
+
+  /* Test FP variants with special input values (infinity, 0).  */
+  VDUP(vector, , float, f, 32, 2, HUGE_VALF);
+  VDUP(vector, q, float, f, 32, 4, 0.0f);
+  VDUP(vector2, q, float, f, 32, 4, 3.2f); /* Restore a normal value.  */
+
+  /* Apply the operator.  */
+  TEST_VRECPS(, float, f, 32, 2);
+  TEST_VRECPS(q, float, f, 32, 4);
+
+#undef CMT
+#define CMT " FP special (infinity, 0) and normal value"
+  CHECK_FP(TEST_MSG, float, 32, 2, PRIx32, expected_fp2, CMT);
+  CHECK_FP(TEST_MSG, float, 32, 4, PRIx32, expected_fp2, CMT);
+
+
+  /* Test FP variants with only special input values (infinity, 0).  */
+  VDUP(vector, , float, f, 32, 2, HUGE_VALF);
+  VDUP(vector, q, float, f, 32, 4, 0.0f);
+  VDUP(vector2, , float, f, 32, 2, 0.0f);
+  VDUP(vector2, q, float, f, 32, 4, HUGE_VALF);
+
+  /* Apply the operator */
+  TEST_VRECPS(, float, f, 32, 2);
+  TEST_VRECPS(q, float, f, 32, 4);
+
+#undef CMT
+#define CMT " FP special (infinity, 0)"
+  CHECK_FP(TEST_MSG, float, 32, 2, PRIx32, expected_fp3, CMT);
+  CHECK_FP(TEST_MSG, float, 32, 4, PRIx32, expected_fp3, CMT);
+}
+
+int main (void)
+{
+  exec_vrecps ();
+  return 0;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vreinterpret.c
@@ -0,0 +1,741 @@
+#include <arm_neon.h>
+#include "arm-neon-ref.h"
+#include "compute-ref-data.h"
+
+/* Expected results for vreinterpret_s8_xx.  */
+VECT_VAR_DECL(expected_s8_1,int,8,8) [] = { 0xf0, 0xff, 0xf1, 0xff,
+					    0xf2, 0xff, 0xf3, 0xff };
+VECT_VAR_DECL(expected_s8_2,int,8,8) [] = { 0xf0, 0xff, 0xff, 0xff,
+					    0xf1, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_s8_3,int,8,8) [] = { 0xf0, 0xff, 0xff, 0xff,
+					    0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_s8_4,int,8,8) [] = { 0xf0, 0xf1, 0xf2, 0xf3,
+					    0xf4, 0xf5, 0xf6, 0xf7 };
+VECT_VAR_DECL(expected_s8_5,int,8,8) [] = { 0xf0, 0xff, 0xf1, 0xff,
+					    0xf2, 0xff, 0xf3, 0xff };
+VECT_VAR_DECL(expected_s8_6,int,8,8) [] = { 0xf0, 0xff, 0xff, 0xff,
+					    0xf1, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_s8_7,int,8,8) [] = { 0xf0, 0xff, 0xff, 0xff,
+					    0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_s8_8,int,8,8) [] = { 0xf0, 0xf1, 0xf2, 0xf3,
+					    0xf4, 0xf5, 0xf6, 0xf7 };
+VECT_VAR_DECL(expected_s8_9,int,8,8) [] = { 0xf0, 0xff, 0xf1, 0xff,
+					    0xf2, 0xff, 0xf3, 0xff };
+
+/* Expected results for vreinterpret_s16_xx.  */
+VECT_VAR_DECL(expected_s16_1,int,16,4) [] = { 0xf1f0, 0xf3f2, 0xf5f4, 0xf7f6 };
+VECT_VAR_DECL(expected_s16_2,int,16,4) [] = { 0xfff0, 0xffff, 0xfff1, 0xffff };
+VECT_VAR_DECL(expected_s16_3,int,16,4) [] = { 0xfff0, 0xffff, 0xffff, 0xffff };
+VECT_VAR_DECL(expected_s16_4,int,16,4) [] = { 0xf1f0, 0xf3f2, 0xf5f4, 0xf7f6 };
+VECT_VAR_DECL(expected_s16_5,int,16,4) [] = { 0xfff0, 0xfff1, 0xfff2, 0xfff3 };
+VECT_VAR_DECL(expected_s16_6,int,16,4) [] = { 0xfff0, 0xffff, 0xfff1, 0xffff };
+VECT_VAR_DECL(expected_s16_7,int,16,4) [] = { 0xfff0, 0xffff, 0xffff, 0xffff };
+VECT_VAR_DECL(expected_s16_8,int,16,4) [] = { 0xf1f0, 0xf3f2, 0xf5f4, 0xf7f6 };
+VECT_VAR_DECL(expected_s16_9,int,16,4) [] = { 0xfff0, 0xfff1, 0xfff2, 0xfff3 };
+
+/* Expected results for vreinterpret_s32_xx.  */
+VECT_VAR_DECL(expected_s32_1,int,32,2) [] = { 0xf3f2f1f0, 0xf7f6f5f4 };
+VECT_VAR_DECL(expected_s32_2,int,32,2) [] = { 0xfff1fff0, 0xfff3fff2 };
+VECT_VAR_DECL(expected_s32_3,int,32,2) [] = { 0xfffffff0, 0xffffffff };
+VECT_VAR_DECL(expected_s32_4,int,32,2) [] = { 0xf3f2f1f0, 0xf7f6f5f4 };
+VECT_VAR_DECL(expected_s32_5,int,32,2) [] = { 0xfff1fff0, 0xfff3fff2 };
+VECT_VAR_DECL(expected_s32_6,int,32,2) [] = { 0xfffffff0, 0xfffffff1 };
+VECT_VAR_DECL(expected_s32_7,int,32,2) [] = { 0xfffffff0, 0xffffffff };
+VECT_VAR_DECL(expected_s32_8,int,32,2) [] = { 0xf3f2f1f0, 0xf7f6f5f4 };
+VECT_VAR_DECL(expected_s32_9,int,32,2) [] = { 0xfff1fff0, 0xfff3fff2 };
+
+/* Expected results for vreinterpret_s64_xx.  */
+VECT_VAR_DECL(expected_s64_1,int,64,1) [] = { 0xf7f6f5f4f3f2f1f0 };
+VECT_VAR_DECL(expected_s64_2,int,64,1) [] = { 0xfff3fff2fff1fff0 };
+VECT_VAR_DECL(expected_s64_3,int,64,1) [] = { 0xfffffff1fffffff0 };
+VECT_VAR_DECL(expected_s64_4,int,64,1) [] = { 0xf7f6f5f4f3f2f1f0 };
+VECT_VAR_DECL(expected_s64_5,int,64,1) [] = { 0xfff3fff2fff1fff0 };
+VECT_VAR_DECL(expected_s64_6,int,64,1) [] = { 0xfffffff1fffffff0 };
+VECT_VAR_DECL(expected_s64_7,int,64,1) [] = { 0xfffffffffffffff0 };
+VECT_VAR_DECL(expected_s64_8,int,64,1) [] = { 0xf7f6f5f4f3f2f1f0 };
+VECT_VAR_DECL(expected_s64_9,int,64,1) [] = { 0xfff3fff2fff1fff0 };
+
+/* Expected results for vreinterpret_u8_xx.  */
+VECT_VAR_DECL(expected_u8_1,uint,8,8) [] = { 0xf0, 0xf1, 0xf2, 0xf3,
+					     0xf4, 0xf5, 0xf6, 0xf7 };
+VECT_VAR_DECL(expected_u8_2,uint,8,8) [] = { 0xf0, 0xff, 0xf1, 0xff,
+					     0xf2, 0xff, 0xf3, 0xff };
+VECT_VAR_DECL(expected_u8_3,uint,8,8) [] = { 0xf0, 0xff, 0xff, 0xff,
+					     0xf1, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_u8_4,uint,8,8) [] = { 0xf0, 0xff, 0xff, 0xff,
+					     0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_u8_5,uint,8,8) [] = { 0xf0, 0xff, 0xf1, 0xff,
+					     0xf2, 0xff, 0xf3, 0xff };
+VECT_VAR_DECL(expected_u8_6,uint,8,8) [] = { 0xf0, 0xff, 0xff, 0xff,
+					     0xf1, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_u8_7,uint,8,8) [] = { 0xf0, 0xff, 0xff, 0xff,
+					     0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_u8_8,uint,8,8) [] = { 0xf0, 0xf1, 0xf2, 0xf3,
+					     0xf4, 0xf5, 0xf6, 0xf7 };
+VECT_VAR_DECL(expected_u8_9,uint,8,8) [] = { 0xf0, 0xff, 0xf1, 0xff,
+					     0xf2, 0xff, 0xf3, 0xff };
+
+/* Expected results for vreinterpret_u16_xx.  */
+VECT_VAR_DECL(expected_u16_1,uint,16,4) [] = { 0xf1f0, 0xf3f2, 0xf5f4, 0xf7f6 };
+VECT_VAR_DECL(expected_u16_2,uint,16,4) [] = { 0xfff0, 0xfff1, 0xfff2, 0xfff3 };
+VECT_VAR_DECL(expected_u16_3,uint,16,4) [] = { 0xfff0, 0xffff, 0xfff1, 0xffff };
+VECT_VAR_DECL(expected_u16_4,uint,16,4) [] = { 0xfff0, 0xffff, 0xffff, 0xffff };
+VECT_VAR_DECL(expected_u16_5,uint,16,4) [] = { 0xf1f0, 0xf3f2, 0xf5f4, 0xf7f6 };
+VECT_VAR_DECL(expected_u16_6,uint,16,4) [] = { 0xfff0, 0xffff, 0xfff1, 0xffff };
+VECT_VAR_DECL(expected_u16_7,uint,16,4) [] = { 0xfff0, 0xffff, 0xffff, 0xffff };
+VECT_VAR_DECL(expected_u16_8,uint,16,4) [] = { 0xf1f0, 0xf3f2, 0xf5f4, 0xf7f6 };
+VECT_VAR_DECL(expected_u16_9,uint,16,4) [] = { 0xfff0, 0xfff1, 0xfff2, 0xfff3 };
+
+/* Expected results for vreinterpret_u32_xx.  */
+VECT_VAR_DECL(expected_u32_1,uint,32,2) [] = { 0xf3f2f1f0, 0xf7f6f5f4 };
+VECT_VAR_DECL(expected_u32_2,uint,32,2) [] = { 0xfff1fff0, 0xfff3fff2 };
+VECT_VAR_DECL(expected_u32_3,uint,32,2) [] = { 0xfffffff0, 0xfffffff1 };
+VECT_VAR_DECL(expected_u32_4,uint,32,2) [] = { 0xfffffff0, 0xffffffff };
+VECT_VAR_DECL(expected_u32_5,uint,32,2) [] = { 0xf3f2f1f0, 0xf7f6f5f4 };
+VECT_VAR_DECL(expected_u32_6,uint,32,2) [] = { 0xfff1fff0, 0xfff3fff2 };
+VECT_VAR_DECL(expected_u32_7,uint,32,2) [] = { 0xfffffff0, 0xffffffff };
+VECT_VAR_DECL(expected_u32_8,uint,32,2) [] = { 0xf3f2f1f0, 0xf7f6f5f4 };
+VECT_VAR_DECL(expected_u32_9,uint,32,2) [] = { 0xfff1fff0, 0xfff3fff2 };
+
+/* Expected results for vreinterpret_u64_xx.  */
+VECT_VAR_DECL(expected_u64_1,uint,64,1) [] = { 0xf7f6f5f4f3f2f1f0 };
+VECT_VAR_DECL(expected_u64_2,uint,64,1) [] = { 0xfff3fff2fff1fff0 };
+VECT_VAR_DECL(expected_u64_3,uint,64,1) [] = { 0xfffffff1fffffff0 };
+VECT_VAR_DECL(expected_u64_4,uint,64,1) [] = { 0xfffffffffffffff0 };
+VECT_VAR_DECL(expected_u64_5,uint,64,1) [] = { 0xf7f6f5f4f3f2f1f0 };
+VECT_VAR_DECL(expected_u64_6,uint,64,1) [] = { 0xfff3fff2fff1fff0 };
+VECT_VAR_DECL(expected_u64_7,uint,64,1) [] = { 0xfffffff1fffffff0 };
+VECT_VAR_DECL(expected_u64_8,uint,64,1) [] = { 0xf7f6f5f4f3f2f1f0 };
+VECT_VAR_DECL(expected_u64_9,uint,64,1) [] = { 0xfff3fff2fff1fff0 };
+
+/* Expected results for vreinterpret_p8_xx.  */
+VECT_VAR_DECL(expected_p8_1,poly,8,8) [] = { 0xf0, 0xf1, 0xf2, 0xf3,
+					     0xf4, 0xf5, 0xf6, 0xf7 };
+VECT_VAR_DECL(expected_p8_2,poly,8,8) [] = { 0xf0, 0xff, 0xf1, 0xff,
+					     0xf2, 0xff, 0xf3, 0xff };
+VECT_VAR_DECL(expected_p8_3,poly,8,8) [] = { 0xf0, 0xff, 0xff, 0xff,
+					     0xf1, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_p8_4,poly,8,8) [] = { 0xf0, 0xff, 0xff, 0xff,
+					     0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_p8_5,poly,8,8) [] = { 0xf0, 0xf1, 0xf2, 0xf3,
+					     0xf4, 0xf5, 0xf6, 0xf7 };
+VECT_VAR_DECL(expected_p8_6,poly,8,8) [] = { 0xf0, 0xff, 0xf1, 0xff,
+					     0xf2, 0xff, 0xf3, 0xff };
+VECT_VAR_DECL(expected_p8_7,poly,8,8) [] = { 0xf0, 0xff, 0xff, 0xff,
+					     0xf1, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_p8_8,poly,8,8) [] = { 0xf0, 0xff, 0xff, 0xff,
+					     0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_p8_9,poly,8,8) [] = { 0xf0, 0xff, 0xf1, 0xff,
+					     0xf2, 0xff, 0xf3, 0xff };
+
+/* Expected results for vreinterpret_p16_xx.  */
+VECT_VAR_DECL(expected_p16_1,poly,16,4) [] = { 0xf1f0, 0xf3f2, 0xf5f4, 0xf7f6 };
+VECT_VAR_DECL(expected_p16_2,poly,16,4) [] = { 0xfff0, 0xfff1, 0xfff2, 0xfff3 };
+VECT_VAR_DECL(expected_p16_3,poly,16,4) [] = { 0xfff0, 0xffff, 0xfff1, 0xffff };
+VECT_VAR_DECL(expected_p16_4,poly,16,4) [] = { 0xfff0, 0xffff, 0xffff, 0xffff };
+VECT_VAR_DECL(expected_p16_5,poly,16,4) [] = { 0xf1f0, 0xf3f2, 0xf5f4, 0xf7f6 };
+VECT_VAR_DECL(expected_p16_6,poly,16,4) [] = { 0xfff0, 0xfff1, 0xfff2, 0xfff3 };
+VECT_VAR_DECL(expected_p16_7,poly,16,4) [] = { 0xfff0, 0xffff, 0xfff1, 0xffff };
+VECT_VAR_DECL(expected_p16_8,poly,16,4) [] = { 0xfff0, 0xffff, 0xffff, 0xffff };
+VECT_VAR_DECL(expected_p16_9,poly,16,4) [] = { 0xf1f0, 0xf3f2, 0xf5f4, 0xf7f6 };
+
+/* Expected results for vreinterpretq_s8_xx.  */
+VECT_VAR_DECL(expected_q_s8_1,int,8,16) [] = { 0xf0, 0xff, 0xf1, 0xff,
+					       0xf2, 0xff, 0xf3, 0xff,
+					       0xf4, 0xff, 0xf5, 0xff,
+					       0xf6, 0xff, 0xf7, 0xff };
+VECT_VAR_DECL(expected_q_s8_2,int,8,16) [] = { 0xf0, 0xff, 0xff, 0xff,
+					       0xf1, 0xff, 0xff, 0xff,
+					       0xf2, 0xff, 0xff, 0xff,
+					       0xf3, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_q_s8_3,int,8,16) [] = { 0xf0, 0xff, 0xff, 0xff,
+					       0xff, 0xff, 0xff, 0xff,
+					       0xf1, 0xff, 0xff, 0xff,
+					       0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_q_s8_4,int,8,16) [] = { 0xf0, 0xf1, 0xf2, 0xf3,
+					       0xf4, 0xf5, 0xf6, 0xf7,
+					       0xf8, 0xf9, 0xfa, 0xfb,
+					       0xfc, 0xfd, 0xfe, 0xff };
+VECT_VAR_DECL(expected_q_s8_5,int,8,16) [] = { 0xf0, 0xff, 0xf1, 0xff,
+					       0xf2, 0xff, 0xf3, 0xff,
+					       0xf4, 0xff, 0xf5, 0xff,
+					       0xf6, 0xff, 0xf7, 0xff };
+VECT_VAR_DECL(expected_q_s8_6,int,8,16) [] = { 0xf0, 0xff, 0xff, 0xff,
+					       0xf1, 0xff, 0xff, 0xff,
+					       0xf2, 0xff, 0xff, 0xff,
+					       0xf3, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_q_s8_7,int,8,16) [] = { 0xf0, 0xff, 0xff, 0xff,
+					       0xff, 0xff, 0xff, 0xff,
+					       0xf1, 0xff, 0xff, 0xff,
+					       0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_q_s8_8,int,8,16) [] = { 0xf0, 0xf1, 0xf2, 0xf3,
+					       0xf4, 0xf5, 0xf6, 0xf7,
+					       0xf8, 0xf9, 0xfa, 0xfb,
+					       0xfc, 0xfd, 0xfe, 0xff };
+VECT_VAR_DECL(expected_q_s8_9,int,8,16) [] = { 0xf0, 0xff, 0xf1, 0xff,
+					       0xf2, 0xff, 0xf3, 0xff,
+					       0xf4, 0xff, 0xf5, 0xff,
+					       0xf6, 0xff, 0xf7, 0xff };
+
+/* Expected results for vreinterpretq_s16_xx.  */
+VECT_VAR_DECL(expected_q_s16_1,int,16,8) [] = { 0xf1f0, 0xf3f2,
+						0xf5f4, 0xf7f6,
+						0xf9f8, 0xfbfa,
+						0xfdfc, 0xfffe };
+VECT_VAR_DECL(expected_q_s16_2,int,16,8) [] = { 0xfff0, 0xffff,
+						0xfff1, 0xffff,
+						0xfff2, 0xffff,
+						0xfff3, 0xffff };
+VECT_VAR_DECL(expected_q_s16_3,int,16,8) [] = { 0xfff0, 0xffff,
+						0xffff, 0xffff,
+						0xfff1, 0xffff,
+						0xffff, 0xffff };
+VECT_VAR_DECL(expected_q_s16_4,int,16,8) [] = { 0xf1f0, 0xf3f2,
+						0xf5f4, 0xf7f6,
+						0xf9f8, 0xfbfa,
+						0xfdfc, 0xfffe };
+VECT_VAR_DECL(expected_q_s16_5,int,16,8) [] = { 0xfff0, 0xfff1,
+						0xfff2, 0xfff3,
+						0xfff4, 0xfff5,
+						0xfff6, 0xfff7 };
+VECT_VAR_DECL(expected_q_s16_6,int,16,8) [] = { 0xfff0, 0xffff,
+						0xfff1, 0xffff,
+						0xfff2, 0xffff,
+						0xfff3, 0xffff };
+VECT_VAR_DECL(expected_q_s16_7,int,16,8) [] = { 0xfff0, 0xffff,
+						0xffff, 0xffff,
+						0xfff1, 0xffff,
+						0xffff, 0xffff };
+VECT_VAR_DECL(expected_q_s16_8,int,16,8) [] = { 0xf1f0, 0xf3f2,
+						0xf5f4, 0xf7f6,
+						0xf9f8, 0xfbfa,
+						0xfdfc, 0xfffe };
+VECT_VAR_DECL(expected_q_s16_9,int,16,8) [] = { 0xfff0, 0xfff1,
+						0xfff2, 0xfff3,
+						0xfff4, 0xfff5,
+						0xfff6, 0xfff7 };
+
+/* Expected results for vreinterpretq_s32_xx.  */
+VECT_VAR_DECL(expected_q_s32_1,int,32,4) [] = { 0xf3f2f1f0, 0xf7f6f5f4,
+						0xfbfaf9f8, 0xfffefdfc };
+VECT_VAR_DECL(expected_q_s32_2,int,32,4) [] = { 0xfff1fff0, 0xfff3fff2,
+						0xfff5fff4, 0xfff7fff6 };
+VECT_VAR_DECL(expected_q_s32_3,int,32,4) [] = { 0xfffffff0, 0xffffffff,
+						0xfffffff1, 0xffffffff };
+VECT_VAR_DECL(expected_q_s32_4,int,32,4) [] = { 0xf3f2f1f0, 0xf7f6f5f4,
+						0xfbfaf9f8, 0xfffefdfc };
+VECT_VAR_DECL(expected_q_s32_5,int,32,4) [] = { 0xfff1fff0, 0xfff3fff2,
+						0xfff5fff4, 0xfff7fff6 };
+VECT_VAR_DECL(expected_q_s32_6,int,32,4) [] = { 0xfffffff0, 0xfffffff1,
+						0xfffffff2, 0xfffffff3 };
+VECT_VAR_DECL(expected_q_s32_7,int,32,4) [] = { 0xfffffff0, 0xffffffff,
+						0xfffffff1, 0xffffffff };
+VECT_VAR_DECL(expected_q_s32_8,int,32,4) [] = { 0xf3f2f1f0, 0xf7f6f5f4,
+						0xfbfaf9f8, 0xfffefdfc };
+VECT_VAR_DECL(expected_q_s32_9,int,32,4) [] = { 0xfff1fff0, 0xfff3fff2,
+						0xfff5fff4, 0xfff7fff6 };
+
+/* Expected results for vreinterpretq_s64_xx.  */
+VECT_VAR_DECL(expected_q_s64_1,int,64,2) [] = { 0xf7f6f5f4f3f2f1f0,
+						0xfffefdfcfbfaf9f8 };
+VECT_VAR_DECL(expected_q_s64_2,int,64,2) [] = { 0xfff3fff2fff1fff0,
+						0xfff7fff6fff5fff4 };
+VECT_VAR_DECL(expected_q_s64_3,int,64,2) [] = { 0xfffffff1fffffff0,
+						0xfffffff3fffffff2 };
+VECT_VAR_DECL(expected_q_s64_4,int,64,2) [] = { 0xf7f6f5f4f3f2f1f0,
+						0xfffefdfcfbfaf9f8 };
+VECT_VAR_DECL(expected_q_s64_5,int,64,2) [] = { 0xfff3fff2fff1fff0,
+						0xfff7fff6fff5fff4 };
+VECT_VAR_DECL(expected_q_s64_6,int,64,2) [] = { 0xfffffff1fffffff0,
+						0xfffffff3fffffff2 };
+VECT_VAR_DECL(expected_q_s64_7,int,64,2) [] = { 0xfffffffffffffff0,
+						0xfffffffffffffff1 };
+VECT_VAR_DECL(expected_q_s64_8,int,64,2) [] = { 0xf7f6f5f4f3f2f1f0,
+						0xfffefdfcfbfaf9f8 };
+VECT_VAR_DECL(expected_q_s64_9,int,64,2) [] = { 0xfff3fff2fff1fff0,
+						0xfff7fff6fff5fff4 };
+
+/* Expected results for vreinterpretq_u8_xx.  */
+VECT_VAR_DECL(expected_q_u8_1,uint,8,16) [] = { 0xf0, 0xf1, 0xf2, 0xf3,
+						0xf4, 0xf5, 0xf6, 0xf7,
+						0xf8, 0xf9, 0xfa, 0xfb,
+						0xfc, 0xfd, 0xfe, 0xff };
+VECT_VAR_DECL(expected_q_u8_2,uint,8,16) [] = { 0xf0, 0xff, 0xf1, 0xff,
+						0xf2, 0xff, 0xf3, 0xff,
+						0xf4, 0xff, 0xf5, 0xff,
+						0xf6, 0xff, 0xf7, 0xff };
+VECT_VAR_DECL(expected_q_u8_3,uint,8,16) [] = { 0xf0, 0xff, 0xff, 0xff,
+						0xf1, 0xff, 0xff, 0xff,
+						0xf2, 0xff, 0xff, 0xff,
+						0xf3, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_q_u8_4,uint,8,16) [] = { 0xf0, 0xff, 0xff, 0xff,
+						0xff, 0xff, 0xff, 0xff,
+						0xf1, 0xff, 0xff, 0xff,
+						0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_q_u8_5,uint,8,16) [] = { 0xf0, 0xff, 0xf1, 0xff,
+						0xf2, 0xff, 0xf3, 0xff,
+						0xf4, 0xff, 0xf5, 0xff,
+						0xf6, 0xff, 0xf7, 0xff };
+VECT_VAR_DECL(expected_q_u8_6,uint,8,16) [] = { 0xf0, 0xff, 0xff, 0xff,
+						0xf1, 0xff, 0xff, 0xff,
+						0xf2, 0xff, 0xff, 0xff,
+						0xf3, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_q_u8_7,uint,8,16) [] = { 0xf0, 0xff, 0xff, 0xff,
+						0xff, 0xff, 0xff, 0xff,
+						0xf1, 0xff, 0xff, 0xff,
+						0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_q_u8_8,uint,8,16) [] = { 0xf0, 0xf1, 0xf2, 0xf3,
+						0xf4, 0xf5, 0xf6, 0xf7,
+						0xf8, 0xf9, 0xfa, 0xfb,
+						0xfc, 0xfd, 0xfe, 0xff };
+VECT_VAR_DECL(expected_q_u8_9,uint,8,16) [] = { 0xf0, 0xff, 0xf1, 0xff,
+						0xf2, 0xff, 0xf3, 0xff,
+						0xf4, 0xff, 0xf5, 0xff,
+						0xf6, 0xff, 0xf7, 0xff };
+
+/* Expected results for vreinterpretq_u16_xx.  */
+VECT_VAR_DECL(expected_q_u16_1,uint,16,8) [] = { 0xf1f0, 0xf3f2,
+						 0xf5f4, 0xf7f6,
+						 0xf9f8, 0xfbfa,
+						 0xfdfc, 0xfffe };
+VECT_VAR_DECL(expected_q_u16_2,uint,16,8) [] = { 0xfff0, 0xfff1,
+						 0xfff2, 0xfff3,
+						 0xfff4, 0xfff5,
+						 0xfff6, 0xfff7 };
+VECT_VAR_DECL(expected_q_u16_3,uint,16,8) [] = { 0xfff0, 0xffff,
+						 0xfff1, 0xffff,
+						 0xfff2, 0xffff,
+						 0xfff3, 0xffff };
+VECT_VAR_DECL(expected_q_u16_4,uint,16,8) [] = { 0xfff0, 0xffff,
+						 0xffff, 0xffff,
+						 0xfff1, 0xffff,
+						 0xffff, 0xffff };
+VECT_VAR_DECL(expected_q_u16_5,uint,16,8) [] = { 0xf1f0, 0xf3f2,
+						 0xf5f4, 0xf7f6,
+						 0xf9f8, 0xfbfa,
+						 0xfdfc, 0xfffe };
+VECT_VAR_DECL(expected_q_u16_6,uint,16,8) [] = { 0xfff0, 0xffff,
+						 0xfff1, 0xffff,
+						 0xfff2, 0xffff,
+						 0xfff3, 0xffff };
+VECT_VAR_DECL(expected_q_u16_7,uint,16,8) [] = { 0xfff0, 0xffff,
+						 0xffff, 0xffff,
+						 0xfff1, 0xffff,
+						 0xffff, 0xffff };
+VECT_VAR_DECL(expected_q_u16_8,uint,16,8) [] = { 0xf1f0, 0xf3f2,
+						 0xf5f4, 0xf7f6,
+						 0xf9f8, 0xfbfa,
+						 0xfdfc, 0xfffe };
+VECT_VAR_DECL(expected_q_u16_9,uint,16,8) [] = { 0xfff0, 0xfff1,
+						 0xfff2, 0xfff3,
+						 0xfff4, 0xfff5,
+						 0xfff6, 0xfff7 };
+
+/* Expected results for vreinterpretq_u32_xx.  */
+VECT_VAR_DECL(expected_q_u32_1,uint,32,4) [] = { 0xf3f2f1f0, 0xf7f6f5f4,
+						 0xfbfaf9f8, 0xfffefdfc };
+VECT_VAR_DECL(expected_q_u32_2,uint,32,4) [] = { 0xfff1fff0, 0xfff3fff2,
+						 0xfff5fff4, 0xfff7fff6 };
+VECT_VAR_DECL(expected_q_u32_3,uint,32,4) [] = { 0xfffffff0, 0xfffffff1,
+						 0xfffffff2, 0xfffffff3 };
+VECT_VAR_DECL(expected_q_u32_4,uint,32,4) [] = { 0xfffffff0, 0xffffffff,
+						 0xfffffff1, 0xffffffff };
+VECT_VAR_DECL(expected_q_u32_5,uint,32,4) [] = { 0xf3f2f1f0, 0xf7f6f5f4,
+						 0xfbfaf9f8, 0xfffefdfc };
+VECT_VAR_DECL(expected_q_u32_6,uint,32,4) [] = { 0xfff1fff0, 0xfff3fff2,
+						 0xfff5fff4, 0xfff7fff6 };
+VECT_VAR_DECL(expected_q_u32_7,uint,32,4) [] = { 0xfffffff0, 0xffffffff,
+						 0xfffffff1, 0xffffffff };
+VECT_VAR_DECL(expected_q_u32_8,uint,32,4) [] = { 0xf3f2f1f0, 0xf7f6f5f4,
+						 0xfbfaf9f8, 0xfffefdfc };
+VECT_VAR_DECL(expected_q_u32_9,uint,32,4) [] = { 0xfff1fff0, 0xfff3fff2,
+						 0xfff5fff4, 0xfff7fff6 };
+
+/* Expected results for vreinterpretq_u64_xx.  */
+VECT_VAR_DECL(expected_q_u64_1,uint,64,2) [] = { 0xf7f6f5f4f3f2f1f0,
+						0xfffefdfcfbfaf9f8 };
+VECT_VAR_DECL(expected_q_u64_2,uint,64,2) [] = { 0xfff3fff2fff1fff0,
+						0xfff7fff6fff5fff4 };
+VECT_VAR_DECL(expected_q_u64_3,uint,64,2) [] = { 0xfffffff1fffffff0,
+						0xfffffff3fffffff2 };
+VECT_VAR_DECL(expected_q_u64_4,uint,64,2) [] = { 0xfffffffffffffff0,
+						0xfffffffffffffff1 };
+VECT_VAR_DECL(expected_q_u64_5,uint,64,2) [] = { 0xf7f6f5f4f3f2f1f0,
+						0xfffefdfcfbfaf9f8 };
+VECT_VAR_DECL(expected_q_u64_6,uint,64,2) [] = { 0xfff3fff2fff1fff0,
+						0xfff7fff6fff5fff4 };
+VECT_VAR_DECL(expected_q_u64_7,uint,64,2) [] = { 0xfffffff1fffffff0,
+						0xfffffff3fffffff2 };
+VECT_VAR_DECL(expected_q_u64_8,uint,64,2) [] = { 0xf7f6f5f4f3f2f1f0,
+						0xfffefdfcfbfaf9f8 };
+VECT_VAR_DECL(expected_q_u64_9,uint,64,2) [] = { 0xfff3fff2fff1fff0,
+						 0xfff7fff6fff5fff4 };
+
+/* Expected results for vreinterpret_f32_xx.  */
+VECT_VAR_DECL(expected_f32_1,hfloat,32,2) [] = { 0xf3f2f1f0, 0xf7f6f5f4 };
+VECT_VAR_DECL(expected_f32_2,hfloat,32,2) [] = { 0xfff1fff0, 0xfff3fff2 };
+VECT_VAR_DECL(expected_f32_3,hfloat,32,2) [] = { 0xfffffff0, 0xfffffff1 };
+VECT_VAR_DECL(expected_f32_4,hfloat,32,2) [] = { 0xfffffff0, 0xffffffff };
+VECT_VAR_DECL(expected_f32_5,hfloat,32,2) [] = { 0xf3f2f1f0, 0xf7f6f5f4 };
+VECT_VAR_DECL(expected_f32_6,hfloat,32,2) [] = { 0xfff1fff0, 0xfff3fff2 };
+VECT_VAR_DECL(expected_f32_7,hfloat,32,2) [] = { 0xfffffff0, 0xfffffff1 };
+VECT_VAR_DECL(expected_f32_8,hfloat,32,2) [] = { 0xfffffff0, 0xffffffff };
+VECT_VAR_DECL(expected_f32_9,hfloat,32,2) [] = { 0xf3f2f1f0, 0xf7f6f5f4 };
+VECT_VAR_DECL(expected_f32_10,hfloat,32,2) [] = { 0xfff1fff0, 0xfff3fff2 };
+
+/* Expected results for vreinterpretq_f32_xx.  */
+VECT_VAR_DECL(expected_q_f32_1,hfloat,32,4) [] = { 0xf3f2f1f0, 0xf7f6f5f4,
+						   0xfbfaf9f8, 0xfffefdfc };
+VECT_VAR_DECL(expected_q_f32_2,hfloat,32,4) [] = { 0xfff1fff0, 0xfff3fff2,
+						   0xfff5fff4, 0xfff7fff6 };
+VECT_VAR_DECL(expected_q_f32_3,hfloat,32,4) [] = { 0xfffffff0, 0xfffffff1,
+						   0xfffffff2, 0xfffffff3 };
+VECT_VAR_DECL(expected_q_f32_4,hfloat,32,4) [] = { 0xfffffff0, 0xffffffff,
+						   0xfffffff1, 0xffffffff };
+VECT_VAR_DECL(expected_q_f32_5,hfloat,32,4) [] = { 0xf3f2f1f0, 0xf7f6f5f4,
+						   0xfbfaf9f8, 0xfffefdfc };
+VECT_VAR_DECL(expected_q_f32_6,hfloat,32,4) [] = { 0xfff1fff0, 0xfff3fff2,
+						   0xfff5fff4, 0xfff7fff6 };
+VECT_VAR_DECL(expected_q_f32_7,hfloat,32,4) [] = { 0xfffffff0, 0xfffffff1,
+						   0xfffffff2, 0xfffffff3 };
+VECT_VAR_DECL(expected_q_f32_8,hfloat,32,4) [] = { 0xfffffff0, 0xffffffff,
+						   0xfffffff1, 0xffffffff };
+VECT_VAR_DECL(expected_q_f32_9,hfloat,32,4) [] = { 0xf3f2f1f0, 0xf7f6f5f4,
+						   0xfbfaf9f8, 0xfffefdfc };
+VECT_VAR_DECL(expected_q_f32_10,hfloat,32,4) [] = { 0xfff1fff0, 0xfff3fff2,
+						    0xfff5fff4, 0xfff7fff6 };
+
+/* Expected results for vreinterpretq_xx_f32.  */
+VECT_VAR_DECL(expected_xx_f32_1,int,8,8) [] = { 0x0, 0x0, 0x80, 0xc1,
+						0x0, 0x0, 0x70, 0xc1 };
+VECT_VAR_DECL(expected_xx_f32_2,int,16,4) [] = { 0x0, 0xc180, 0x0, 0xc170 };
+VECT_VAR_DECL(expected_xx_f32_3,int,32,2) [] = { 0xc1800000, 0xc1700000 };
+VECT_VAR_DECL(expected_xx_f32_4,int,64,1) [] = { 0xc1700000c1800000 };
+VECT_VAR_DECL(expected_xx_f32_5,uint,8,8) [] = { 0x0, 0x0, 0x80, 0xc1,
+						 0x0, 0x0, 0x70, 0xc1 };
+VECT_VAR_DECL(expected_xx_f32_6,uint,16,4) [] = { 0x0, 0xc180, 0x0, 0xc170 };
+VECT_VAR_DECL(expected_xx_f32_7,uint,32,2) [] = { 0xc1800000, 0xc1700000 };
+VECT_VAR_DECL(expected_xx_f32_8,uint,64,1) [] = { 0xc1700000c1800000 };
+VECT_VAR_DECL(expected_xx_f32_9,poly,8,8) [] = { 0x0, 0x0, 0x80, 0xc1,
+						 0x0, 0x0, 0x70, 0xc1 };
+VECT_VAR_DECL(expected_xx_f32_10,poly,16,4) [] = { 0x0, 0xc180, 0x0, 0xc170 };
+
+/* Expected results for vreinterpretq_xx_f32.  */
+VECT_VAR_DECL(expected_q_xx_f32_1,int,8,16) [] = { 0x0, 0x0, 0x80, 0xc1,
+						   0x0, 0x0, 0x70, 0xc1,
+						   0x0, 0x0, 0x60, 0xc1,
+						   0x0, 0x0, 0x50, 0xc1 };
+VECT_VAR_DECL(expected_q_xx_f32_2,int,16,8) [] = { 0x0, 0xc180, 0x0, 0xc170,
+						   0x0, 0xc160, 0x0, 0xc150 };
+VECT_VAR_DECL(expected_q_xx_f32_3,int,32,4) [] = { 0xc1800000, 0xc1700000,
+						   0xc1600000, 0xc1500000 };
+VECT_VAR_DECL(expected_q_xx_f32_4,int,64,2) [] = { 0xc1700000c1800000,
+						   0xc1500000c1600000 };
+VECT_VAR_DECL(expected_q_xx_f32_5,uint,8,16) [] = { 0x0, 0x0, 0x80, 0xc1,
+						    0x0, 0x0, 0x70, 0xc1,
+						    0x0, 0x0, 0x60, 0xc1,
+						    0x0, 0x0, 0x50, 0xc1 };
+VECT_VAR_DECL(expected_q_xx_f32_6,uint,16,8) [] = { 0x0, 0xc180, 0x0, 0xc170,
+						    0x0, 0xc160, 0x0, 0xc150 };
+VECT_VAR_DECL(expected_q_xx_f32_7,uint,32,4) [] = { 0xc1800000, 0xc1700000,
+						    0xc1600000, 0xc1500000 };
+VECT_VAR_DECL(expected_q_xx_f32_8,uint,64,2) [] = { 0xc1700000c1800000,
+						    0xc1500000c1600000 };
+VECT_VAR_DECL(expected_q_xx_f32_9,poly,8,16) [] = { 0x0, 0x0, 0x80, 0xc1,
+						    0x0, 0x0, 0x70, 0xc1,
+						    0x0, 0x0, 0x60, 0xc1,
+						    0x0, 0x0, 0x50, 0xc1 };
+VECT_VAR_DECL(expected_q_xx_f32_10,poly,16,8) [] = { 0x0, 0xc180, 0x0, 0xc170,
+						     0x0, 0xc160, 0x0, 0xc150 };
+
+#define TEST_MSG "VREINTERPRET/VREINTERPRETQ"
+
+void exec_vreinterpret (void)
+{
+  int i;
+
+  /* Basic test: y=vreinterpret(x), then store the result.  */
+#define TEST_VREINTERPRET(Q, T1, T2, W, N, TS1, TS2, WS, NS, EXPECTED)	\
+  VECT_VAR(vector_res, T1, W, N) =					\
+    vreinterpret##Q##_##T2##W##_##TS2##WS(VECT_VAR(vector, TS1, WS, NS)); \
+  vst1##Q##_##T2##W(VECT_VAR(result, T1, W, N),				\
+		    VECT_VAR(vector_res, T1, W, N));			\
+  CHECK(TEST_MSG, T1, W, N, PRIx##W, EXPECTED, "");
+
+#define TEST_VREINTERPRET_POLY(Q, T1, T2, W, N, TS1, TS2, WS, NS, EXPECTED) \
+  VECT_VAR(vector_res, T1, W, N) =					\
+    vreinterpret##Q##_##T2##W##_##TS2##WS(VECT_VAR(vector, TS1, WS, NS)); \
+  vst1##Q##_##T2##W(VECT_VAR(result, T1, W, N),				\
+		    VECT_VAR(vector_res, T1, W, N));			\
+  CHECK(TEST_MSG, T1, W, N, PRIx##W, EXPECTED, "");
+
+#define TEST_VREINTERPRET_FP(Q, T1, T2, W, N, TS1, TS2, WS, NS, EXPECTED) \
+  VECT_VAR(vector_res, T1, W, N) =					\
+    vreinterpret##Q##_##T2##W##_##TS2##WS(VECT_VAR(vector, TS1, WS, NS)); \
+  vst1##Q##_##T2##W(VECT_VAR(result, T1, W, N),				\
+		    VECT_VAR(vector_res, T1, W, N));			\
+  CHECK_FP(TEST_MSG, T1, W, N, PRIx##W, EXPECTED, "");
+
+  DECL_VARIABLE_ALL_VARIANTS(vector);
+  DECL_VARIABLE_ALL_VARIANTS(vector_res);
+
+  clean_results ();
+
+
+  /* Initialize input "vector" from "buffer".  */
+  TEST_MACRO_ALL_VARIANTS_2_5(VLOAD, vector, buffer);
+  VLOAD(vector, buffer, , float, f, 32, 2);
+  VLOAD(vector, buffer, q, float, f, 32, 4);
+
+  /* vreinterpret_s8_xx.  */
+  TEST_VREINTERPRET(, int, s, 8, 8, int, s, 16, 4, expected_s8_1);
+  TEST_VREINTERPRET(, int, s, 8, 8, int, s, 32, 2, expected_s8_2);
+  TEST_VREINTERPRET(, int, s, 8, 8, int, s, 64, 1, expected_s8_3);
+  TEST_VREINTERPRET(, int, s, 8, 8, uint, u, 8, 8, expected_s8_4);
+  TEST_VREINTERPRET(, int, s, 8, 8, uint, u, 16, 4, expected_s8_5);
+  TEST_VREINTERPRET(, int, s, 8, 8, uint, u, 32, 2, expected_s8_6);
+  TEST_VREINTERPRET(, int, s, 8, 8, uint, u, 64, 1, expected_s8_7);
+  TEST_VREINTERPRET(, int, s, 8, 8, poly, p, 8, 8, expected_s8_8);
+  TEST_VREINTERPRET(, int, s, 8, 8, poly, p, 16, 4, expected_s8_9);
+
+  /* vreinterpret_s16_xx.  */
+  TEST_VREINTERPRET(, int, s, 16, 4, int, s, 8, 8, expected_s16_1);
+  TEST_VREINTERPRET(, int, s, 16, 4, int, s, 32, 2, expected_s16_2);
+  TEST_VREINTERPRET(, int, s, 16, 4, int, s, 64, 1, expected_s16_3);
+  TEST_VREINTERPRET(, int, s, 16, 4, uint, u, 8, 8, expected_s16_4);
+  TEST_VREINTERPRET(, int, s, 16, 4, uint, u, 16, 4, expected_s16_5);
+  TEST_VREINTERPRET(, int, s, 16, 4, uint, u, 32, 2, expected_s16_6);
+  TEST_VREINTERPRET(, int, s, 16, 4, uint, u, 64, 1, expected_s16_7);
+  TEST_VREINTERPRET(, int, s, 16, 4, poly, p, 8, 8, expected_s16_8);
+  TEST_VREINTERPRET(, int, s, 16, 4, poly, p, 16, 4, expected_s16_9);
+
+  /* vreinterpret_s32_xx.  */
+  TEST_VREINTERPRET(, int, s, 32, 2, int, s, 8, 8, expected_s32_1);
+  TEST_VREINTERPRET(, int, s, 32, 2, int, s, 16, 4, expected_s32_2);
+  TEST_VREINTERPRET(, int, s, 32, 2, int, s, 64, 1, expected_s32_3);
+  TEST_VREINTERPRET(, int, s, 32, 2, uint, u, 8, 8, expected_s32_4);
+  TEST_VREINTERPRET(, int, s, 32, 2, uint, u, 16, 4, expected_s32_5);
+  TEST_VREINTERPRET(, int, s, 32, 2, uint, u, 32, 2, expected_s32_6);
+  TEST_VREINTERPRET(, int, s, 32, 2, uint, u, 64, 1, expected_s32_7);
+  TEST_VREINTERPRET(, int, s, 32, 2, poly, p, 8, 8, expected_s32_8);
+  TEST_VREINTERPRET(, int, s, 32, 2, poly, p, 16, 4, expected_s32_9);
+
+  /* vreinterpret_s64_xx.  */
+  TEST_VREINTERPRET(, int, s, 64, 1, int, s, 8, 8, expected_s64_1);
+  TEST_VREINTERPRET(, int, s, 64, 1, int, s, 16, 4, expected_s64_2);
+  TEST_VREINTERPRET(, int, s, 64, 1, int, s, 32, 2, expected_s64_3);
+  TEST_VREINTERPRET(, int, s, 64, 1, uint, u, 8, 8, expected_s64_4);
+  TEST_VREINTERPRET(, int, s, 64, 1, uint, u, 16, 4, expected_s64_5);
+  TEST_VREINTERPRET(, int, s, 64, 1, uint, u, 32, 2, expected_s64_6);
+  TEST_VREINTERPRET(, int, s, 64, 1, uint, u, 64, 1, expected_s64_7);
+  TEST_VREINTERPRET(, int, s, 64, 1, poly, p, 8, 8, expected_s64_8);
+  TEST_VREINTERPRET(, int, s, 64, 1, poly, p, 16, 4, expected_s64_9);
+
+  /* vreinterpret_u8_xx.  */
+  TEST_VREINTERPRET(, uint, u, 8, 8, int, s, 8, 8, expected_u8_1);
+  TEST_VREINTERPRET(, uint, u, 8, 8, int, s, 16, 4, expected_u8_2);
+  TEST_VREINTERPRET(, uint, u, 8, 8, int, s, 32, 2, expected_u8_3);
+  TEST_VREINTERPRET(, uint, u, 8, 8, int, s, 64, 1, expected_u8_4);
+  TEST_VREINTERPRET(, uint, u, 8, 8, uint, u, 16, 4, expected_u8_5);
+  TEST_VREINTERPRET(, uint, u, 8, 8, uint, u, 32, 2, expected_u8_6);
+  TEST_VREINTERPRET(, uint, u, 8, 8, uint, u, 64, 1, expected_u8_7);
+  TEST_VREINTERPRET(, uint, u, 8, 8, poly, p, 8, 8, expected_u8_8);
+  TEST_VREINTERPRET(, uint, u, 8, 8, poly, p, 16, 4, expected_u8_9);
+
+  /* vreinterpret_u16_xx.  */
+  TEST_VREINTERPRET(, uint, u, 16, 4, int, s, 8, 8, expected_u16_1);
+  TEST_VREINTERPRET(, uint, u, 16, 4, int, s, 16, 4, expected_u16_2);
+  TEST_VREINTERPRET(, uint, u, 16, 4, int, s, 32, 2, expected_u16_3);
+  TEST_VREINTERPRET(, uint, u, 16, 4, int, s, 64, 1, expected_u16_4);
+  TEST_VREINTERPRET(, uint, u, 16, 4, uint, u, 8, 8, expected_u16_5);
+  TEST_VREINTERPRET(, uint, u, 16, 4, uint, u, 32, 2, expected_u16_6);
+  TEST_VREINTERPRET(, uint, u, 16, 4, uint, u, 64, 1, expected_u16_7);
+  TEST_VREINTERPRET(, uint, u, 16, 4, poly, p, 8, 8, expected_u16_8);
+  TEST_VREINTERPRET(, uint, u, 16, 4, poly, p, 16, 4, expected_u16_9);
+
+  /* vreinterpret_u32_xx.  */
+  TEST_VREINTERPRET(, uint, u, 32, 2, int, s, 8, 8, expected_u32_1);
+  TEST_VREINTERPRET(, uint, u, 32, 2, int, s, 16, 4, expected_u32_2);
+  TEST_VREINTERPRET(, uint, u, 32, 2, int, s, 32, 2, expected_u32_3);
+  TEST_VREINTERPRET(, uint, u, 32, 2, int, s, 64, 1, expected_u32_4);
+  TEST_VREINTERPRET(, uint, u, 32, 2, uint, u, 8, 8, expected_u32_5);
+  TEST_VREINTERPRET(, uint, u, 32, 2, uint, u, 16, 4, expected_u32_6);
+  TEST_VREINTERPRET(, uint, u, 32, 2, uint, u, 64, 1, expected_u32_7);
+  TEST_VREINTERPRET(, uint, u, 32, 2, poly, p, 8, 8, expected_u32_8);
+  TEST_VREINTERPRET(, uint, u, 32, 2, poly, p, 16, 4, expected_u32_9);
+
+  /* vreinterpret_u64_xx.  */
+  TEST_VREINTERPRET(, uint, u, 64, 1, int, s, 8, 8, expected_u64_1);
+  TEST_VREINTERPRET(, uint, u, 64, 1, int, s, 16, 4, expected_u64_2);
+  TEST_VREINTERPRET(, uint, u, 64, 1, int, s, 32, 2, expected_u64_3);
+  TEST_VREINTERPRET(, uint, u, 64, 1, int, s, 64, 1, expected_u64_4);
+  TEST_VREINTERPRET(, uint, u, 64, 1, uint, u, 8, 8, expected_u64_5);
+  TEST_VREINTERPRET(, uint, u, 64, 1, uint, u, 16, 4, expected_u64_6);
+  TEST_VREINTERPRET(, uint, u, 64, 1, uint, u, 32, 2, expected_u64_7);
+  TEST_VREINTERPRET(, uint, u, 64, 1, poly, p, 8, 8, expected_u64_8);
+  TEST_VREINTERPRET(, uint, u, 64, 1, poly, p, 16, 4, expected_u64_9);
+
+  /* vreinterpret_p8_xx.  */
+  TEST_VREINTERPRET_POLY(, poly, p, 8, 8, int, s, 8, 8, expected_p8_1);
+  TEST_VREINTERPRET_POLY(, poly, p, 8, 8, int, s, 16, 4, expected_p8_2);
+  TEST_VREINTERPRET_POLY(, poly, p, 8, 8, int, s, 32, 2, expected_p8_3);
+  TEST_VREINTERPRET_POLY(, poly, p, 8, 8, int, s, 64, 1, expected_p8_4);
+  TEST_VREINTERPRET_POLY(, poly, p, 8, 8, uint, u, 8, 8, expected_p8_5);
+  TEST_VREINTERPRET_POLY(, poly, p, 8, 8, uint, u, 16, 4, expected_p8_6);
+  TEST_VREINTERPRET_POLY(, poly, p, 8, 8, uint, u, 32, 2, expected_p8_7);
+  TEST_VREINTERPRET_POLY(, poly, p, 8, 8, uint, u, 64, 1, expected_p8_8);
+  TEST_VREINTERPRET_POLY(, poly, p, 8, 8, poly, p, 16, 4, expected_p8_9);
+
+  /* vreinterpret_p16_xx.  */
+  TEST_VREINTERPRET_POLY(, poly, p, 16, 4, int, s, 8, 8, expected_p16_1);
+  TEST_VREINTERPRET_POLY(, poly, p, 16, 4, int, s, 16, 4, expected_p16_2);
+  TEST_VREINTERPRET_POLY(, poly, p, 16, 4, int, s, 32, 2, expected_p16_3);
+  TEST_VREINTERPRET_POLY(, poly, p, 16, 4, int, s, 64, 1, expected_p16_4);
+  TEST_VREINTERPRET_POLY(, poly, p, 16, 4, uint, u, 8, 8, expected_p16_5);
+  TEST_VREINTERPRET_POLY(, poly, p, 16, 4, uint, u, 16, 4, expected_p16_6);
+  TEST_VREINTERPRET_POLY(, poly, p, 16, 4, uint, u, 32, 2, expected_p16_7);
+  TEST_VREINTERPRET_POLY(, poly, p, 16, 4, uint, u, 64, 1, expected_p16_8);
+  TEST_VREINTERPRET_POLY(, poly, p, 16, 4, poly, p, 8, 8, expected_p16_9);
+
+  /* vreinterpretq_s8_xx.  */
+  TEST_VREINTERPRET(q, int, s, 8, 16, int, s, 16, 8, expected_q_s8_1);
+  TEST_VREINTERPRET(q, int, s, 8, 16, int, s, 32, 4, expected_q_s8_2);
+  TEST_VREINTERPRET(q, int, s, 8, 16, int, s, 64, 2, expected_q_s8_3);
+  TEST_VREINTERPRET(q, int, s, 8, 16, uint, u, 8, 16, expected_q_s8_4);
+  TEST_VREINTERPRET(q, int, s, 8, 16, uint, u, 16, 8, expected_q_s8_5);
+  TEST_VREINTERPRET(q, int, s, 8, 16, uint, u, 32, 4, expected_q_s8_6);
+  TEST_VREINTERPRET(q, int, s, 8, 16, uint, u, 64, 2, expected_q_s8_7);
+  TEST_VREINTERPRET(q, int, s, 8, 16, poly, p, 8, 16, expected_q_s8_8);
+  TEST_VREINTERPRET(q, int, s, 8, 16, poly, p, 16, 8, expected_q_s8_9);
+
+  /* vreinterpretq_s16_xx.  */
+  TEST_VREINTERPRET(q, int, s, 16, 8, int, s, 8, 16, expected_q_s16_1);
+  TEST_VREINTERPRET(q, int, s, 16, 8, int, s, 32, 4, expected_q_s16_2);
+  TEST_VREINTERPRET(q, int, s, 16, 8, int, s, 64, 2, expected_q_s16_3);
+  TEST_VREINTERPRET(q, int, s, 16, 8, uint, u, 8, 16, expected_q_s16_4);
+  TEST_VREINTERPRET(q, int, s, 16, 8, uint, u, 16, 8, expected_q_s16_5);
+  TEST_VREINTERPRET(q, int, s, 16, 8, uint, u, 32, 4, expected_q_s16_6);
+  TEST_VREINTERPRET(q, int, s, 16, 8, uint, u, 64, 2, expected_q_s16_7);
+  TEST_VREINTERPRET(q, int, s, 16, 8, poly, p, 8, 16, expected_q_s16_8);
+  TEST_VREINTERPRET(q, int, s, 16, 8, poly, p, 16, 8, expected_q_s16_9);
+
+  /* vreinterpretq_s32_xx.  */
+  TEST_VREINTERPRET(q, int, s, 32, 4, int, s, 8, 16, expected_q_s32_1);
+  TEST_VREINTERPRET(q, int, s, 32, 4, int, s, 16, 8, expected_q_s32_2);
+  TEST_VREINTERPRET(q, int, s, 32, 4, int, s, 64, 2, expected_q_s32_3);
+  TEST_VREINTERPRET(q, int, s, 32, 4, uint, u, 8, 16, expected_q_s32_4);
+  TEST_VREINTERPRET(q, int, s, 32, 4, uint, u, 16, 8, expected_q_s32_5);
+  TEST_VREINTERPRET(q, int, s, 32, 4, uint, u, 32, 4, expected_q_s32_6);
+  TEST_VREINTERPRET(q, int, s, 32, 4, uint, u, 64, 2, expected_q_s32_7);
+  TEST_VREINTERPRET(q, int, s, 32, 4, poly, p, 8, 16, expected_q_s32_8);
+  TEST_VREINTERPRET(q, int, s, 32, 4, poly, p, 16, 8, expected_q_s32_9);
+
+  /* vreinterpretq_s64_xx.  */
+  TEST_VREINTERPRET(q, int, s, 64, 2, int, s, 8, 16, expected_q_s64_1);
+  TEST_VREINTERPRET(q, int, s, 64, 2, int, s, 16, 8, expected_q_s64_2);
+  TEST_VREINTERPRET(q, int, s, 64, 2, int, s, 32, 4, expected_q_s64_3);
+  TEST_VREINTERPRET(q, int, s, 64, 2, uint, u, 8, 16, expected_q_s64_4);
+  TEST_VREINTERPRET(q, int, s, 64, 2, uint, u, 16, 8, expected_q_s64_5);
+  TEST_VREINTERPRET(q, int, s, 64, 2, uint, u, 32, 4, expected_q_s64_6);
+  TEST_VREINTERPRET(q, int, s, 64, 2, uint, u, 64, 2, expected_q_s64_7);
+  TEST_VREINTERPRET(q, int, s, 64, 2, poly, p, 8, 16, expected_q_s64_8);
+  TEST_VREINTERPRET(q, int, s, 64, 2, poly, p, 16, 8, expected_q_s64_9);
+
+  /* vreinterpretq_u8_xx.  */
+  TEST_VREINTERPRET(q, uint, u, 8, 16, int, s, 8, 16, expected_q_u8_1);
+  TEST_VREINTERPRET(q, uint, u, 8, 16, int, s, 16, 8, expected_q_u8_2);
+  TEST_VREINTERPRET(q, uint, u, 8, 16, int, s, 32, 4, expected_q_u8_3);
+  TEST_VREINTERPRET(q, uint, u, 8, 16, int, s, 64, 2, expected_q_u8_4);
+  TEST_VREINTERPRET(q, uint, u, 8, 16, uint, u, 16, 8, expected_q_u8_5);
+  TEST_VREINTERPRET(q, uint, u, 8, 16, uint, u, 32, 4, expected_q_u8_6);
+  TEST_VREINTERPRET(q, uint, u, 8, 16, uint, u, 64, 2, expected_q_u8_7);
+  TEST_VREINTERPRET(q, uint, u, 8, 16, poly, p, 8, 16, expected_q_u8_8);
+  TEST_VREINTERPRET(q, uint, u, 8, 16, poly, p, 16, 8, expected_q_u8_9);
+
+  /* vreinterpretq_u16_xx.  */
+  TEST_VREINTERPRET(q, uint, u, 16, 8, int, s, 8, 16, expected_q_u16_1);
+  TEST_VREINTERPRET(q, uint, u, 16, 8, int, s, 16, 8, expected_q_u16_2);
+  TEST_VREINTERPRET(q, uint, u, 16, 8, int, s, 32, 4, expected_q_u16_3);
+  TEST_VREINTERPRET(q, uint, u, 16, 8, int, s, 64, 2, expected_q_u16_4);
+  TEST_VREINTERPRET(q, uint, u, 16, 8, uint, u, 8, 16, expected_q_u16_5);
+  TEST_VREINTERPRET(q, uint, u, 16, 8, uint, u, 32, 4, expected_q_u16_6);
+  TEST_VREINTERPRET(q, uint, u, 16, 8, uint, u, 64, 2, expected_q_u16_7);
+  TEST_VREINTERPRET(q, uint, u, 16, 8, poly, p, 8, 16, expected_q_u16_8);
+  TEST_VREINTERPRET(q, uint, u, 16, 8, poly, p, 16, 8, expected_q_u16_9);
+
+  /* vreinterpretq_u32_xx.  */
+  TEST_VREINTERPRET(q, uint, u, 32, 4, int, s, 8, 16, expected_q_u32_1);
+  TEST_VREINTERPRET(q, uint, u, 32, 4, int, s, 16, 8, expected_q_u32_2);
+  TEST_VREINTERPRET(q, uint, u, 32, 4, int, s, 32, 4, expected_q_u32_3);
+  TEST_VREINTERPRET(q, uint, u, 32, 4, int, s, 64, 2, expected_q_u32_4);
+  TEST_VREINTERPRET(q, uint, u, 32, 4, uint, u, 8, 16, expected_q_u32_5);
+  TEST_VREINTERPRET(q, uint, u, 32, 4, uint, u, 16, 8, expected_q_u32_6);
+  TEST_VREINTERPRET(q, uint, u, 32, 4, uint, u, 64, 2, expected_q_u32_7);
+  TEST_VREINTERPRET(q, uint, u, 32, 4, poly, p, 8, 16, expected_q_u32_8);
+  TEST_VREINTERPRET(q, uint, u, 32, 4, poly, p, 16, 8, expected_q_u32_9);
+
+  /* vreinterpretq_u64_xx.  */
+  TEST_VREINTERPRET(q, uint, u, 64, 2, int, s, 8, 16, expected_q_u64_1);
+  TEST_VREINTERPRET(q, uint, u, 64, 2, int, s, 16, 8, expected_q_u64_2);
+  TEST_VREINTERPRET(q, uint, u, 64, 2, int, s, 32, 4, expected_q_u64_3);
+  TEST_VREINTERPRET(q, uint, u, 64, 2, int, s, 64, 2, expected_q_u64_4);
+  TEST_VREINTERPRET(q, uint, u, 64, 2, uint, u, 8, 16, expected_q_u64_5);
+  TEST_VREINTERPRET(q, uint, u, 64, 2, uint, u, 16, 8, expected_q_u64_6);
+  TEST_VREINTERPRET(q, uint, u, 64, 2, uint, u, 32, 4, expected_q_u64_7);
+  TEST_VREINTERPRET(q, uint, u, 64, 2, poly, p, 8, 16, expected_q_u64_8);
+  TEST_VREINTERPRET(q, uint, u, 64, 2, poly, p, 16, 8, expected_q_u64_9);
+
+  /* vreinterpret_f32_xx.  */
+  TEST_VREINTERPRET_FP(, float, f, 32, 2, int, s, 8, 8, expected_f32_1);
+  TEST_VREINTERPRET_FP(, float, f, 32, 2, int, s, 16, 4, expected_f32_2);
+  TEST_VREINTERPRET_FP(, float, f, 32, 2, int, s, 32, 2, expected_f32_3);
+  TEST_VREINTERPRET_FP(, float, f, 32, 2, int, s, 64, 1, expected_f32_4);
+  TEST_VREINTERPRET_FP(, float, f, 32, 2, uint, u, 8, 8, expected_f32_5);
+  TEST_VREINTERPRET_FP(, float, f, 32, 2, uint, u, 16, 4, expected_f32_6);
+  TEST_VREINTERPRET_FP(, float, f, 32, 2, uint, u, 32, 2, expected_f32_7);
+  TEST_VREINTERPRET_FP(, float, f, 32, 2, uint, u, 64, 1, expected_f32_8);
+  TEST_VREINTERPRET_FP(, float, f, 32, 2, poly, p, 8, 8, expected_f32_9);
+  TEST_VREINTERPRET_FP(, float, f, 32, 2, poly, p, 16, 4, expected_f32_10);
+
+  /* vreinterpretq_f32_xx.  */
+  TEST_VREINTERPRET_FP(q, float, f, 32, 4, int, s, 8, 16, expected_q_f32_1);
+  TEST_VREINTERPRET_FP(q, float, f, 32, 4, int, s, 16, 8, expected_q_f32_2);
+  TEST_VREINTERPRET_FP(q, float, f, 32, 4, int, s, 32, 4, expected_q_f32_3);
+  TEST_VREINTERPRET_FP(q, float, f, 32, 4, int, s, 64, 2, expected_q_f32_4);
+  TEST_VREINTERPRET_FP(q, float, f, 32, 4, uint, u, 8, 16, expected_q_f32_5);
+  TEST_VREINTERPRET_FP(q, float, f, 32, 4, uint, u, 16, 8, expected_q_f32_6);
+  TEST_VREINTERPRET_FP(q, float, f, 32, 4, uint, u, 32, 4, expected_q_f32_7);
+  TEST_VREINTERPRET_FP(q, float, f, 32, 4, uint, u, 64, 2, expected_q_f32_8);
+  TEST_VREINTERPRET_FP(q, float, f, 32, 4, poly, p, 8, 16, expected_q_f32_9);
+  TEST_VREINTERPRET_FP(q, float, f, 32, 4, poly, p, 16, 8, expected_q_f32_10);
+
+  /* vreinterpret_xx_f32.  */
+  TEST_VREINTERPRET(, int, s, 8, 8, float, f, 32, 2, expected_xx_f32_1);
+  TEST_VREINTERPRET(, int, s, 16, 4, float, f, 32, 2, expected_xx_f32_2);
+  TEST_VREINTERPRET(, int, s, 32, 2, float, f, 32, 2, expected_xx_f32_3);
+  TEST_VREINTERPRET(, int, s, 64, 1, float, f, 32, 2, expected_xx_f32_4);
+  TEST_VREINTERPRET(, uint, u, 8, 8, float, f, 32, 2, expected_xx_f32_5);
+  TEST_VREINTERPRET(, uint, u, 16, 4, float, f, 32, 2, expected_xx_f32_6);
+  TEST_VREINTERPRET(, uint, u, 32, 2, float, f, 32, 2, expected_xx_f32_7);
+  TEST_VREINTERPRET(, uint, u, 64, 1, float, f, 32, 2, expected_xx_f32_8);
+  TEST_VREINTERPRET_POLY(, poly, p, 8, 8, float, f, 32, 2, expected_xx_f32_9);
+  TEST_VREINTERPRET_POLY(, poly, p, 16, 4, float, f, 32, 2, expected_xx_f32_10);
+
+  /* vreinterpretq_xx_f32.  */
+  TEST_VREINTERPRET(q, int, s, 8, 16, float, f, 32, 4, expected_q_xx_f32_1);
+  TEST_VREINTERPRET(q, int, s, 16, 8, float, f, 32, 4, expected_q_xx_f32_2);
+  TEST_VREINTERPRET(q, int, s, 32, 4, float, f, 32, 4, expected_q_xx_f32_3);
+  TEST_VREINTERPRET(q, int, s, 64, 2, float, f, 32, 4, expected_q_xx_f32_4);
+  TEST_VREINTERPRET(q, uint, u, 8, 16, float, f, 32, 4, expected_q_xx_f32_5);
+  TEST_VREINTERPRET(q, uint, u, 16, 8, float, f, 32, 4, expected_q_xx_f32_6);
+  TEST_VREINTERPRET(q, uint, u, 32, 4, float, f, 32, 4, expected_q_xx_f32_7);
+  TEST_VREINTERPRET(q, uint, u, 64, 2, float, f, 32, 4, expected_q_xx_f32_8);
+  TEST_VREINTERPRET_POLY(q, poly, p, 8, 16, float, f, 32, 4, expected_q_xx_f32_9);
+  TEST_VREINTERPRET_POLY(q, poly, p, 16, 8, float, f, 32, 4, expected_q_xx_f32_10);
+}
+
+int main (void)
+{
+  exec_vreinterpret ();
+  return 0;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vrev.c
@@ -0,0 +1,200 @@
+#include <arm_neon.h>
+#include "arm-neon-ref.h"
+#include "compute-ref-data.h"
+
+/* Expected results for vrev16.  */
+VECT_VAR_DECL(expected_vrev16,int,8,8) [] = { 0xf1, 0xf0, 0xf3, 0xf2,
+					      0xf5, 0xf4, 0xf7, 0xf6 };
+VECT_VAR_DECL(expected_vrev16,uint,8,8) [] = { 0xf1, 0xf0, 0xf3, 0xf2,
+					       0xf5, 0xf4, 0xf7, 0xf6 };
+VECT_VAR_DECL(expected_vrev16,poly,8,8) [] = { 0xf1, 0xf0, 0xf3, 0xf2,
+					       0xf5, 0xf4, 0xf7, 0xf6 };
+VECT_VAR_DECL(expected_vrev16,int,8,16) [] = { 0xf1, 0xf0, 0xf3, 0xf2,
+					       0xf5, 0xf4, 0xf7, 0xf6,
+					       0xf9, 0xf8, 0xfb, 0xfa,
+					       0xfd, 0xfc, 0xff, 0xfe };
+VECT_VAR_DECL(expected_vrev16,uint,8,16) [] = { 0xf1, 0xf0, 0xf3, 0xf2,
+						0xf5, 0xf4, 0xf7, 0xf6,
+						0xf9, 0xf8, 0xfb, 0xfa,
+						0xfd, 0xfc, 0xff, 0xfe };
+VECT_VAR_DECL(expected_vrev16,poly,8,16) [] = { 0xf1, 0xf0, 0xf3, 0xf2,
+						0xf5, 0xf4, 0xf7, 0xf6,
+						0xf9, 0xf8, 0xfb, 0xfa,
+						0xfd, 0xfc, 0xff, 0xfe };
+
+/* Expected results for vrev32.  */
+VECT_VAR_DECL(expected_vrev32,int,8,8) [] = { 0xf3, 0xf2, 0xf1, 0xf0,
+					      0xf7, 0xf6, 0xf5, 0xf4 };
+VECT_VAR_DECL(expected_vrev32,int,16,4) [] = { 0xfff1, 0xfff0, 0xfff3, 0xfff2 };
+VECT_VAR_DECL(expected_vrev32,uint,8,8) [] = { 0xf3, 0xf2, 0xf1, 0xf0,
+					       0xf7, 0xf6, 0xf5, 0xf4 };
+VECT_VAR_DECL(expected_vrev32,uint,16,4) [] = { 0xfff1, 0xfff0, 0xfff3, 0xfff2 };
+VECT_VAR_DECL(expected_vrev32,poly,8,8) [] = { 0xf3, 0xf2, 0xf1, 0xf0,
+					       0xf7, 0xf6, 0xf5, 0xf4 };
+VECT_VAR_DECL(expected_vrev32,poly,16,4) [] = { 0xfff1, 0xfff0, 0xfff3, 0xfff2 };
+VECT_VAR_DECL(expected_vrev32,int,8,16) [] = { 0xf3, 0xf2, 0xf1, 0xf0,
+					       0xf7, 0xf6, 0xf5, 0xf4,
+					       0xfb, 0xfa, 0xf9, 0xf8,
+					       0xff, 0xfe, 0xfd, 0xfc };
+VECT_VAR_DECL(expected_vrev32,int,16,8) [] = { 0xfff1, 0xfff0, 0xfff3, 0xfff2,
+					       0xfff5, 0xfff4, 0xfff7, 0xfff6 };
+VECT_VAR_DECL(expected_vrev32,uint,8,16) [] = { 0xf3, 0xf2, 0xf1, 0xf0,
+						0xf7, 0xf6, 0xf5, 0xf4,
+						0xfb, 0xfa, 0xf9, 0xf8,
+						0xff, 0xfe, 0xfd, 0xfc };
+VECT_VAR_DECL(expected_vrev32,uint,16,8) [] = { 0xfff1, 0xfff0, 0xfff3, 0xfff2,
+						0xfff5, 0xfff4, 0xfff7, 0xfff6 };
+VECT_VAR_DECL(expected_vrev32,poly,8,16) [] = { 0xf3, 0xf2, 0xf1, 0xf0,
+						0xf7, 0xf6, 0xf5, 0xf4,
+						0xfb, 0xfa, 0xf9, 0xf8,
+						0xff, 0xfe, 0xfd, 0xfc };
+VECT_VAR_DECL(expected_vrev32,poly,16,8) [] = { 0xfff1, 0xfff0, 0xfff3, 0xfff2,
+						0xfff5, 0xfff4, 0xfff7, 0xfff6 };
+
+/* Expected results for vrev64.  */
+VECT_VAR_DECL(expected_vrev64,int,8,8) [] = { 0xf7, 0xf6, 0xf5, 0xf4,
+					      0xf3, 0xf2, 0xf1, 0xf0 };
+VECT_VAR_DECL(expected_vrev64,int,16,4) [] = { 0xfff3, 0xfff2, 0xfff1, 0xfff0 };
+VECT_VAR_DECL(expected_vrev64,int,32,2) [] = { 0xfffffff1, 0xfffffff0 };
+VECT_VAR_DECL(expected_vrev64,uint,8,8) [] = { 0xf7, 0xf6, 0xf5, 0xf4, 0xf3,
+					       0xf2, 0xf1, 0xf0 };
+VECT_VAR_DECL(expected_vrev64,uint,16,4) [] = { 0xfff3, 0xfff2, 0xfff1, 0xfff0 };
+VECT_VAR_DECL(expected_vrev64,uint,32,2) [] = { 0xfffffff1, 0xfffffff0 };
+VECT_VAR_DECL(expected_vrev64,poly,8,8) [] = { 0xf7, 0xf6, 0xf5, 0xf4,
+					       0xf3, 0xf2, 0xf1, 0xf0 };
+VECT_VAR_DECL(expected_vrev64,poly,16,4) [] = { 0xfff3, 0xfff2, 0xfff1, 0xfff0 };
+VECT_VAR_DECL(expected_vrev64,hfloat,32,2) [] = { 0xc1700000, 0xc1800000 };
+VECT_VAR_DECL(expected_vrev64,int,8,16) [] = { 0xf7, 0xf6, 0xf5, 0xf4,
+					       0xf3, 0xf2, 0xf1, 0xf0,
+					       0xff, 0xfe, 0xfd, 0xfc,
+					       0xfb, 0xfa, 0xf9, 0xf8 };
+VECT_VAR_DECL(expected_vrev64,int,16,8) [] = { 0xfff3, 0xfff2, 0xfff1, 0xfff0,
+					       0xfff7, 0xfff6, 0xfff5, 0xfff4 };
+VECT_VAR_DECL(expected_vrev64,int,32,4) [] = { 0xfffffff1, 0xfffffff0,
+					       0xfffffff3, 0xfffffff2 };
+VECT_VAR_DECL(expected_vrev64,uint,8,16) [] = { 0xf7, 0xf6, 0xf5, 0xf4,
+						0xf3, 0xf2, 0xf1, 0xf0,
+						0xff, 0xfe, 0xfd, 0xfc,
+						0xfb, 0xfa, 0xf9, 0xf8 };
+VECT_VAR_DECL(expected_vrev64,uint,16,8) [] = { 0xfff3, 0xfff2, 0xfff1, 0xfff0,
+						0xfff7, 0xfff6, 0xfff5, 0xfff4 };
+VECT_VAR_DECL(expected_vrev64,uint,32,4) [] = { 0xfffffff1, 0xfffffff0,
+						0xfffffff3, 0xfffffff2 };
+VECT_VAR_DECL(expected_vrev64,poly,8,16) [] = { 0xf7, 0xf6, 0xf5, 0xf4,
+						0xf3, 0xf2, 0xf1, 0xf0,
+						0xff, 0xfe, 0xfd, 0xfc,
+						0xfb, 0xfa, 0xf9, 0xf8 };
+VECT_VAR_DECL(expected_vrev64,poly,16,8) [] = { 0xfff3, 0xfff2, 0xfff1, 0xfff0,
+						0xfff7, 0xfff6, 0xfff5, 0xfff4 };
+VECT_VAR_DECL(expected_vrev64,hfloat,32,4) [] = { 0xc1700000, 0xc1800000,
+						  0xc1500000, 0xc1600000 };
+
+void exec_vrev (void)
+{
+  /* Basic test: y=vrev(x), then store the result.  */
+#define TEST_VREV(Q, T1, T2, W, N, W2)					\
+  VECT_VAR(vector_res, T1, W, N) =					\
+    vrev##W2##Q##_##T2##W(VECT_VAR(vector, T1, W, N));			\
+  vst1##Q##_##T2##W(VECT_VAR(result, T1, W, N), VECT_VAR(vector_res, T1, W, N))
+
+  DECL_VARIABLE_ALL_VARIANTS(vector);
+  DECL_VARIABLE_ALL_VARIANTS(vector_res);
+
+  clean_results ();
+
+  /* Initialize input "vector" from "buffer".  */
+  TEST_MACRO_ALL_VARIANTS_2_5(VLOAD, vector, buffer);
+  VLOAD(vector, buffer, , float, f, 32, 2);
+  VLOAD(vector, buffer, q, float, f, 32, 4);
+
+  /* Check vrev in each of the existing combinations.  */
+#define TEST_MSG "VREV16"
+  TEST_VREV(, int, s, 8, 8, 16);
+  TEST_VREV(, uint, u, 8, 8, 16);
+  TEST_VREV(, poly, p, 8, 8, 16);
+  TEST_VREV(q, int, s, 8, 16, 16);
+  TEST_VREV(q, uint, u, 8, 16, 16);
+  TEST_VREV(q, poly, p, 8, 16, 16);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_vrev16, "");
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_vrev16, "");
+  CHECK(TEST_MSG, poly, 8, 8, PRIx8, expected_vrev16, "");
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_vrev16, "");
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_vrev16, "");
+  CHECK(TEST_MSG, poly, 8, 16, PRIx8, expected_vrev16, "");
+
+#undef TEST_MSG
+#define TEST_MSG "VREV32"
+  TEST_VREV(, int, s, 8, 8, 32);
+  TEST_VREV(, int, s, 16, 4, 32);
+  TEST_VREV(, uint, u, 8, 8, 32);
+  TEST_VREV(, uint, u, 16, 4, 32);
+  TEST_VREV(, poly, p, 8, 8, 32);
+  TEST_VREV(, poly, p, 16, 4, 32);
+  TEST_VREV(q, int, s, 8, 16, 32);
+  TEST_VREV(q, int, s, 16, 8, 32);
+  TEST_VREV(q, uint, u, 8, 16, 32);
+  TEST_VREV(q, uint, u, 16, 8, 32);
+  TEST_VREV(q, poly, p, 8, 16, 32);
+  TEST_VREV(q, poly, p, 16, 8, 32);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_vrev32, "");
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_vrev32, "");
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_vrev32, "");
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_vrev32, "");
+  CHECK(TEST_MSG, poly, 8, 8, PRIx8, expected_vrev32, "");
+  CHECK(TEST_MSG, poly, 16, 4, PRIx16, expected_vrev32, "");
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_vrev32, "");
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_vrev32, "");
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_vrev32, "");
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_vrev32, "");
+  CHECK(TEST_MSG, poly, 8, 16, PRIx8, expected_vrev32, "");
+  CHECK(TEST_MSG, poly, 16, 8, PRIx16, expected_vrev32, "");
+
+#undef TEST_MSG
+#define TEST_MSG "VREV64"
+  TEST_VREV(, int, s, 8, 8, 64);
+  TEST_VREV(, int, s, 16, 4, 64);
+  TEST_VREV(, int, s, 32, 2, 64);
+  TEST_VREV(, uint, u, 8, 8, 64);
+  TEST_VREV(, uint, u, 16, 4, 64);
+  TEST_VREV(, uint, u, 32, 2, 64);
+  TEST_VREV(, poly, p, 8, 8, 64);
+  TEST_VREV(, poly, p, 16, 4, 64);
+  TEST_VREV(q, int, s, 8, 16, 64);
+  TEST_VREV(q, int, s, 16, 8, 64);
+  TEST_VREV(q, int, s, 32, 4, 64);
+  TEST_VREV(q, uint, u, 8, 16, 64);
+  TEST_VREV(q, uint, u, 16, 8, 64);
+  TEST_VREV(q, uint, u, 32, 4, 64);
+  TEST_VREV(q, poly, p, 8, 16, 64);
+  TEST_VREV(q, poly, p, 16, 8, 64);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_vrev64, "");
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_vrev64, "");
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_vrev64, "");
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_vrev64, "");
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_vrev64, "");
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_vrev64, "");
+  CHECK(TEST_MSG, poly, 8, 8, PRIx8, expected_vrev64, "");
+  CHECK(TEST_MSG, poly, 16, 4, PRIx16, expected_vrev64, "");
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_vrev64, "");
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_vrev64, "");
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_vrev64, "");
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_vrev64, "");
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_vrev64, "");
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_vrev64, "");
+  CHECK(TEST_MSG, poly, 8, 16, PRIx8, expected_vrev64, "");
+  CHECK(TEST_MSG, poly, 16, 8, PRIx16, expected_vrev64, "");
+
+  TEST_VREV(, float, f, 32, 2, 64);
+  TEST_VREV(q, float, f, 32, 4, 64);
+  CHECK_FP(TEST_MSG, float, 32, 2, PRIx32, expected_vrev64, "");
+  CHECK_FP(TEST_MSG, float, 32, 4, PRIx32, expected_vrev64, "");
+}
+
+int main (void)
+{
+  exec_vrev ();
+  return 0;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vrshl.c
@@ -0,0 +1,627 @@
+#include <arm_neon.h>
+#include "arm-neon-ref.h"
+#include "compute-ref-data.h"
+
+/* Expected results with input=0.  */
+VECT_VAR_DECL(expected_0,int,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,int,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,int,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,int,64,1) [] = { 0x0 };
+VECT_VAR_DECL(expected_0,uint,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					  0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,uint,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,uint,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,uint,64,1) [] = { 0x0 };
+VECT_VAR_DECL(expected_0,int,8,16) [] = { 0x0, 0x0, 0x0, 0x0,
+					  0x0, 0x0, 0x0, 0x0,
+					  0x0, 0x0, 0x0, 0x0,
+					  0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,int,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					  0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,int,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,int,64,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,uint,8,16) [] = { 0x0, 0x0, 0x0, 0x0,
+					   0x0, 0x0, 0x0, 0x0,
+					   0x0, 0x0, 0x0, 0x0,
+					   0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,uint,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					   0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,uint,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,uint,64,2) [] = { 0x0, 0x0 };
+
+/* Expected results with input=0 and negative shift amount.  */
+VECT_VAR_DECL(expected_0_sh_neg,int,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+						0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_sh_neg,int,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_sh_neg,int,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_sh_neg,int,64,1) [] = { 0x0 };
+VECT_VAR_DECL(expected_0_sh_neg,uint,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+						 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_sh_neg,uint,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_sh_neg,uint,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_sh_neg,uint,64,1) [] = { 0x0 };
+VECT_VAR_DECL(expected_0_sh_neg,int,8,16) [] = { 0x0, 0x0, 0x0, 0x0,
+						 0x0, 0x0, 0x0, 0x0,
+						 0x0, 0x0, 0x0, 0x0,
+						 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_sh_neg,int,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+						 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_sh_neg,int,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_sh_neg,int,64,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_sh_neg,uint,8,16) [] = { 0x0, 0x0, 0x0, 0x0,
+						  0x0, 0x0, 0x0, 0x0,
+						  0x0, 0x0, 0x0, 0x0,
+						  0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_sh_neg,uint,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+						  0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_sh_neg,uint,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0_sh_neg,uint,64,2) [] = { 0x0, 0x0 };
+
+/* Expected results.  */
+VECT_VAR_DECL(expected,int,8,8) [] = { 0xe0, 0xe2, 0xe4, 0xe6,
+				       0xe8, 0xea, 0xec, 0xee };
+VECT_VAR_DECL(expected,int,16,4) [] = { 0xff80, 0xff88, 0xff90, 0xff98 };
+VECT_VAR_DECL(expected,int,32,2) [] = { 0xfffff000, 0xfffff100 };
+VECT_VAR_DECL(expected,int,64,1) [] = { 0xfffffffffffffffe };
+VECT_VAR_DECL(expected,uint,8,8) [] = { 0xe0, 0xe2, 0xe4, 0xe6,
+					0xe8, 0xea, 0xec, 0xee };
+VECT_VAR_DECL(expected,uint,16,4) [] = { 0xff80, 0xff88, 0xff90, 0xff98 };
+VECT_VAR_DECL(expected,uint,32,2) [] = { 0xfffff000, 0xfffff100 };
+VECT_VAR_DECL(expected,uint,64,1) [] = { 0x1ffffffffffffffe };
+VECT_VAR_DECL(expected,int,8,16) [] = { 0x0, 0x0, 0x0, 0x0,
+					0x0, 0x0, 0x0, 0x0,
+					0x0, 0x0, 0x0, 0x0,
+					0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected,int,16,8) [] = { 0x0, 0x1000, 0x2000, 0x3000,
+					0x4000, 0x5000, 0x6000, 0x7000 };
+VECT_VAR_DECL(expected,int,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected,int,64,2) [] = { 0x0, 0x8000000000000000 };
+VECT_VAR_DECL(expected,uint,8,16) [] = { 0x0, 0x0, 0x0, 0x0,
+					 0x0, 0x0, 0x0, 0x0,
+					 0x0, 0x0, 0x0, 0x0,
+					 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected,uint,16,8) [] = { 0x0, 0x1000, 0x2000, 0x3000,
+					 0x4000, 0x5000, 0x6000, 0x7000 };
+VECT_VAR_DECL(expected,uint,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected,uint,64,2) [] = { 0x0, 0x8000000000000000 };
+
+/* Expected results with negative shift amount.  */
+VECT_VAR_DECL(expected_sh_neg,int,8,8) [] = { 0xf8, 0xf9, 0xf9, 0xfa,
+					      0xfa, 0xfb, 0xfb, 0xfc };
+VECT_VAR_DECL(expected_sh_neg,int,16,4) [] = { 0xfffc, 0xfffc, 0xfffd, 0xfffd };
+VECT_VAR_DECL(expected_sh_neg,int,32,2) [] = { 0xfffffffe, 0xfffffffe };
+VECT_VAR_DECL(expected_sh_neg,int,64,1) [] = { 0xffffffffffffffff };
+VECT_VAR_DECL(expected_sh_neg,uint,8,8) [] = { 0x78, 0x79, 0x79, 0x7a,
+					       0x7a, 0x7b, 0x7b, 0x7c };
+VECT_VAR_DECL(expected_sh_neg,uint,16,4) [] = { 0x3ffc, 0x3ffc, 0x3ffd, 0x3ffd };
+VECT_VAR_DECL(expected_sh_neg,uint,32,2) [] = { 0x1ffffffe, 0x1ffffffe };
+VECT_VAR_DECL(expected_sh_neg,uint,64,1) [] = { 0xfffffffffffffff };
+VECT_VAR_DECL(expected_sh_neg,int,8,16) [] = { 0x0, 0x0, 0x0, 0x0,
+					       0x0, 0x0, 0x0, 0x0,
+					       0x0, 0x0, 0x0, 0x0,
+					       0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_sh_neg,int,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					       0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_sh_neg,int,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_sh_neg,int,64,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_sh_neg,uint,8,16) [] = { 0x2, 0x2, 0x2, 0x2,
+						0x2, 0x2, 0x2, 0x2,
+						0x2, 0x2, 0x2, 0x2,
+						0x2, 0x2, 0x2, 0x2 };
+VECT_VAR_DECL(expected_sh_neg,uint,16,8) [] = { 0x20, 0x20, 0x20, 0x20,
+						0x20, 0x20, 0x20, 0x20 };
+VECT_VAR_DECL(expected_sh_neg,uint,32,4) [] = { 0x80000, 0x80000,
+						0x80000, 0x80000 };
+VECT_VAR_DECL(expected_sh_neg,uint,64,2) [] = { 0x100000000000, 0x100000000000 };
+
+/* Expected results with max input value shifted by -1 to test
+   round_const.  */
+VECT_VAR_DECL(expected_max_sh_minus1,int,8,8) [] = { 0x40, 0x40, 0x40, 0x40,
+						     0x40, 0x40, 0x40, 0x40 };
+VECT_VAR_DECL(expected_max_sh_minus1,int,16,4) [] = { 0x4000, 0x4000,
+						      0x4000, 0x4000 };
+VECT_VAR_DECL(expected_max_sh_minus1,int,32,2) [] = { 0x40000000, 0x40000000 };
+VECT_VAR_DECL(expected_max_sh_minus1,int,64,1) [] = { 0x4000000000000000 };
+VECT_VAR_DECL(expected_max_sh_minus1,uint,8,8) [] = { 0x80, 0x80, 0x80, 0x80,
+						      0x80, 0x80, 0x80, 0x80 };
+VECT_VAR_DECL(expected_max_sh_minus1,uint,16,4) [] = { 0x8000, 0x8000,
+						       0x8000, 0x8000 };
+VECT_VAR_DECL(expected_max_sh_minus1,uint,32,2) [] = { 0x80000000, 0x80000000 };
+VECT_VAR_DECL(expected_max_sh_minus1,uint,64,1) [] = { 0x8000000000000000 };
+VECT_VAR_DECL(expected_max_sh_minus1,int,8,16) [] = { 0x40, 0x40, 0x40, 0x40,
+						      0x40, 0x40, 0x40, 0x40,
+						      0x40, 0x40, 0x40, 0x40,
+						      0x40, 0x40, 0x40, 0x40 };
+VECT_VAR_DECL(expected_max_sh_minus1,int,16,8) [] = { 0x4000, 0x4000,
+						      0x4000, 0x4000,
+						      0x4000, 0x4000,
+						      0x4000, 0x4000 };
+VECT_VAR_DECL(expected_max_sh_minus1,int,32,4) [] = { 0x40000000, 0x40000000,
+						      0x40000000, 0x40000000 };
+VECT_VAR_DECL(expected_max_sh_minus1,int,64,2) [] = { 0x4000000000000000,
+						      0x4000000000000000 };
+VECT_VAR_DECL(expected_max_sh_minus1,uint,8,16) [] = { 0x80, 0x80, 0x80, 0x80,
+						       0x80, 0x80, 0x80, 0x80,
+						       0x80, 0x80, 0x80, 0x80,
+						       0x80, 0x80, 0x80, 0x80 };
+VECT_VAR_DECL(expected_max_sh_minus1,uint,16,8) [] = { 0x8000, 0x8000,
+						       0x8000, 0x8000,
+						       0x8000, 0x8000,
+						       0x8000, 0x8000 };
+VECT_VAR_DECL(expected_max_sh_minus1,uint,32,4) [] = { 0x80000000, 0x80000000,
+						       0x80000000, 0x80000000 };
+VECT_VAR_DECL(expected_max_sh_minus1,uint,64,2) [] = { 0x8000000000000000,
+						       0x8000000000000000 };
+
+/* Expected results with max input value shifted by -3 to test
+   round_const.  */
+VECT_VAR_DECL(expected_max_sh_minus3,int,8,8) [] = { 0x10, 0x10, 0x10, 0x10,
+						     0x10, 0x10, 0x10, 0x10 };
+VECT_VAR_DECL(expected_max_sh_minus3,int,16,4) [] = { 0x1000, 0x1000,
+						      0x1000, 0x1000 };
+VECT_VAR_DECL(expected_max_sh_minus3,int,32,2) [] = { 0x10000000, 0x10000000 };
+VECT_VAR_DECL(expected_max_sh_minus3,int,64,1) [] = { 0x1000000000000000 };
+VECT_VAR_DECL(expected_max_sh_minus3,uint,8,8) [] = { 0x20, 0x20, 0x20, 0x20,
+						      0x20, 0x20, 0x20, 0x20 };
+VECT_VAR_DECL(expected_max_sh_minus3,uint,16,4) [] = { 0x2000, 0x2000,
+						       0x2000, 0x2000 };
+VECT_VAR_DECL(expected_max_sh_minus3,uint,32,2) [] = { 0x20000000, 0x20000000 };
+VECT_VAR_DECL(expected_max_sh_minus3,uint,64,1) [] = { 0x2000000000000000 };
+VECT_VAR_DECL(expected_max_sh_minus3,int,8,16) [] = { 0x10, 0x10, 0x10, 0x10,
+						      0x10, 0x10, 0x10, 0x10,
+						      0x10, 0x10, 0x10, 0x10,
+						      0x10, 0x10, 0x10, 0x10 };
+VECT_VAR_DECL(expected_max_sh_minus3,int,16,8) [] = { 0x1000, 0x1000,
+						      0x1000, 0x1000,
+						      0x1000, 0x1000,
+						      0x1000, 0x1000 };
+VECT_VAR_DECL(expected_max_sh_minus3,int,32,4) [] = { 0x10000000, 0x10000000,
+						      0x10000000, 0x10000000 };
+VECT_VAR_DECL(expected_max_sh_minus3,int,64,2) [] = { 0x1000000000000000,
+						      0x1000000000000000 };
+VECT_VAR_DECL(expected_max_sh_minus3,uint,8,16) [] = { 0x20, 0x20, 0x20, 0x20,
+						       0x20, 0x20, 0x20, 0x20,
+						       0x20, 0x20, 0x20, 0x20,
+						       0x20, 0x20, 0x20, 0x20 };
+VECT_VAR_DECL(expected_max_sh_minus3,uint,16,8) [] = { 0x2000, 0x2000,
+						       0x2000, 0x2000,
+						       0x2000, 0x2000,
+						       0x2000, 0x2000 };
+VECT_VAR_DECL(expected_max_sh_minus3,uint,32,4) [] = { 0x20000000, 0x20000000,
+						       0x20000000, 0x20000000 };
+VECT_VAR_DECL(expected_max_sh_minus3,uint,64,2) [] = { 0x2000000000000000,
+						       0x2000000000000000 };
+
+/* Expected results with negative shift by vector width.  */
+VECT_VAR_DECL(expected_max_sh_minus_width,int,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+							  0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_max_sh_minus_width,int,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_max_sh_minus_width,int,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_max_sh_minus_width,int,64,1) [] = { 0x0 };
+VECT_VAR_DECL(expected_max_sh_minus_width,uint,8,8) [] = { 0x1, 0x1, 0x1, 0x1,
+							   0x1, 0x1, 0x1, 0x1 };
+VECT_VAR_DECL(expected_max_sh_minus_width,uint,16,4) [] = { 0x1, 0x1, 0x1, 0x1 };
+VECT_VAR_DECL(expected_max_sh_minus_width,uint,32,2) [] = { 0x1, 0x1 };
+VECT_VAR_DECL(expected_max_sh_minus_width,uint,64,1) [] = { 0x1 };
+VECT_VAR_DECL(expected_max_sh_minus_width,int,8,16) [] = { 0x0, 0x0, 0x0, 0x0,
+							   0x0, 0x0, 0x0, 0x0,
+							   0x0, 0x0, 0x0, 0x0,
+							   0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_max_sh_minus_width,int,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+							   0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_max_sh_minus_width,int,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_max_sh_minus_width,int,64,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_max_sh_minus_width,uint,8,16) [] = { 0x1, 0x1, 0x1, 0x1,
+							    0x1, 0x1, 0x1, 0x1,
+							    0x1, 0x1, 0x1, 0x1,
+							    0x1, 0x1, 0x1, 0x1 };
+VECT_VAR_DECL(expected_max_sh_minus_width,uint,16,8) [] = { 0x1, 0x1, 0x1, 0x1,
+							    0x1, 0x1, 0x1, 0x1 };
+VECT_VAR_DECL(expected_max_sh_minus_width,uint,32,4) [] = { 0x1, 0x1, 0x1, 0x1 };
+VECT_VAR_DECL(expected_max_sh_minus_width,uint,64,2) [] = { 0x1, 0x1 };
+
+/* Expected results with large shift amount.  */
+VECT_VAR_DECL(expected_max_sh_large,int,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+						    0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_max_sh_large,int,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_max_sh_large,int,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_max_sh_large,int,64,1) [] = { 0x0 };
+VECT_VAR_DECL(expected_max_sh_large,uint,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+						     0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_max_sh_large,uint,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_max_sh_large,uint,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_max_sh_large,uint,64,1) [] = { 0x0 };
+VECT_VAR_DECL(expected_max_sh_large,int,8,16) [] = { 0x0, 0x0, 0x0, 0x0,
+						     0x0, 0x0, 0x0, 0x0,
+						     0x0, 0x0, 0x0, 0x0,
+						     0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_max_sh_large,int,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+						     0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_max_sh_large,int,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_max_sh_large,int,64,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_max_sh_large,uint,8,16) [] = { 0x0, 0x0, 0x0, 0x0,
+						      0x0, 0x0, 0x0, 0x0,
+						      0x0, 0x0, 0x0, 0x0,
+						      0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_max_sh_large,uint,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+						      0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_max_sh_large,uint,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_max_sh_large,uint,64,2) [] = { 0x0, 0x0 };
+
+/* Expected results with large negative shift amount.  */
+VECT_VAR_DECL(expected_max_sh_large_neg,int,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+							0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_max_sh_large_neg,int,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_max_sh_large_neg,int,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_max_sh_large_neg,int,64,1) [] = { 0x0 };
+VECT_VAR_DECL(expected_max_sh_large_neg,uint,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+							 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_max_sh_large_neg,uint,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_max_sh_large_neg,uint,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_max_sh_large_neg,uint,64,1) [] = { 0x0 };
+VECT_VAR_DECL(expected_max_sh_large_neg,int,8,16) [] = { 0x0, 0x0, 0x0, 0x0,
+							 0x0, 0x0, 0x0, 0x0,
+							 0x0, 0x0, 0x0, 0x0,
+							 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_max_sh_large_neg,int,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+							 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_max_sh_large_neg,int,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_max_sh_large_neg,int,64,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_max_sh_large_neg,uint,8,16) [] = { 0x0, 0x0, 0x0, 0x0,
+							  0x0, 0x0, 0x0, 0x0,
+							  0x0, 0x0, 0x0, 0x0,
+							  0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_max_sh_large_neg,uint,16,8) [] = { 0x1, 0x1, 0x1, 0x1,
+							  0x1, 0x1, 0x1, 0x1 };
+VECT_VAR_DECL(expected_max_sh_large_neg,uint,32,4) [] = { 0x1, 0x1, 0x1, 0x1 };
+VECT_VAR_DECL(expected_max_sh_large_neg,uint,64,2) [] = { 0x1, 0x1 };
+
+#define TEST_MSG "VRSHL/VRSHLQ"
+void exec_vrshl (void)
+{
+  /* Basic test: v3=vrshl(v1,v2), then store the result.  */
+#define TEST_VRSHL(T3, Q, T1, T2, W, N)					\
+  VECT_VAR(vector_res, T1, W, N) =					\
+    vrshl##Q##_##T2##W(VECT_VAR(vector, T1, W, N),			\
+		       VECT_VAR(vector_shift, T3, W, N));		\
+  vst1##Q##_##T2##W(VECT_VAR(result, T1, W, N), VECT_VAR(vector_res, T1, W, N))
+
+  DECL_VARIABLE_ALL_VARIANTS(vector);
+  DECL_VARIABLE_ALL_VARIANTS(vector_res);
+
+  DECL_VARIABLE_SIGNED_VARIANTS(vector_shift);
+
+  clean_results ();
+
+  /* Fill input vector with 0, to check behavior on limits.  */
+  VDUP(vector, , int, s, 8, 8, 0);
+  VDUP(vector, , int, s, 16, 4, 0);
+  VDUP(vector, , int, s, 32, 2, 0);
+  VDUP(vector, , int, s, 64, 1, 0);
+  VDUP(vector, , uint, u, 8, 8, 0);
+  VDUP(vector, , uint, u, 16, 4, 0);
+  VDUP(vector, , uint, u, 32, 2, 0);
+  VDUP(vector, , uint, u, 64, 1, 0);
+  VDUP(vector, q, int, s, 8, 16, 0);
+  VDUP(vector, q, int, s, 16, 8, 0);
+  VDUP(vector, q, int, s, 32, 4, 0);
+  VDUP(vector, q, int, s, 64, 2, 0);
+  VDUP(vector, q, uint, u, 8, 16, 0);
+  VDUP(vector, q, uint, u, 16, 8, 0);
+  VDUP(vector, q, uint, u, 32, 4, 0);
+  VDUP(vector, q, uint, u, 64, 2, 0);
+
+  /* Choose init value arbitrarily, will be used as shift amount.  */
+  /* Use values equal to one-less-than the type width to check
+     behaviour on limits.  */
+  VDUP(vector_shift, , int, s, 8, 8, 7);
+  VDUP(vector_shift, , int, s, 16, 4, 15);
+  VDUP(vector_shift, , int, s, 32, 2, 31);
+  VDUP(vector_shift, , int, s, 64, 1, 63);
+  VDUP(vector_shift, q, int, s, 8, 16, 7);
+  VDUP(vector_shift, q, int, s, 16, 8, 15);
+  VDUP(vector_shift, q, int, s, 32, 4, 31);
+  VDUP(vector_shift, q, int, s, 64, 2, 63);
+
+  TEST_MACRO_ALL_VARIANTS_1_5(TEST_VRSHL, int);
+
+#define CMT " (with input = 0)"
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_0, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_0, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_0, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_0, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_0, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_0, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_0, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_0, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_0, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_0, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_0, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_0, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_0, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_0, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_0, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_0, CMT);
+
+
+  /* Use negative shift amounts.  */
+  VDUP(vector_shift, , int, s, 8, 8, -1);
+  VDUP(vector_shift, , int, s, 16, 4, -2);
+  VDUP(vector_shift, , int, s, 32, 2, -3);
+  VDUP(vector_shift, , int, s, 64, 1, -4);
+  VDUP(vector_shift, q, int, s, 8, 16, -7);
+  VDUP(vector_shift, q, int, s, 16, 8, -11);
+  VDUP(vector_shift, q, int, s, 32, 4, -13);
+  VDUP(vector_shift, q, int, s, 64, 2, -20);
+
+  TEST_MACRO_ALL_VARIANTS_1_5(TEST_VRSHL, int);
+
+#undef CMT
+#define CMT " (input 0 and negative shift amount)"
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_0_sh_neg, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_0_sh_neg, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_0_sh_neg, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_0_sh_neg, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_0_sh_neg, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_0_sh_neg, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_0_sh_neg, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_0_sh_neg, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_0_sh_neg, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_0_sh_neg, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_0_sh_neg, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_0_sh_neg, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_0_sh_neg, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_0_sh_neg, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_0_sh_neg, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_0_sh_neg, CMT);
+
+
+  /* Test again, with predefined input values.  */
+  TEST_MACRO_ALL_VARIANTS_2_5(VLOAD, vector, buffer);
+
+  /* Choose init value arbitrarily, will be used as shift amount.  */
+  VDUP(vector_shift, , int, s, 8, 8, 1);
+  VDUP(vector_shift, , int, s, 16, 4, 3);
+  VDUP(vector_shift, , int, s, 32, 2, 8);
+  VDUP(vector_shift, , int, s, 64, 1, -3);
+  VDUP(vector_shift, q, int, s, 8, 16, 10);
+  VDUP(vector_shift, q, int, s, 16, 8, 12);
+  VDUP(vector_shift, q, int, s, 32, 4, 32);
+  VDUP(vector_shift, q, int, s, 64, 2, 63);
+
+  TEST_MACRO_ALL_VARIANTS_1_5(TEST_VRSHL, int);
+
+#undef CMT
+#define CMT ""
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected, CMT);
+
+
+  /* Use negative shift amounts.  */
+  VDUP(vector_shift, , int, s, 8, 8, -1);
+  VDUP(vector_shift, , int, s, 16, 4, -2);
+  VDUP(vector_shift, , int, s, 32, 2, -3);
+  VDUP(vector_shift, , int, s, 64, 1, -4);
+  VDUP(vector_shift, q, int, s, 8, 16, -7);
+  VDUP(vector_shift, q, int, s, 16, 8, -11);
+  VDUP(vector_shift, q, int, s, 32, 4, -13);
+  VDUP(vector_shift, q, int, s, 64, 2, -20);
+
+  TEST_MACRO_ALL_VARIANTS_1_5(TEST_VRSHL, int);
+
+#undef CMT
+#define CMT " (negative shift amount)"
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_sh_neg, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_sh_neg, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_sh_neg, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_sh_neg, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_sh_neg, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_sh_neg, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_sh_neg, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_sh_neg, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_sh_neg, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_sh_neg, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_sh_neg, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_sh_neg, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_sh_neg, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_sh_neg, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_sh_neg, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_sh_neg, CMT);
+
+  /* Fill input vector with max value, to check behavior on limits.  */
+  VDUP(vector, , int, s, 8, 8, 0x7F);
+  VDUP(vector, , int, s, 16, 4, 0x7FFF);
+  VDUP(vector, , int, s, 32, 2, 0x7FFFFFFF);
+  VDUP(vector, , int, s, 64, 1, 0x7FFFFFFFFFFFFFFFLL);
+  VDUP(vector, , uint, u, 8, 8, 0xFF);
+  VDUP(vector, , uint, u, 16, 4, 0xFFFF);
+  VDUP(vector, , uint, u, 32, 2, 0xFFFFFFFF);
+  VDUP(vector, , uint, u, 64, 1, 0xFFFFFFFFFFFFFFFFULL);
+  VDUP(vector, q, int, s, 8, 16, 0x7F);
+  VDUP(vector, q, int, s, 16, 8, 0x7FFF);
+  VDUP(vector, q, int, s, 32, 4, 0x7FFFFFFF);
+  VDUP(vector, q, int, s, 64, 2, 0x7FFFFFFFFFFFFFFFLL);
+  VDUP(vector, q, uint, u, 8, 16, 0xFF);
+  VDUP(vector, q, uint, u, 16, 8, 0xFFFF);
+  VDUP(vector, q, uint, u, 32, 4, 0xFFFFFFFF);
+  VDUP(vector, q, uint, u, 64, 2, 0xFFFFFFFFFFFFFFFFULL);
+
+  /* Use -1 shift amount to check overflow with round_const.  */
+  VDUP(vector_shift, , int, s, 8, 8, -1);
+  VDUP(vector_shift, , int, s, 16, 4, -1);
+  VDUP(vector_shift, , int, s, 32, 2, -1);
+  VDUP(vector_shift, , int, s, 64, 1, -1);
+  VDUP(vector_shift, q, int, s, 8, 16, -1);
+  VDUP(vector_shift, q, int, s, 16, 8, -1);
+  VDUP(vector_shift, q, int, s, 32, 4, -1);
+  VDUP(vector_shift, q, int, s, 64, 2, -1);
+
+  TEST_MACRO_ALL_VARIANTS_1_5(TEST_VRSHL, int);
+
+#undef CMT
+#define CMT " (max input, shift by -1)"
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_max_sh_minus1, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_max_sh_minus1, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_max_sh_minus1, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_max_sh_minus1, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_max_sh_minus1, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_max_sh_minus1, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_max_sh_minus1, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_max_sh_minus1, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_max_sh_minus1, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_max_sh_minus1, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_max_sh_minus1, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_max_sh_minus1, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_max_sh_minus1, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_max_sh_minus1, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_max_sh_minus1, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_max_sh_minus1, CMT);
+
+  /* Use -3 shift amount to check overflow with round_const.  */
+  VDUP(vector_shift, , int, s, 8, 8, -3);
+  VDUP(vector_shift, , int, s, 16, 4, -3);
+  VDUP(vector_shift, , int, s, 32, 2, -3);
+  VDUP(vector_shift, , int, s, 64, 1, -3);
+  VDUP(vector_shift, q, int, s, 8, 16, -3);
+  VDUP(vector_shift, q, int, s, 16, 8, -3);
+  VDUP(vector_shift, q, int, s, 32, 4, -3);
+  VDUP(vector_shift, q, int, s, 64, 2, -3);
+
+  TEST_MACRO_ALL_VARIANTS_1_5(TEST_VRSHL, int);
+
+#undef CMT
+#define CMT " (check rounding constant: max input, shift by -3)"
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_max_sh_minus3, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_max_sh_minus3, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_max_sh_minus3, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_max_sh_minus3, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_max_sh_minus3, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_max_sh_minus3, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_max_sh_minus3, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_max_sh_minus3, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_max_sh_minus3, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_max_sh_minus3, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_max_sh_minus3, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_max_sh_minus3, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_max_sh_minus3, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_max_sh_minus3, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_max_sh_minus3, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_max_sh_minus3, CMT);
+
+
+  /* Use negative shift amount as large as input vector width.  */
+  VDUP(vector_shift, , int, s, 8, 8, -8);
+  VDUP(vector_shift, , int, s, 16, 4, -16);
+  VDUP(vector_shift, , int, s, 32, 2, -32);
+  VDUP(vector_shift, , int, s, 64, 1, -64);
+  VDUP(vector_shift, q, int, s, 8, 16, -8);
+  VDUP(vector_shift, q, int, s, 16, 8, -16);
+  VDUP(vector_shift, q, int, s, 32, 4, -32);
+  VDUP(vector_shift, q, int, s, 64, 2, -64);
+
+  TEST_MACRO_ALL_VARIANTS_1_5(TEST_VRSHL, int);
+
+#undef CMT
+#define CMT " (max input, right shift by vector width)"
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_max_sh_minus_width, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_max_sh_minus_width, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_max_sh_minus_width, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_max_sh_minus_width, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_max_sh_minus_width, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_max_sh_minus_width, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_max_sh_minus_width, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_max_sh_minus_width, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_max_sh_minus_width, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_max_sh_minus_width, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_max_sh_minus_width, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_max_sh_minus_width, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_max_sh_minus_width, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_max_sh_minus_width, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_max_sh_minus_width, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_max_sh_minus_width, CMT);
+
+
+  /* Test large shift amount.  */
+  VDUP(vector_shift, , int, s, 8, 8, 10);
+  VDUP(vector_shift, , int, s, 16, 4, 20);
+  VDUP(vector_shift, , int, s, 32, 2, 33);
+  VDUP(vector_shift, , int, s, 64, 1, 65);
+  VDUP(vector_shift, q, int, s, 8, 16, 9);
+  VDUP(vector_shift, q, int, s, 16, 8, 16);
+  VDUP(vector_shift, q, int, s, 32, 4, 32);
+  VDUP(vector_shift, q, int, s, 64, 2, 64);
+
+  TEST_MACRO_ALL_VARIANTS_1_5(TEST_VRSHL, int);
+
+#undef CMT
+#define CMT " (max input, large shift amount)"
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_max_sh_large, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_max_sh_large, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_max_sh_large, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_max_sh_large, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_max_sh_large, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_max_sh_large, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_max_sh_large, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_max_sh_large, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_max_sh_large, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_max_sh_large, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_max_sh_large, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_max_sh_large, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_max_sh_large, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_max_sh_large, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_max_sh_large, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_max_sh_large, CMT);
+
+  
+  /* Test large negative shift amount.  */
+  VDUP(vector_shift, , int, s, 8, 8, -10);
+  VDUP(vector_shift, , int, s, 16, 4, -20);
+  VDUP(vector_shift, , int, s, 32, 2, -33);
+  VDUP(vector_shift, , int, s, 64, 1, -65);
+  VDUP(vector_shift, q, int, s, 8, 16, -9);
+  VDUP(vector_shift, q, int, s, 16, 8, -16);
+  VDUP(vector_shift, q, int, s, 32, 4, -32);
+  VDUP(vector_shift, q, int, s, 64, 2, -64);
+
+  TEST_MACRO_ALL_VARIANTS_1_5(TEST_VRSHL, int);
+
+#undef CMT
+#define CMT " (max input, large negative shift amount)"
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_max_sh_large_neg, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_max_sh_large_neg, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_max_sh_large_neg, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_max_sh_large_neg, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_max_sh_large_neg, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_max_sh_large_neg, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_max_sh_large_neg, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_max_sh_large_neg, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_max_sh_large_neg, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_max_sh_large_neg, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_max_sh_large_neg, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_max_sh_large_neg, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_max_sh_large_neg, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_max_sh_large_neg, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_max_sh_large_neg, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_max_sh_large_neg, CMT);
+}
+
+int main (void)
+{
+  exec_vrshl ();
+  return 0;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vrshr_n.c
@@ -0,0 +1,504 @@
+#include <arm_neon.h>
+#include "arm-neon-ref.h"
+#include "compute-ref-data.h"
+
+/* Expected results.  */
+VECT_VAR_DECL(expected,int,8,8) [] = { 0xf8, 0xf9, 0xf9, 0xfa,
+				       0xfa, 0xfb, 0xfb, 0xfc };
+VECT_VAR_DECL(expected,int,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected,int,32,2) [] = { 0xfffffffc, 0xfffffffc };
+VECT_VAR_DECL(expected,int,64,1) [] = { 0x0 };
+VECT_VAR_DECL(expected,uint,8,8) [] = { 0x3c, 0x3c, 0x3d, 0x3d,
+					0x3d, 0x3d, 0x3e, 0x3e };
+VECT_VAR_DECL(expected,uint,16,4) [] = { 0x1ffe, 0x1ffe, 0x1ffe, 0x1ffe };
+VECT_VAR_DECL(expected,uint,32,2) [] = { 0x8000000, 0x8000000 };
+VECT_VAR_DECL(expected,uint,64,1) [] = { 0x80000000 };
+VECT_VAR_DECL(expected,int,8,16) [] = { 0xf8, 0xf9, 0xf9, 0xfa,
+					0xfa, 0xfb, 0xfb, 0xfc,
+					0xfc, 0xfd, 0xfd, 0xfe,
+					0xfe, 0xff, 0xff, 0x0 };
+VECT_VAR_DECL(expected,int,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected,int,32,4) [] = { 0xfffffffc, 0xfffffffc,
+					0xfffffffd, 0xfffffffd };
+VECT_VAR_DECL(expected,int,64,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected,uint,8,16) [] = { 0x3c, 0x3c, 0x3d, 0x3d,
+					 0x3d, 0x3d, 0x3e, 0x3e,
+					 0x3e, 0x3e, 0x3f, 0x3f,
+					 0x3f, 0x3f, 0x40, 0x40 };
+VECT_VAR_DECL(expected,uint,16,8) [] = { 0x1ffe, 0x1ffe, 0x1ffe, 0x1ffe,
+					 0x1fff, 0x1fff, 0x1fff, 0x1fff };
+VECT_VAR_DECL(expected,uint,32,4) [] = { 0x8000000, 0x8000000,
+					 0x8000000, 0x8000000 };
+VECT_VAR_DECL(expected,uint,64,2) [] = { 0x80000000, 0x80000000 };
+
+/* Expected results with maximum input and max shift amount.  */
+VECT_VAR_DECL(expected_max_sh_max,int,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+						  0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_max_sh_max,int,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_max_sh_max,int,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_max_sh_max,int,64,1) [] = { 0x0 };
+VECT_VAR_DECL(expected_max_sh_max,uint,8,8) [] = { 0x1, 0x1, 0x1, 0x1,
+						   0x1, 0x1, 0x1, 0x1 };
+VECT_VAR_DECL(expected_max_sh_max,uint,16,4) [] = { 0x1, 0x1, 0x1, 0x1 };
+VECT_VAR_DECL(expected_max_sh_max,uint,32,2) [] = { 0x1, 0x1 };
+VECT_VAR_DECL(expected_max_sh_max,uint,64,1) [] = { 0x1 };
+VECT_VAR_DECL(expected_max_sh_max,int,8,16) [] = { 0x0, 0x0, 0x0, 0x0,
+						   0x0, 0x0, 0x0, 0x0,
+						   0x0, 0x0, 0x0, 0x0,
+						   0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_max_sh_max,int,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+						   0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_max_sh_max,int,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_max_sh_max,int,64,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_max_sh_max,uint,8,16) [] = { 0x1, 0x1, 0x1, 0x1,
+						    0x1, 0x1, 0x1, 0x1,
+						    0x1, 0x1, 0x1, 0x1,
+						    0x1, 0x1, 0x1, 0x1 };
+VECT_VAR_DECL(expected_max_sh_max,uint,16,8) [] = { 0x1, 0x1, 0x1, 0x1,
+						    0x1, 0x1, 0x1, 0x1 };
+VECT_VAR_DECL(expected_max_sh_max,uint,32,4) [] = { 0x1, 0x1, 0x1, 0x1 };
+VECT_VAR_DECL(expected_max_sh_max,uint,64,2) [] = { 0x1, 0x1 };
+
+/* Expected results with maximum input and shift by 1.  */
+VECT_VAR_DECL(expected_max_sh_1,int,8,8) [] = { 0x40, 0x40, 0x40, 0x40,
+						0x40, 0x40, 0x40, 0x40 };
+VECT_VAR_DECL(expected_max_sh_1,int,16,4) [] = { 0x4000, 0x4000,
+						 0x4000, 0x4000 };
+VECT_VAR_DECL(expected_max_sh_1,int,32,2) [] = { 0x40000000, 0x40000000 };
+VECT_VAR_DECL(expected_max_sh_1,int,64,1) [] = { 0x4000000000000000 };
+VECT_VAR_DECL(expected_max_sh_1,uint,8,8) [] = { 0x80, 0x80, 0x80, 0x80,
+						 0x80, 0x80, 0x80, 0x80 };
+VECT_VAR_DECL(expected_max_sh_1,uint,16,4) [] = { 0x8000, 0x8000,
+						  0x8000, 0x8000 };
+VECT_VAR_DECL(expected_max_sh_1,uint,32,2) [] = { 0x80000000, 0x80000000 };
+VECT_VAR_DECL(expected_max_sh_1,uint,64,1) [] = { 0x8000000000000000 };
+VECT_VAR_DECL(expected_max_sh_1,int,8,16) [] = { 0x40, 0x40, 0x40, 0x40,
+						 0x40, 0x40, 0x40, 0x40,
+						 0x40, 0x40, 0x40, 0x40,
+						 0x40, 0x40, 0x40, 0x40 };
+VECT_VAR_DECL(expected_max_sh_1,int,16,8) [] = { 0x4000, 0x4000,
+						 0x4000, 0x4000,
+						 0x4000, 0x4000,
+						 0x4000, 0x4000 };
+VECT_VAR_DECL(expected_max_sh_1,int,32,4) [] = { 0x40000000, 0x40000000,
+						 0x40000000, 0x40000000 };
+VECT_VAR_DECL(expected_max_sh_1,int,64,2) [] = { 0x4000000000000000,
+						 0x4000000000000000 };
+VECT_VAR_DECL(expected_max_sh_1,uint,8,16) [] = { 0x80, 0x80, 0x80, 0x80,
+						  0x80, 0x80, 0x80, 0x80,
+						  0x80, 0x80, 0x80, 0x80,
+						  0x80, 0x80, 0x80, 0x80 };
+VECT_VAR_DECL(expected_max_sh_1,uint,16,8) [] = { 0x8000, 0x8000,
+						  0x8000, 0x8000,
+						  0x8000, 0x8000,
+						  0x8000, 0x8000 };
+VECT_VAR_DECL(expected_max_sh_1,uint,32,4) [] = { 0x80000000, 0x80000000,
+						  0x80000000, 0x80000000 };
+VECT_VAR_DECL(expected_max_sh_1,uint,64,2) [] = { 0x8000000000000000,
+						  0x8000000000000000 };
+
+/* Expected results with maximum input and shift by 3.  */
+VECT_VAR_DECL(expected_max_sh_3,int,8,8) [] = { 0x10, 0x10, 0x10, 0x10,
+						0x10, 0x10, 0x10, 0x10 };
+VECT_VAR_DECL(expected_max_sh_3,int,16,4) [] = { 0x1000, 0x1000,
+						 0x1000, 0x1000 };
+VECT_VAR_DECL(expected_max_sh_3,int,32,2) [] = { 0x10000000, 0x10000000 };
+VECT_VAR_DECL(expected_max_sh_3,int,64,1) [] = { 0x1000000000000000 };
+VECT_VAR_DECL(expected_max_sh_3,uint,8,8) [] = { 0x20, 0x20, 0x20, 0x20,
+						 0x20, 0x20, 0x20, 0x20 };
+VECT_VAR_DECL(expected_max_sh_3,uint,16,4) [] = { 0x2000, 0x2000,
+						  0x2000, 0x2000 };
+VECT_VAR_DECL(expected_max_sh_3,uint,32,2) [] = { 0x20000000, 0x20000000 };
+VECT_VAR_DECL(expected_max_sh_3,uint,64,1) [] = { 0x2000000000000000 };
+VECT_VAR_DECL(expected_max_sh_3,int,8,16) [] = { 0x10, 0x10, 0x10, 0x10,
+						 0x10, 0x10, 0x10, 0x10,
+						 0x10, 0x10, 0x10, 0x10,
+						 0x10, 0x10, 0x10, 0x10 };
+VECT_VAR_DECL(expected_max_sh_3,int,16,8) [] = { 0x1000, 0x1000,
+						 0x1000, 0x1000,
+						 0x1000, 0x1000,
+						 0x1000, 0x1000 };
+VECT_VAR_DECL(expected_max_sh_3,int,32,4) [] = { 0x10000000, 0x10000000,
+						 0x10000000, 0x10000000 };
+VECT_VAR_DECL(expected_max_sh_3,int,64,2) [] = { 0x1000000000000000,
+						 0x1000000000000000 };
+VECT_VAR_DECL(expected_max_sh_3,uint,8,16) [] = { 0x20, 0x20, 0x20, 0x20,
+						  0x20, 0x20, 0x20, 0x20,
+						  0x20, 0x20, 0x20, 0x20,
+						  0x20, 0x20, 0x20, 0x20 };
+VECT_VAR_DECL(expected_max_sh_3,uint,16,8) [] = { 0x2000, 0x2000,
+						  0x2000, 0x2000,
+						  0x2000, 0x2000,
+						  0x2000, 0x2000 };
+VECT_VAR_DECL(expected_max_sh_3,uint,32,4) [] = { 0x20000000, 0x20000000,
+						  0x20000000, 0x20000000 };
+VECT_VAR_DECL(expected_max_sh_3,uint,64,2) [] = { 0x2000000000000000,
+						  0x2000000000000000 };
+
+/* Expected results with max negative input (for signed types, shift
+   by 1.  */
+VECT_VAR_DECL(expected_max_neg_sh_1,int,8,8) [] = { 0xc0, 0xc0, 0xc0, 0xc0,
+						    0xc0, 0xc0, 0xc0, 0xc0 };
+VECT_VAR_DECL(expected_max_neg_sh_1,int,16,4) [] = { 0xc000, 0xc000,
+						     0xc000, 0xc000 };
+VECT_VAR_DECL(expected_max_neg_sh_1,int,32,2) [] = { 0xc0000000, 0xc0000000 };
+VECT_VAR_DECL(expected_max_neg_sh_1,int,64,1) [] = { 0xc000000000000000 };
+VECT_VAR_DECL(expected_max_neg_sh_1,uint,8,8) [] = { 0x80, 0x80, 0x80, 0x80,
+						     0x80, 0x80, 0x80, 0x80 };
+VECT_VAR_DECL(expected_max_neg_sh_1,uint,16,4) [] = { 0x8000, 0x8000,
+						      0x8000, 0x8000 };
+VECT_VAR_DECL(expected_max_neg_sh_1,uint,32,2) [] = { 0x80000000, 0x80000000 };
+VECT_VAR_DECL(expected_max_neg_sh_1,uint,64,1) [] = { 0x8000000000000000 };
+VECT_VAR_DECL(expected_max_neg_sh_1,int,8,16) [] = { 0xc0, 0xc0, 0xc0, 0xc0,
+						     0xc0, 0xc0, 0xc0, 0xc0,
+						     0xc0, 0xc0, 0xc0, 0xc0,
+						     0xc0, 0xc0, 0xc0, 0xc0 };
+VECT_VAR_DECL(expected_max_neg_sh_1,int,16,8) [] = { 0xc000, 0xc000,
+						     0xc000, 0xc000,
+						     0xc000, 0xc000,
+						     0xc000, 0xc000 };
+VECT_VAR_DECL(expected_max_neg_sh_1,int,32,4) [] = { 0xc0000000, 0xc0000000,
+						     0xc0000000, 0xc0000000 };
+VECT_VAR_DECL(expected_max_neg_sh_1,int,64,2) [] = { 0xc000000000000000,
+						     0xc000000000000000 };
+VECT_VAR_DECL(expected_max_neg_sh_1,uint,8,16) [] = { 0x80, 0x80, 0x80, 0x80,
+						      0x80, 0x80, 0x80, 0x80,
+						      0x80, 0x80, 0x80, 0x80,
+						      0x80, 0x80, 0x80, 0x80 };
+VECT_VAR_DECL(expected_max_neg_sh_1,uint,16,8) [] = { 0x8000, 0x8000,
+						      0x8000, 0x8000,
+						      0x8000, 0x8000,
+						      0x8000, 0x8000 };
+VECT_VAR_DECL(expected_max_neg_sh_1,uint,32,4) [] = { 0x80000000, 0x80000000,
+						      0x80000000, 0x80000000 };
+VECT_VAR_DECL(expected_max_neg_sh_1,uint,64,2) [] = { 0x8000000000000000,
+						      0x8000000000000000 };
+
+/* Expected results with max negative input (for signed types, shift
+   by 3.  */
+VECT_VAR_DECL(expected_max_neg_sh_3,int,8,8) [] = { 0xf0, 0xf0, 0xf0, 0xf0,
+						    0xf0, 0xf0, 0xf0, 0xf0 };
+VECT_VAR_DECL(expected_max_neg_sh_3,int,16,4) [] = { 0xf000, 0xf000,
+						     0xf000, 0xf000 };
+VECT_VAR_DECL(expected_max_neg_sh_3,int,32,2) [] = { 0xf0000000, 0xf0000000 };
+VECT_VAR_DECL(expected_max_neg_sh_3,int,64,1) [] = { 0xf000000000000000 };
+VECT_VAR_DECL(expected_max_neg_sh_3,uint,8,8) [] = { 0x20, 0x20, 0x20, 0x20,
+						     0x20, 0x20, 0x20, 0x20 };
+VECT_VAR_DECL(expected_max_neg_sh_3,uint,16,4) [] = { 0x2000, 0x2000,
+						      0x2000, 0x2000 };
+VECT_VAR_DECL(expected_max_neg_sh_3,uint,32,2) [] = { 0x20000000, 0x20000000 };
+VECT_VAR_DECL(expected_max_neg_sh_3,uint,64,1) [] = { 0x2000000000000000 };
+VECT_VAR_DECL(expected_max_neg_sh_3,int,8,16) [] = { 0xf0, 0xf0, 0xf0, 0xf0,
+						     0xf0, 0xf0, 0xf0, 0xf0,
+						     0xf0, 0xf0, 0xf0, 0xf0,
+						     0xf0, 0xf0, 0xf0, 0xf0 };
+VECT_VAR_DECL(expected_max_neg_sh_3,int,16,8) [] = { 0xf000, 0xf000,
+						     0xf000, 0xf000,
+						     0xf000, 0xf000,
+						     0xf000, 0xf000 };
+VECT_VAR_DECL(expected_max_neg_sh_3,int,32,4) [] = { 0xf0000000, 0xf0000000,
+						     0xf0000000, 0xf0000000 };
+VECT_VAR_DECL(expected_max_neg_sh_3,int,64,2) [] = { 0xf000000000000000,
+						     0xf000000000000000 };
+VECT_VAR_DECL(expected_max_neg_sh_3,uint,8,16) [] = { 0x20, 0x20, 0x20, 0x20,
+						      0x20, 0x20, 0x20, 0x20,
+						      0x20, 0x20, 0x20, 0x20,
+						      0x20, 0x20, 0x20, 0x20 };
+VECT_VAR_DECL(expected_max_neg_sh_3,uint,16,8) [] = { 0x2000, 0x2000,
+						      0x2000, 0x2000,
+						      0x2000, 0x2000,
+						      0x2000, 0x2000 };
+VECT_VAR_DECL(expected_max_neg_sh_3,uint,32,4) [] = { 0x20000000, 0x20000000,
+						      0x20000000, 0x20000000 };
+VECT_VAR_DECL(expected_max_neg_sh_3,uint,64,2) [] = { 0x2000000000000000,
+						      0x2000000000000000 };
+
+#define TEST_MSG "VRSHR_N"
+void exec_vrshr_n (void)
+{
+  /* Basic test: y=vrshr_n(x,v), then store the result.  */
+#define TEST_VRSHR_N(Q, T1, T2, W, N, V)				\
+  VECT_VAR(vector_res, T1, W, N) =					\
+    vrshr##Q##_n_##T2##W(VECT_VAR(vector, T1, W, N),			\
+			 V);						\
+  vst1##Q##_##T2##W(VECT_VAR(result, T1, W, N), VECT_VAR(vector_res, T1, W, N))
+
+  DECL_VARIABLE_ALL_VARIANTS(vector);
+  DECL_VARIABLE_ALL_VARIANTS(vector_res);
+
+  clean_results ();
+
+  /* Initialize input "vector" from "buffer".  */
+  TEST_MACRO_ALL_VARIANTS_2_5(VLOAD, vector, buffer);
+
+  /* Choose shift amount arbitrarily.  */
+  TEST_VRSHR_N(, int, s, 8, 8, 1);
+  TEST_VRSHR_N(, int, s, 16, 4, 12);
+  TEST_VRSHR_N(, int, s, 32, 2, 2);
+  TEST_VRSHR_N(, int, s, 64, 1, 32);
+  TEST_VRSHR_N(, uint, u, 8, 8, 2);
+  TEST_VRSHR_N(, uint, u, 16, 4, 3);
+  TEST_VRSHR_N(, uint, u, 32, 2, 5);
+  TEST_VRSHR_N(, uint, u, 64, 1, 33);
+
+  TEST_VRSHR_N(q, int, s, 8, 16, 1);
+  TEST_VRSHR_N(q, int, s, 16, 8, 12);
+  TEST_VRSHR_N(q, int, s, 32, 4, 2);
+  TEST_VRSHR_N(q, int, s, 64, 2, 32);
+  TEST_VRSHR_N(q, uint, u, 8, 16, 2);
+  TEST_VRSHR_N(q, uint, u, 16, 8, 3);
+  TEST_VRSHR_N(q, uint, u, 32, 4, 5);
+  TEST_VRSHR_N(q, uint, u, 64, 2, 33);
+
+#define CMT ""
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected, CMT);
+
+
+  /* Use maximum positive input value.  */
+  VDUP(vector, , int, s, 8, 8, 0x7F);
+  VDUP(vector, , int, s, 16, 4, 0x7FFF);
+  VDUP(vector, , int, s, 32, 2, 0x7FFFFFFF);
+  VDUP(vector, , int, s, 64, 1, 0x7FFFFFFFFFFFFFFFLL);
+  VDUP(vector, , uint, u, 8, 8, 0xFF);
+  VDUP(vector, , uint, u, 16, 4, 0xFFFF);
+  VDUP(vector, , uint, u, 32, 2, 0xFFFFFFFF);
+  VDUP(vector, , uint, u, 64, 1, 0xFFFFFFFFFFFFFFFFULL);
+  VDUP(vector, q, int, s, 8, 16, 0x7F);
+  VDUP(vector, q, int, s, 16, 8, 0x7FFF);
+  VDUP(vector, q, int, s, 32, 4, 0x7FFFFFFF);
+  VDUP(vector, q, int, s, 64, 2, 0x7FFFFFFFFFFFFFFFLL);
+  VDUP(vector, q, uint, u, 8, 16, 0xFF);
+  VDUP(vector, q, uint, u, 16, 8, 0xFFFF);
+  VDUP(vector, q, uint, u, 32, 4, 0xFFFFFFFF);
+  VDUP(vector, q, uint, u, 64, 2, 0xFFFFFFFFFFFFFFFFULL);
+
+  /* Use max shift amount, to exercise saturation.  */
+  TEST_VRSHR_N(, int, s, 8, 8, 8);
+  TEST_VRSHR_N(, int, s, 16, 4, 16);
+  TEST_VRSHR_N(, int, s, 32, 2, 32);
+  TEST_VRSHR_N(, int, s, 64, 1, 64);
+  TEST_VRSHR_N(, uint, u, 8, 8, 8);
+  TEST_VRSHR_N(, uint, u, 16, 4, 16);
+  TEST_VRSHR_N(, uint, u, 32, 2, 32);
+  TEST_VRSHR_N(, uint, u, 64, 1, 64);
+  TEST_VRSHR_N(q, int, s, 8, 16, 8);
+  TEST_VRSHR_N(q, int, s, 16, 8, 16);
+  TEST_VRSHR_N(q, int, s, 32, 4, 32);
+  TEST_VRSHR_N(q, int, s, 64, 2, 64);
+  TEST_VRSHR_N(q, uint, u, 8, 16, 8);
+  TEST_VRSHR_N(q, uint, u, 16, 8, 16);
+  TEST_VRSHR_N(q, uint, u, 32, 4, 32);
+  TEST_VRSHR_N(q, uint, u, 64, 2, 64);
+
+#undef CMT
+#define CMT " (overflow test: max shift amount, max positive input)"
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_max_sh_max, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_max_sh_max, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_max_sh_max, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_max_sh_max, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_max_sh_max, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_max_sh_max, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_max_sh_max, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_max_sh_max, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_max_sh_max, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_max_sh_max, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_max_sh_max, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_max_sh_max, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_max_sh_max, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_max_sh_max, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_max_sh_max, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_max_sh_max, CMT);
+
+
+  /* Use 1 as shift amount, to exercise saturation.  */
+  TEST_VRSHR_N(, int, s, 8, 8, 1);
+  TEST_VRSHR_N(, int, s, 16, 4, 1);
+  TEST_VRSHR_N(, int, s, 32, 2, 1);
+  TEST_VRSHR_N(, int, s, 64, 1, 1);
+  TEST_VRSHR_N(, uint, u, 8, 8, 1);
+  TEST_VRSHR_N(, uint, u, 16, 4, 1);
+  TEST_VRSHR_N(, uint, u, 32, 2, 1);
+  TEST_VRSHR_N(, uint, u, 64, 1, 1);
+  TEST_VRSHR_N(q, int, s, 8, 16, 1);
+  TEST_VRSHR_N(q, int, s, 16, 8, 1);
+  TEST_VRSHR_N(q, int, s, 32, 4, 1);
+  TEST_VRSHR_N(q, int, s, 64, 2, 1);
+  TEST_VRSHR_N(q, uint, u, 8, 16, 1);
+  TEST_VRSHR_N(q, uint, u, 16, 8, 1);
+  TEST_VRSHR_N(q, uint, u, 32, 4, 1);
+  TEST_VRSHR_N(q, uint, u, 64, 2, 1);
+
+#undef CMT
+#define CMT " (overflow test: shift by 1, with max input)"
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_max_sh_1, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_max_sh_1, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_max_sh_1, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_max_sh_1, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_max_sh_1, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_max_sh_1, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_max_sh_1, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_max_sh_1, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_max_sh_1, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_max_sh_1, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_max_sh_1, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_max_sh_1, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_max_sh_1, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_max_sh_1, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_max_sh_1, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_max_sh_1, CMT);
+
+
+  /* Use 3 as shift amount, to exercise saturation.  */
+  TEST_VRSHR_N(, int, s, 8, 8, 3);
+  TEST_VRSHR_N(, int, s, 16, 4, 3);
+  TEST_VRSHR_N(, int, s, 32, 2, 3);
+  TEST_VRSHR_N(, int, s, 64, 1, 3);
+  TEST_VRSHR_N(, uint, u, 8, 8, 3);
+  TEST_VRSHR_N(, uint, u, 16, 4, 3);
+  TEST_VRSHR_N(, uint, u, 32, 2, 3);
+  TEST_VRSHR_N(, uint, u, 64, 1, 3);
+  TEST_VRSHR_N(q, int, s, 8, 16, 3);
+  TEST_VRSHR_N(q, int, s, 16, 8, 3);
+  TEST_VRSHR_N(q, int, s, 32, 4, 3);
+  TEST_VRSHR_N(q, int, s, 64, 2, 3);
+  TEST_VRSHR_N(q, uint, u, 8, 16, 3);
+  TEST_VRSHR_N(q, uint, u, 16, 8, 3);
+  TEST_VRSHR_N(q, uint, u, 32, 4, 3);
+  TEST_VRSHR_N(q, uint, u, 64, 2, 3);
+
+#undef CMT
+#define CMT " (overflow test: shift by 3, with max input)"
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_max_sh_3, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_max_sh_3, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_max_sh_3, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_max_sh_3, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_max_sh_3, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_max_sh_3, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_max_sh_3, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_max_sh_3, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_max_sh_3, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_max_sh_3, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_max_sh_3, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_max_sh_3, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_max_sh_3, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_max_sh_3, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_max_sh_3, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_max_sh_3, CMT);
+
+
+  /* Use minimum negative input for signed types.  */
+  VDUP(vector, , int, s, 8, 8, 0x80);
+  VDUP(vector, , int, s, 16, 4, 0x8000);
+  VDUP(vector, , int, s, 32, 2, 0x80000000);
+  VDUP(vector, , int, s, 64, 1, 0x8000000000000000LL);
+  VDUP(vector, , uint, u, 8, 8, 0xFF);
+  VDUP(vector, , uint, u, 16, 4, 0xFFFF);
+  VDUP(vector, , uint, u, 32, 2, 0xFFFFFFFF);
+  VDUP(vector, , uint, u, 64, 1, 0xFFFFFFFFFFFFFFFFULL);
+  VDUP(vector, q, int, s, 8, 16, 0x80);
+  VDUP(vector, q, int, s, 16, 8, 0x8000);
+  VDUP(vector, q, int, s, 32, 4, 0x80000000);
+  VDUP(vector, q, int, s, 64, 2, 0x8000000000000000LL);
+  VDUP(vector, q, uint, u, 8, 16, 0xFF);
+  VDUP(vector, q, uint, u, 16, 8, 0xFFFF);
+  VDUP(vector, q, uint, u, 32, 4, 0xFFFFFFFF);
+  VDUP(vector, q, uint, u, 64, 2, 0xFFFFFFFFFFFFFFFFULL);
+
+
+  /* Use 1 as shift amount, to exercise saturation code.  */
+  TEST_VRSHR_N(, int, s, 8, 8, 1);
+  TEST_VRSHR_N(, int, s, 16, 4, 1);
+  TEST_VRSHR_N(, int, s, 32, 2, 1);
+  TEST_VRSHR_N(, int, s, 64, 1, 1);
+  TEST_VRSHR_N(, uint, u, 8, 8, 1);
+  TEST_VRSHR_N(, uint, u, 16, 4, 1);
+  TEST_VRSHR_N(, uint, u, 32, 2, 1);
+  TEST_VRSHR_N(, uint, u, 64, 1, 1);
+  TEST_VRSHR_N(q, int, s, 8, 16, 1);
+  TEST_VRSHR_N(q, int, s, 16, 8, 1);
+  TEST_VRSHR_N(q, int, s, 32, 4, 1);
+  TEST_VRSHR_N(q, int, s, 64, 2, 1);
+  TEST_VRSHR_N(q, uint, u, 8, 16, 1);
+  TEST_VRSHR_N(q, uint, u, 16, 8, 1);
+  TEST_VRSHR_N(q, uint, u, 32, 4, 1);
+  TEST_VRSHR_N(q, uint, u, 64, 2, 1);
+
+#undef CMT
+#define CMT " (overflow test: shift by 1, with negative input)"
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_max_neg_sh_1, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_max_neg_sh_1, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_max_neg_sh_1, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_max_neg_sh_1, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_max_neg_sh_1, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_max_neg_sh_1, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_max_neg_sh_1, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_max_neg_sh_1, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_max_neg_sh_1, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_max_neg_sh_1, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_max_neg_sh_1, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_max_neg_sh_1, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_max_neg_sh_1, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_max_neg_sh_1, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_max_neg_sh_1, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_max_neg_sh_1, CMT);
+
+
+  /* Use 3 as shift amount, to exercise saturation code.  */
+  TEST_VRSHR_N(, int, s, 8, 8, 3);
+  TEST_VRSHR_N(, int, s, 16, 4, 3);
+  TEST_VRSHR_N(, int, s, 32, 2, 3);
+  TEST_VRSHR_N(, int, s, 64, 1, 3);
+  TEST_VRSHR_N(, uint, u, 8, 8, 3);
+  TEST_VRSHR_N(, uint, u, 16, 4, 3);
+  TEST_VRSHR_N(, uint, u, 32, 2, 3);
+  TEST_VRSHR_N(, uint, u, 64, 1, 3);
+  TEST_VRSHR_N(q, int, s, 8, 16, 3);
+  TEST_VRSHR_N(q, int, s, 16, 8, 3);
+  TEST_VRSHR_N(q, int, s, 32, 4, 3);
+  TEST_VRSHR_N(q, int, s, 64, 2, 3);
+  TEST_VRSHR_N(q, uint, u, 8, 16, 3);
+  TEST_VRSHR_N(q, uint, u, 16, 8, 3);
+  TEST_VRSHR_N(q, uint, u, 32, 4, 3);
+  TEST_VRSHR_N(q, uint, u, 64, 2, 3);
+
+#undef CMT
+#define CMT " (overflow test: shift by 3, with negative input)"
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_max_neg_sh_3, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_max_neg_sh_3, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_max_neg_sh_3, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_max_neg_sh_3, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_max_neg_sh_3, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_max_neg_sh_3, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_max_neg_sh_3, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_max_neg_sh_3, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_max_neg_sh_3, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_max_neg_sh_3, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_max_neg_sh_3, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_max_neg_sh_3, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_max_neg_sh_3, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_max_neg_sh_3, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_max_neg_sh_3, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_max_neg_sh_3, CMT);
+}
+
+int main (void)
+{
+  exec_vrshr_n ();
+  return 0;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vrshrn_n.c
@@ -0,0 +1,143 @@
+#include <arm_neon.h>
+#include "arm-neon-ref.h"
+#include "compute-ref-data.h"
+
+/* Expected results with input=0.  */
+VECT_VAR_DECL(expected_0,int,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,int,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,int,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,uint,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					  0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,uint,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_0,uint,32,2) [] = { 0x0, 0x0 };
+
+/* Expected results.  */
+VECT_VAR_DECL(expected,int,8,8) [] = { 0xf8, 0xf9, 0xf9, 0xfa,
+				       0xfa, 0xfb, 0xfb, 0xfc };
+VECT_VAR_DECL(expected,int,16,4) [] = { 0xfff8, 0xfff9, 0xfff9, 0xfffa };
+VECT_VAR_DECL(expected,int,32,2) [] = { 0xfffffffc, 0xfffffffc };
+VECT_VAR_DECL(expected,uint,8,8) [] = { 0xfc, 0xfc, 0xfd, 0xfd,
+					0xfd, 0xfd, 0xfe, 0xfe };
+VECT_VAR_DECL(expected,uint,16,4) [] = { 0xfffe, 0xfffe, 0xfffe, 0xfffe };
+VECT_VAR_DECL(expected,uint,32,2) [] = { 0xfffffffe, 0xfffffffe };
+
+/* Expected results with large shift amount.  */
+VECT_VAR_DECL(expected_sh_large,int,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+						0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_sh_large,int,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_sh_large,int,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_sh_large,uint,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+						 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_sh_large,uint,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_sh_large,uint,32,2) [] = { 0x0, 0x0 };
+
+#define TEST_MSG "VRSHRN_N"
+void exec_vrshrn_n (void)
+{
+  /* Basic test: v2=vrshrn_n(v1,v), then store the result.  */
+#define TEST_VRSHRN_N(T1, T2, W, N, W2, V)				\
+  VECT_VAR(vector_res, T1, W2, N) =					\
+    vrshrn_n_##T2##W(VECT_VAR(vector, T1, W, N),			\
+		     V);						\
+  vst1_##T2##W2(VECT_VAR(result, T1, W2, N), VECT_VAR(vector_res, T1, W2, N))
+
+  /* vector is twice as large as vector_res.  */
+  DECL_VARIABLE(vector, int, 16, 8);
+  DECL_VARIABLE(vector, int, 32, 4);
+  DECL_VARIABLE(vector, int, 64, 2);
+  DECL_VARIABLE(vector, uint, 16, 8);
+  DECL_VARIABLE(vector, uint, 32, 4);
+  DECL_VARIABLE(vector, uint, 64, 2);
+
+  DECL_VARIABLE(vector_res, int, 8, 8);
+  DECL_VARIABLE(vector_res, int, 16, 4);
+  DECL_VARIABLE(vector_res, int, 32, 2);
+  DECL_VARIABLE(vector_res, uint, 8, 8);
+  DECL_VARIABLE(vector_res, uint, 16, 4);
+  DECL_VARIABLE(vector_res, uint, 32, 2);
+
+  clean_results ();
+
+  /* Fill input vector with 0, to check behavior on limits.  */
+  VDUP(vector, q, int, s, 16, 8, 0);
+  VDUP(vector, q, int, s, 32, 4, 0);
+  VDUP(vector, q, int, s, 64, 2, 0);
+  VDUP(vector, q, uint, u, 16, 8, 0);
+  VDUP(vector, q, uint, u, 32, 4, 0);
+  VDUP(vector, q, uint, u, 64, 2, 0);
+
+  /* Choose shift amount arbitrarily.  */
+  TEST_VRSHRN_N(int, s, 16, 8, 8, 1);
+  TEST_VRSHRN_N(int, s, 32, 4, 16, 1);
+  TEST_VRSHRN_N(int, s, 64, 2, 32, 2);
+  TEST_VRSHRN_N(uint, u, 16, 8, 8, 2);
+  TEST_VRSHRN_N(uint, u, 32, 4, 16, 3);
+  TEST_VRSHRN_N(uint, u, 64, 2, 32, 3);
+
+#define CMT " (with input = 0)"
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_0, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_0, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_0, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_0, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_0, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_0, CMT);
+
+
+  /* Test again, with predefined input values.  */
+  VLOAD(vector, buffer, q, int, s, 16, 8);
+  VLOAD(vector, buffer, q, int, s, 32, 4);
+  VLOAD(vector, buffer, q, int, s, 64, 2);
+  VLOAD(vector, buffer, q, uint, u, 16, 8);
+  VLOAD(vector, buffer, q, uint, u, 32, 4);
+  VLOAD(vector, buffer, q, uint, u, 64, 2);
+
+  /* Choose shift amount arbitrarily.  */
+  TEST_VRSHRN_N(int, s, 16, 8, 8, 1);
+  TEST_VRSHRN_N(int, s, 32, 4, 16, 1);
+  TEST_VRSHRN_N(int, s, 64, 2, 32, 2);
+  TEST_VRSHRN_N(uint, u, 16, 8, 8, 2);
+  TEST_VRSHRN_N(uint, u, 32, 4, 16, 3);
+  TEST_VRSHRN_N(uint, u, 64, 2, 32, 3);
+
+#undef CMT
+#define CMT ""
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected, CMT);
+
+
+  /* Fill input arbitrary values.  */
+  VDUP(vector, q, int, s, 16, 8, 30);
+  VDUP(vector, q, int, s, 32, 4, 0);
+  VDUP(vector, q, int, s, 64, 2, 0);
+  VDUP(vector, q, uint, u, 16, 8, 0xFFF0);
+  VDUP(vector, q, uint, u, 32, 4, 0xFFFFFFF0);
+  VDUP(vector, q, uint, u, 64, 2, 0);
+
+  /* Choose large shift amount arbitrarily.  */
+  TEST_VRSHRN_N(int, s, 16, 8, 8, 7);
+  TEST_VRSHRN_N(int, s, 32, 4, 16, 14);
+  TEST_VRSHRN_N(int, s, 64, 2, 32, 31);
+  TEST_VRSHRN_N(uint, u, 16, 8, 8, 7);
+  TEST_VRSHRN_N(uint, u, 32, 4, 16, 16);
+  TEST_VRSHRN_N(uint, u, 64, 2, 32, 3);
+
+#undef CMT
+#define CMT " (with large shift amount)"
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_sh_large, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_sh_large, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_sh_large, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_sh_large, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_sh_large, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_sh_large, CMT);
+}
+
+int main (void)
+{
+  exec_vrshrn_n ();
+  return 0;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vrsqrte.c
@@ -0,0 +1,157 @@
+#include <arm_neon.h>
+#include "arm-neon-ref.h"
+#include "compute-ref-data.h"
+#include <math.h>
+
+/* Expected results.  */
+VECT_VAR_DECL(expected,uint,32,2) [] = { 0xffffffff, 0xffffffff };
+VECT_VAR_DECL(expected,uint,32,4) [] = { 0x9c800000, 0x9c800000,
+					 0x9c800000, 0x9c800000 };
+VECT_VAR_DECL(expected,hfloat,32,2) [] = { 0x3e498000, 0x3e498000 };
+VECT_VAR_DECL(expected,hfloat,32,4) [] = { 0x3e700000, 0x3e700000,
+					   0x3e700000, 0x3e700000 };
+
+/* Expected results with large uint #1.  */
+VECT_VAR_DECL(expected_1,uint,32,2) [] = { 0x80000000, 0x80000000 };
+VECT_VAR_DECL(expected_1,uint,32,4) [] = { 0xae800000, 0xae800000,
+					   0xae800000, 0xae800000 };
+
+/* Expected results with large uint #2.  */
+VECT_VAR_DECL(expected_2,uint,32,2) [] = { 0xb4800000, 0xb4800000 };
+VECT_VAR_DECL(expected_2,uint,32,4) [] = { 0xed000000, 0xed000000,
+					   0xed000000, 0xed000000 };
+
+/* Expected results with FP special inputs values (NaNs, ...).  */
+VECT_VAR_DECL(expected_fp1,hfloat,32,2) [] = { 0x7fc00000, 0x7fc00000 };
+VECT_VAR_DECL(expected_fp1,hfloat,32,4) [] = { 0x7f800000, 0x7f800000,
+					       0x7f800000, 0x7f800000 };
+
+/* Expected results with FP special inputs values
+   (negative, infinity).  */
+VECT_VAR_DECL(expected_fp2,hfloat,32,2) [] = { 0x7fc00000, 0x7fc00000 };
+VECT_VAR_DECL(expected_fp2,hfloat,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+
+/* Expected results with FP special inputs values
+   (-0, -infinity).  */
+VECT_VAR_DECL(expected_fp3,hfloat,32,2) [] = { 0xff800000, 0xff800000 };
+VECT_VAR_DECL(expected_fp3,hfloat,32,4) [] = { 0x7fc00000, 0x7fc00000,
+					       0x7fc00000, 0x7fc00000 };
+
+#define TEST_MSG "VRSQRTE/VRSQRTEQ"
+void exec_vrsqrte(void)
+{
+  int i;
+
+  /* Basic test: y=vrsqrte(x), then store the result.  */
+#define TEST_VRSQRTE(Q, T1, T2, W, N)			\
+  VECT_VAR(vector_res, T1, W, N) =			\
+    vrsqrte##Q##_##T2##W(VECT_VAR(vector, T1, W, N));	\
+  vst1##Q##_##T2##W(VECT_VAR(result, T1, W, N),		\
+		    VECT_VAR(vector_res, T1, W, N))
+
+  DECL_VARIABLE(vector, uint, 32, 2);
+  DECL_VARIABLE(vector, float, 32, 2);
+  DECL_VARIABLE(vector, uint, 32, 4);
+  DECL_VARIABLE(vector, float, 32, 4);
+
+  DECL_VARIABLE(vector_res, uint, 32, 2);
+  DECL_VARIABLE(vector_res, float, 32, 2);
+  DECL_VARIABLE(vector_res, uint, 32, 4);
+  DECL_VARIABLE(vector_res, float, 32, 4);
+
+  clean_results ();
+
+  /* Choose init value arbitrarily.  */
+  VDUP(vector, , uint, u, 32, 2, 0x12345678);
+  VDUP(vector, , float, f, 32, 2, 25.799999f);
+  VDUP(vector, q, uint, u, 32, 4, 0xABCDEF10);
+  VDUP(vector, q, float, f, 32, 4, 18.2f);
+
+  /* Apply the operator.  */
+  TEST_VRSQRTE(, uint, u, 32, 2);
+  TEST_VRSQRTE(, float, f, 32, 2);
+  TEST_VRSQRTE(q, uint, u, 32, 4);
+  TEST_VRSQRTE(q, float, f, 32, 4);
+
+#define CMT ""
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected, CMT);
+  CHECK_FP(TEST_MSG, float, 32, 2, PRIx32, expected, CMT);
+  CHECK_FP(TEST_MSG, float, 32, 4, PRIx32, expected, CMT);
+
+
+  /* Don't test FP variants with negative inputs.  */
+  /* Use input with various values of bits 30 and 31.  */
+  VDUP(vector, , uint, u, 32, 2, 0xFFFFFFFF);
+  VDUP(vector, q, uint, u, 32, 4, 0x89081234);
+
+  /* Apply the operator.  */
+  TEST_VRSQRTE(, uint, u, 32, 2);
+  TEST_VRSQRTE(q, uint, u, 32, 4);
+
+#undef CMT
+#define CMT " (large uint #1)"
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_1, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_1, CMT);
+
+
+  /* Choose init value arbitrarily.  */
+  VDUP(vector, , uint, u, 32, 2, 0x80000000);
+  VDUP(vector, q, uint, u, 32, 4, 0x4ABCDEF0);
+
+  /* Apply the operator.  */
+  TEST_VRSQRTE(, uint, u, 32, 2);
+  TEST_VRSQRTE(q, uint, u, 32, 4);
+
+#undef CMT
+#define CMT " (large uint #2)"
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_2, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_2, CMT);
+
+
+  /* Test FP variants with special input values (NaNs, ...).  */
+  VDUP(vector, , float, f, 32, 2, NAN);
+  VDUP(vector, q, float, f, 32, 4, 0.0f);
+
+  /* Apply the operator.  */
+  TEST_VRSQRTE(, float, f, 32, 2);
+  TEST_VRSQRTE(q, float, f, 32, 4);
+
+#undef CMT
+#define CMT " FP special (NaN, 0)"
+  CHECK_FP(TEST_MSG, float, 32, 2, PRIx32, expected_fp1, CMT);
+  CHECK_FP(TEST_MSG, float, 32, 4, PRIx32, expected_fp1, CMT);
+
+
+  /* Test FP variants with special input values (negative, infinity).  */
+  VDUP(vector, , float, f, 32, 2, -1.0f);
+  VDUP(vector, q, float, f, 32, 4, HUGE_VALF);
+
+  /* Apply the operator.  */
+  TEST_VRSQRTE(, float, f, 32, 2);
+  TEST_VRSQRTE(q, float, f, 32, 4);
+
+#undef CMT
+#define CMT " FP special (negative, infinity)"
+  CHECK_FP(TEST_MSG, float, 32, 2, PRIx32, expected_fp2, CMT);
+  CHECK_FP(TEST_MSG, float, 32, 4, PRIx32, expected_fp2, CMT);
+
+  /* Test FP variants with special input values (-0, -infinity).  */
+  VDUP(vector, , float, f, 32, 2, -0.0f);
+  VDUP(vector, q, float, f, 32, 4, -HUGE_VALF);
+
+  /* Apply the operator.  */
+  TEST_VRSQRTE(, float, f, 32, 2);
+  TEST_VRSQRTE(q, float, f, 32, 4);
+
+#undef CMT
+#define CMT " FP special (-0, -infinity)"
+  CHECK_FP(TEST_MSG, float, 32, 2, PRIx32, expected_fp3, CMT);
+  CHECK_FP(TEST_MSG, float, 32, 4, PRIx32, expected_fp3, CMT);
+}
+
+int main (void)
+{
+  exec_vrsqrte ();
+  return 0;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vrsqrts.c
@@ -0,0 +1,118 @@
+#include <arm_neon.h>
+#include "arm-neon-ref.h"
+#include "compute-ref-data.h"
+#include <math.h>
+
+/* Expected results.  */
+VECT_VAR_DECL(expected,hfloat,32,2) [] = { 0xc2796b84, 0xc2796b84 };
+VECT_VAR_DECL(expected,hfloat,32,4) [] = { 0xc0e4a3d8, 0xc0e4a3d8,
+					   0xc0e4a3d8, 0xc0e4a3d8 };
+
+/* Expected results with input=NaN.  */
+VECT_VAR_DECL(expected_nan,hfloat,32,2) [] = { 0x7fc00000, 0x7fc00000 };
+VECT_VAR_DECL(expected_nan,hfloat,32,4) [] = { 0x7fc00000, 0x7fc00000,
+					       0x7fc00000, 0x7fc00000 };
+
+/* Expected results with FP special inputs values (infinity, 0).  */
+VECT_VAR_DECL(expected_fp1,hfloat,32,2) [] = { 0xff800000, 0xff800000 };
+VECT_VAR_DECL(expected_fp1,hfloat,32,4) [] = { 0x3fc00000, 0x3fc00000,
+					       0x3fc00000, 0x3fc00000 };
+
+/* Expected results with only FP special inputs values (infinity,
+   0).  */
+VECT_VAR_DECL(expected_fp2,hfloat,32,2) [] = { 0x3fc00000, 0x3fc00000 };
+VECT_VAR_DECL(expected_fp2,hfloat,32,4) [] = { 0x3fc00000, 0x3fc00000,
+					       0x3fc00000, 0x3fc00000 };
+
+#define TEST_MSG "VRSQRTS/VRSQRTSQ"
+void exec_vrsqrts(void)
+{
+  int i;
+
+  /* Basic test: y=vrsqrts(x), then store the result.  */
+#define TEST_VRSQRTS(Q, T1, T2, W, N)			\
+  VECT_VAR(vector_res, T1, W, N) =			\
+    vrsqrts##Q##_##T2##W(VECT_VAR(vector, T1, W, N),	\
+			 VECT_VAR(vector2, T1, W, N));	\
+  vst1##Q##_##T2##W(VECT_VAR(result, T1, W, N),		\
+		    VECT_VAR(vector_res, T1, W, N))
+
+  /* No need for integer variants.  */
+  DECL_VARIABLE(vector, float, 32, 2);
+  DECL_VARIABLE(vector, float, 32, 4);
+
+  DECL_VARIABLE(vector2, float, 32, 2);
+  DECL_VARIABLE(vector2, float, 32, 4);
+
+  DECL_VARIABLE(vector_res, float, 32, 2);
+  DECL_VARIABLE(vector_res, float, 32, 4);
+
+  clean_results ();
+
+  /* Choose init value arbitrarily.  */
+  VDUP(vector, , float, f, 32, 2, 12.9f);
+  VDUP(vector, q, float, f, 32, 4, 9.1f);
+
+  VDUP(vector2, , float, f, 32, 2, 9.9f);
+  VDUP(vector2, q, float, f, 32, 4, 1.9f);
+
+  /* Apply the operator.  */
+  TEST_VRSQRTS(, float, f, 32, 2);
+  TEST_VRSQRTS(q, float, f, 32, 4);
+
+#define CMT ""
+  CHECK_FP(TEST_MSG, float, 32, 2, PRIx32, expected, CMT);
+  CHECK_FP(TEST_MSG, float, 32, 4, PRIx32, expected, CMT);
+
+
+  /* Test FP variants with special input values (NaN).  */
+  VDUP(vector, , float, f, 32, 2, NAN);
+  VDUP(vector2, q, float, f, 32, 4, NAN);
+
+  /* Apply the operator.  */
+  TEST_VRSQRTS(, float, f, 32, 2);
+  TEST_VRSQRTS(q, float, f, 32, 4);
+
+#undef CMT
+#define CMT " FP special (NAN) and normal values"
+  CHECK_FP(TEST_MSG, float, 32, 2, PRIx32, expected_nan, CMT);
+  CHECK_FP(TEST_MSG, float, 32, 4, PRIx32, expected_nan, CMT);
+
+
+  /* Test FP variants with special input values (infinity, 0).  */
+  VDUP(vector, , float, f, 32, 2, HUGE_VALF);
+  VDUP(vector, q, float, f, 32, 4, 0.0f);
+  /* Restore a normal value in vector2.  */
+  VDUP(vector2, q, float, f, 32, 4, 3.2f);
+
+  /* Apply the operator.  */
+  TEST_VRSQRTS(, float, f, 32, 2);
+  TEST_VRSQRTS(q, float, f, 32, 4);
+
+#undef CMT
+#define CMT " FP special (infinity, 0) and normal values"
+  CHECK_FP(TEST_MSG, float, 32, 2, PRIx32, expected_fp1, CMT);
+  CHECK_FP(TEST_MSG, float, 32, 4, PRIx32, expected_fp1, CMT);
+
+
+  /* Test FP variants with only special input values (infinity, 0).  */
+  VDUP(vector, , float, f, 32, 2, HUGE_VALF);
+  VDUP(vector, q, float, f, 32, 4, 0.0f);
+  VDUP(vector2, , float, f, 32, 2, -0.0f);
+  VDUP(vector2, q, float, f, 32, 4, HUGE_VALF);
+
+  /* Apply the operator.  */
+  TEST_VRSQRTS(, float, f, 32, 2);
+  TEST_VRSQRTS(q, float, f, 32, 4);
+
+#undef CMT
+#define CMT " only FP special (infinity, 0)"
+  CHECK_FP(TEST_MSG, float, 32, 2, PRIx32, expected_fp2, CMT);
+  CHECK_FP(TEST_MSG, float, 32, 4, PRIx32, expected_fp2, CMT);
+}
+
+int main (void)
+{
+  exec_vrsqrts ();
+  return 0;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vrsra_n.c
@@ -0,0 +1,553 @@
+#include <arm_neon.h>
+#include "arm-neon-ref.h"
+#include "compute-ref-data.h"
+
+/* Expected results.  */
+VECT_VAR_DECL(expected,int,8,8) [] = { 0xf9, 0xfa, 0xfb, 0xfc,
+				       0xfd, 0xfe, 0xff, 0x0 };
+VECT_VAR_DECL(expected,int,16,4) [] = { 0xfff0, 0xfff1, 0xfff2, 0xfff3 };
+VECT_VAR_DECL(expected,int,32,2) [] = { 0xfffffffd, 0xfffffffe };
+VECT_VAR_DECL(expected,int,64,1) [] = { 0xfffffffffffffff0 };
+VECT_VAR_DECL(expected,uint,8,8) [] = { 0x5, 0x6, 0x7, 0x8,
+					0x9, 0xa, 0xb, 0xc };
+VECT_VAR_DECL(expected,uint,16,4) [] = { 0xfffd, 0xfffe, 0xffff, 0x0 };
+VECT_VAR_DECL(expected,uint,32,2) [] = { 0xfffffff4, 0xfffffff5 };
+VECT_VAR_DECL(expected,uint,64,1) [] = { 0xfffffffffffffff0 };
+VECT_VAR_DECL(expected,int,8,16) [] = { 0xf9, 0xfa, 0xfb, 0xfc,
+					0xfd, 0xfe, 0xff, 0x0,
+					0x1, 0x2, 0x3, 0x4,
+					0x5, 0x6, 0x7, 0x8 };
+VECT_VAR_DECL(expected,int,16,8) [] = { 0xfff0, 0xfff1, 0xfff2, 0xfff3,
+					0xfff4, 0xfff5, 0xfff6, 0xfff7 };
+VECT_VAR_DECL(expected,int,32,4) [] = { 0xfffffffd, 0xfffffffe,
+					0xffffffff, 0x0 };
+VECT_VAR_DECL(expected,int,64,2) [] = { 0xfffffffffffffff0, 0xfffffffffffffff1 };
+VECT_VAR_DECL(expected,uint,8,16) [] = { 0x5, 0x6, 0x7, 0x8,
+					 0x9, 0xa, 0xb, 0xc,
+					 0xd, 0xe, 0xf, 0x10,
+					 0x11, 0x12, 0x13, 0x14 };
+VECT_VAR_DECL(expected,uint,16,8) [] = { 0xfffd, 0xfffe, 0xffff, 0x0,
+					 0x1, 0x2, 0x3, 0x4 };
+VECT_VAR_DECL(expected,uint,32,4) [] = { 0xfffffff4, 0xfffffff5,
+					 0xfffffff6, 0xfffffff7 };
+VECT_VAR_DECL(expected,uint,64,2) [] = { 0xfffffffffffffff0,
+					 0xfffffffffffffff1 };
+
+/* Expected results with max input and shift by 1.  */
+VECT_VAR_DECL(expected_max_sh1,int,8,8) [] = { 0x40, 0x40, 0x40, 0x40,
+					       0x40, 0x40, 0x40, 0x40 };
+VECT_VAR_DECL(expected_max_sh1,int,16,4) [] = { 0x4000, 0x4000, 0x4000, 0x4000 };
+VECT_VAR_DECL(expected_max_sh1,int,32,2) [] = { 0x40000000, 0x40000000 };
+VECT_VAR_DECL(expected_max_sh1,int,64,1) [] = { 0x4000000000000000 };
+VECT_VAR_DECL(expected_max_sh1,uint,8,8) [] = { 0x80, 0x80, 0x80, 0x80,
+						0x80, 0x80, 0x80, 0x80 };
+VECT_VAR_DECL(expected_max_sh1,uint,16,4) [] = { 0x8000, 0x8000,
+						 0x8000, 0x8000 };
+VECT_VAR_DECL(expected_max_sh1,uint,32,2) [] = { 0x80000000, 0x80000000 };
+VECT_VAR_DECL(expected_max_sh1,uint,64,1) [] = { 0x8000000000000000 };
+VECT_VAR_DECL(expected_max_sh1,int,8,16) [] = { 0x40, 0x40, 0x40, 0x40,
+						0x40, 0x40, 0x40, 0x40,
+						0x40, 0x40, 0x40, 0x40,
+						0x40, 0x40, 0x40, 0x40 };
+VECT_VAR_DECL(expected_max_sh1,int,16,8) [] = { 0x4000, 0x4000, 0x4000, 0x4000,
+						0x4000, 0x4000, 0x4000, 0x4000 };
+VECT_VAR_DECL(expected_max_sh1,int,32,4) [] = { 0x40000000, 0x40000000,
+						0x40000000, 0x40000000 };
+VECT_VAR_DECL(expected_max_sh1,int,64,2) [] = { 0x4000000000000000,
+						0x4000000000000000 };
+VECT_VAR_DECL(expected_max_sh1,uint,8,16) [] = { 0x80, 0x80, 0x80, 0x80,
+						 0x80, 0x80, 0x80, 0x80,
+						 0x80, 0x80, 0x80, 0x80,
+						 0x80, 0x80, 0x80, 0x80 };
+VECT_VAR_DECL(expected_max_sh1,uint,16,8) [] = { 0x8000, 0x8000,
+						 0x8000, 0x8000,
+						 0x8000, 0x8000,
+						 0x8000, 0x8000 };
+VECT_VAR_DECL(expected_max_sh1,uint,32,4) [] = { 0x80000000, 0x80000000,
+						 0x80000000, 0x80000000 };
+VECT_VAR_DECL(expected_max_sh1,uint,64,2) [] = { 0x8000000000000000,
+						 0x8000000000000000 };
+
+/* Expected results with max input and shift by 3.  */
+VECT_VAR_DECL(expected_max_sh3,int,8,8) [] = { 0x10, 0x10, 0x10, 0x10,
+					       0x10, 0x10, 0x10, 0x10 };
+VECT_VAR_DECL(expected_max_sh3,int,16,4) [] = { 0x1000, 0x1000, 0x1000, 0x1000 };
+VECT_VAR_DECL(expected_max_sh3,int,32,2) [] = { 0x10000000, 0x10000000 };
+VECT_VAR_DECL(expected_max_sh3,int,64,1) [] = { 0x1000000000000000 };
+VECT_VAR_DECL(expected_max_sh3,uint,8,8) [] = { 0x20, 0x20, 0x20, 0x20,
+						0x20, 0x20, 0x20, 0x20 };
+VECT_VAR_DECL(expected_max_sh3,uint,16,4) [] = { 0x2000, 0x2000,
+						 0x2000, 0x2000 };
+VECT_VAR_DECL(expected_max_sh3,uint,32,2) [] = { 0x20000000, 0x20000000 };
+VECT_VAR_DECL(expected_max_sh3,uint,64,1) [] = { 0x2000000000000000 };
+VECT_VAR_DECL(expected_max_sh3,int,8,16) [] = { 0x10, 0x10, 0x10, 0x10,
+						0x10, 0x10, 0x10, 0x10,
+						0x10, 0x10, 0x10, 0x10,
+						0x10, 0x10, 0x10, 0x10 };
+VECT_VAR_DECL(expected_max_sh3,int,16,8) [] = { 0x1000, 0x1000, 0x1000, 0x1000,
+						0x1000, 0x1000, 0x1000, 0x1000 };
+VECT_VAR_DECL(expected_max_sh3,int,32,4) [] = { 0x10000000, 0x10000000,
+						0x10000000, 0x10000000 };
+VECT_VAR_DECL(expected_max_sh3,int,64,2) [] = { 0x1000000000000000,
+						0x1000000000000000 };
+VECT_VAR_DECL(expected_max_sh3,uint,8,16) [] = { 0x20, 0x20, 0x20, 0x20,
+						 0x20, 0x20, 0x20, 0x20,
+						 0x20, 0x20, 0x20, 0x20,
+						 0x20, 0x20, 0x20, 0x20 };
+VECT_VAR_DECL(expected_max_sh3,uint,16,8) [] = { 0x2000, 0x2000,
+						 0x2000, 0x2000,
+						 0x2000, 0x2000,
+						 0x2000, 0x2000 };
+VECT_VAR_DECL(expected_max_sh3,uint,32,4) [] = { 0x20000000, 0x20000000,
+						 0x20000000, 0x20000000 };
+VECT_VAR_DECL(expected_max_sh3,uint,64,2) [] = { 0x2000000000000000,
+						 0x2000000000000000 };
+
+/* Expected results with max input and shift by type width.  */
+VECT_VAR_DECL(expected_max_shmax,int,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+						 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_max_shmax,int,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_max_shmax,int,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_max_shmax,int,64,1) [] = { 0x0 };
+VECT_VAR_DECL(expected_max_shmax,uint,8,8) [] = { 0x1, 0x1, 0x1, 0x1,
+						  0x1, 0x1, 0x1, 0x1 };
+VECT_VAR_DECL(expected_max_shmax,uint,16,4) [] = { 0x1, 0x1, 0x1, 0x1 };
+VECT_VAR_DECL(expected_max_shmax,uint,32,2) [] = { 0x1, 0x1 };
+VECT_VAR_DECL(expected_max_shmax,uint,64,1) [] = { 0x1 };
+VECT_VAR_DECL(expected_max_shmax,int,8,16) [] = { 0x0, 0x0, 0x0, 0x0,
+						  0x0, 0x0, 0x0, 0x0,
+						  0x0, 0x0, 0x0, 0x0,
+						  0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_max_shmax,int,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+						  0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_max_shmax,int,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_max_shmax,int,64,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_max_shmax,uint,8,16) [] = { 0x1, 0x1, 0x1, 0x1,
+						   0x1, 0x1, 0x1, 0x1,
+						   0x1, 0x1, 0x1, 0x1,
+						   0x1, 0x1, 0x1, 0x1 };
+VECT_VAR_DECL(expected_max_shmax,uint,16,8) [] = { 0x1, 0x1, 0x1, 0x1,
+						   0x1, 0x1, 0x1, 0x1 };
+VECT_VAR_DECL(expected_max_shmax,uint,32,4) [] = { 0x1, 0x1, 0x1, 0x1 };
+VECT_VAR_DECL(expected_max_shmax,uint,64,2) [] = { 0x1, 0x1 };
+
+/* Expected results with min negative input and shift by 1.  */
+VECT_VAR_DECL(expected_min_sh1,int,8,8) [] = { 0xc0, 0xc0, 0xc0, 0xc0,
+					       0xc0, 0xc0, 0xc0, 0xc0 };
+VECT_VAR_DECL(expected_min_sh1,int,16,4) [] = { 0xc000, 0xc000, 0xc000, 0xc000 };
+VECT_VAR_DECL(expected_min_sh1,int,32,2) [] = { 0xc0000000, 0xc0000000 };
+VECT_VAR_DECL(expected_min_sh1,int,64,1) [] = { 0xc000000000000000 };
+VECT_VAR_DECL(expected_min_sh1,uint,8,8) [] = { 0x1, 0x1, 0x1, 0x1,
+						0x1, 0x1, 0x1, 0x1 };
+VECT_VAR_DECL(expected_min_sh1,uint,16,4) [] = { 0x1, 0x1, 0x1, 0x1 };
+VECT_VAR_DECL(expected_min_sh1,uint,32,2) [] = { 0x1, 0x1 };
+VECT_VAR_DECL(expected_min_sh1,uint,64,1) [] = { 0x1 };
+VECT_VAR_DECL(expected_min_sh1,int,8,16) [] = { 0xc0, 0xc0, 0xc0, 0xc0,
+						0xc0, 0xc0, 0xc0, 0xc0,
+						0xc0, 0xc0, 0xc0, 0xc0,
+						0xc0, 0xc0, 0xc0, 0xc0 };
+VECT_VAR_DECL(expected_min_sh1,int,16,8) [] = { 0xc000, 0xc000, 0xc000, 0xc000,
+						0xc000, 0xc000, 0xc000, 0xc000 };
+VECT_VAR_DECL(expected_min_sh1,int,32,4) [] = { 0xc0000000, 0xc0000000,
+						0xc0000000, 0xc0000000 };
+VECT_VAR_DECL(expected_min_sh1,int,64,2) [] = { 0xc000000000000000,
+						0xc000000000000000 };
+VECT_VAR_DECL(expected_min_sh1,uint,8,16) [] = { 0x1, 0x1, 0x1, 0x1,
+						 0x1, 0x1, 0x1, 0x1,
+						 0x1, 0x1, 0x1, 0x1,
+						 0x1, 0x1, 0x1, 0x1 };
+VECT_VAR_DECL(expected_min_sh1,uint,16,8) [] = { 0x1, 0x1, 0x1, 0x1,
+						 0x1, 0x1, 0x1, 0x1 };
+VECT_VAR_DECL(expected_min_sh1,uint,32,4) [] = { 0x1, 0x1, 0x1, 0x1 };
+VECT_VAR_DECL(expected_min_sh1,uint,64,2) [] = { 0x1, 0x1 };
+
+/* Expected results with min negative input and shift by 3.  */
+VECT_VAR_DECL(expected_min_sh3,int,8,8) [] = { 0xf0, 0xf0, 0xf0, 0xf0,
+					       0xf0, 0xf0, 0xf0, 0xf0 };
+VECT_VAR_DECL(expected_min_sh3,int,16,4) [] = { 0xf000, 0xf000, 0xf000, 0xf000 };
+VECT_VAR_DECL(expected_min_sh3,int,32,2) [] = { 0xf0000000, 0xf0000000 };
+VECT_VAR_DECL(expected_min_sh3,int,64,1) [] = { 0xf000000000000000 };
+VECT_VAR_DECL(expected_min_sh3,uint,8,8) [] = { 0x1, 0x1, 0x1, 0x1,
+						0x1, 0x1, 0x1, 0x1 };
+VECT_VAR_DECL(expected_min_sh3,uint,16,4) [] = { 0x1, 0x1, 0x1, 0x1 };
+VECT_VAR_DECL(expected_min_sh3,uint,32,2) [] = { 0x1, 0x1 };
+VECT_VAR_DECL(expected_min_sh3,uint,64,1) [] = { 0x1 };
+VECT_VAR_DECL(expected_min_sh3,int,8,16) [] = { 0xf0, 0xf0, 0xf0, 0xf0,
+						0xf0, 0xf0, 0xf0, 0xf0,
+						0xf0, 0xf0, 0xf0, 0xf0,
+						0xf0, 0xf0, 0xf0, 0xf0 };
+VECT_VAR_DECL(expected_min_sh3,int,16,8) [] = { 0xf000, 0xf000, 0xf000, 0xf000,
+						0xf000, 0xf000, 0xf000, 0xf000 };
+VECT_VAR_DECL(expected_min_sh3,int,32,4) [] = { 0xf0000000, 0xf0000000,
+						0xf0000000, 0xf0000000 };
+VECT_VAR_DECL(expected_min_sh3,int,64,2) [] = { 0xf000000000000000,
+						0xf000000000000000 };
+VECT_VAR_DECL(expected_min_sh3,uint,8,16) [] = { 0x1, 0x1, 0x1, 0x1,
+						 0x1, 0x1, 0x1, 0x1,
+						 0x1, 0x1, 0x1, 0x1,
+						 0x1, 0x1, 0x1, 0x1 };
+VECT_VAR_DECL(expected_min_sh3,uint,16,8) [] = { 0x1, 0x1, 0x1, 0x1,
+						 0x1, 0x1, 0x1, 0x1 };
+VECT_VAR_DECL(expected_min_sh3,uint,32,4) [] = { 0x1, 0x1, 0x1, 0x1 };
+VECT_VAR_DECL(expected_min_sh3,uint,64,2) [] = { 0x1, 0x1 };
+
+/* Expected results with min negative input and shift by type width.  */
+VECT_VAR_DECL(expected_min_shmax,int,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+						 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_min_shmax,int,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_min_shmax,int,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_min_shmax,int,64,1) [] = { 0x0 };
+VECT_VAR_DECL(expected_min_shmax,uint,8,8) [] = { 0x1, 0x1, 0x1, 0x1,
+						  0x1, 0x1, 0x1, 0x1 };
+VECT_VAR_DECL(expected_min_shmax,uint,16,4) [] = { 0x1, 0x1, 0x1, 0x1 };
+VECT_VAR_DECL(expected_min_shmax,uint,32,2) [] = { 0x1, 0x1 };
+VECT_VAR_DECL(expected_min_shmax,uint,64,1) [] = { 0x1 };
+VECT_VAR_DECL(expected_min_shmax,int,8,16) [] = { 0x0, 0x0, 0x0, 0x0,
+						  0x0, 0x0, 0x0, 0x0,
+						  0x0, 0x0, 0x0, 0x0,
+						  0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_min_shmax,int,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+						  0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_min_shmax,int,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_min_shmax,int,64,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_min_shmax,uint,8,16) [] = { 0x1, 0x1, 0x1, 0x1,
+						   0x1, 0x1, 0x1, 0x1,
+						   0x1, 0x1, 0x1, 0x1,
+						   0x1, 0x1, 0x1, 0x1 };
+VECT_VAR_DECL(expected_min_shmax,uint,16,8) [] = { 0x1, 0x1, 0x1, 0x1,
+						   0x1, 0x1, 0x1, 0x1 };
+VECT_VAR_DECL(expected_min_shmax,uint,32,4) [] = { 0x1, 0x1, 0x1, 0x1 };
+VECT_VAR_DECL(expected_min_shmax,uint,64,2) [] = { 0x1, 0x1 };
+
+#define TEST_MSG "VRSRA_N"
+void exec_vrsra_n (void)
+{
+  /* Basic test: y=vrsra_n(x,v), then store the result.  */
+#define TEST_VRSRA_N(Q, T1, T2, W, N, V)				\
+  VECT_VAR(vector_res, T1, W, N) =					\
+    vrsra##Q##_n_##T2##W(VECT_VAR(vector, T1, W, N),			\
+			 VECT_VAR(vector2, T1, W, N),			\
+			 V);						\
+  vst1##Q##_##T2##W(VECT_VAR(result, T1, W, N), VECT_VAR(vector_res, T1, W, N))
+
+  DECL_VARIABLE_ALL_VARIANTS(vector);
+  DECL_VARIABLE_ALL_VARIANTS(vector2);
+  DECL_VARIABLE_ALL_VARIANTS(vector_res);
+
+  clean_results ();
+
+  /* Initialize input "vector" from "buffer".  */
+  TEST_MACRO_ALL_VARIANTS_2_5(VLOAD, vector, buffer);
+
+  /* Choose arbitrary initialization values.  */
+  VDUP(vector2, , int, s, 8, 8, 0x11);
+  VDUP(vector2, , int, s, 16, 4, 0x22);
+  VDUP(vector2, , int, s, 32, 2, 0x33);
+  VDUP(vector2, , int, s, 64, 1, 0x44);
+  VDUP(vector2, , uint, u, 8, 8, 0x55);
+  VDUP(vector2, , uint, u, 16, 4, 0x66);
+  VDUP(vector2, , uint, u, 32, 2, 0x77);
+  VDUP(vector2, , uint, u, 64, 1, 0x88);
+
+  VDUP(vector2, q, int, s, 8, 16, 0x11);
+  VDUP(vector2, q, int, s, 16, 8, 0x22);
+  VDUP(vector2, q, int, s, 32, 4, 0x33);
+  VDUP(vector2, q, int, s, 64, 2, 0x44);
+  VDUP(vector2, q, uint, u, 8, 16, 0x55);
+  VDUP(vector2, q, uint, u, 16, 8, 0x66);
+  VDUP(vector2, q, uint, u, 32, 4, 0x77);
+  VDUP(vector2, q, uint, u, 64, 2, 0x88);
+
+  /* Choose shift amount arbitrarily.  */
+  TEST_VRSRA_N(, int, s, 8, 8, 1);
+  TEST_VRSRA_N(, int, s, 16, 4, 12);
+  TEST_VRSRA_N(, int, s, 32, 2, 2);
+  TEST_VRSRA_N(, int, s, 64, 1, 32);
+  TEST_VRSRA_N(, uint, u, 8, 8, 2);
+  TEST_VRSRA_N(, uint, u, 16, 4, 3);
+  TEST_VRSRA_N(, uint, u, 32, 2, 5);
+  TEST_VRSRA_N(, uint, u, 64, 1, 33);
+
+  TEST_VRSRA_N(q, int, s, 8, 16, 1);
+  TEST_VRSRA_N(q, int, s, 16, 8, 12);
+  TEST_VRSRA_N(q, int, s, 32, 4, 2);
+  TEST_VRSRA_N(q, int, s, 64, 2, 32);
+  TEST_VRSRA_N(q, uint, u, 8, 16, 2);
+  TEST_VRSRA_N(q, uint, u, 16, 8, 3);
+  TEST_VRSRA_N(q, uint, u, 32, 4, 5);
+  TEST_VRSRA_N(q, uint, u, 64, 2, 33);
+
+#define CMT ""
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected, CMT);
+
+
+  /* Initialize the accumulator with 0.  */
+  VDUP(vector, , int, s, 8, 8, 0);
+  VDUP(vector, , int, s, 16, 4, 0);
+  VDUP(vector, , int, s, 32, 2, 0);
+  VDUP(vector, , int, s, 64, 1, 0);
+  VDUP(vector, , uint, u, 8, 8, 0);
+  VDUP(vector, , uint, u, 16, 4, 0);
+  VDUP(vector, , uint, u, 32, 2, 0);
+  VDUP(vector, , uint, u, 64, 1, 0);
+  VDUP(vector, q, int, s, 8, 16, 0);
+  VDUP(vector, q, int, s, 16, 8, 0);
+  VDUP(vector, q, int, s, 32, 4, 0);
+  VDUP(vector, q, int, s, 64, 2, 0);
+  VDUP(vector, q, uint, u, 8, 16, 0);
+  VDUP(vector, q, uint, u, 16, 8, 0);
+  VDUP(vector, q, uint, u, 32, 4, 0);
+  VDUP(vector, q, uint, u, 64, 2, 0);
+
+  /* Initialize with max values to check overflow.  */
+  VDUP(vector2, , int, s, 8, 8, 0x7F);
+  VDUP(vector2, , int, s, 16, 4, 0x7FFF);
+  VDUP(vector2, , int, s, 32, 2, 0x7FFFFFFF);
+  VDUP(vector2, , int, s, 64, 1, 0x7FFFFFFFFFFFFFFFLL);
+  VDUP(vector2, , uint, u, 8, 8, 0xFF);
+  VDUP(vector2, , uint, u, 16, 4, 0xFFFF);
+  VDUP(vector2, , uint, u, 32, 2, 0xFFFFFFFF);
+  VDUP(vector2, , uint, u, 64, 1, 0xFFFFFFFFFFFFFFFFULL);
+  VDUP(vector2, q, int, s, 8, 16, 0x7F);
+  VDUP(vector2, q, int, s, 16, 8, 0x7FFF);
+  VDUP(vector2, q, int, s, 32, 4, 0x7FFFFFFF);
+  VDUP(vector2, q, int, s, 64, 2, 0x7FFFFFFFFFFFFFFFLL);
+  VDUP(vector2, q, uint, u, 8, 16, 0xFF);
+  VDUP(vector2, q, uint, u, 16, 8, 0xFFFF);
+  VDUP(vector2, q, uint, u, 32, 4, 0xFFFFFFFF);
+  VDUP(vector2, q, uint, u, 64, 2, 0xFFFFFFFFFFFFFFFFULL);
+
+  /* Shift by 1 to check overflow with rounding constant.  */
+  TEST_VRSRA_N(, int, s, 8, 8, 1);
+  TEST_VRSRA_N(, int, s, 16, 4, 1);
+  TEST_VRSRA_N(, int, s, 32, 2, 1);
+  TEST_VRSRA_N(, int, s, 64, 1, 1);
+  TEST_VRSRA_N(, uint, u, 8, 8, 1);
+  TEST_VRSRA_N(, uint, u, 16, 4, 1);
+  TEST_VRSRA_N(, uint, u, 32, 2, 1);
+  TEST_VRSRA_N(, uint, u, 64, 1, 1);
+  TEST_VRSRA_N(q, int, s, 8, 16, 1);
+  TEST_VRSRA_N(q, int, s, 16, 8, 1);
+  TEST_VRSRA_N(q, int, s, 32, 4, 1);
+  TEST_VRSRA_N(q, int, s, 64, 2, 1);
+  TEST_VRSRA_N(q, uint, u, 8, 16, 1);
+  TEST_VRSRA_N(q, uint, u, 16, 8, 1);
+  TEST_VRSRA_N(q, uint, u, 32, 4, 1);
+  TEST_VRSRA_N(q, uint, u, 64, 2, 1);
+
+#undef CMT
+#define CMT " (checking overflow: shift by 1, max input)"
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_max_sh1, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_max_sh1, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_max_sh1, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_max_sh1, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_max_sh1, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_max_sh1, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_max_sh1, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_max_sh1, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_max_sh1, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_max_sh1, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_max_sh1, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_max_sh1, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_max_sh1, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_max_sh1, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_max_sh1, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_max_sh1, CMT);
+
+
+  /* Shift by 3 to check overflow with rounding constant.  */
+  TEST_VRSRA_N(, int, s, 8, 8, 3);
+  TEST_VRSRA_N(, int, s, 16, 4, 3);
+  TEST_VRSRA_N(, int, s, 32, 2, 3);
+  TEST_VRSRA_N(, int, s, 64, 1, 3);
+  TEST_VRSRA_N(, uint, u, 8, 8, 3);
+  TEST_VRSRA_N(, uint, u, 16, 4, 3);
+  TEST_VRSRA_N(, uint, u, 32, 2, 3);
+  TEST_VRSRA_N(, uint, u, 64, 1, 3);
+  TEST_VRSRA_N(q, int, s, 8, 16, 3);
+  TEST_VRSRA_N(q, int, s, 16, 8, 3);
+  TEST_VRSRA_N(q, int, s, 32, 4, 3);
+  TEST_VRSRA_N(q, int, s, 64, 2, 3);
+  TEST_VRSRA_N(q, uint, u, 8, 16, 3);
+  TEST_VRSRA_N(q, uint, u, 16, 8, 3);
+  TEST_VRSRA_N(q, uint, u, 32, 4, 3);
+  TEST_VRSRA_N(q, uint, u, 64, 2, 3);
+
+#undef CMT
+#define CMT " (checking overflow: shift by 3, max input)"
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_max_sh3, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_max_sh3, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_max_sh3, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_max_sh3, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_max_sh3, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_max_sh3, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_max_sh3, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_max_sh3, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_max_sh3, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_max_sh3, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_max_sh3, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_max_sh3, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_max_sh3, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_max_sh3, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_max_sh3, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_max_sh3, CMT);
+
+
+  /* Shift by max to check overflow with rounding constant.  */
+  TEST_VRSRA_N(, int, s, 8, 8, 8);
+  TEST_VRSRA_N(, int, s, 16, 4, 16);
+  TEST_VRSRA_N(, int, s, 32, 2, 32);
+  TEST_VRSRA_N(, int, s, 64, 1, 64);
+  TEST_VRSRA_N(, uint, u, 8, 8, 8);
+  TEST_VRSRA_N(, uint, u, 16, 4, 16);
+  TEST_VRSRA_N(, uint, u, 32, 2, 32);
+  TEST_VRSRA_N(, uint, u, 64, 1, 64);
+  TEST_VRSRA_N(q, int, s, 8, 16, 8);
+  TEST_VRSRA_N(q, int, s, 16, 8, 16);
+  TEST_VRSRA_N(q, int, s, 32, 4, 32);
+  TEST_VRSRA_N(q, int, s, 64, 2, 64);
+  TEST_VRSRA_N(q, uint, u, 8, 16, 8);
+  TEST_VRSRA_N(q, uint, u, 16, 8, 16);
+  TEST_VRSRA_N(q, uint, u, 32, 4, 32);
+  TEST_VRSRA_N(q, uint, u, 64, 2, 64);
+
+#undef CMT
+#define CMT " (checking overflow: shift by max, max input)"
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_max_shmax, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_max_shmax, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_max_shmax, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_max_shmax, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_max_shmax, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_max_shmax, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_max_shmax, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_max_shmax, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_max_shmax, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_max_shmax, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_max_shmax, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_max_shmax, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_max_shmax, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_max_shmax, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_max_shmax, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_max_shmax, CMT);
+
+
+  /* Initialize with min values to check overflow.  */
+  VDUP(vector2, , int, s, 8, 8, 0x80);
+  VDUP(vector2, , int, s, 16, 4, 0x8000);
+  VDUP(vector2, , int, s, 32, 2, 0x80000000);
+  VDUP(vector2, , int, s, 64, 1, 0x8000000000000000LL);
+  VDUP(vector2, q, int, s, 8, 16, 0x80);
+  VDUP(vector2, q, int, s, 16, 8, 0x8000);
+  VDUP(vector2, q, int, s, 32, 4, 0x80000000);
+  VDUP(vector2, q, int, s, 64, 2, 0x8000000000000000ULL);
+
+  /* Shift by 1 to check overflow with rounding constant.  */
+  TEST_VRSRA_N(, int, s, 8, 8, 1);
+  TEST_VRSRA_N(, int, s, 16, 4, 1);
+  TEST_VRSRA_N(, int, s, 32, 2, 1);
+  TEST_VRSRA_N(, int, s, 64, 1, 1);
+  TEST_VRSRA_N(q, int, s, 8, 16, 1);
+  TEST_VRSRA_N(q, int, s, 16, 8, 1);
+  TEST_VRSRA_N(q, int, s, 32, 4, 1);
+  TEST_VRSRA_N(q, int, s, 64, 2, 1);
+
+#undef CMT
+#define CMT " (checking overflow: shift by 1, min negative input)"
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_min_sh1, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_min_sh1, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_min_sh1, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_min_sh1, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_min_sh1, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_min_sh1, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_min_sh1, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_min_sh1, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_min_sh1, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_min_sh1, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_min_sh1, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_min_sh1, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_min_sh1, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_min_sh1, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_min_sh1, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_min_sh1, CMT);
+
+
+  /* Shift by 3 to check overflow with rounding constant.  */
+  TEST_VRSRA_N(, int, s, 8, 8, 3);
+  TEST_VRSRA_N(, int, s, 16, 4, 3);
+  TEST_VRSRA_N(, int, s, 32, 2, 3);
+  TEST_VRSRA_N(, int, s, 64, 1, 3);
+  TEST_VRSRA_N(q, int, s, 8, 16, 3);
+  TEST_VRSRA_N(q, int, s, 16, 8, 3);
+  TEST_VRSRA_N(q, int, s, 32, 4, 3);
+  TEST_VRSRA_N(q, int, s, 64, 2, 3);
+
+#undef CMT
+#define CMT " (checking overflow: shift by 3, min negative input)"
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_min_sh3, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_min_sh3, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_min_sh3, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_min_sh3, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_min_sh3, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_min_sh3, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_min_sh3, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_min_sh3, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_min_sh3, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_min_sh3, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_min_sh3, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_min_sh3, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_min_sh3, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_min_sh3, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_min_sh3, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_min_sh3, CMT);
+
+
+  /* Shift by max to check overflow with rounding constant.  */
+  TEST_VRSRA_N(, int, s, 8, 8, 8);
+  TEST_VRSRA_N(, int, s, 16, 4, 16);
+  TEST_VRSRA_N(, int, s, 32, 2, 32);
+  TEST_VRSRA_N(, int, s, 64, 1, 64);
+  TEST_VRSRA_N(q, int, s, 8, 16, 8);
+  TEST_VRSRA_N(q, int, s, 16, 8, 16);
+  TEST_VRSRA_N(q, int, s, 32, 4, 32);
+  TEST_VRSRA_N(q, int, s, 64, 2, 64);
+
+#undef CMT
+#define CMT " (checking overflow: shift by max, min negative input)"
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_min_shmax, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_min_shmax, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_min_shmax, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_min_shmax, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_min_shmax, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_min_shmax, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_min_shmax, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_min_shmax, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_min_shmax, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_min_shmax, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_min_shmax, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_min_shmax, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_min_shmax, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_min_shmax, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_min_shmax, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_min_shmax, CMT);
+}
+
+int main (void)
+{
+  exec_vrsra_n ();
+  return 0;
+}
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vsXi_n.inc
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vsXi_n.inc
@@ -68,7 +68,24 @@ void FNNAME (INSN_NAME) (void)
   TEST_VSXI_N(INSN_NAME, q, poly, p, 8, 16, 3);
   TEST_VSXI_N(INSN_NAME, q, poly, p, 16, 8, 12);
 
-  CHECK_RESULTS (TEST_MSG, "");
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected, "");
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected, "");
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected, "");
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected, "");
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected, "");
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected, "");
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected, "");
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected, "");
+  CHECK(TEST_MSG, poly, 8, 8, PRIx8, expected, "");
+  CHECK(TEST_MSG, poly, 16, 4, PRIx16, expected, "");
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected, "");
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected, "");
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected, "");
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected, "");
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected, "");
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected, "");
+  CHECK(TEST_MSG, poly, 8, 16, PRIx8, expected, "");
+  CHECK(TEST_MSG, poly, 16, 8, PRIx16, expected, "");
 
 #ifdef EXTRA_TESTS
   EXTRA_TESTS();
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vset_lane.c
@@ -0,0 +1,99 @@
+#include <arm_neon.h>
+#include "arm-neon-ref.h"
+#include "compute-ref-data.h"
+
+/* Expected results.  */
+VECT_VAR_DECL(expected,int,8,8) [] = { 0xf0, 0xf1, 0xf2, 0xf3,
+				       0xf4, 0xf5, 0xf6, 0x11 };
+VECT_VAR_DECL(expected,int,16,4) [] = { 0xfff0, 0xfff1, 0xfff2, 0x22 };
+VECT_VAR_DECL(expected,int,32,2) [] = { 0xfffffff0, 0x33 };
+VECT_VAR_DECL(expected,int,64,1) [] = { 0x44 };
+VECT_VAR_DECL(expected,uint,8,8) [] = { 0xf0, 0xf1, 0xf2, 0xf3,
+					0xf4, 0xf5, 0x55, 0xf7 };
+VECT_VAR_DECL(expected,uint,16,4) [] = { 0xfff0, 0xfff1, 0x66, 0xfff3 };
+VECT_VAR_DECL(expected,uint,32,2) [] = { 0xfffffff0, 0x77 };
+VECT_VAR_DECL(expected,uint,64,1) [] = { 0x88 };
+VECT_VAR_DECL(expected,poly,8,8) [] = { 0xf0, 0xf1, 0xf2, 0xf3,
+					0xf4, 0xf5, 0x55, 0xf7 };
+VECT_VAR_DECL(expected,poly,16,4) [] = { 0xfff0, 0xfff1, 0x66, 0xfff3 };
+VECT_VAR_DECL(expected,hfloat,32,2) [] = { 0xc1800000, 0x4204cccd };
+VECT_VAR_DECL(expected,int,8,16) [] = { 0xf0, 0xf1, 0xf2, 0xf3,
+					0xf4, 0xf5, 0xf6, 0xf7,
+					0xf8, 0xf9, 0xfa, 0xfb,
+					0xfc, 0xfd, 0xfe, 0x99 };
+VECT_VAR_DECL(expected,int,16,8) [] = { 0xfff0, 0xfff1, 0xfff2, 0xfff3,
+					0xfff4, 0xaa, 0xfff6, 0xfff7 };
+VECT_VAR_DECL(expected,int,32,4) [] = { 0xfffffff0, 0xfffffff1,
+					0xfffffff2, 0xbb };
+VECT_VAR_DECL(expected,int,64,2) [] = { 0xfffffffffffffff0, 0xcc };
+VECT_VAR_DECL(expected,uint,8,16) [] = { 0xf0, 0xf1, 0xf2, 0xf3,
+					 0xf4, 0xf5, 0xf6, 0xf7,
+					 0xf8, 0xf9, 0xfa, 0xfb,
+					 0xfc, 0xfd, 0xdd, 0xff };
+VECT_VAR_DECL(expected,uint,16,8) [] = { 0xfff0, 0xfff1, 0xfff2, 0xfff3,
+					 0xfff4, 0xfff5, 0xee, 0xfff7 };
+VECT_VAR_DECL(expected,uint,32,4) [] = { 0xfffffff0, 0xfffffff1,
+					 0xff, 0xfffffff3 };
+VECT_VAR_DECL(expected,uint,64,2) [] = { 0xfffffffffffffff0, 0x11 };
+VECT_VAR_DECL(expected,poly,8,16) [] = { 0xf0, 0xf1, 0xf2, 0xf3,
+					 0xf4, 0xf5, 0xf6, 0xf7,
+					 0xf8, 0xf9, 0xfa, 0xfb,
+					 0xfc, 0xfd, 0xdd, 0xff };
+VECT_VAR_DECL(expected,poly,16,8) [] = { 0xfff0, 0xfff1, 0xfff2, 0xfff3,
+					 0xfff4, 0xfff5, 0xee, 0xfff7 };
+VECT_VAR_DECL(expected,hfloat,32,4) [] = { 0xc1800000, 0xc1700000,
+					   0xc1600000, 0x41333333 };
+
+#define TEST_MSG "VSET_LANE/VSET_LANEQ"
+void exec_vset_lane (void)
+{
+  /* vec=vset_lane(val, vec, lane), then store the result.  */
+#define TEST_VSET_LANE(Q, T1, T2, W, N, V, L)				\
+  VECT_VAR(vector, T1, W, N) =						\
+    vset##Q##_lane_##T2##W(V,						\
+			   VECT_VAR(vector, T1, W, N),			\
+			   L);						\
+  vst1##Q##_##T2##W(VECT_VAR(result, T1, W, N), VECT_VAR(vector, T1, W, N))
+
+  DECL_VARIABLE_ALL_VARIANTS(vector);
+
+  clean_results ();
+
+  /* Initialize input "vector" from "buffer".  */
+  TEST_MACRO_ALL_VARIANTS_2_5(VLOAD, vector, buffer);
+  VLOAD(vector, buffer, , float, f, 32, 2);
+  VLOAD(vector, buffer, q, float, f, 32, 4);
+
+  /* Choose value and lane arbitrarily.  */
+  TEST_VSET_LANE(, int, s, 8, 8, 0x11, 7);
+  TEST_VSET_LANE(, int, s, 16, 4, 0x22, 3);
+  TEST_VSET_LANE(, int, s, 32, 2, 0x33, 1);
+  TEST_VSET_LANE(, int, s, 64, 1, 0x44, 0);
+  TEST_VSET_LANE(, uint, u, 8, 8, 0x55, 6);
+  TEST_VSET_LANE(, uint, u, 16, 4, 0x66, 2);
+  TEST_VSET_LANE(, uint, u, 32, 2, 0x77, 1);
+  TEST_VSET_LANE(, uint, u, 64, 1, 0x88, 0);
+  TEST_VSET_LANE(, poly, p, 8, 8, 0x55, 6);
+  TEST_VSET_LANE(, poly, p, 16, 4, 0x66, 2);
+  TEST_VSET_LANE(, float, f, 32, 2, 33.2f, 1);
+
+  TEST_VSET_LANE(q, int, s, 8, 16, 0x99, 15);
+  TEST_VSET_LANE(q, int, s, 16, 8, 0xAA, 5);
+  TEST_VSET_LANE(q, int, s, 32, 4, 0xBB, 3);
+  TEST_VSET_LANE(q, int, s, 64, 2, 0xCC, 1);
+  TEST_VSET_LANE(q, uint, u, 8, 16, 0xDD, 14);
+  TEST_VSET_LANE(q, uint, u, 16, 8, 0xEE, 6);
+  TEST_VSET_LANE(q, uint, u, 32, 4, 0xFF, 2);
+  TEST_VSET_LANE(q, uint, u, 64, 2, 0x11, 1);
+  TEST_VSET_LANE(q, poly, p, 8, 16, 0xDD, 14);
+  TEST_VSET_LANE(q, poly, p, 16, 8, 0xEE, 6);
+  TEST_VSET_LANE(q, float, f, 32, 4, 11.2f, 3);
+
+  CHECK_RESULTS(TEST_MSG, "");
+}
+
+int main (void)
+{
+  exec_vset_lane ();
+  return 0;
+}
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vshl.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vshl.c
@@ -13,11 +13,6 @@ VECT_VAR_DECL(expected,uint,8,8) [] = { 0xe0, 0xe2, 0xe4, 0xe6,
 VECT_VAR_DECL(expected,uint,16,4) [] = { 0xff80, 0xff88, 0xff90, 0xff98 };
 VECT_VAR_DECL(expected,uint,32,2) [] = { 0xfffff000, 0xfffff100 };
 VECT_VAR_DECL(expected,uint,64,1) [] = { 0xffffffffffffff80 };
-VECT_VAR_DECL(expected,poly,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,4) [] = { 0x3333, 0x3333,
-					 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,2) [] = { 0x33333333, 0x33333333 };
 VECT_VAR_DECL(expected,int,8,16) [] = { 0x0, 0x20, 0x40, 0x60,
 					0x80, 0xa0, 0xc0, 0xe0,
 					0x0, 0x20, 0x40, 0x60,
@@ -36,14 +31,6 @@ VECT_VAR_DECL(expected,uint,16,8) [] = { 0x0, 0x1000, 0x2000, 0x3000,
 VECT_VAR_DECL(expected,uint,32,4) [] = { 0x0, 0x40000000,
 					 0x80000000, 0xc0000000 };
 VECT_VAR_DECL(expected,uint,64,2) [] = { 0x0, 0x8000000000000000 };
-VECT_VAR_DECL(expected,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,4) [] = { 0x33333333, 0x33333333,
-					   0x33333333, 0x33333333 };
 
 /* Expected results with large shift amount.  */
 VECT_VAR_DECL(expected_large_shift,int,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
@@ -56,11 +43,6 @@ VECT_VAR_DECL(expected_large_shift,uint,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
 VECT_VAR_DECL(expected_large_shift,uint,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
 VECT_VAR_DECL(expected_large_shift,uint,32,2) [] = { 0x0, 0x0 };
 VECT_VAR_DECL(expected_large_shift,uint,64,1) [] = { 0x0 };
-VECT_VAR_DECL(expected_large_shift,poly,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-						    0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected_large_shift,poly,16,4) [] = { 0x3333, 0x3333,
-						     0x3333, 0x3333 };
-VECT_VAR_DECL(expected_large_shift,hfloat,32,2) [] = { 0x33333333, 0x33333333 };
 VECT_VAR_DECL(expected_large_shift,int,8,16) [] = { 0x0, 0x0, 0x0, 0x0,
 						    0x0, 0x0, 0x0, 0x0,
 						    0x0, 0x0, 0x0, 0x0,
@@ -77,16 +59,6 @@ VECT_VAR_DECL(expected_large_shift,uint,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
 						     0x0, 0x0, 0x0, 0x0 };
 VECT_VAR_DECL(expected_large_shift,uint,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
 VECT_VAR_DECL(expected_large_shift,uint,64,2) [] = { 0x0, 0x0 };
-VECT_VAR_DECL(expected_large_shift,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-						     0x33, 0x33, 0x33, 0x33,
-						     0x33, 0x33, 0x33, 0x33,
-						     0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected_large_shift,poly,16,8) [] = { 0x3333, 0x3333,
-						     0x3333, 0x3333,
-						     0x3333, 0x3333,
-						     0x3333, 0x3333 };
-VECT_VAR_DECL(expected_large_shift,hfloat,32,4) [] = { 0x33333333, 0x33333333,
-						       0x33333333, 0x33333333 };
 
 
 /* Expected results with negative shift amount.  */
@@ -103,12 +75,6 @@ VECT_VAR_DECL(expected_negative_shift,uint,16,4) [] = { 0x7ff8, 0x7ff8,
 VECT_VAR_DECL(expected_negative_shift,uint,32,2) [] = { 0x3ffffffc,
 							0x3ffffffc };
 VECT_VAR_DECL(expected_negative_shift,uint,64,1) [] = { 0xfffffffffffffff };
-VECT_VAR_DECL(expected_negative_shift,poly,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-						       0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected_negative_shift,poly,16,4) [] = { 0x3333, 0x3333,
-							0x3333, 0x3333 };
-VECT_VAR_DECL(expected_negative_shift,hfloat,32,2) [] = { 0x33333333,
-							  0x33333333 };
 VECT_VAR_DECL(expected_negative_shift,int,8,16) [] = { 0xfc, 0xfc, 0xfc, 0xfc,
 						       0xfd, 0xfd, 0xfd, 0xfd,
 						       0xfe, 0xfe, 0xfe, 0xfe,
@@ -133,18 +99,6 @@ VECT_VAR_DECL(expected_negative_shift,uint,32,4) [] = { 0x1ffffffe, 0x1ffffffe,
 							0x1ffffffe, 0x1ffffffe };
 VECT_VAR_DECL(expected_negative_shift,uint,64,2) [] = { 0x7ffffffffffffff,
 							0x7ffffffffffffff };
-VECT_VAR_DECL(expected_negative_shift,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-							0x33, 0x33, 0x33, 0x33,
-							0x33, 0x33, 0x33, 0x33,
-							0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected_negative_shift,poly,16,8) [] = { 0x3333, 0x3333,
-							0x3333, 0x3333,
-							0x3333, 0x3333,
-							0x3333, 0x3333 };
-VECT_VAR_DECL(expected_negative_shift,hfloat,32,4) [] = { 0x33333333,
-							  0x33333333,
-							  0x33333333,
-							  0x33333333 };
 
 
 #ifndef INSN_NAME
@@ -187,7 +141,22 @@ void FNNAME (INSN_NAME) (void)
   /* Execute the tests.  */
   TEST_MACRO_ALL_VARIANTS_1_5(TEST_VSHL, int);
 
-  CHECK_RESULTS (TEST_MSG, "");
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected, "");
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected, "");
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected, "");
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected, "");
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected, "");
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected, "");
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected, "");
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected, "");
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected, "");
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected, "");
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected, "");
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected, "");
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected, "");
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected, "");
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected, "");
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected, "");
 
 
   /* Test large shift amount (larger or equal to the type width.  */
@@ -203,7 +172,23 @@ void FNNAME (INSN_NAME) (void)
   /* Execute the tests.  */
   TEST_MACRO_ALL_VARIANTS_1_5(TEST_VSHL, int);
 
-  CHECK_RESULTS_NAMED (TEST_MSG, expected_large_shift, "(large shift amount)");
+#define COMMENT1 "(large shift amount)"
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_large_shift, COMMENT1);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_large_shift, COMMENT1);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_large_shift, COMMENT1);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_large_shift, COMMENT1);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_large_shift, COMMENT1);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_large_shift, COMMENT1);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_large_shift, COMMENT1);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_large_shift, COMMENT1);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_large_shift, COMMENT1);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_large_shift, COMMENT1);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_large_shift, COMMENT1);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_large_shift, COMMENT1);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_large_shift, COMMENT1);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_large_shift, COMMENT1);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_large_shift, COMMENT1);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_large_shift, COMMENT1);
 
 
   /* Test negative shift amount. */
@@ -219,7 +204,23 @@ void FNNAME (INSN_NAME) (void)
   /* Execute the tests.  */
   TEST_MACRO_ALL_VARIANTS_1_5(TEST_VSHL, int);
 
-  CHECK_RESULTS_NAMED (TEST_MSG, expected_negative_shift, "(negative shift amount)");
+#define COMMENT2 "(negative shift amount)"
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_negative_shift, COMMENT2);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_negative_shift, COMMENT2);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_negative_shift, COMMENT2);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_negative_shift, COMMENT2);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_negative_shift, COMMENT2);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_negative_shift, COMMENT2);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_negative_shift, COMMENT2);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_negative_shift, COMMENT2);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_negative_shift, COMMENT2);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_negative_shift, COMMENT2);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_negative_shift, COMMENT2);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected_negative_shift, COMMENT2);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_negative_shift, COMMENT2);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_negative_shift, COMMENT2);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_negative_shift, COMMENT2);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected_negative_shift, COMMENT2);
 }
 
 int main (void)
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vshl_n.c
@@ -0,0 +1,96 @@
+#include <arm_neon.h>
+#include "arm-neon-ref.h"
+#include "compute-ref-data.h"
+
+/* Expected results.  */
+VECT_VAR_DECL(expected,int,8,8) [] = { 0xe0, 0xe2, 0xe4, 0xe6,
+				       0xe8, 0xea, 0xec, 0xee };
+VECT_VAR_DECL(expected,int,16,4) [] = { 0xffe0, 0xffe2, 0xffe4, 0xffe6 };
+VECT_VAR_DECL(expected,int,32,2) [] = { 0xffffff80, 0xffffff88 };
+VECT_VAR_DECL(expected,int,64,1) [] = { 0xffffffffffffffc0 };
+VECT_VAR_DECL(expected,uint,8,8) [] = { 0xc0, 0xc4, 0xc8, 0xcc,
+					0xd0, 0xd4, 0xd8, 0xdc };
+VECT_VAR_DECL(expected,uint,16,4) [] = { 0xff00, 0xff10, 0xff20, 0xff30 };
+VECT_VAR_DECL(expected,uint,32,2) [] = { 0xffffff80, 0xffffff88 };
+VECT_VAR_DECL(expected,uint,64,1) [] = { 0xffffffffffffffe0 };
+VECT_VAR_DECL(expected,int,8,16) [] = { 0x0, 0x20, 0x40, 0x60,
+					0x80, 0xa0, 0xc0, 0xe0,
+					0x0, 0x20, 0x40, 0x60,
+					0x80, 0xa0, 0xc0, 0xe0 };
+VECT_VAR_DECL(expected,int,16,8) [] = { 0xffe0, 0xffe2, 0xffe4, 0xffe6,
+					0xffe8, 0xffea, 0xffec, 0xffee };
+VECT_VAR_DECL(expected,int,32,4) [] = { 0xffffffc0, 0xffffffc4,
+					0xffffffc8, 0xffffffcc };
+VECT_VAR_DECL(expected,int,64,2) [] = { 0xffffffffffffffc0, 0xffffffffffffffc4 };
+VECT_VAR_DECL(expected,uint,8,16) [] = { 0xc0, 0xc4, 0xc8, 0xcc,
+					 0xd0, 0xd4, 0xd8, 0xdc,
+					 0xe0, 0xe4, 0xe8, 0xec,
+					 0xf0, 0xf4, 0xf8, 0xfc };
+VECT_VAR_DECL(expected,uint,16,8) [] = { 0xff80, 0xff88, 0xff90, 0xff98,
+					 0xffa0, 0xffa8, 0xffb0, 0xffb8 };
+VECT_VAR_DECL(expected,uint,32,4) [] = { 0xffffffc0, 0xffffffc4,
+					 0xffffffc8, 0xffffffcc };
+VECT_VAR_DECL(expected,uint,64,2) [] = { 0xffffffffffffffe0,
+					 0xffffffffffffffe2 };
+
+#define TEST_MSG "VSHL_N"
+void exec_vshl_n (void)
+{
+  /* Basic test: v2=vshl_n(v1,v), then store the result.  */
+#define TEST_VSHL_N(Q, T1, T2, W, N, V)					\
+  VECT_VAR(vector_res, T1, W, N) =					\
+    vshl##Q##_n_##T2##W(VECT_VAR(vector, T1, W, N),			\
+			V);						\
+  vst1##Q##_##T2##W(VECT_VAR(result, T1, W, N), VECT_VAR(vector_res, T1, W, N))
+
+  DECL_VARIABLE_ALL_VARIANTS(vector);
+  DECL_VARIABLE_ALL_VARIANTS(vector_res);
+
+  clean_results ();
+
+  /* Initialize input "vector" from "buffer".  */
+  TEST_MACRO_ALL_VARIANTS_2_5(VLOAD, vector, buffer);
+
+  /* Choose shift amount arbitrarily.  */
+  TEST_VSHL_N(, int, s, 8, 8, 1);
+  TEST_VSHL_N(, int, s, 16, 4, 1);
+  TEST_VSHL_N(, int, s, 32, 2, 3);
+  TEST_VSHL_N(, int, s, 64, 1, 2);
+  TEST_VSHL_N(, uint, u, 8, 8, 2);
+  TEST_VSHL_N(, uint, u, 16, 4, 4);
+  TEST_VSHL_N(, uint, u, 32, 2, 3);
+  TEST_VSHL_N(, uint, u, 64, 1, 1);
+
+  TEST_VSHL_N(q, int, s, 8, 16, 5);
+  TEST_VSHL_N(q, int, s, 16, 8, 1);
+  TEST_VSHL_N(q, int, s, 32, 4, 2);
+  TEST_VSHL_N(q, int, s, 64, 2, 2);
+  TEST_VSHL_N(q, uint, u, 8, 16, 2);
+  TEST_VSHL_N(q, uint, u, 16, 8, 3);
+  TEST_VSHL_N(q, uint, u, 32, 4, 2);
+  TEST_VSHL_N(q, uint, u, 64, 2, 1);
+
+#define CMT ""
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected, CMT);
+}
+
+int main (void)
+{
+  exec_vshl_n ();
+  return 0;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vshll_n.c
@@ -0,0 +1,56 @@
+#include <arm_neon.h>
+#include "arm-neon-ref.h"
+#include "compute-ref-data.h"
+
+/* Expected results.  */
+VECT_VAR_DECL(expected,int,16,8) [] = { 0xffe0, 0xffe2, 0xffe4, 0xffe6,
+					0xffe8, 0xffea, 0xffec, 0xffee };
+VECT_VAR_DECL(expected,int,32,4) [] = { 0xffffffe0, 0xffffffe2,
+					0xffffffe4, 0xffffffe6 };
+VECT_VAR_DECL(expected,int,64,2) [] = { 0xffffffffffffff80, 0xffffffffffffff88 };
+VECT_VAR_DECL(expected,uint,16,8) [] = { 0x3c0, 0x3c4, 0x3c8, 0x3cc,
+					 0x3d0, 0x3d4, 0x3d8, 0x3dc };
+VECT_VAR_DECL(expected,uint,32,4) [] = { 0xfff00, 0xfff10, 0xfff20, 0xfff30 };
+VECT_VAR_DECL(expected,uint,64,2) [] = { 0x7ffffff80, 0x7ffffff88 };
+
+#define TEST_MSG "VSHLL_N"
+void exec_vshll_n (void)
+{
+  /* Basic test: v2=vshll_n(v1,v), then store the result.  */
+#define TEST_VSHLL_N(T1, T2, W, W2, N, V)				\
+  VECT_VAR(vector_res, T1, W2, N) =					\
+    vshll##_n_##T2##W(VECT_VAR(vector, T1, W, N),			\
+		      V);						\
+  vst1q##_##T2##W2(VECT_VAR(result, T1, W2, N), VECT_VAR(vector_res, T1, W2, N))
+
+  DECL_VARIABLE_ALL_VARIANTS(vector);
+  DECL_VARIABLE_ALL_VARIANTS(vector_res);
+
+  clean_results ();
+
+  /* Initialize input "vector" from "buffer".  */
+  TEST_MACRO_ALL_VARIANTS_2_5(VLOAD, vector, buffer);
+
+  /* Choose shift amount arbitrarily.  */
+  TEST_VSHLL_N(int, s, 8, 16, 8, 1);
+  TEST_VSHLL_N(int, s, 16, 32, 4, 1);
+  TEST_VSHLL_N(int, s, 32, 64, 2, 3);
+  TEST_VSHLL_N(uint, u, 8, 16, 8, 2);
+  TEST_VSHLL_N(uint, u, 16, 32, 4, 4);
+  TEST_VSHLL_N(uint, u, 32, 64, 2, 3);
+
+#undef CMT
+#define CMT ""
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected, CMT);
+}
+
+int main (void)
+{
+  exec_vshll_n ();
+  return 0;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vshr_n.c
@@ -0,0 +1,95 @@
+#include <arm_neon.h>
+#include "arm-neon-ref.h"
+#include "compute-ref-data.h"
+
+/* Expected results.  */
+VECT_VAR_DECL(expected,int,8,8) [] = { 0xf8, 0xf8, 0xf9, 0xf9,
+				       0xfa, 0xfa, 0xfb, 0xfb };
+VECT_VAR_DECL(expected,int,16,4) [] = { 0xffff, 0xffff, 0xffff, 0xffff };
+VECT_VAR_DECL(expected,int,32,2) [] = { 0xfffffffc, 0xfffffffc };
+VECT_VAR_DECL(expected,int,64,1) [] = { 0xffffffffffffffff };
+VECT_VAR_DECL(expected,uint,8,8) [] = { 0x3c, 0x3c, 0x3c, 0x3c,
+					0x3d, 0x3d, 0x3d, 0x3d };
+VECT_VAR_DECL(expected,uint,16,4) [] = { 0x1ffe, 0x1ffe, 0x1ffe, 0x1ffe };
+VECT_VAR_DECL(expected,uint,32,2) [] = { 0x7ffffff, 0x7ffffff };
+VECT_VAR_DECL(expected,uint,64,1) [] = { 0x7fffffff };
+VECT_VAR_DECL(expected,int,8,16) [] = { 0xf8, 0xf8, 0xf9, 0xf9,
+					0xfa, 0xfa, 0xfb, 0xfb,
+					0xfc, 0xfc, 0xfd, 0xfd,
+					0xfe, 0xfe, 0xff, 0xff };
+VECT_VAR_DECL(expected,int,16,8) [] = { 0xffff, 0xffff, 0xffff, 0xffff,
+					0xffff, 0xffff, 0xffff, 0xffff };
+VECT_VAR_DECL(expected,int,32,4) [] = { 0xfffffffc, 0xfffffffc,
+					0xfffffffc, 0xfffffffc };
+VECT_VAR_DECL(expected,int,64,2) [] = { 0xffffffffffffffff, 0xffffffffffffffff };
+VECT_VAR_DECL(expected,uint,8,16) [] = { 0x3c, 0x3c, 0x3c, 0x3c,
+					 0x3d, 0x3d, 0x3d, 0x3d,
+					 0x3e, 0x3e, 0x3e, 0x3e,
+					 0x3f, 0x3f, 0x3f, 0x3f };
+VECT_VAR_DECL(expected,uint,16,8) [] = { 0x1ffe, 0x1ffe, 0x1ffe, 0x1ffe,
+					 0x1ffe, 0x1ffe, 0x1ffe, 0x1ffe };
+VECT_VAR_DECL(expected,uint,32,4) [] = { 0x7ffffff, 0x7ffffff,
+					 0x7ffffff, 0x7ffffff };
+VECT_VAR_DECL(expected,uint,64,2) [] = { 0x7fffffff, 0x7fffffff };
+
+#define TEST_MSG "VSHR_N"
+void exec_vshr_n (void)
+{
+  /* Basic test: y=vshr_n(x,v), then store the result.  */
+#define TEST_VSHR_N(Q, T1, T2, W, N, V)					\
+  VECT_VAR(vector_res, T1, W, N) =					\
+    vshr##Q##_n_##T2##W(VECT_VAR(vector, T1, W, N),			\
+			V);						\
+  vst1##Q##_##T2##W(VECT_VAR(result, T1, W, N), VECT_VAR(vector_res, T1, W, N))
+
+  DECL_VARIABLE_ALL_VARIANTS(vector);
+  DECL_VARIABLE_ALL_VARIANTS(vector_res);
+
+  clean_results ();
+
+  /* Initialize input "vector" from "buffer".  */
+  TEST_MACRO_ALL_VARIANTS_2_5(VLOAD, vector, buffer);
+
+  /* Choose shift amount arbitrarily.  */
+  TEST_VSHR_N(, int, s, 8, 8, 1);
+  TEST_VSHR_N(, int, s, 16, 4, 12);
+  TEST_VSHR_N(, int, s, 32, 2, 2);
+  TEST_VSHR_N(, int, s, 64, 1, 32);
+  TEST_VSHR_N(, uint, u, 8, 8, 2);
+  TEST_VSHR_N(, uint, u, 16, 4, 3);
+  TEST_VSHR_N(, uint, u, 32, 2, 5);
+  TEST_VSHR_N(, uint, u, 64, 1, 33);
+
+  TEST_VSHR_N(q, int, s, 8, 16, 1);
+  TEST_VSHR_N(q, int, s, 16, 8, 12);
+  TEST_VSHR_N(q, int, s, 32, 4, 2);
+  TEST_VSHR_N(q, int, s, 64, 2, 32);
+  TEST_VSHR_N(q, uint, u, 8, 16, 2);
+  TEST_VSHR_N(q, uint, u, 16, 8, 3);
+  TEST_VSHR_N(q, uint, u, 32, 4, 5);
+  TEST_VSHR_N(q, uint, u, 64, 2, 33);
+
+#define CMT ""
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected, CMT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected, CMT);
+}
+
+int main (void)
+{
+  exec_vshr_n ();
+  return 0;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vshrn_n.c
@@ -0,0 +1,70 @@
+#include <arm_neon.h>
+#include "arm-neon-ref.h"
+#include "compute-ref-data.h"
+
+/* Expected results.  */
+VECT_VAR_DECL(expected,int,8,8) [] = { 0xf8, 0xf8, 0xf9, 0xf9,
+				       0xfa, 0xfa, 0xfb, 0xfb };
+VECT_VAR_DECL(expected,int,16,4) [] = { 0xfff8, 0xfff8, 0xfff9, 0xfff9 };
+VECT_VAR_DECL(expected,int,32,2) [] = { 0xfffffffc, 0xfffffffc };
+VECT_VAR_DECL(expected,uint,8,8) [] = { 0xfc, 0xfc, 0xfc, 0xfc,
+					0xfd, 0xfd, 0xfd, 0xfd };
+VECT_VAR_DECL(expected,uint,16,4) [] = { 0xfffe, 0xfffe, 0xfffe, 0xfffe };
+VECT_VAR_DECL(expected,uint,32,2) [] = { 0xfffffffe, 0xfffffffe };
+
+#define TEST_MSG "VSHRN_N"
+void exec_vshrn_n (void)
+{
+  /* Basic test: y=vshrn_n(x,v), then store the result.  */
+#define TEST_VSHRN_N(T1, T2, W, W2, N, V)				\
+  VECT_VAR(vector_res, T1, W2, N) =					\
+    vshrn_n_##T2##W(VECT_VAR(vector, T1, W, N),				\
+		    V);							\
+  vst1_##T2##W2(VECT_VAR(result, T1, W2, N), VECT_VAR(vector_res, T1, W2, N))
+
+  /* vector is twice as large as vector_res.  */
+  DECL_VARIABLE(vector, int, 16, 8);
+  DECL_VARIABLE(vector, int, 32, 4);
+  DECL_VARIABLE(vector, int, 64, 2);
+  DECL_VARIABLE(vector, uint, 16, 8);
+  DECL_VARIABLE(vector, uint, 32, 4);
+  DECL_VARIABLE(vector, uint, 64, 2);
+
+  DECL_VARIABLE(vector_res, int, 8, 8);
+  DECL_VARIABLE(vector_res, int, 16, 4);
+  DECL_VARIABLE(vector_res, int, 32, 2);
+  DECL_VARIABLE(vector_res, uint, 8, 8);
+  DECL_VARIABLE(vector_res, uint, 16, 4);
+  DECL_VARIABLE(vector_res, uint, 32, 2);
+
+  clean_results ();
+
+  VLOAD(vector, buffer, q, int, s, 16, 8);
+  VLOAD(vector, buffer, q, int, s, 32, 4);
+  VLOAD(vector, buffer, q, int, s, 64, 2);
+  VLOAD(vector, buffer, q, uint, u, 16, 8);
+  VLOAD(vector, buffer, q, uint, u, 32, 4);
+  VLOAD(vector, buffer, q, uint, u, 64, 2);
+
+  /* Choose shift amount arbitrarily.  */
+  TEST_VSHRN_N(int, s, 16, 8, 8, 1);
+  TEST_VSHRN_N(int, s, 32, 16, 4, 1);
+  TEST_VSHRN_N(int, s, 64, 32, 2, 2);
+  TEST_VSHRN_N(uint, u, 16, 8, 8, 2);
+  TEST_VSHRN_N(uint, u, 32, 16, 4, 3);
+  TEST_VSHRN_N(uint, u, 64, 32, 2, 3);
+
+#define CMT ""
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected, CMT);
+}
+
+int main (void)
+{
+  exec_vshrn_n ();
+  return 0;
+}
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vshuffle.inc
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vshuffle.inc
@@ -120,15 +120,40 @@ void FNNAME (INSN_NAME) (void)
   TEST_EXTRA_CHUNK(poly, 16, 8, 1);		\
   TEST_EXTRA_CHUNK(float, 32, 4, 1)
 
+  /* vshuffle support all vector types except [u]int64x1 and
+     [u]int64x2.  */
+#define CHECK_RESULTS_VSHUFFLE(test_name,EXPECTED,comment)		\
+  {									\
+    CHECK(test_name, int, 8, 8, PRIx8, EXPECTED, comment);		\
+    CHECK(test_name, int, 16, 4, PRIx16, EXPECTED, comment);		\
+    CHECK(test_name, int, 32, 2, PRIx32, EXPECTED, comment);		\
+    CHECK(test_name, uint, 8, 8, PRIx8, EXPECTED, comment);		\
+    CHECK(test_name, uint, 16, 4, PRIx16, EXPECTED, comment);		\
+    CHECK(test_name, uint, 32, 2, PRIx32, EXPECTED, comment);		\
+    CHECK(test_name, poly, 8, 8, PRIx8, EXPECTED, comment);		\
+    CHECK(test_name, poly, 16, 4, PRIx16, EXPECTED, comment);		\
+    CHECK_FP(test_name, float, 32, 2, PRIx32, EXPECTED, comment);	\
+									\
+    CHECK(test_name, int, 8, 16, PRIx8, EXPECTED, comment);		\
+    CHECK(test_name, int, 16, 8, PRIx16, EXPECTED, comment);		\
+    CHECK(test_name, int, 32, 4, PRIx32, EXPECTED, comment);		\
+    CHECK(test_name, uint, 8, 16, PRIx8, EXPECTED, comment);		\
+    CHECK(test_name, uint, 16, 8, PRIx16, EXPECTED, comment);		\
+    CHECK(test_name, uint, 32, 4, PRIx32, EXPECTED, comment);		\
+    CHECK(test_name, poly, 8, 16, PRIx8, EXPECTED, comment);		\
+    CHECK(test_name, poly, 16, 8, PRIx16, EXPECTED, comment);		\
+    CHECK_FP(test_name, float, 32, 4, PRIx32, EXPECTED, comment);	\
+  }									\
+
   clean_results ();
 
   /* Execute the tests.  */
   TEST_ALL_VSHUFFLE(INSN_NAME);
 
-  CHECK_RESULTS_NAMED (TEST_MSG, expected0, "(chunk 0)");
+  CHECK_RESULTS_VSHUFFLE (TEST_MSG, expected0, "(chunk 0)");
 
   TEST_ALL_EXTRA_CHUNKS();
-  CHECK_RESULTS_NAMED (TEST_MSG, expected1, "(chunk 1)");
+  CHECK_RESULTS_VSHUFFLE (TEST_MSG, expected1, "(chunk 1)");
 }
 
 int main (void)
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vsli_n.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vsli_n.c
@@ -23,7 +23,6 @@ VECT_VAR_DECL(expected,uint,64,1) [] = { 0x10 };
 VECT_VAR_DECL(expected,poly,8,8) [] = { 0x50, 0x51, 0x52, 0x53,
 					0x50, 0x51, 0x52, 0x53 };
 VECT_VAR_DECL(expected,poly,16,4) [] = { 0x7bf0, 0x7bf1, 0x7bf2, 0x7bf3 };
-VECT_VAR_DECL(expected,hfloat,32,2) [] = { 0x33333333, 0x33333333 };
 VECT_VAR_DECL(expected,int,8,16) [] = { 0xd0, 0xd1, 0xd2, 0xd3,
 					0xd4, 0xd5, 0xd6, 0xd7,
 					0xd8, 0xd9, 0xda, 0xdb,
@@ -48,8 +47,6 @@ VECT_VAR_DECL(expected,poly,8,16) [] = { 0x60, 0x61, 0x62, 0x63,
 					 0x64, 0x65, 0x66, 0x67 };
 VECT_VAR_DECL(expected,poly,16,8) [] = { 0x3ff0, 0x3ff1, 0x3ff2, 0x3ff3,
 					 0x3ff4, 0x3ff5, 0x3ff6, 0x3ff7 };
-VECT_VAR_DECL(expected,hfloat,32,4) [] = { 0x33333333, 0x33333333,
-					   0x33333333, 0x33333333 };
 
 /* Expected results with max shift amount.  */
 VECT_VAR_DECL(expected_max_shift,int,8,8) [] = { 0x70, 0x71, 0x72, 0x73,
@@ -68,7 +65,6 @@ VECT_VAR_DECL(expected_max_shift,poly,8,8) [] = { 0x70, 0x71, 0x72, 0x73,
 						  0x74, 0x75, 0x76, 0x77 };
 VECT_VAR_DECL(expected_max_shift,poly,16,4) [] = { 0x7ff0, 0x7ff1,
 						   0x7ff2, 0x7ff3 };
-VECT_VAR_DECL(expected_max_shift,hfloat,32,2) [] = { 0x33333333, 0x33333333 };
 VECT_VAR_DECL(expected_max_shift,int,8,16) [] = { 0x70, 0x71, 0x72, 0x73,
 						  0x74, 0x75, 0x76, 0x77,
 						  0x78, 0x79, 0x7a, 0x7b,
@@ -95,8 +91,6 @@ VECT_VAR_DECL(expected_max_shift,poly,8,16) [] = { 0x70, 0x71, 0x72, 0x73,
 						   0x7c, 0x7d, 0x7e, 0x7f };
 VECT_VAR_DECL(expected_max_shift,poly,16,8) [] = { 0xfff0, 0xfff1, 0xfff2, 0xfff3,
 						   0xfff4, 0xfff5, 0xfff6, 0xfff7 };
-VECT_VAR_DECL(expected_max_shift,hfloat,32,4) [] = { 0x33333333, 0x33333333,
-						     0x33333333, 0x33333333 };
 
 #include "vsXi_n.inc"
 
@@ -158,5 +152,23 @@ void vsli_extra(void)
   TEST_VSXI_N(INSN_NAME, q, poly, p, 8, 16, 7);
   TEST_VSXI_N(INSN_NAME, q, poly, p, 16, 8, 15);
 
-  CHECK_RESULTS_NAMED (TEST_MSG, expected_max_shift, "(max shift amount)");
+#define COMMENT "(max shift amount)"
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_max_shift, COMMENT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_max_shift, COMMENT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_max_shift, COMMENT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_max_shift, COMMENT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_max_shift, COMMENT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_max_shift, COMMENT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_max_shift, COMMENT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_max_shift, COMMENT);
+  CHECK(TEST_MSG, poly, 8, 8, PRIx8, expected_max_shift, COMMENT);
+  CHECK(TEST_MSG, poly, 16, 4, PRIx16, expected_max_shift, COMMENT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_max_shift, COMMENT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_max_shift, COMMENT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_max_shift, COMMENT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_max_shift, COMMENT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_max_shift, COMMENT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_max_shift, COMMENT);
+  CHECK(TEST_MSG, poly, 8, 16, PRIx8, expected_max_shift, COMMENT);
+  CHECK(TEST_MSG, poly, 16, 8, PRIx16, expected_max_shift, COMMENT);
 }
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vsra_n.c
@@ -0,0 +1,117 @@
+#include <arm_neon.h>
+#include "arm-neon-ref.h"
+#include "compute-ref-data.h"
+
+/* Expected results.  */
+VECT_VAR_DECL(expected,int,8,8) [] = { 0xf8, 0xf9, 0xfa, 0xfb,
+				       0xfc, 0xfd, 0xfe, 0xff };
+VECT_VAR_DECL(expected,int,16,4) [] = { 0xfff0, 0xfff1, 0xfff2, 0xfff3 };
+VECT_VAR_DECL(expected,int,32,2) [] = { 0xfffffffc, 0xfffffffd };
+VECT_VAR_DECL(expected,int,64,1) [] = { 0xfffffffffffffff0 };
+VECT_VAR_DECL(expected,uint,8,8) [] = { 0x5, 0x6, 0x7, 0x8,
+					0x9, 0xa, 0xb, 0xc };
+VECT_VAR_DECL(expected,uint,16,4) [] = { 0xfffc, 0xfffd, 0xfffe, 0xffff };
+VECT_VAR_DECL(expected,uint,32,2) [] = { 0xfffffff3, 0xfffffff4 };
+VECT_VAR_DECL(expected,uint,64,1) [] = { 0xfffffffffffffff0 };
+VECT_VAR_DECL(expected,int,8,16) [] = { 0xf8, 0xf9, 0xfa, 0xfb,
+					0xfc, 0xfd, 0xfe, 0xff,
+					0x0, 0x1, 0x2, 0x3,
+					0x4, 0x5, 0x6, 0x7 };
+VECT_VAR_DECL(expected,int,16,8) [] = { 0xfff0, 0xfff1, 0xfff2, 0xfff3,
+					0xfff4, 0xfff5, 0xfff6, 0xfff7 };
+VECT_VAR_DECL(expected,int,32,4) [] = { 0xfffffffc, 0xfffffffd,
+					0xfffffffe, 0xffffffff };
+VECT_VAR_DECL(expected,int,64,2) [] = { 0xfffffffffffffff0,
+					0xfffffffffffffff1 };
+VECT_VAR_DECL(expected,uint,8,16) [] = { 0x5, 0x6, 0x7, 0x8,
+					 0x9, 0xa, 0xb, 0xc,
+					 0xd, 0xe, 0xf, 0x10,
+					 0x11, 0x12, 0x13, 0x14 };
+VECT_VAR_DECL(expected,uint,16,8) [] = { 0xfffc, 0xfffd, 0xfffe, 0xffff,
+					 0x0, 0x1, 0x2, 0x3 };
+VECT_VAR_DECL(expected,uint,32,4) [] = { 0xfffffff3, 0xfffffff4,
+					 0xfffffff5, 0xfffffff6 };
+VECT_VAR_DECL(expected,uint,64,2) [] = { 0xfffffffffffffff0,
+					 0xfffffffffffffff1 };
+
+#define TEST_MSG "VSRA_N"
+void exec_vsra_n (void)
+{
+  /* Basic test: y=vsra_n(x,v), then store the result.  */
+#define TEST_VSRA_N(Q, T1, T2, W, N, V)					\
+  VECT_VAR(vector_res, T1, W, N) =					\
+    vsra##Q##_n_##T2##W(VECT_VAR(vector, T1, W, N),			\
+			VECT_VAR(vector2, T1, W, N),			\
+			V);						\
+  vst1##Q##_##T2##W(VECT_VAR(result, T1, W, N), VECT_VAR(vector_res, T1, W, N))
+
+  DECL_VARIABLE_ALL_VARIANTS(vector);
+  DECL_VARIABLE_ALL_VARIANTS(vector2);
+  DECL_VARIABLE_ALL_VARIANTS(vector_res);
+
+  clean_results ();
+
+  /* Initialize input "vector" from "buffer".  */
+  TEST_MACRO_ALL_VARIANTS_2_5(VLOAD, vector, buffer);
+
+  /* Choose arbitrary initialization values.  */
+  VDUP(vector2, , int, s, 8, 8, 0x11);
+  VDUP(vector2, , int, s, 16, 4, 0x22);
+  VDUP(vector2, , int, s, 32, 2, 0x33);
+  VDUP(vector2, , int, s, 64, 1, 0x44);
+  VDUP(vector2, , uint, u, 8, 8, 0x55);
+  VDUP(vector2, , uint, u, 16, 4, 0x66);
+  VDUP(vector2, , uint, u, 32, 2, 0x77);
+  VDUP(vector2, , uint, u, 64, 1, 0x88);
+
+  VDUP(vector2, q, int, s, 8, 16, 0x11);
+  VDUP(vector2, q, int, s, 16, 8, 0x22);
+  VDUP(vector2, q, int, s, 32, 4, 0x33);
+  VDUP(vector2, q, int, s, 64, 2, 0x44);
+  VDUP(vector2, q, uint, u, 8, 16, 0x55);
+  VDUP(vector2, q, uint, u, 16, 8, 0x66);
+  VDUP(vector2, q, uint, u, 32, 4, 0x77);
+  VDUP(vector2, q, uint, u, 64, 2, 0x88);
+
+  /* Choose shift amount arbitrarily.  */
+  TEST_VSRA_N(, int, s, 8, 8, 1);
+  TEST_VSRA_N(, int, s, 16, 4, 12);
+  TEST_VSRA_N(, int, s, 32, 2, 2);
+  TEST_VSRA_N(, int, s, 64, 1, 32);
+  TEST_VSRA_N(, uint, u, 8, 8, 2);
+  TEST_VSRA_N(, uint, u, 16, 4, 3);
+  TEST_VSRA_N(, uint, u, 32, 2, 5);
+  TEST_VSRA_N(, uint, u, 64, 1, 33);
+
+  TEST_VSRA_N(q, int, s, 8, 16, 1);
+  TEST_VSRA_N(q, int, s, 16, 8, 12);
+  TEST_VSRA_N(q, int, s, 32, 4, 2);
+  TEST_VSRA_N(q, int, s, 64, 2, 32);
+  TEST_VSRA_N(q, uint, u, 8, 16, 2);
+  TEST_VSRA_N(q, uint, u, 16, 8, 3);
+  TEST_VSRA_N(q, uint, u, 32, 4, 5);
+  TEST_VSRA_N(q, uint, u, 64, 2, 33);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected, "");
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected, "");
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected, "");
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected, "");
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected, "");
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected, "");
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected, "");
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected, "");
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected, "");
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected, "");
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected, "");
+  CHECK(TEST_MSG, int, 64, 2, PRIx64, expected, "");
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected, "");
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected, "");
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected, "");
+  CHECK(TEST_MSG, uint, 64, 2, PRIx64, expected, "");
+}
+
+int main (void)
+{
+  exec_vsra_n ();
+  return 0;
+}
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vsri_n.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vsri_n.c
@@ -23,7 +23,6 @@ VECT_VAR_DECL(expected,uint,64,1) [] = { 0xe000000000000000 };
 VECT_VAR_DECL(expected,poly,8,8) [] = { 0xc5, 0xc5, 0xc5, 0xc5,
 					0xc5, 0xc5, 0xc5, 0xc5 };
 VECT_VAR_DECL(expected,poly,16,4) [] = { 0xffc0, 0xffc0, 0xffc0, 0xffc0 };
-VECT_VAR_DECL(expected,hfloat,32,2) [] = { 0x33333333, 0x33333333 };
 VECT_VAR_DECL(expected,int,8,16) [] = { 0xf7, 0xf7, 0xf7, 0xf7,
 					0xf7, 0xf7, 0xf7, 0xf7,
 					0xff, 0xff, 0xff, 0xff,
@@ -50,8 +49,6 @@ VECT_VAR_DECL(expected,poly,8,16) [] = { 0xe1, 0xe1, 0xe1, 0xe1,
 					 0xe1, 0xe1, 0xe1, 0xe1 };
 VECT_VAR_DECL(expected,poly,16,8) [] = { 0xfff0, 0xfff0, 0xfff0, 0xfff0,
 					 0xfff0, 0xfff0, 0xfff0, 0xfff0 };
-VECT_VAR_DECL(expected,hfloat,32,4) [] = { 0x33333333, 0x33333333,
-					   0x33333333, 0x33333333 };
 
 /* Expected results with max shift amount.  */
 VECT_VAR_DECL(expected_max_shift,int,8,8) [] = { 0xf0, 0xf1, 0xf2, 0xf3,
@@ -70,7 +67,6 @@ VECT_VAR_DECL(expected_max_shift,poly,8,8) [] = { 0xf0, 0xf1, 0xf2, 0xf3,
 						  0xf4, 0xf5, 0xf6, 0xf7 };
 VECT_VAR_DECL(expected_max_shift,poly,16,4) [] = { 0xfff0, 0xfff1,
 						   0xfff2, 0xfff3 };
-VECT_VAR_DECL(expected_max_shift,hfloat,32,2) [] = { 0x33333333, 0x33333333 };
 VECT_VAR_DECL(expected_max_shift,int,8,16) [] = { 0xf0, 0xf1, 0xf2, 0xf3,
 						  0xf4, 0xf5, 0xf6, 0xf7,
 						  0xf8, 0xf9, 0xfa, 0xfb,
@@ -97,8 +93,6 @@ VECT_VAR_DECL(expected_max_shift,poly,8,16) [] = { 0xf0, 0xf1, 0xf2, 0xf3,
 						   0xfc, 0xfd, 0xfe, 0xff };
 VECT_VAR_DECL(expected_max_shift,poly,16,8) [] = { 0xfff0, 0xfff1, 0xfff2, 0xfff3,
 						   0xfff4, 0xfff5, 0xfff6, 0xfff7 };
-VECT_VAR_DECL(expected_max_shift,hfloat,32,4) [] = { 0x33333333, 0x33333333,
-						     0x33333333, 0x33333333 };
 
 #include "vsXi_n.inc"
 
@@ -160,5 +154,23 @@ void vsri_extra(void)
   TEST_VSXI_N(INSN_NAME, q, poly, p, 8, 16, 8);
   TEST_VSXI_N(INSN_NAME, q, poly, p, 16, 8, 16);
 
-  CHECK_RESULTS_NAMED (TEST_MSG, expected_max_shift, "(max shift amount)");
+#define COMMENT "(max shift amount)"
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_max_shift, COMMENT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_max_shift, COMMENT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_max_shift, COMMENT);
+  CHECK(TEST_MSG, int, 64, 1, PRIx64, expected_max_shift, COMMENT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_max_shift, COMMENT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_max_shift, COMMENT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_max_shift, COMMENT);
+  CHECK(TEST_MSG, uint, 64, 1, PRIx64, expected_max_shift, COMMENT);
+  CHECK(TEST_MSG, poly, 8, 8, PRIx8, expected_max_shift, COMMENT);
+  CHECK(TEST_MSG, poly, 16, 4, PRIx16, expected_max_shift, COMMENT);
+  CHECK(TEST_MSG, int, 8, 16, PRIx8, expected_max_shift, COMMENT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_max_shift, COMMENT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_max_shift, COMMENT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_max_shift, COMMENT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_max_shift, COMMENT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_max_shift, COMMENT);
+  CHECK(TEST_MSG, poly, 8, 16, PRIx8, expected_max_shift, COMMENT);
+  CHECK(TEST_MSG, poly, 16, 8, PRIx16, expected_max_shift, COMMENT);
 }
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst1_lane.c
@@ -0,0 +1,93 @@
+#include <arm_neon.h>
+#include "arm-neon-ref.h"
+#include "compute-ref-data.h"
+
+/* Expected results.  */
+VECT_VAR_DECL(expected,int,8,8) [] = { 0xf7, 0x33, 0x33, 0x33,
+				       0x33, 0x33, 0x33, 0x33 };
+VECT_VAR_DECL(expected,int,16,4) [] = { 0xfff3, 0x3333, 0x3333, 0x3333 };
+VECT_VAR_DECL(expected,int,32,2) [] = { 0xfffffff1, 0x33333333 };
+VECT_VAR_DECL(expected,int,64,1) [] = { 0xfffffffffffffff0 };
+VECT_VAR_DECL(expected,uint,8,8) [] = { 0xf6, 0x33, 0x33, 0x33,
+					0x33, 0x33, 0x33, 0x33 };
+VECT_VAR_DECL(expected,uint,16,4) [] = { 0xfff2, 0x3333, 0x3333, 0x3333 };
+VECT_VAR_DECL(expected,uint,32,2) [] = { 0xfffffff0, 0x33333333 };
+VECT_VAR_DECL(expected,uint,64,1) [] = { 0xfffffffffffffff0 };
+VECT_VAR_DECL(expected,poly,8,8) [] = { 0xf6, 0x33, 0x33, 0x33,
+					0x33, 0x33, 0x33, 0x33 };
+VECT_VAR_DECL(expected,poly,16,4) [] = { 0xfff2, 0x3333, 0x3333, 0x3333 };
+VECT_VAR_DECL(expected,hfloat,32,2) [] = { 0xc1700000, 0x33333333 };
+VECT_VAR_DECL(expected,int,8,16) [] = { 0xff, 0x33, 0x33, 0x33,
+					0x33, 0x33, 0x33, 0x33,
+					0x33, 0x33, 0x33, 0x33,
+					0x33, 0x33, 0x33, 0x33 };
+VECT_VAR_DECL(expected,int,16,8) [] = { 0xfff5, 0x3333, 0x3333, 0x3333,
+					0x3333, 0x3333, 0x3333, 0x3333 };
+VECT_VAR_DECL(expected,int,32,4) [] = { 0xfffffff1, 0x33333333,
+					0x33333333, 0x33333333 };
+VECT_VAR_DECL(expected,int,64,2) [] = { 0xfffffffffffffff1, 0x3333333333333333 };
+VECT_VAR_DECL(expected,uint,8,16) [] = { 0xfa, 0x33, 0x33, 0x33,
+					 0x33, 0x33, 0x33, 0x33,
+					 0x33, 0x33, 0x33, 0x33,
+					 0x33, 0x33, 0x33, 0x33 };
+VECT_VAR_DECL(expected,uint,16,8) [] = { 0xfff4, 0x3333, 0x3333, 0x3333,
+					 0x3333, 0x3333, 0x3333, 0x3333 };
+VECT_VAR_DECL(expected,uint,32,4) [] = { 0xfffffff3, 0x33333333,
+					 0x33333333, 0x33333333 };
+VECT_VAR_DECL(expected,uint,64,2) [] = { 0xfffffffffffffff0,
+					 0x3333333333333333 };
+VECT_VAR_DECL(expected,poly,8,16) [] = { 0xfa, 0x33, 0x33, 0x33,
+					 0x33, 0x33, 0x33, 0x33,
+					 0x33, 0x33, 0x33, 0x33,
+					 0x33, 0x33, 0x33, 0x33 };
+VECT_VAR_DECL(expected,poly,16,8) [] = { 0xfff4, 0x3333, 0x3333, 0x3333,
+					 0x3333, 0x3333, 0x3333, 0x3333 };
+VECT_VAR_DECL(expected,hfloat,32,4) [] = { 0xc1700000, 0x33333333,
+					   0x33333333, 0x33333333 };
+
+#define TEST_MSG "VST1_LANE/VST1_LANEQ"
+void exec_vst1_lane (void)
+{
+#define TEST_VST1_LANE(Q, T1, T2, W, N, L)		\
+  VECT_VAR(vector, T1, W, N) =				\
+    vld1##Q##_##T2##W(VECT_VAR(buffer, T1, W, N));	\
+  vst1##Q##_lane_##T2##W(VECT_VAR(result, T1, W, N),	\
+			 VECT_VAR(vector, T1, W, N), L)
+
+  DECL_VARIABLE_ALL_VARIANTS(vector);
+
+  clean_results ();
+
+  /* Choose lane arbitrarily.  */
+  TEST_VST1_LANE(, int, s, 8, 8, 7);
+  TEST_VST1_LANE(, int, s, 16, 4, 3);
+  TEST_VST1_LANE(, int, s, 32, 2, 1);
+  TEST_VST1_LANE(, int, s, 64, 1, 0);
+  TEST_VST1_LANE(, uint, u, 8, 8, 6);
+  TEST_VST1_LANE(, uint, u, 16, 4, 2);
+  TEST_VST1_LANE(, uint, u, 32, 2, 0);
+  TEST_VST1_LANE(, uint, u, 64, 1, 0);
+  TEST_VST1_LANE(, poly, p, 8, 8, 6);
+  TEST_VST1_LANE(, poly, p, 16, 4, 2);
+  TEST_VST1_LANE(, float, f, 32, 2, 1);
+
+  TEST_VST1_LANE(q, int, s, 8, 16, 15);
+  TEST_VST1_LANE(q, int, s, 16, 8, 5);
+  TEST_VST1_LANE(q, int, s, 32, 4, 1);
+  TEST_VST1_LANE(q, int, s, 64, 2, 1);
+  TEST_VST1_LANE(q, uint, u, 8, 16, 10);
+  TEST_VST1_LANE(q, uint, u, 16, 8, 4);
+  TEST_VST1_LANE(q, uint, u, 32, 4, 3);
+  TEST_VST1_LANE(q, uint, u, 64, 2, 0);
+  TEST_VST1_LANE(q, poly, p, 8, 16, 10);
+  TEST_VST1_LANE(q, poly, p, 16, 8, 4);
+  TEST_VST1_LANE(q, float, f, 32, 4, 1);
+
+  CHECK_RESULTS(TEST_MSG, "");
+}
+
+int main (void)
+{
+  exec_vst1_lane ();
+  return 0;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst2_lane_f32_indices_1.c
@@ -0,0 +1,15 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+void
+f_vst2_lane_f32 (float32_t * p, float32x2x2_t v)
+{
+  /* { dg-error "lane 2 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  vst2_lane_f32 (p, v, 2);
+  /* { dg-error "lane -1 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  vst2_lane_f32 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst2_lane_f64_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+/* { dg-skip-if "" { arm*-*-* } } */
+
+void
+f_vst2_lane_f64 (float64_t * p, float64x1x2_t v)
+{
+  /* { dg-error "lane 1 out of range 0 - 0" "" { xfail arm*-*-* } 0 } */
+  vst2_lane_f64 (p, v, 1);
+  /* { dg-error "lane -1 out of range 0 - 0" "" { xfail arm*-*-* } 0 } */
+  vst2_lane_f64 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst2_lane_p8_indices_1.c
@@ -0,0 +1,15 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+void
+f_vst2_lane_p8 (poly8_t * p, poly8x8x2_t v)
+{
+  /* { dg-error "lane 8 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  vst2_lane_p8 (p, v, 8);
+  /* { dg-error "lane -1 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  vst2_lane_p8 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst2_lane_s16_indices_1.c
@@ -0,0 +1,15 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+void
+f_vst2_lane_s16 (int16_t * p, int16x4x2_t v)
+{
+  /* { dg-error "lane 4 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  vst2_lane_s16 (p, v, 4);
+  /* { dg-error "lane -1 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  vst2_lane_s16 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst2_lane_s32_indices_1.c
@@ -0,0 +1,15 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+void
+f_vst2_lane_s32 (int32_t * p, int32x2x2_t v)
+{
+  /* { dg-error "lane 2 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  vst2_lane_s32 (p, v, 2);
+  /* { dg-error "lane -1 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  vst2_lane_s32 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst2_lane_s64_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+/* { dg-skip-if "" { arm*-*-* } } */
+
+void
+f_vst2_lane_s64 (int64_t * p, int64x1x2_t v)
+{
+  /* { dg-error "lane 1 out of range 0 - 0" "" { xfail arm*-*-* } 0 } */
+  vst2_lane_s64 (p, v, 1);
+  /* { dg-error "lane -1 out of range 0 - 0" "" { xfail arm*-*-* } 0 } */
+  vst2_lane_s64 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst2_lane_s8_indices_1.c
@@ -0,0 +1,15 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+void
+f_vst2_lane_s8 (int8_t * p, int8x8x2_t v)
+{
+  /* { dg-error "lane 8 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  vst2_lane_s8 (p, v, 8);
+  /* { dg-error "lane -1 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  vst2_lane_s8 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst2_lane_u16_indices_1.c
@@ -0,0 +1,15 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+void
+f_vst2_lane_u16 (uint16_t * p, uint16x4x2_t v)
+{
+  /* { dg-error "lane 4 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  vst2_lane_u16 (p, v, 4);
+  /* { dg-error "lane -1 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  vst2_lane_u16 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst2_lane_u32_indices_1.c
@@ -0,0 +1,15 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+void
+f_vst2_lane_u32 (uint32_t * p, uint32x2x2_t v)
+{
+  /* { dg-error "lane 2 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  vst2_lane_u32 (p, v, 2);
+  /* { dg-error "lane -1 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  vst2_lane_u32 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst2_lane_u64_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+/* { dg-skip-if "" { arm*-*-* } } */
+
+void
+f_vst2_lane_u64 (uint64_t * p, uint64x1x2_t v)
+{
+  /* { dg-error "lane 1 out of range 0 - 0" "" { xfail arm*-*-* } 0 } */
+  vst2_lane_u64 (p, v, 1);
+  /* { dg-error "lane -1 out of range 0 - 0" "" { xfail arm*-*-* } 0 } */
+  vst2_lane_u64 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst2_lane_u8_indices_1.c
@@ -0,0 +1,15 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+void
+f_vst2_lane_u8 (uint8_t * p, uint8x8x2_t v)
+{
+  /* { dg-error "lane 8 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  vst2_lane_u8 (p, v, 8);
+  /* { dg-error "lane -1 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  vst2_lane_u8 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst2q_lane_f32_indices_1.c
@@ -0,0 +1,15 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+void
+f_vst2q_lane_f32 (float32_t * p, float32x4x2_t v)
+{
+  /* { dg-error "lane 4 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  vst2q_lane_f32 (p, v, 4);
+  /* { dg-error "lane -1 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  vst2q_lane_f32 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst2q_lane_f64_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+/* { dg-skip-if "" { arm*-*-* } } */
+
+void
+f_vst2q_lane_f64 (float64_t * p, float64x2x2_t v)
+{
+  /* { dg-error "lane 2 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  vst2q_lane_f64 (p, v, 2);
+  /* { dg-error "lane -1 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  vst2q_lane_f64 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst2q_lane_p8_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+/* { dg-skip-if "" { arm*-*-* } } */
+
+void
+f_vst2q_lane_p8 (poly8_t * p, poly8x16x2_t v)
+{
+  /* { dg-error "lane 16 out of range 0 - 15" "" { xfail arm*-*-* } 0 } */
+  vst2q_lane_p8 (p, v, 16);
+  /* { dg-error "lane -1 out of range 0 - 15" "" { xfail arm*-*-* } 0 } */
+  vst2q_lane_p8 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst2q_lane_s16_indices_1.c
@@ -0,0 +1,15 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+void
+f_vst2q_lane_s16 (int16_t * p, int16x8x2_t v)
+{
+  /* { dg-error "lane 8 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  vst2q_lane_s16 (p, v, 8);
+  /* { dg-error "lane -1 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  vst2q_lane_s16 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst2q_lane_s32_indices_1.c
@@ -0,0 +1,15 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+void
+f_vst2q_lane_s32 (int32_t * p, int32x4x2_t v)
+{
+  /* { dg-error "lane 4 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  vst2q_lane_s32 (p, v, 4);
+  /* { dg-error "lane -1 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  vst2q_lane_s32 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst2q_lane_s64_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+/* { dg-skip-if "" { arm*-*-* } } */
+
+void
+f_vst2q_lane_s64 (int64_t * p, int64x2x2_t v)
+{
+  /* { dg-error "lane 2 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  vst2q_lane_s64 (p, v, 2);
+  /* { dg-error "lane -1 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  vst2q_lane_s64 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst2q_lane_s8_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+/* { dg-skip-if "" { arm*-*-* } } */
+
+void
+f_vst2q_lane_s8 (int8_t * p, int8x16x2_t v)
+{
+  /* { dg-error "lane 16 out of range 0 - 15" "" { xfail arm*-*-* } 0 } */
+  vst2q_lane_s8 (p, v, 16);
+  /* { dg-error "lane -1 out of range 0 - 15" "" { xfail arm*-*-* } 0 } */
+  vst2q_lane_s8 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst2q_lane_u16_indices_1.c
@@ -0,0 +1,15 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+void
+f_vst2q_lane_u16 (uint16_t * p, uint16x8x2_t v)
+{
+  /* { dg-error "lane 8 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  vst2q_lane_u16 (p, v, 8);
+  /* { dg-error "lane -1 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  vst2q_lane_u16 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst2q_lane_u32_indices_1.c
@@ -0,0 +1,15 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+void
+f_vst2q_lane_u32 (uint32_t * p, uint32x4x2_t v)
+{
+  /* { dg-error "lane 4 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  vst2q_lane_u32 (p, v, 4);
+  /* { dg-error "lane -1 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  vst2q_lane_u32 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst2q_lane_u64_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+/* { dg-skip-if "" { arm*-*-* } } */
+
+void
+f_vst2q_lane_u64 (uint64_t * p, uint64x2x2_t v)
+{
+  /* { dg-error "lane 2 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  vst2q_lane_u64 (p, v, 2);
+  /* { dg-error "lane -1 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  vst2q_lane_u64 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst2q_lane_u8_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+/* { dg-skip-if "" { arm*-*-* } } */
+
+void
+f_vst2q_lane_u8 (uint8_t * p, uint8x16x2_t v)
+{
+  /* { dg-error "lane 16 out of range 0 - 15" "" { xfail arm*-*-* } 0 } */
+  vst2q_lane_u8 (p, v, 16);
+  /* { dg-error "lane -1 out of range 0 - 15" "" { xfail arm*-*-* } 0 } */
+  vst2q_lane_u8 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst3_lane_f32_indices_1.c
@@ -0,0 +1,15 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+void
+f_vst3_lane_f32 (float32_t * p, float32x2x3_t v)
+{
+  /* { dg-error "lane 2 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  vst3_lane_f32 (p, v, 2);
+  /* { dg-error "lane -1 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  vst3_lane_f32 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst3_lane_f64_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+/* { dg-skip-if "" { arm*-*-* } } */
+
+void
+f_vst3_lane_f64 (float64_t * p, float64x1x3_t v)
+{
+  /* { dg-error "lane 1 out of range 0 - 0" "" { xfail arm*-*-* } 0 } */
+  vst3_lane_f64 (p, v, 1);
+  /* { dg-error "lane -1 out of range 0 - 0" "" { xfail arm*-*-* } 0 } */
+  vst3_lane_f64 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst3_lane_p8_indices_1.c
@@ -0,0 +1,15 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+void
+f_vst3_lane_p8 (poly8_t * p, poly8x8x3_t v)
+{
+  /* { dg-error "lane 8 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  vst3_lane_p8 (p, v, 8);
+  /* { dg-error "lane -1 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  vst3_lane_p8 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst3_lane_s16_indices_1.c
@@ -0,0 +1,15 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+void
+f_vst3_lane_s16 (int16_t * p, int16x4x3_t v)
+{
+  /* { dg-error "lane 4 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  vst3_lane_s16 (p, v, 4);
+  /* { dg-error "lane -1 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  vst3_lane_s16 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst3_lane_s32_indices_1.c
@@ -0,0 +1,15 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+void
+f_vst3_lane_s32 (int32_t * p, int32x2x3_t v)
+{
+  /* { dg-error "lane 2 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  vst3_lane_s32 (p, v, 2);
+  /* { dg-error "lane -1 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  vst3_lane_s32 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst3_lane_s64_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+/* { dg-skip-if "" { arm*-*-* } } */
+
+void
+f_vst3_lane_s64 (int64_t * p, int64x1x3_t v)
+{
+  /* { dg-error "lane 1 out of range 0 - 0" "" { xfail arm*-*-* } 0 } */
+  vst3_lane_s64 (p, v, 1);
+  /* { dg-error "lane -1 out of range 0 - 0" "" { xfail arm*-*-* } 0 } */
+  vst3_lane_s64 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst3_lane_s8_indices_1.c
@@ -0,0 +1,15 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+void
+f_vst3_lane_s8 (int8_t * p, int8x8x3_t v)
+{
+  /* { dg-error "lane 8 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  vst3_lane_s8 (p, v, 8);
+  /* { dg-error "lane -1 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  vst3_lane_s8 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst3_lane_u16_indices_1.c
@@ -0,0 +1,15 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+void
+f_vst3_lane_u16 (uint16_t * p, uint16x4x3_t v)
+{
+  /* { dg-error "lane 4 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  vst3_lane_u16 (p, v, 4);
+  /* { dg-error "lane -1 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  vst3_lane_u16 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst3_lane_u32_indices_1.c
@@ -0,0 +1,15 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+void
+f_vst3_lane_u32 (uint32_t * p, uint32x2x3_t v)
+{
+  /* { dg-error "lane 2 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  vst3_lane_u32 (p, v, 2);
+  /* { dg-error "lane -1 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  vst3_lane_u32 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst3_lane_u64_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+/* { dg-skip-if "" { arm*-*-* } } */
+
+void
+f_vst3_lane_u64 (uint64_t * p, uint64x1x3_t v)
+{
+  /* { dg-error "lane 1 out of range 0 - 0" "" { xfail arm*-*-* } 0 } */
+  vst3_lane_u64 (p, v, 1);
+  /* { dg-error "lane -1 out of range 0 - 0" "" { xfail arm*-*-* } 0 } */
+  vst3_lane_u64 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst3_lane_u8_indices_1.c
@@ -0,0 +1,15 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+void
+f_vst3_lane_u8 (uint8_t * p, uint8x8x3_t v)
+{
+  /* { dg-error "lane 8 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  vst3_lane_u8 (p, v, 8);
+  /* { dg-error "lane -1 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  vst3_lane_u8 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst3q_lane_f32_indices_1.c
@@ -0,0 +1,15 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+void
+f_vst3q_lane_f32 (float32_t * p, float32x4x3_t v)
+{
+  /* { dg-error "lane 4 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  vst3q_lane_f32 (p, v, 4);
+  /* { dg-error "lane -1 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  vst3q_lane_f32 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst3q_lane_f64_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+/* { dg-skip-if "" { arm*-*-* } } */
+
+void
+f_vst3q_lane_f64 (float64_t * p, float64x2x3_t v)
+{
+  /* { dg-error "lane 2 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  vst3q_lane_f64 (p, v, 2);
+  /* { dg-error "lane -1 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  vst3q_lane_f64 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst3q_lane_p8_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+/* { dg-skip-if "" { arm*-*-* } } */
+
+void
+f_vst3q_lane_p8 (poly8_t * p, poly8x16x3_t v)
+{
+  /* { dg-error "lane 16 out of range 0 - 15" "" { xfail arm*-*-* } 0 } */
+  vst3q_lane_p8 (p, v, 16);
+  /* { dg-error "lane -1 out of range 0 - 15" "" { xfail arm*-*-* } 0 } */
+  vst3q_lane_p8 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst3q_lane_s16_indices_1.c
@@ -0,0 +1,15 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+void
+f_vst3q_lane_s16 (int16_t * p, int16x8x3_t v)
+{
+  /* { dg-error "lane 8 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  vst3q_lane_s16 (p, v, 8);
+  /* { dg-error "lane -1 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  vst3q_lane_s16 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst3q_lane_s32_indices_1.c
@@ -0,0 +1,15 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+void
+f_vst3q_lane_s32 (int32_t * p, int32x4x3_t v)
+{
+  /* { dg-error "lane 4 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  vst3q_lane_s32 (p, v, 4);
+  /* { dg-error "lane -1 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  vst3q_lane_s32 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst3q_lane_s64_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+/* { dg-skip-if "" { arm*-*-* } } */
+
+void
+f_vst3q_lane_s64 (int64_t * p, int64x2x3_t v)
+{
+  /* { dg-error "lane 2 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  vst3q_lane_s64 (p, v, 2);
+  /* { dg-error "lane -1 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  vst3q_lane_s64 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst3q_lane_s8_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+/* { dg-skip-if "" { arm*-*-* } } */
+
+void
+f_vst3q_lane_s8 (int8_t * p, int8x16x3_t v)
+{
+  /* { dg-error "lane 16 out of range 0 - 15" "" { xfail arm*-*-* } 0 } */
+  vst3q_lane_s8 (p, v, 16);
+  /* { dg-error "lane -1 out of range 0 - 15" "" { xfail arm*-*-* } 0 } */
+  vst3q_lane_s8 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst3q_lane_u16_indices_1.c
@@ -0,0 +1,15 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+void
+f_vst3q_lane_u16 (uint16_t * p, uint16x8x3_t v)
+{
+  /* { dg-error "lane 8 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  vst3q_lane_u16 (p, v, 8);
+  /* { dg-error "lane -1 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  vst3q_lane_u16 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst3q_lane_u32_indices_1.c
@@ -0,0 +1,15 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+void
+f_vst3q_lane_u32 (uint32_t * p, uint32x4x3_t v)
+{
+  /* { dg-error "lane 4 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  vst3q_lane_u32 (p, v, 4);
+  /* { dg-error "lane -1 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  vst3q_lane_u32 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst3q_lane_u64_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+/* { dg-skip-if "" { arm*-*-* } } */
+
+void
+f_vst3q_lane_u64 (uint64_t * p, uint64x2x3_t v)
+{
+  /* { dg-error "lane 2 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  vst3q_lane_u64 (p, v, 2);
+  /* { dg-error "lane -1 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  vst3q_lane_u64 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst3q_lane_u8_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+/* { dg-skip-if "" { arm*-*-* } } */
+
+void
+f_vst3q_lane_u8 (uint8_t * p, uint8x16x3_t v)
+{
+  /* { dg-error "lane 16 out of range 0 - 15" "" { xfail arm*-*-* } 0 } */
+  vst3q_lane_u8 (p, v, 16);
+  /* { dg-error "lane -1 out of range 0 - 15" "" { xfail arm*-*-* } 0 } */
+  vst3q_lane_u8 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst4_lane_f32_indices_1.c
@@ -0,0 +1,15 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+void
+f_vst4_lane_f32 (float32_t * p, float32x2x4_t v)
+{
+  /* { dg-error "lane 2 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  vst4_lane_f32 (p, v, 2);
+  /* { dg-error "lane -1 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  vst4_lane_f32 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst4_lane_f64_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+/* { dg-skip-if "" { arm*-*-* } } */
+
+void
+f_vst4_lane_f64 (float64_t * p, float64x1x4_t v)
+{
+  /* { dg-error "lane 1 out of range 0 - 0" "" { xfail arm*-*-* } 0 } */
+  vst4_lane_f64 (p, v, 1);
+  /* { dg-error "lane -1 out of range 0 - 0" "" { xfail arm*-*-* } 0 } */
+  vst4_lane_f64 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst4_lane_p8_indices_1.c
@@ -0,0 +1,15 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+void
+f_vst4_lane_p8 (poly8_t * p, poly8x8x4_t v)
+{
+  /* { dg-error "lane 8 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  vst4_lane_p8 (p, v, 8);
+  /* { dg-error "lane -1 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  vst4_lane_p8 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst4_lane_s16_indices_1.c
@@ -0,0 +1,15 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+void
+f_vst4_lane_s16 (int16_t * p, int16x4x4_t v)
+{
+  /* { dg-error "lane 4 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  vst4_lane_s16 (p, v, 4);
+  /* { dg-error "lane -1 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  vst4_lane_s16 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst4_lane_s32_indices_1.c
@@ -0,0 +1,15 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+void
+f_vst4_lane_s32 (int32_t * p, int32x2x4_t v)
+{
+  /* { dg-error "lane 2 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  vst4_lane_s32 (p, v, 2);
+  /* { dg-error "lane -1 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  vst4_lane_s32 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst4_lane_s64_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+/* { dg-skip-if "" { arm*-*-* } } */
+
+void
+f_vst4_lane_s64 (int64_t * p, int64x1x4_t v)
+{
+  /* { dg-error "lane 1 out of range 0 - 0" "" { xfail arm*-*-* } 0 } */
+  vst4_lane_s64 (p, v, 1);
+  /* { dg-error "lane -1 out of range 0 - 0" "" { xfail arm*-*-* } 0 } */
+  vst4_lane_s64 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst4_lane_s8_indices_1.c
@@ -0,0 +1,15 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+void
+f_vst4_lane_s8 (int8_t * p, int8x8x4_t v)
+{
+  /* { dg-error "lane 8 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  vst4_lane_s8 (p, v, 8);
+  /* { dg-error "lane -1 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  vst4_lane_s8 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst4_lane_u16_indices_1.c
@@ -0,0 +1,15 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+void
+f_vst4_lane_u16 (uint16_t * p, uint16x4x4_t v)
+{
+  /* { dg-error "lane 4 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  vst4_lane_u16 (p, v, 4);
+  /* { dg-error "lane -1 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  vst4_lane_u16 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst4_lane_u32_indices_1.c
@@ -0,0 +1,15 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+void
+f_vst4_lane_u32 (uint32_t * p, uint32x2x4_t v)
+{
+  /* { dg-error "lane 2 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  vst4_lane_u32 (p, v, 2);
+  /* { dg-error "lane -1 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  vst4_lane_u32 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst4_lane_u64_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+/* { dg-skip-if "" { arm*-*-* } } */
+
+void
+f_vst4_lane_u64 (uint64_t * p, uint64x1x4_t v)
+{
+  /* { dg-error "lane 1 out of range 0 - 0" "" { xfail arm*-*-* } 0 } */
+  vst4_lane_u64 (p, v, 1);
+  /* { dg-error "lane -1 out of range 0 - 0" "" { xfail arm*-*-* } 0 } */
+  vst4_lane_u64 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst4_lane_u8_indices_1.c
@@ -0,0 +1,15 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+void
+f_vst4_lane_u8 (uint8_t * p, uint8x8x4_t v)
+{
+  /* { dg-error "lane 8 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  vst4_lane_u8 (p, v, 8);
+  /* { dg-error "lane -1 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  vst4_lane_u8 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst4q_lane_f32_indices_1.c
@@ -0,0 +1,15 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+void
+f_vst4q_lane_f32 (float32_t * p, float32x4x4_t v)
+{
+  /* { dg-error "lane 4 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  vst4q_lane_f32 (p, v, 4);
+  /* { dg-error "lane -1 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  vst4q_lane_f32 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst4q_lane_f64_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+/* { dg-skip-if "" { arm*-*-* } } */
+
+void
+f_vst4q_lane_f64 (float64_t * p, float64x2x4_t v)
+{
+  /* { dg-error "lane 2 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  vst4q_lane_f64 (p, v, 2);
+  /* { dg-error "lane -1 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  vst4q_lane_f64 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst4q_lane_p8_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+/* { dg-skip-if "" { arm*-*-* } } */
+
+void
+f_vst4q_lane_p8 (poly8_t * p, poly8x16x4_t v)
+{
+  /* { dg-error "lane 16 out of range 0 - 15" "" { xfail arm*-*-* } 0 } */
+  vst4q_lane_p8 (p, v, 16);
+  /* { dg-error "lane -1 out of range 0 - 15" "" { xfail arm*-*-* } 0 } */
+  vst4q_lane_p8 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst4q_lane_s16_indices_1.c
@@ -0,0 +1,15 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+void
+f_vst4q_lane_s16 (int16_t * p, int16x8x4_t v)
+{
+  /* { dg-error "lane 8 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  vst4q_lane_s16 (p, v, 8);
+  /* { dg-error "lane -1 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  vst4q_lane_s16 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst4q_lane_s32_indices_1.c
@@ -0,0 +1,15 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+void
+f_vst4q_lane_s32 (int32_t * p, int32x4x4_t v)
+{
+  /* { dg-error "lane 4 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  vst4q_lane_s32 (p, v, 4);
+  /* { dg-error "lane -1 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  vst4q_lane_s32 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst4q_lane_s64_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+/* { dg-skip-if "" { arm*-*-* } } */
+
+void
+f_vst4q_lane_s64 (int64_t * p, int64x2x4_t v)
+{
+  /* { dg-error "lane 2 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  vst4q_lane_s64 (p, v, 2);
+  /* { dg-error "lane -1 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  vst4q_lane_s64 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst4q_lane_s8_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+/* { dg-skip-if "" { arm*-*-* } } */
+
+void
+f_vst4q_lane_s8 (int8_t * p, int8x16x4_t v)
+{
+  /* { dg-error "lane 16 out of range 0 - 15" "" { xfail arm*-*-* } 0 } */
+  vst4q_lane_s8 (p, v, 16);
+  /* { dg-error "lane -1 out of range 0 - 15" "" { xfail arm*-*-* } 0 } */
+  vst4q_lane_s8 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst4q_lane_u16_indices_1.c
@@ -0,0 +1,15 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+void
+f_vst4q_lane_u16 (uint16_t * p, uint16x8x4_t v)
+{
+  /* { dg-error "lane 8 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  vst4q_lane_u16 (p, v, 8);
+  /* { dg-error "lane -1 out of range 0 - 7" "" { xfail arm*-*-* } 0 } */
+  vst4q_lane_u16 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst4q_lane_u32_indices_1.c
@@ -0,0 +1,15 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+
+void
+f_vst4q_lane_u32 (uint32_t * p, uint32x4x4_t v)
+{
+  /* { dg-error "lane 4 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  vst4q_lane_u32 (p, v, 4);
+  /* { dg-error "lane -1 out of range 0 - 3" "" { xfail arm*-*-* } 0 } */
+  vst4q_lane_u32 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst4q_lane_u64_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+/* { dg-skip-if "" { arm*-*-* } } */
+
+void
+f_vst4q_lane_u64 (uint64_t * p, uint64x2x4_t v)
+{
+  /* { dg-error "lane 2 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  vst4q_lane_u64 (p, v, 2);
+  /* { dg-error "lane -1 out of range 0 - 1" "" { xfail arm*-*-* } 0 } */
+  vst4q_lane_u64 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst4q_lane_u8_indices_1.c
@@ -0,0 +1,16 @@
+#include <arm_neon.h>
+
+/* { dg-do compile } */
+/* { dg-skip-if "" { *-*-* } { "-fno-fat-lto-objects" } } */
+/* { dg-excess-errors "" { xfail arm*-*-* } } */
+/* { dg-skip-if "" { arm*-*-* } } */
+
+void
+f_vst4q_lane_u8 (uint8_t * p, uint8x16x4_t v)
+{
+  /* { dg-error "lane 16 out of range 0 - 15" "" { xfail arm*-*-* } 0 } */
+  vst4q_lane_u8 (p, v, 16);
+  /* { dg-error "lane -1 out of range 0 - 15" "" { xfail arm*-*-* } 0 } */
+  vst4q_lane_u8 (p, v, -1);
+  return;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vstX_lane.c
@@ -0,0 +1,578 @@
+#include <arm_neon.h>
+#include "arm-neon-ref.h"
+#include "compute-ref-data.h"
+
+/* Expected results for vst2, chunk 0.  */
+VECT_VAR_DECL(expected_st2_0,int,8,8) [] = { 0xf0, 0xf1, 0x0, 0x0,
+					     0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st2_0,int,16,4) [] = { 0xfff0, 0xfff1, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st2_0,int,32,2) [] = { 0xfffffff0, 0xfffffff1 };
+VECT_VAR_DECL(expected_st2_0,uint,8,8) [] = { 0xf0, 0xf1, 0x0, 0x0,
+					      0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st2_0,uint,16,4) [] = { 0xfff0, 0xfff1, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st2_0,uint,32,2) [] = { 0xfffffff0, 0xfffffff1 };
+VECT_VAR_DECL(expected_st2_0,poly,8,8) [] = { 0xf0, 0xf1, 0x0, 0x0,
+					      0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st2_0,poly,16,4) [] = { 0xfff0, 0xfff1, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st2_0,hfloat,32,2) [] = { 0xc1800000, 0xc1700000 };
+VECT_VAR_DECL(expected_st2_0,int,16,8) [] = { 0xfff0, 0xfff1, 0x0, 0x0,
+					      0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st2_0,int,32,4) [] = { 0xfffffff0, 0xfffffff1, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st2_0,uint,16,8) [] = { 0xfff0, 0xfff1, 0x0, 0x0,
+					       0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st2_0,uint,32,4) [] = { 0xfffffff0, 0xfffffff1,
+					       0x0, 0x0 };
+VECT_VAR_DECL(expected_st2_0,poly,16,8) [] = { 0xfff0, 0xfff1, 0x0, 0x0,
+					       0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st2_0,hfloat,32,4) [] = { 0xc1800000, 0xc1700000,
+						 0x0, 0x0 };
+
+/* Expected results for vst2, chunk 1.  */
+VECT_VAR_DECL(expected_st2_1,int,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					     0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st2_1,int,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st2_1,int,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_st2_1,uint,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					      0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st2_1,uint,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st2_1,uint,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_st2_1,poly,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					      0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st2_1,poly,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st2_1,hfloat,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_st2_1,int,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					      0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st2_1,int,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st2_1,uint,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					       0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st2_1,uint,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st2_1,poly,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					       0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st2_1,hfloat,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+
+/* Expected results for vst3, chunk 0.  */
+VECT_VAR_DECL(expected_st3_0,int,8,8) [] = { 0xf0, 0xf1, 0xf2, 0x0,
+					     0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st3_0,int,16,4) [] = { 0xfff0, 0xfff1, 0xfff2, 0x0 };
+VECT_VAR_DECL(expected_st3_0,int,32,2) [] = { 0xfffffff0, 0xfffffff1 };
+VECT_VAR_DECL(expected_st3_0,uint,8,8) [] = { 0xf0, 0xf1, 0xf2, 0x0,
+					      0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st3_0,uint,16,4) [] = { 0xfff0, 0xfff1, 0xfff2, 0x0 };
+VECT_VAR_DECL(expected_st3_0,uint,32,2) [] = { 0xfffffff0, 0xfffffff1 };
+VECT_VAR_DECL(expected_st3_0,poly,8,8) [] = { 0xf0, 0xf1, 0xf2, 0x0,
+					      0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st3_0,poly,16,4) [] = { 0xfff0, 0xfff1, 0xfff2, 0x0 };
+VECT_VAR_DECL(expected_st3_0,hfloat,32,2) [] = { 0xc1800000, 0xc1700000 };
+VECT_VAR_DECL(expected_st3_0,int,16,8) [] = { 0xfff0, 0xfff1, 0xfff2, 0x0,
+					      0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st3_0,int,32,4) [] = { 0xfffffff0, 0xfffffff1,
+					      0xfffffff2, 0x0 };
+VECT_VAR_DECL(expected_st3_0,uint,16,8) [] = { 0xfff0, 0xfff1, 0xfff2, 0x0,
+					       0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st3_0,uint,32,4) [] = { 0xfffffff0, 0xfffffff1,
+					       0xfffffff2, 0x0 };
+VECT_VAR_DECL(expected_st3_0,poly,16,8) [] = { 0xfff0, 0xfff1, 0xfff2, 0x0,
+					       0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st3_0,hfloat,32,4) [] = { 0xc1800000, 0xc1700000,
+						 0xc1600000, 0x0 };
+
+/* Expected results for vst3, chunk 1.  */
+VECT_VAR_DECL(expected_st3_1,int,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					     0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st3_1,int,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st3_1,int,32,2) [] = { 0xfffffff2, 0x0 };
+VECT_VAR_DECL(expected_st3_1,uint,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					      0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st3_1,uint,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st3_1,uint,32,2) [] = { 0xfffffff2, 0x0 };
+VECT_VAR_DECL(expected_st3_1,poly,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					      0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st3_1,poly,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st3_1,hfloat,32,2) [] = { 0xc1600000, 0x0 };
+VECT_VAR_DECL(expected_st3_1,int,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					      0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st3_1,int,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st3_1,uint,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					       0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st3_1,uint,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st3_1,poly,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					       0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st3_1,hfloat,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+
+/* Expected results for vst3, chunk 2.  */
+VECT_VAR_DECL(expected_st3_2,int,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					     0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st3_2,int,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st3_2,int,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_st3_2,uint,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					      0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st3_2,uint,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st3_2,uint,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_st3_2,poly,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					      0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st3_2,poly,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st3_2,hfloat,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_st3_2,int,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					      0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st3_2,int,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st3_2,uint,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					       0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st3_2,uint,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st3_2,poly,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					       0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st3_2,hfloat,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+
+/* Expected results for vst4, chunk 0.  */
+VECT_VAR_DECL(expected_st4_0,int,8,8) [] = { 0xf0, 0xf1, 0xf2, 0xf3,
+					     0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st4_0,int,16,4) [] = { 0xfff0, 0xfff1, 0xfff2, 0xfff3 };
+VECT_VAR_DECL(expected_st4_0,int,32,2) [] = { 0xfffffff0, 0xfffffff1 };
+VECT_VAR_DECL(expected_st4_0,uint,8,8) [] = { 0xf0, 0xf1, 0xf2, 0xf3,
+					      0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st4_0,uint,16,4) [] = { 0xfff0, 0xfff1, 0xfff2, 0xfff3 };
+VECT_VAR_DECL(expected_st4_0,uint,32,2) [] = { 0xfffffff0, 0xfffffff1 };
+VECT_VAR_DECL(expected_st4_0,poly,8,8) [] = { 0xf0, 0xf1, 0xf2, 0xf3,
+					      0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st4_0,poly,16,4) [] = { 0xfff0, 0xfff1, 0xfff2, 0xfff3 };
+VECT_VAR_DECL(expected_st4_0,hfloat,32,2) [] = { 0xc1800000, 0xc1700000 };
+VECT_VAR_DECL(expected_st4_0,int,16,8) [] = { 0xfff0, 0xfff1, 0xfff2, 0xfff3,
+					      0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st4_0,int,32,4) [] = { 0xfffffff0, 0xfffffff1,
+					      0xfffffff2, 0xfffffff3 };
+VECT_VAR_DECL(expected_st4_0,uint,16,8) [] = { 0xfff0, 0xfff1, 0xfff2, 0xfff3,
+					       0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st4_0,uint,32,4) [] = { 0xfffffff0, 0xfffffff1,
+					       0xfffffff2, 0xfffffff3 };
+VECT_VAR_DECL(expected_st4_0,poly,16,8) [] = { 0xfff0, 0xfff1, 0xfff2, 0xfff3,
+					       0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st4_0,hfloat,32,4) [] = { 0xc1800000, 0xc1700000,
+						 0xc1600000, 0xc1500000 };
+
+/* Expected results for vst4, chunk 1.  */
+VECT_VAR_DECL(expected_st4_1,int,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					     0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st4_1,int,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st4_1,int,32,2) [] = { 0xfffffff2, 0xfffffff3 };
+VECT_VAR_DECL(expected_st4_1,uint,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					      0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st4_1,uint,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st4_1,uint,32,2) [] = { 0xfffffff2, 0xfffffff3 };
+VECT_VAR_DECL(expected_st4_1,poly,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					      0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st4_1,poly,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st4_1,hfloat,32,2) [] = { 0xc1600000, 0xc1500000 };
+VECT_VAR_DECL(expected_st4_1,int,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					      0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st4_1,int,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st4_1,uint,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					       0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st4_1,uint,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st4_1,poly,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					       0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st4_1,hfloat,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+
+/* Expected results for vst4, chunk 2.  */
+VECT_VAR_DECL(expected_st4_2,int,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					     0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st4_2,int,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st4_2,int,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_st4_2,uint,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					      0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st4_2,uint,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st4_2,uint,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_st4_2,poly,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					      0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st4_2,poly,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st4_2,hfloat,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_st4_2,int,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					      0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st4_2,int,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st4_2,uint,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					       0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st4_2,uint,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st4_2,poly,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					       0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st4_2,hfloat,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+
+/* Expected results for vst4, chunk 3.  */
+VECT_VAR_DECL(expected_st4_3,int,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					     0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st4_3,int,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st4_3,int,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_st4_3,uint,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					      0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st4_3,uint,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st4_3,uint,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_st4_3,poly,8,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					      0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st4_3,poly,16,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st4_3,hfloat,32,2) [] = { 0x0, 0x0 };
+VECT_VAR_DECL(expected_st4_3,int,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					      0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st4_3,int,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st4_3,uint,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					       0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st4_3,uint,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st4_3,poly,16,8) [] = { 0x0, 0x0, 0x0, 0x0,
+					       0x0, 0x0, 0x0, 0x0 };
+VECT_VAR_DECL(expected_st4_3,hfloat,32,4) [] = { 0x0, 0x0, 0x0, 0x0 };
+
+/* Declare additional input buffers as needed.  */
+/* Input buffers for vld2_lane.  */
+VECT_VAR_DECL_INIT(buffer_vld2_lane, int, 8, 2);
+VECT_VAR_DECL_INIT(buffer_vld2_lane, int, 16, 2);
+VECT_VAR_DECL_INIT(buffer_vld2_lane, int, 32, 2);
+VECT_VAR_DECL_INIT(buffer_vld2_lane, int, 64, 2);
+VECT_VAR_DECL_INIT(buffer_vld2_lane, uint, 8, 2);
+VECT_VAR_DECL_INIT(buffer_vld2_lane, uint, 16, 2);
+VECT_VAR_DECL_INIT(buffer_vld2_lane, uint, 32, 2);
+VECT_VAR_DECL_INIT(buffer_vld2_lane, uint, 64, 2);
+VECT_VAR_DECL_INIT(buffer_vld2_lane, poly, 8, 2);
+VECT_VAR_DECL_INIT(buffer_vld2_lane, poly, 16, 2);
+VECT_VAR_DECL_INIT(buffer_vld2_lane, float, 32, 2);
+
+/* Input buffers for vld3_lane.  */
+VECT_VAR_DECL_INIT(buffer_vld3_lane, int, 8, 3);
+VECT_VAR_DECL_INIT(buffer_vld3_lane, int, 16, 3);
+VECT_VAR_DECL_INIT(buffer_vld3_lane, int, 32, 3);
+VECT_VAR_DECL_INIT(buffer_vld3_lane, int, 64, 3);
+VECT_VAR_DECL_INIT(buffer_vld3_lane, uint, 8, 3);
+VECT_VAR_DECL_INIT(buffer_vld3_lane, uint, 16, 3);
+VECT_VAR_DECL_INIT(buffer_vld3_lane, uint, 32, 3);
+VECT_VAR_DECL_INIT(buffer_vld3_lane, uint, 64, 3);
+VECT_VAR_DECL_INIT(buffer_vld3_lane, poly, 8, 3);
+VECT_VAR_DECL_INIT(buffer_vld3_lane, poly, 16, 3);
+VECT_VAR_DECL_INIT(buffer_vld3_lane, float, 32, 3);
+
+/* Input buffers for vld4_lane.  */
+VECT_VAR_DECL_INIT(buffer_vld4_lane, int, 8, 4);
+VECT_VAR_DECL_INIT(buffer_vld4_lane, int, 16, 4);
+VECT_VAR_DECL_INIT(buffer_vld4_lane, int, 32, 4);
+VECT_VAR_DECL_INIT(buffer_vld4_lane, int, 64, 4);
+VECT_VAR_DECL_INIT(buffer_vld4_lane, uint, 8, 4);
+VECT_VAR_DECL_INIT(buffer_vld4_lane, uint, 16, 4);
+VECT_VAR_DECL_INIT(buffer_vld4_lane, uint, 32, 4);
+VECT_VAR_DECL_INIT(buffer_vld4_lane, uint, 64, 4);
+VECT_VAR_DECL_INIT(buffer_vld4_lane, poly, 8, 4);
+VECT_VAR_DECL_INIT(buffer_vld4_lane, poly, 16, 4);
+VECT_VAR_DECL_INIT(buffer_vld4_lane, float, 32, 4);
+
+void exec_vstX_lane (void)
+{
+  /* In this case, input variables are arrays of vectors.  */
+#define DECL_VSTX_LANE(T1, W, N, X)					\
+  VECT_ARRAY_TYPE(T1, W, N, X) VECT_ARRAY_VAR(vector, T1, W, N, X);	\
+  VECT_ARRAY_TYPE(T1, W, N, X) VECT_ARRAY_VAR(vector_src, T1, W, N, X);	\
+  VECT_VAR_DECL(result_bis_##X, T1, W, N)[X * N]
+
+  /* We need to use a temporary result buffer (result_bis), because
+     the one used for other tests is not large enough. A subset of the
+     result data is moved from result_bis to result, and it is this
+     subset which is used to check the actual behaviour. The next
+     macro enables to move another chunk of data from result_bis to
+     result.  */
+  /* We also use another extra input buffer (buffer_src), which we
+     fill with 0xAA, and which it used to load a vector from which we
+     read a given lane.  */
+#define TEST_VSTX_LANE(Q, T1, T2, W, N, X, L)				 \
+  memset (VECT_VAR(buffer_src, T1, W, N), 0xAA,				 \
+	  sizeof(VECT_VAR(buffer_src, T1, W, N)));			 \
+  memset (VECT_VAR(result_bis_##X, T1, W, N), 0,			 \
+	  sizeof(VECT_VAR(result_bis_##X, T1, W, N)));			 \
+									 \
+  VECT_ARRAY_VAR(vector_src, T1, W, N, X) =				 \
+    vld##X##Q##_##T2##W(VECT_VAR(buffer_src, T1, W, N));		 \
+									 \
+  VECT_ARRAY_VAR(vector, T1, W, N, X) =					 \
+    /* Use dedicated init buffer, of size X.  */			 \
+    vld##X##Q##_lane_##T2##W(VECT_VAR(buffer_vld##X##_lane, T1, W, X),	 \
+			     VECT_ARRAY_VAR(vector_src, T1, W, N, X),	 \
+			     L);					 \
+  vst##X##Q##_lane_##T2##W(VECT_VAR(result_bis_##X, T1, W, N),		 \
+			   VECT_ARRAY_VAR(vector, T1, W, N, X),		 \
+			   L);						 \
+  memcpy(VECT_VAR(result, T1, W, N), VECT_VAR(result_bis_##X, T1, W, N), \
+	 sizeof(VECT_VAR(result, T1, W, N)));
+
+  /* Overwrite "result" with the contents of "result_bis"[Y].  */
+#define TEST_EXTRA_CHUNK(T1, W, N, X, Y)		\
+  memcpy(VECT_VAR(result, T1, W, N),			\
+	 &(VECT_VAR(result_bis_##X, T1, W, N)[Y*N]),	\
+	 sizeof(VECT_VAR(result, T1, W, N)));
+
+  /* We need all variants in 64 bits, but there is no 64x2 variant,
+     nor 128 bits vectors of int8/uint8/poly8.  */
+#define DECL_ALL_VSTX_LANE(X)			\
+  DECL_VSTX_LANE(int, 8, 8, X);			\
+  DECL_VSTX_LANE(int, 16, 4, X);		\
+  DECL_VSTX_LANE(int, 32, 2, X);		\
+  DECL_VSTX_LANE(uint, 8, 8, X);		\
+  DECL_VSTX_LANE(uint, 16, 4, X);		\
+  DECL_VSTX_LANE(uint, 32, 2, X);		\
+  DECL_VSTX_LANE(poly, 8, 8, X);		\
+  DECL_VSTX_LANE(poly, 16, 4, X);		\
+  DECL_VSTX_LANE(float, 32, 2, X);		\
+  DECL_VSTX_LANE(int, 16, 8, X);		\
+  DECL_VSTX_LANE(int, 32, 4, X);		\
+  DECL_VSTX_LANE(uint, 16, 8, X);		\
+  DECL_VSTX_LANE(uint, 32, 4, X);		\
+  DECL_VSTX_LANE(poly, 16, 8, X);		\
+  DECL_VSTX_LANE(float, 32, 4, X)
+
+#define DUMMY_ARRAY(V, T, W, N, L) VECT_VAR_DECL(V,T,W,N)[N*L]
+
+  /* Use the same lanes regardless of the size of the array (X), for
+     simplicity.  */
+#define TEST_ALL_VSTX_LANE(X)			\
+  TEST_VSTX_LANE(, int, s, 8, 8, X, 7);		\
+  TEST_VSTX_LANE(, int, s, 16, 4, X, 2);	\
+  TEST_VSTX_LANE(, int, s, 32, 2, X, 0);	\
+  TEST_VSTX_LANE(, float, f, 32, 2, X, 0);	\
+  TEST_VSTX_LANE(, uint, u, 8, 8, X, 4);	\
+  TEST_VSTX_LANE(, uint, u, 16, 4, X, 3);	\
+  TEST_VSTX_LANE(, uint, u, 32, 2, X, 1);	\
+  TEST_VSTX_LANE(, poly, p, 8, 8, X, 4);	\
+  TEST_VSTX_LANE(, poly, p, 16, 4, X, 3);	\
+  TEST_VSTX_LANE(q, int, s, 16, 8, X, 6);	\
+  TEST_VSTX_LANE(q, int, s, 32, 4, X, 2);	\
+  TEST_VSTX_LANE(q, uint, u, 16, 8, X, 5);	\
+  TEST_VSTX_LANE(q, uint, u, 32, 4, X, 0);	\
+  TEST_VSTX_LANE(q, poly, p, 16, 8, X, 5);	\
+  TEST_VSTX_LANE(q, float, f, 32, 4, X, 2)
+
+#define TEST_ALL_EXTRA_CHUNKS(X, Y)		\
+  TEST_EXTRA_CHUNK(int, 8, 8, X, Y);		\
+  TEST_EXTRA_CHUNK(int, 16, 4, X, Y);		\
+  TEST_EXTRA_CHUNK(int, 32, 2, X, Y);		\
+  TEST_EXTRA_CHUNK(uint, 8, 8, X, Y);		\
+  TEST_EXTRA_CHUNK(uint, 16, 4, X, Y);		\
+  TEST_EXTRA_CHUNK(uint, 32, 2, X, Y);		\
+  TEST_EXTRA_CHUNK(poly, 8, 8, X, Y);		\
+  TEST_EXTRA_CHUNK(poly, 16, 4, X, Y);		\
+  TEST_EXTRA_CHUNK(float, 32, 2, X, Y);		\
+  TEST_EXTRA_CHUNK(int, 16, 8, X, Y);		\
+  TEST_EXTRA_CHUNK(int, 32, 4, X, Y);		\
+  TEST_EXTRA_CHUNK(uint, 16, 8, X, Y);		\
+  TEST_EXTRA_CHUNK(uint, 32, 4, X, Y);		\
+  TEST_EXTRA_CHUNK(poly, 16, 8, X, Y);		\
+  TEST_EXTRA_CHUNK(float, 32, 4, X, Y)
+
+  /* Declare the temporary buffers / variables.  */
+  DECL_ALL_VSTX_LANE(2);
+  DECL_ALL_VSTX_LANE(3);
+  DECL_ALL_VSTX_LANE(4);
+
+  /* Define dummy input arrays, large enough for x4 vectors.  */
+  DUMMY_ARRAY(buffer_src, int, 8, 8, 4);
+  DUMMY_ARRAY(buffer_src, int, 16, 4, 4);
+  DUMMY_ARRAY(buffer_src, int, 32, 2, 4);
+  DUMMY_ARRAY(buffer_src, uint, 8, 8, 4);
+  DUMMY_ARRAY(buffer_src, uint, 16, 4, 4);
+  DUMMY_ARRAY(buffer_src, uint, 32, 2, 4);
+  DUMMY_ARRAY(buffer_src, poly, 8, 8, 4);
+  DUMMY_ARRAY(buffer_src, poly, 16, 4, 4);
+  DUMMY_ARRAY(buffer_src, float, 32, 2, 4);
+  DUMMY_ARRAY(buffer_src, int, 16, 8, 4);
+  DUMMY_ARRAY(buffer_src, int, 32, 4, 4);
+  DUMMY_ARRAY(buffer_src, uint, 16, 8, 4);
+  DUMMY_ARRAY(buffer_src, uint, 32, 4, 4);
+  DUMMY_ARRAY(buffer_src, poly, 16, 8, 4);
+  DUMMY_ARRAY(buffer_src, float, 32, 4, 4);
+
+  /* Check vst2_lane/vst2q_lane.  */
+  clean_results ();
+#define TEST_MSG "VST2_LANE/VST2Q_LANE"
+  TEST_ALL_VSTX_LANE(2);
+
+#define CMT " (chunk 0)"
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_st2_0, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_st2_0, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_st2_0, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_st2_0, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_st2_0, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_st2_0, CMT);
+  CHECK(TEST_MSG, poly, 8, 8, PRIx8, expected_st2_0, CMT);
+  CHECK(TEST_MSG, poly, 16, 4, PRIx16, expected_st2_0, CMT);
+  CHECK_FP(TEST_MSG, float, 32, 2, PRIx32, expected_st2_0, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_st2_0, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_st2_0, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_st2_0, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_st2_0, CMT);
+  CHECK(TEST_MSG, poly, 16, 8, PRIx16, expected_st2_0, CMT);
+  CHECK_FP(TEST_MSG, float, 32, 4, PRIx32, expected_st2_0, CMT);
+
+  TEST_ALL_EXTRA_CHUNKS(2, 1);
+#undef CMT
+#define CMT " chunk 1"
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_st2_1, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_st2_1, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_st2_1, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_st2_1, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_st2_1, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_st2_1, CMT);
+  CHECK(TEST_MSG, poly, 8, 8, PRIx8, expected_st2_1, CMT);
+  CHECK(TEST_MSG, poly, 16, 4, PRIx16, expected_st2_1, CMT);
+  CHECK_FP(TEST_MSG, float, 32, 2, PRIx32, expected_st2_1, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_st2_1, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_st2_1, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_st2_1, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_st2_1, CMT);
+  CHECK(TEST_MSG, poly, 16, 8, PRIx16, expected_st2_1, CMT);
+  CHECK_FP(TEST_MSG, float, 32, 4, PRIx32, expected_st2_1, CMT);
+
+
+  /* Check vst3_lane/vst3q_lane.  */
+  clean_results ();
+#undef TEST_MSG
+#define TEST_MSG "VST3_LANE/VST3Q_LANE"
+  TEST_ALL_VSTX_LANE(3);
+
+#undef CMT
+#define CMT " (chunk 0)"
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_st3_0, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_st3_0, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_st3_0, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_st3_0, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_st3_0, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_st3_0, CMT);
+  CHECK(TEST_MSG, poly, 8, 8, PRIx8, expected_st3_0, CMT);
+  CHECK(TEST_MSG, poly, 16, 4, PRIx16, expected_st3_0, CMT);
+  CHECK_FP(TEST_MSG, float, 32, 2, PRIx32, expected_st3_0, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_st3_0, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_st3_0, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_st3_0, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_st3_0, CMT);
+  CHECK(TEST_MSG, poly, 16, 8, PRIx16, expected_st3_0, CMT);
+  CHECK_FP(TEST_MSG, float, 32, 4, PRIx32, expected_st3_0, CMT);
+
+  TEST_ALL_EXTRA_CHUNKS(3, 1);
+
+#undef CMT
+#define CMT " (chunk 1)"
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_st3_1, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_st3_1, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_st3_1, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_st3_1, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_st3_1, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_st3_1, CMT);
+  CHECK(TEST_MSG, poly, 8, 8, PRIx8, expected_st3_1, CMT);
+  CHECK(TEST_MSG, poly, 16, 4, PRIx16, expected_st3_1, CMT);
+  CHECK_FP(TEST_MSG, float, 32, 2, PRIx32, expected_st3_1, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_st3_1, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_st3_1, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_st3_1, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_st3_1, CMT);
+  CHECK(TEST_MSG, poly, 16, 8, PRIx16, expected_st3_1, CMT);
+  CHECK_FP(TEST_MSG, float, 32, 4, PRIx32, expected_st3_1, CMT);
+
+  TEST_ALL_EXTRA_CHUNKS(3, 2);
+
+#undef CMT
+#define CMT " (chunk 2)"
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_st3_2, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_st3_2, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_st3_2, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_st3_2, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_st3_2, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_st3_2, CMT);
+  CHECK(TEST_MSG, poly, 8, 8, PRIx8, expected_st3_2, CMT);
+  CHECK(TEST_MSG, poly, 16, 4, PRIx16, expected_st3_2, CMT);
+  CHECK_FP(TEST_MSG, float, 32, 2, PRIx32, expected_st3_2, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_st3_2, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_st3_2, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_st3_2, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_st3_2, CMT);
+  CHECK(TEST_MSG, poly, 16, 8, PRIx16, expected_st3_2, CMT);
+  CHECK_FP(TEST_MSG, float, 32, 4, PRIx32, expected_st3_2, CMT);
+
+
+  /* Check vst4_lane/vst4q_lane.  */
+  clean_results ();
+#undef TEST_MSG
+#define TEST_MSG "VST4_LANE/VST4Q_LANE"
+  TEST_ALL_VSTX_LANE(4);
+
+#undef CMT
+#define CMT " (chunk 0)"
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_st4_0, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_st4_0, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_st4_0, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_st4_0, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_st4_0, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_st4_0, CMT);
+  CHECK(TEST_MSG, poly, 8, 8, PRIx8, expected_st4_0, CMT);
+  CHECK(TEST_MSG, poly, 16, 4, PRIx16, expected_st4_0, CMT);
+  CHECK_FP(TEST_MSG, float, 32, 2, PRIx32, expected_st4_0, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_st4_0, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_st4_0, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_st4_0, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_st4_0, CMT);
+  CHECK(TEST_MSG, poly, 16, 8, PRIx16, expected_st4_0, CMT);
+  CHECK_FP(TEST_MSG, float, 32, 4, PRIx32, expected_st4_0, CMT);
+
+  TEST_ALL_EXTRA_CHUNKS(4, 1);
+
+#undef CMT
+#define CMT " (chunk 1)"
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_st4_1, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_st4_1, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_st4_1, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_st4_1, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_st4_1, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_st4_1, CMT);
+  CHECK(TEST_MSG, poly, 8, 8, PRIx8, expected_st4_1, CMT);
+  CHECK(TEST_MSG, poly, 16, 4, PRIx16, expected_st4_1, CMT);
+  CHECK_FP(TEST_MSG, float, 32, 2, PRIx32, expected_st4_1, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_st4_1, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_st4_1, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_st4_1, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_st4_1, CMT);
+  CHECK(TEST_MSG, poly, 16, 8, PRIx16, expected_st4_1, CMT);
+  CHECK_FP(TEST_MSG, float, 32, 4, PRIx32, expected_st4_1, CMT);
+
+  TEST_ALL_EXTRA_CHUNKS(4, 2);
+
+#undef CMT
+#define CMT " (chunk 2)"
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_st4_2, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_st4_2, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_st4_2, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_st4_2, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_st4_2, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_st4_2, CMT);
+  CHECK(TEST_MSG, poly, 8, 8, PRIx8, expected_st4_2, CMT);
+  CHECK(TEST_MSG, poly, 16, 4, PRIx16, expected_st4_2, CMT);
+  CHECK_FP(TEST_MSG, float, 32, 2, PRIx32, expected_st4_2, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_st4_2, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_st4_2, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_st4_2, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_st4_2, CMT);
+  CHECK(TEST_MSG, poly, 16, 8, PRIx16, expected_st4_2, CMT);
+  CHECK_FP(TEST_MSG, float, 32, 4, PRIx32, expected_st4_2, CMT);
+
+  TEST_ALL_EXTRA_CHUNKS(4, 3);
+
+#undef CMT
+#define CMT " (chunk 3)"
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_st4_3, CMT);
+  CHECK(TEST_MSG, int, 16, 4, PRIx16, expected_st4_3, CMT);
+  CHECK(TEST_MSG, int, 32, 2, PRIx32, expected_st4_3, CMT);
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_st4_3, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_st4_3, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_st4_3, CMT);
+  CHECK(TEST_MSG, poly, 8, 8, PRIx8, expected_st4_3, CMT);
+  CHECK(TEST_MSG, poly, 16, 4, PRIx16, expected_st4_3, CMT);
+  CHECK_FP(TEST_MSG, float, 32, 2, PRIx32, expected_st4_3, CMT);
+  CHECK(TEST_MSG, int, 16, 8, PRIx16, expected_st4_3, CMT);
+  CHECK(TEST_MSG, int, 32, 4, PRIx32, expected_st4_3, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_st4_3, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_st4_3, CMT);
+  CHECK(TEST_MSG, poly, 16, 8, PRIx16, expected_st4_3, CMT);
+  CHECK_FP(TEST_MSG, float, 32, 4, PRIx32, expected_st4_3, CMT);
+}
+
+int main (void)
+{
+  exec_vstX_lane ();
+  return 0;
+}
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vsub.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vsub.c
@@ -18,10 +18,6 @@ VECT_VAR_DECL(expected,uint,8,8) [] = { 0xdc, 0xdd, 0xde, 0xdf,
 VECT_VAR_DECL(expected,uint,16,4) [] = { 0xffd2, 0xffd3, 0xffd4, 0xffd5 };
 VECT_VAR_DECL(expected,uint,32,2) [] = { 0xffffffc8, 0xffffffc9 };
 VECT_VAR_DECL(expected,uint,64,1) [] = { 0xffffffffffffffee };
-VECT_VAR_DECL(expected,poly,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,2) [] = { 0x33333333, 0x33333333 };
 VECT_VAR_DECL(expected,int,8,16) [] = { 0xfa, 0xfb, 0xfc, 0xfd,
 					0xfe, 0xff, 0x0, 0x1,
 					0x2, 0x3, 0x4, 0x5,
@@ -41,14 +37,6 @@ VECT_VAR_DECL(expected,uint,32,4) [] = { 0xffffffb9, 0xffffffba,
 					 0xffffffbb, 0xffffffbc };
 VECT_VAR_DECL(expected,uint,64,2) [] = { 0xffffffffffffffed,
 					 0xffffffffffffffee };
-VECT_VAR_DECL(expected,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,4) [] = { 0x33333333, 0x33333333,
-					  0x33333333, 0x33333333 };
 
 /* Expected results for float32 variants. Needs to be separated since
    the generic test function does not test floating-point
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vsubl.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vsubl.c
@@ -6,43 +6,13 @@
 #define TEST_MSG "VSUBL"
 
 /* Expected results.  */
-VECT_VAR_DECL(expected,int,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-				       0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,int,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,int,32,2) [] = { 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,int,64,1) [] = { 0x3333333333333333 };
-VECT_VAR_DECL(expected,uint,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,uint,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,uint,32,2) [] = { 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,uint,64,1) [] = { 0x3333333333333333 };
-VECT_VAR_DECL(expected,poly,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,2) [] = { 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,int,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
 VECT_VAR_DECL(expected,int,16,8) [] = { 0xfffd, 0xfffe, 0xffff, 0x0,
 					0x1, 0x2, 0x3, 0x4 };
 VECT_VAR_DECL(expected,int,32,4) [] = { 0xfffffffe, 0xffffffff, 0x0, 0x1 };
 VECT_VAR_DECL(expected,int,64,2) [] = { 0x0, 0x1 };
-VECT_VAR_DECL(expected,uint,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
 VECT_VAR_DECL(expected,uint,16,8) [] = { 0xfffd, 0xfffe, 0xffff, 0x0,
 					 0x1, 0x2, 0x3, 0x4 };
 VECT_VAR_DECL(expected,uint,32,4) [] = { 0xffffffff, 0x0, 0x1, 0x2 };
 VECT_VAR_DECL(expected,uint,64,2) [] = { 0x0, 0x1 };
-VECT_VAR_DECL(expected,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,4) [] = { 0x33333333, 0x33333333,
-					   0x33333333, 0x33333333 };
 
 #include "vXXXl.inc"
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vsubw.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vsubw.c
@@ -6,45 +6,15 @@
 #define TEST_MSG "VSUBW"
 
 /* Expected results.  */
-VECT_VAR_DECL(expected,int,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-				       0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,int,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,int,32,2) [] = { 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,int,64,1) [] = { 0x3333333333333333 };
-VECT_VAR_DECL(expected,uint,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,uint,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,uint,32,2) [] = { 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,uint,64,1) [] = { 0x3333333333333333 };
-VECT_VAR_DECL(expected,poly,8,8) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,4) [] = { 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,2) [] = { 0x33333333, 0x33333333 };
-VECT_VAR_DECL(expected,int,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33,
-					0x33, 0x33, 0x33, 0x33 };
 VECT_VAR_DECL(expected,int,16,8) [] = { 0xfffd, 0xfffe, 0xffff, 0x0,
 					0x1, 0x2, 0x3, 0x4 };
 VECT_VAR_DECL(expected,int,32,4) [] = { 0xfffffffe, 0xffffffff, 0x0, 0x1 };
 VECT_VAR_DECL(expected,int,64,2) [] = { 0x0, 0x1 };
-VECT_VAR_DECL(expected,uint,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
 VECT_VAR_DECL(expected,uint,16,8) [] = { 0xfefd, 0xfefe, 0xfeff, 0xff00,
 					 0xff01, 0xff02, 0xff03, 0xff04 };
 VECT_VAR_DECL(expected,uint,32,4) [] = { 0xfffeffff, 0xffff0000,
 					 0xffff0001, 0xffff0002 };
 VECT_VAR_DECL(expected,uint,64,2) [] = { 0xffffffff00000000,
 					 0xffffffff00000001 };
-VECT_VAR_DECL(expected,poly,8,16) [] = { 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33,
-					 0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected,poly,16,8) [] = { 0x3333, 0x3333, 0x3333, 0x3333,
-					 0x3333, 0x3333, 0x3333, 0x3333 };
-VECT_VAR_DECL(expected,hfloat,32,4) [] = { 0x33333333, 0x33333333,
-					   0x33333333, 0x33333333 };
 
 #include "vXXXw.inc"
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vtbX.c
@@ -0,0 +1,289 @@
+#include <arm_neon.h>
+#include "arm-neon-ref.h"
+#include "compute-ref-data.h"
+
+/* Expected results for vtbl1.  */
+VECT_VAR_DECL(expected_vtbl1,int,8,8) [] = { 0x0, 0xf2, 0xf2, 0xf2,
+					     0x0, 0x0, 0xf2, 0xf2 };
+VECT_VAR_DECL(expected_vtbl1,uint,8,8) [] = { 0x0, 0xf3, 0xf3, 0xf3,
+					      0x0, 0x0, 0xf3, 0xf3 };
+VECT_VAR_DECL(expected_vtbl1,poly,8,8) [] = { 0x0, 0xf3, 0xf3, 0xf3,
+					      0x0, 0x0, 0xf3, 0xf3 };
+
+/* Expected results for vtbl2.  */
+VECT_VAR_DECL(expected_vtbl2,int,8,8) [] = { 0xf6, 0xf3, 0xf3, 0xf3,
+					     0x0, 0x0, 0xf3, 0xf3 };
+VECT_VAR_DECL(expected_vtbl2,uint,8,8) [] = { 0xf6, 0xf5, 0xf5, 0xf5,
+					      0x0, 0x0, 0xf5, 0xf5 };
+VECT_VAR_DECL(expected_vtbl2,poly,8,8) [] = { 0xf6, 0xf5, 0xf5, 0xf5,
+					      0x0, 0x0, 0xf5, 0xf5 };
+
+/* Expected results for vtbl3.  */
+VECT_VAR_DECL(expected_vtbl3,int,8,8) [] = { 0xf8, 0xf4, 0xf4, 0xf4,
+					     0xff, 0x0, 0xf4, 0xf4 };
+VECT_VAR_DECL(expected_vtbl3,uint,8,8) [] = { 0xf8, 0xf7, 0xf7, 0xf7,
+					      0xff, 0x0, 0xf7, 0xf7 };
+VECT_VAR_DECL(expected_vtbl3,poly,8,8) [] = { 0xf8, 0xf7, 0xf7, 0xf7,
+					      0xff, 0x0, 0xf7, 0xf7 };
+
+/* Expected results for vtbl4.  */
+VECT_VAR_DECL(expected_vtbl4,int,8,8) [] = { 0xfa, 0xf5, 0xf5, 0xf5,
+					    0x3, 0x0, 0xf5, 0xf5 };
+VECT_VAR_DECL(expected_vtbl4,uint,8,8) [] = { 0xfa, 0xf9, 0xf9, 0xf9,
+					     0x3, 0x0, 0xf9, 0xf9 };
+VECT_VAR_DECL(expected_vtbl4,poly,8,8) [] = { 0xfa, 0xf9, 0xf9, 0xf9,
+					     0x3, 0x0, 0xf9, 0xf9 };
+
+/* Expected results for vtbx1.  */
+VECT_VAR_DECL(expected_vtbx1,int,8,8) [] = { 0x33, 0xf2, 0xf2, 0xf2,
+					    0x33, 0x33, 0xf2, 0xf2 };
+VECT_VAR_DECL(expected_vtbx1,uint,8,8) [] = { 0xcc, 0xf3, 0xf3, 0xf3,
+					     0xcc, 0xcc, 0xf3, 0xf3 };
+VECT_VAR_DECL(expected_vtbx1,poly,8,8) [] = { 0xcc, 0xf3, 0xf3, 0xf3,
+					     0xcc, 0xcc, 0xf3, 0xf3 };
+
+/* Expected results for vtbx2.  */
+VECT_VAR_DECL(expected_vtbx2,int,8,8) [] = { 0xf6, 0xf3, 0xf3, 0xf3,
+					    0x33, 0x33, 0xf3, 0xf3 };
+VECT_VAR_DECL(expected_vtbx2,uint,8,8) [] = { 0xf6, 0xf5, 0xf5, 0xf5,
+					     0xcc, 0xcc, 0xf5, 0xf5 };
+VECT_VAR_DECL(expected_vtbx2,poly,8,8) [] = { 0xf6, 0xf5, 0xf5, 0xf5,
+					     0xcc, 0xcc, 0xf5, 0xf5 };
+
+/* Expected results for vtbx3.  */
+VECT_VAR_DECL(expected_vtbx3,int,8,8) [] = { 0xf8, 0xf4, 0xf4, 0xf4,
+					    0xff, 0x33, 0xf4, 0xf4 };
+VECT_VAR_DECL(expected_vtbx3,uint,8,8) [] = { 0xf8, 0xf7, 0xf7, 0xf7,
+					     0xff, 0xcc, 0xf7, 0xf7 };
+VECT_VAR_DECL(expected_vtbx3,poly,8,8) [] = { 0xf8, 0xf7, 0xf7, 0xf7,
+					     0xff, 0xcc, 0xf7, 0xf7 };
+
+/* Expected results for vtbx4.  */
+VECT_VAR_DECL(expected_vtbx4,int,8,8) [] = { 0xfa, 0xf5, 0xf5, 0xf5,
+					     0x3, 0x33, 0xf5, 0xf5 };
+VECT_VAR_DECL(expected_vtbx4,uint,8,8) [] = { 0xfa, 0xf9, 0xf9, 0xf9,
+					      0x3, 0xcc, 0xf9, 0xf9 };
+VECT_VAR_DECL(expected_vtbx4,poly,8,8) [] = { 0xfa, 0xf9, 0xf9, 0xf9,
+					      0x3, 0xcc, 0xf9, 0xf9 };
+
+void exec_vtbX (void)
+{
+  int i;
+
+  /* In this case, input variables are arrays of vectors.  */
+#define DECL_VTBX(T1, W, N, X)						\
+  VECT_ARRAY_TYPE(T1, W, N, X) VECT_ARRAY_VAR(table_vector, T1, W, N, X)
+
+  /* The vtbl1 variant is different from vtbl{2,3,4} because it takes a
+     vector as 1st param, instead of an array of vectors.  */
+#define TEST_VTBL1(T1, T2, T3, W, N)			\
+  VECT_VAR(table_vector, T1, W, N) =			\
+    vld1##_##T2##W((T1##W##_t *)lookup_table);		\
+							\
+  VECT_VAR(vector_res, T1, W, N) =			\
+    vtbl1_##T2##W(VECT_VAR(table_vector, T1, W, N),	\
+		  VECT_VAR(vector, T3, W, N));		\
+  vst1_##T2##W(VECT_VAR(result, T1, W, N),		\
+	       VECT_VAR(vector_res, T1, W, N));
+
+#define TEST_VTBLX(T1, T2, T3, W, N, X)					\
+  VECT_ARRAY_VAR(table_vector, T1, W, N, X) =				\
+    vld##X##_##T2##W((T1##W##_t *)lookup_table);			\
+									\
+  VECT_VAR(vector_res, T1, W, N) =					\
+    vtbl##X##_##T2##W(VECT_ARRAY_VAR(table_vector, T1, W, N, X),	\
+		      VECT_VAR(vector, T3, W, N));			\
+  vst1_##T2##W(VECT_VAR(result, T1, W, N),				\
+	       VECT_VAR(vector_res, T1, W, N));
+
+  /* We need to define a lookup table.  */
+  uint8_t lookup_table[32];
+
+  DECL_VARIABLE(vector, int, 8, 8);
+  DECL_VARIABLE(vector, uint, 8, 8);
+  DECL_VARIABLE(vector, poly, 8, 8);
+  DECL_VARIABLE(vector_res, int, 8, 8);
+  DECL_VARIABLE(vector_res, uint, 8, 8);
+  DECL_VARIABLE(vector_res, poly, 8, 8);
+
+  /* For vtbl1.  */
+  DECL_VARIABLE(table_vector, int, 8, 8);
+  DECL_VARIABLE(table_vector, uint, 8, 8);
+  DECL_VARIABLE(table_vector, poly, 8, 8);
+
+  /* For vtbx*.  */
+  DECL_VARIABLE(default_vector, int, 8, 8);
+  DECL_VARIABLE(default_vector, uint, 8, 8);
+  DECL_VARIABLE(default_vector, poly, 8, 8);
+
+  /* We need only 8 bits variants.  */
+#define DECL_ALL_VTBLX(X)			\
+  DECL_VTBX(int, 8, 8, X);			\
+  DECL_VTBX(uint, 8, 8, X);			\
+  DECL_VTBX(poly, 8, 8, X)
+
+#define TEST_ALL_VTBL1()			\
+  TEST_VTBL1(int, s, int, 8, 8);		\
+  TEST_VTBL1(uint, u, uint, 8, 8);		\
+  TEST_VTBL1(poly, p, uint, 8, 8)
+
+#define TEST_ALL_VTBLX(X)			\
+  TEST_VTBLX(int, s, int, 8, 8, X);		\
+  TEST_VTBLX(uint, u, uint, 8, 8, X);		\
+  TEST_VTBLX(poly, p, uint, 8, 8, X)
+
+  /* Declare the temporary buffers / variables.  */
+  DECL_ALL_VTBLX(2);
+  DECL_ALL_VTBLX(3);
+  DECL_ALL_VTBLX(4);
+
+  /* Fill the lookup table.  */
+  for (i=0; i<32; i++) {
+    lookup_table[i] = i-15;
+  }
+
+  /* Choose init value arbitrarily, will be used as table index.  */
+  VDUP(vector, , int, s, 8, 8, 1);
+  VDUP(vector, , uint, u, 8, 8, 2);
+  VDUP(vector, , poly, p, 8, 8, 2);
+
+  /* To ensure coverage, add some indexes larger than 8,16 and 32
+     except: lane 0 (by 6), lane 1 (by 8) and lane 2 (by 9).  */
+  VSET_LANE(vector, , int, s, 8, 8, 0, 10);
+  VSET_LANE(vector, , int, s, 8, 8, 4, 20);
+  VSET_LANE(vector, , int, s, 8, 8, 5, 40);
+  VSET_LANE(vector, , uint, u, 8, 8, 0, 10);
+  VSET_LANE(vector, , uint, u, 8, 8, 4, 20);
+  VSET_LANE(vector, , uint, u, 8, 8, 5, 40);
+  VSET_LANE(vector, , poly, p, 8, 8, 0, 10);
+  VSET_LANE(vector, , poly, p, 8, 8, 4, 20);
+  VSET_LANE(vector, , poly, p, 8, 8, 5, 40);
+
+
+  /* Check vtbl1.  */
+  clean_results ();
+#define TEST_MSG "VTBL1"
+  TEST_ALL_VTBL1();
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_vtbl1, "");
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_vtbl1, "");
+  CHECK(TEST_MSG, poly, 8, 8, PRIx8, expected_vtbl1, "");
+
+  /* Check vtbl2.  */
+  clean_results ();
+#undef TEST_MSG
+#define TEST_MSG "VTBL2"
+  TEST_ALL_VTBLX(2);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_vtbl2, "");
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_vtbl2, "");
+  CHECK(TEST_MSG, poly, 8, 8, PRIx8, expected_vtbl2, "");
+
+  /* Check vtbl3.  */
+  clean_results ();
+#undef TEST_MSG
+#define TEST_MSG "VTBL3"
+  TEST_ALL_VTBLX(3);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_vtbl3, "");
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_vtbl3, "");
+  CHECK(TEST_MSG, poly, 8, 8, PRIx8, expected_vtbl3, "");
+
+  /* Check vtbl4.  */
+  clean_results ();
+#undef TEST_MSG
+#define TEST_MSG "VTBL4"
+  TEST_ALL_VTBLX(4);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_vtbl4, "");
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_vtbl4, "");
+  CHECK(TEST_MSG, poly, 8, 8, PRIx8, expected_vtbl4, "");
+
+
+  /* Now test VTBX.  */
+
+  /* The vtbx1 variant is different from vtbx{2,3,4} because it takes a
+     vector as 1st param, instead of an array of vectors.  */
+#define TEST_VTBX1(T1, T2, T3, W, N)			\
+  VECT_VAR(table_vector, T1, W, N) =			\
+    vld1##_##T2##W((T1##W##_t *)lookup_table);		\
+							\
+  VECT_VAR(vector_res, T1, W, N) =			\
+    vtbx1_##T2##W(VECT_VAR(default_vector, T1, W, N),	\
+		  VECT_VAR(table_vector, T1, W, N),	\
+		  VECT_VAR(vector, T3, W, N));		\
+  vst1_##T2##W(VECT_VAR(result, T1, W, N),		\
+	       VECT_VAR(vector_res, T1, W, N));
+
+#define TEST_VTBXX(T1, T2, T3, W, N, X)					\
+  VECT_ARRAY_VAR(table_vector, T1, W, N, X) =				\
+    vld##X##_##T2##W((T1##W##_t *)lookup_table);			\
+									\
+  VECT_VAR(vector_res, T1, W, N) =					\
+    vtbx##X##_##T2##W(VECT_VAR(default_vector, T1, W, N),		\
+		      VECT_ARRAY_VAR(table_vector, T1, W, N, X),	\
+		      VECT_VAR(vector, T3, W, N));			\
+  vst1_##T2##W(VECT_VAR(result, T1, W, N),				\
+	       VECT_VAR(vector_res, T1, W, N));
+
+#define TEST_ALL_VTBX1()			\
+  TEST_VTBX1(int, s, int, 8, 8);		\
+  TEST_VTBX1(uint, u, uint, 8, 8);		\
+  TEST_VTBX1(poly, p, uint, 8, 8)
+
+#define TEST_ALL_VTBXX(X)			\
+  TEST_VTBXX(int, s, int, 8, 8, X);		\
+  TEST_VTBXX(uint, u, uint, 8, 8, X);		\
+  TEST_VTBXX(poly, p, uint, 8, 8, X)
+
+  /* Choose init value arbitrarily, will be used as default value.  */
+  VDUP(default_vector, , int, s, 8, 8, 0x33);
+  VDUP(default_vector, , uint, u, 8, 8, 0xCC);
+  VDUP(default_vector, , poly, p, 8, 8, 0xCC);
+
+  /* Check vtbx1.  */
+  clean_results ();
+#undef TEST_MSG
+#define TEST_MSG "VTBX1"
+  TEST_ALL_VTBX1();
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_vtbx1, "");
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_vtbx1, "");
+  CHECK(TEST_MSG, poly, 8, 8, PRIx8, expected_vtbx1, "");
+
+  /* Check vtbx2.  */
+  clean_results ();
+#undef TEST_MSG
+#define TEST_MSG "VTBX2"
+  TEST_ALL_VTBXX(2);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_vtbx2, "");
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_vtbx2, "");
+  CHECK(TEST_MSG, poly, 8, 8, PRIx8, expected_vtbx2, "");
+
+  /* Check vtbx3.  */
+  clean_results ();
+#undef TEST_MSG
+#define TEST_MSG "VTBX3"
+  TEST_ALL_VTBXX(3);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_vtbx3, "");
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_vtbx3, "");
+  CHECK(TEST_MSG, poly, 8, 8, PRIx8, expected_vtbx3, "");
+
+  /* Check vtbx4.  */
+  clean_results ();
+#undef TEST_MSG
+#define TEST_MSG "VTBX4"
+  TEST_ALL_VTBXX(4);
+
+  CHECK(TEST_MSG, int, 8, 8, PRIx8, expected_vtbx4, "");
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_vtbx4, "");
+  CHECK(TEST_MSG, poly, 8, 8, PRIx8, expected_vtbx4, "");
+}
+
+int main (void)
+{
+  exec_vtbX ();
+  return 0;
+}
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vtrn.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vtrn.c
@@ -8,12 +8,10 @@ VECT_VAR_DECL(expected0,int,8,8) [] = { 0xf0, 0xf1, 0x11, 0x11,
 					0xf2, 0xf3, 0x11, 0x11 };
 VECT_VAR_DECL(expected0,int,16,4) [] = { 0xfff0, 0xfff1, 0x22, 0x22 };
 VECT_VAR_DECL(expected0,int,32,2) [] = { 0xfffffff0, 0xfffffff1 };
-VECT_VAR_DECL(expected0,int,64,1) [] = { 0x3333333333333333 };
 VECT_VAR_DECL(expected0,uint,8,8) [] = { 0xf0, 0xf1, 0x55, 0x55,
 					 0xf2, 0xf3, 0x55, 0x55 };
 VECT_VAR_DECL(expected0,uint,16,4) [] = { 0xfff0, 0xfff1, 0x66, 0x66 };
 VECT_VAR_DECL(expected0,uint,32,2) [] = { 0xfffffff0, 0xfffffff1 };
-VECT_VAR_DECL(expected0,uint,64,1) [] = { 0x3333333333333333 };
 VECT_VAR_DECL(expected0,poly,8,8) [] = { 0xf0, 0xf1, 0x55, 0x55,
 					 0xf2, 0xf3, 0x55, 0x55 };
 VECT_VAR_DECL(expected0,poly,16,4) [] = { 0xfff0, 0xfff1, 0x66, 0x66 };
@@ -25,8 +23,6 @@ VECT_VAR_DECL(expected0,int,8,16) [] = { 0xf0, 0xf1, 0x11, 0x11,
 VECT_VAR_DECL(expected0,int,16,8) [] = { 0xfff0, 0xfff1, 0x22, 0x22,
 					 0xfff2, 0xfff3, 0x22, 0x22 };
 VECT_VAR_DECL(expected0,int,32,4) [] = { 0xfffffff0, 0xfffffff1, 0x33, 0x33 };
-VECT_VAR_DECL(expected0,int,64,2) [] = { 0x3333333333333333,
-					 0x3333333333333333 };
 VECT_VAR_DECL(expected0,uint,8,16) [] = { 0xf0, 0xf1, 0x55, 0x55,
 					  0xf2, 0xf3, 0x55, 0x55,
 					  0xf4, 0xf5, 0x55, 0x55,
@@ -34,8 +30,6 @@ VECT_VAR_DECL(expected0,uint,8,16) [] = { 0xf0, 0xf1, 0x55, 0x55,
 VECT_VAR_DECL(expected0,uint,16,8) [] = { 0xfff0, 0xfff1, 0x66, 0x66,
 					  0xfff2, 0xfff3, 0x66, 0x66 };
 VECT_VAR_DECL(expected0,uint,32,4) [] = { 0xfffffff0, 0xfffffff1, 0x77, 0x77 };
-VECT_VAR_DECL(expected0,uint,64,2) [] = { 0x3333333333333333,
-					  0x3333333333333333 };
 VECT_VAR_DECL(expected0,poly,8,16) [] = { 0xf0, 0xf1, 0x55, 0x55,
 					  0xf2, 0xf3, 0x55, 0x55,
 					  0xf4, 0xf5, 0x55, 0x55,
@@ -50,12 +44,10 @@ VECT_VAR_DECL(expected1,int,8,8) [] = { 0xf4, 0xf5, 0x11, 0x11,
 					0xf6, 0xf7, 0x11, 0x11 };
 VECT_VAR_DECL(expected1,int,16,4) [] = { 0xfff2, 0xfff3, 0x22, 0x22 };
 VECT_VAR_DECL(expected1,int,32,2) [] = { 0x33, 0x33 };
-VECT_VAR_DECL(expected1,int,64,1) [] = { 0x3333333333333333 };
 VECT_VAR_DECL(expected1,uint,8,8) [] = { 0xf4, 0xf5, 0x55, 0x55,
 					 0xf6, 0xf7, 0x55, 0x55 };
 VECT_VAR_DECL(expected1,uint,16,4) [] = { 0xfff2, 0xfff3, 0x66, 0x66 };
 VECT_VAR_DECL(expected1,uint,32,2) [] = { 0x77, 0x77 };
-VECT_VAR_DECL(expected1,uint,64,1) [] = { 0x3333333333333333 };
 VECT_VAR_DECL(expected1,poly,8,8) [] = { 0xf4, 0xf5, 0x55, 0x55,
 					 0xf6, 0xf7, 0x55, 0x55 };
 VECT_VAR_DECL(expected1,poly,16,4) [] = { 0xfff2, 0xfff3, 0x66, 0x66 };
@@ -67,8 +59,6 @@ VECT_VAR_DECL(expected1,int,8,16) [] = { 0xf8, 0xf9, 0x11, 0x11,
 VECT_VAR_DECL(expected1,int,16,8) [] = { 0xfff4, 0xfff5, 0x22, 0x22,
 					 0xfff6, 0xfff7, 0x22, 0x22 };
 VECT_VAR_DECL(expected1,int,32,4) [] = { 0xfffffff2, 0xfffffff3, 0x33, 0x33 };
-VECT_VAR_DECL(expected1,int,64,2) [] = { 0x3333333333333333,
-					 0x3333333333333333 };
 VECT_VAR_DECL(expected1,uint,8,16) [] = { 0xf8, 0xf9, 0x55, 0x55,
 					  0xfa, 0xfb, 0x55, 0x55,
 					  0xfc, 0xfd, 0x55, 0x55,
@@ -76,8 +66,6 @@ VECT_VAR_DECL(expected1,uint,8,16) [] = { 0xf8, 0xf9, 0x55, 0x55,
 VECT_VAR_DECL(expected1,uint,16,8) [] = { 0xfff4, 0xfff5, 0x66, 0x66,
 					  0xfff6, 0xfff7, 0x66, 0x66 };
 VECT_VAR_DECL(expected1,uint,32,4) [] = { 0xfffffff2, 0xfffffff3, 0x77, 0x77 };
-VECT_VAR_DECL(expected1,uint,64,2) [] = { 0x3333333333333333,
-					  0x3333333333333333 };
 VECT_VAR_DECL(expected1,poly,8,16) [] = { 0xf8, 0xf9, 0x55, 0x55,
 					  0xfa, 0xfb, 0x55, 0x55,
 					  0xfc, 0xfd, 0x55, 0x55,
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vtst.c
@@ -0,0 +1,120 @@
+#include <arm_neon.h>
+#include "arm-neon-ref.h"
+#include "compute-ref-data.h"
+
+/* Expected results with signed input.  */
+VECT_VAR_DECL(expected_signed,uint,8,8) [] = { 0x0, 0xff, 0xff, 0xff,
+					       0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_signed,uint,16,4) [] = { 0x0, 0xffff, 0x0, 0xffff };
+VECT_VAR_DECL(expected_signed,uint,32,2) [] = { 0x0, 0xffffffff };
+VECT_VAR_DECL(expected_signed,uint,8,16) [] = { 0x0, 0xff, 0xff, 0xff,
+						0xff, 0xff, 0xff, 0xff,
+						0xff, 0xff, 0xff, 0xff,
+						0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_signed,uint,16,8) [] = { 0x0, 0xffff, 0x0, 0xffff,
+						0xffff, 0xffff, 0xffff, 0xffff };
+VECT_VAR_DECL(expected_signed,uint,32,4) [] = { 0x0, 0xffffffff,
+						0x0, 0xffffffff };
+
+/* Expected results with unsigned input.  */
+VECT_VAR_DECL(expected_unsigned,uint,8,8) [] = { 0x0, 0xff, 0xff, 0xff,
+						 0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_unsigned,uint,16,4) [] = { 0x0, 0xffff, 0x0, 0xffff };
+VECT_VAR_DECL(expected_unsigned,uint,32,2) [] = { 0x0, 0xffffffff };
+VECT_VAR_DECL(expected_unsigned,uint,8,16) [] = { 0x0, 0xff, 0xff, 0xff,
+						  0xff, 0xff, 0xff, 0xff,
+						  0xff, 0xff, 0xff, 0xff,
+						  0xff, 0xff, 0xff, 0xff };
+VECT_VAR_DECL(expected_unsigned,uint,16,8) [] = { 0x0, 0xffff,
+						  0x0, 0xffff,
+						  0xffff, 0xffff,
+						  0xffff, 0xffff };
+VECT_VAR_DECL(expected_unsigned,uint,32,4) [] = { 0x0, 0xffffffff,
+						  0x0, 0xffffffff };
+
+#ifndef INSN_NAME
+#define INSN_NAME vtst
+#define TEST_MSG "VTST/VTSTQ"
+#endif
+
+/* We can't use the standard ref_v_binary_op.c template because vtst
+   has no 64 bits variant, and outputs are always of uint type.  */
+#define FNNAME1(NAME) void exec_ ## NAME (void)
+#define FNNAME(NAME) FNNAME1(NAME)
+
+FNNAME (INSN_NAME)
+{
+  /* Basic test: y=OP(x,x), then store the result.  */
+#define TEST_BINARY_OP1(INSN, Q, T1, T2, W, N)		\
+  VECT_VAR(vector_res, uint, W, N) =			\
+    INSN##Q##_##T2##W(VECT_VAR(vector, T1, W, N),	\
+		      VECT_VAR(vector2, T1, W, N));	\
+  vst1##Q##_u##W(VECT_VAR(result, uint, W, N),		\
+		 VECT_VAR(vector_res, uint, W, N))
+
+#define TEST_BINARY_OP(INSN, Q, T1, T2, W, N)	\
+  TEST_BINARY_OP1(INSN, Q, T1, T2, W, N)	\
+
+  DECL_VARIABLE_ALL_VARIANTS(vector);
+  DECL_VARIABLE_ALL_VARIANTS(vector2);
+  DECL_VARIABLE_UNSIGNED_VARIANTS(vector_res);
+
+
+  clean_results ();
+
+  /* Initialize input "vector" from "buffer".  */
+  TEST_MACRO_ALL_VARIANTS_2_5(VLOAD, vector, buffer);
+
+  /* Choose init value arbitrarily, will be used as comparison
+     value.  */
+  VDUP(vector2, , int, s, 8, 8, 15);
+  VDUP(vector2, , int, s, 16, 4, 5);
+  VDUP(vector2, , int, s, 32, 2, 1);
+  VDUP(vector2, , uint, u, 8, 8, 15);
+  VDUP(vector2, , uint, u, 16, 4, 5);
+  VDUP(vector2, , uint, u, 32, 2, 1);
+  VDUP(vector2, q, int, s, 8, 16, 15);
+  VDUP(vector2, q, int, s, 16, 8, 5);
+  VDUP(vector2, q, int, s, 32, 4, 1);
+  VDUP(vector2, q, uint, u, 8, 16, 15);
+  VDUP(vector2, q, uint, u, 16, 8, 5);
+  VDUP(vector2, q, uint, u, 32, 4, 1);
+
+#define TEST_MACRO_NO64BIT_VARIANT_1_5(MACRO, VAR, T1, T2)	\
+  MACRO(VAR, , T1, T2, 8, 8);					\
+  MACRO(VAR, , T1, T2, 16, 4);					\
+  MACRO(VAR, , T1, T2, 32, 2);					\
+  MACRO(VAR, q, T1, T2, 8, 16);					\
+  MACRO(VAR, q, T1, T2, 16, 8);					\
+  MACRO(VAR, q, T1, T2, 32, 4)
+
+  /* Split the test, as both signed and unsigned variants output their
+     result in an unsigned form (thus the same output variable is used
+     in these tests).  */
+  TEST_MACRO_NO64BIT_VARIANT_1_5(TEST_BINARY_OP, INSN_NAME, int, s);
+
+#define CMT " (signed input)"
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_signed, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_signed, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_signed, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_signed, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_signed, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_signed, CMT);
+
+  TEST_MACRO_NO64BIT_VARIANT_1_5(TEST_BINARY_OP, INSN_NAME, uint, u);
+
+#undef CMT
+#define CMT " (unsigned input)"
+  CHECK(TEST_MSG, uint, 8, 8, PRIx8, expected_unsigned, CMT);
+  CHECK(TEST_MSG, uint, 16, 4, PRIx16, expected_unsigned, CMT);
+  CHECK(TEST_MSG, uint, 32, 2, PRIx32, expected_unsigned, CMT);
+  CHECK(TEST_MSG, uint, 8, 16, PRIx8, expected_unsigned, CMT);
+  CHECK(TEST_MSG, uint, 16, 8, PRIx16, expected_unsigned, CMT);
+  CHECK(TEST_MSG, uint, 32, 4, PRIx32, expected_unsigned, CMT);
+}
+
+int main (void)
+{
+  exec_vtst ();
+  return 0;
+}
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vuzp.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vuzp.c
@@ -9,14 +9,12 @@ VECT_VAR_DECL(expected0,int,8,8) [] = { 0xf0, 0xf1, 0xf2, 0xf3,
 VECT_VAR_DECL(expected0,int,16,4) [] = { 0xfff0, 0xfff1,
 					 0xfff2, 0xfff3 };
 VECT_VAR_DECL(expected0,int,32,2) [] = { 0xfffffff0, 0xfffffff1 };
-VECT_VAR_DECL(expected0,int,64,1) [] = { 0x3333333333333333 };
 VECT_VAR_DECL(expected0,uint,8,8) [] = { 0xf0, 0xf1, 0xf2, 0xf3,
 					 0xf4, 0xf5, 0xf6, 0xf7 };
 VECT_VAR_DECL(expected0,uint,16,4) [] = { 0xfff0, 0xfff1,
 					  0xfff2, 0xfff3 };
 VECT_VAR_DECL(expected0,uint,32,2) [] = { 0xfffffff0,
 					  0xfffffff1 };
-VECT_VAR_DECL(expected0,uint,64,1) [] = { 0x3333333333333333 };
 VECT_VAR_DECL(expected0,poly,8,8) [] = { 0xf0, 0xf1, 0xf2, 0xf3,
 					 0xf4, 0xf5, 0xf6, 0xf7 };
 VECT_VAR_DECL(expected0,poly,16,4) [] = { 0xfff0, 0xfff1,
@@ -32,8 +30,6 @@ VECT_VAR_DECL(expected0,int,16,8) [] = { 0xfff0, 0xfff1,
 					 0xfff6, 0xfff7 };
 VECT_VAR_DECL(expected0,int,32,4) [] = { 0xfffffff0, 0xfffffff1,
 					 0xfffffff2, 0xfffffff3 };
-VECT_VAR_DECL(expected0,int,64,2) [] = { 0x3333333333333333,
-					 0x3333333333333333 };
 VECT_VAR_DECL(expected0,uint,8,16) [] = { 0xf0, 0xf1, 0xf2, 0xf3,
 					  0xf4, 0xf5, 0xf6, 0xf7,
 					  0xf8, 0xf9, 0xfa, 0xfb,
@@ -44,8 +40,6 @@ VECT_VAR_DECL(expected0,uint,16,8) [] = { 0xfff0, 0xfff1,
 					  0xfff6, 0xfff7 };
 VECT_VAR_DECL(expected0,uint,32,4) [] = { 0xfffffff0, 0xfffffff1,
 					  0xfffffff2, 0xfffffff3 };
-VECT_VAR_DECL(expected0,uint,64,2) [] = { 0x3333333333333333,
-					  0x3333333333333333 };
 VECT_VAR_DECL(expected0,poly,8,16) [] = { 0xf0, 0xf1, 0xf2, 0xf3,
 					  0xf4, 0xf5, 0xf6, 0xf7,
 					  0xf8, 0xf9, 0xfa, 0xfb,
@@ -62,12 +56,10 @@ VECT_VAR_DECL(expected1,int,8,8) [] = { 0x11, 0x11, 0x11, 0x11,
 					0x11, 0x11, 0x11, 0x11 };
 VECT_VAR_DECL(expected1,int,16,4) [] = { 0x22, 0x22, 0x22, 0x22 };
 VECT_VAR_DECL(expected1,int,32,2) [] = { 0x33, 0x33 };
-VECT_VAR_DECL(expected1,int,64,1) [] = { 0x3333333333333333 };
 VECT_VAR_DECL(expected1,uint,8,8) [] = { 0x55, 0x55, 0x55, 0x55,
 					 0x55, 0x55, 0x55, 0x55 };
 VECT_VAR_DECL(expected1,uint,16,4) [] = { 0x66, 0x66, 0x66, 0x66 };
 VECT_VAR_DECL(expected1,uint,32,2) [] = { 0x77, 0x77 };
-VECT_VAR_DECL(expected1,uint,64,1) [] = { 0x3333333333333333 };
 VECT_VAR_DECL(expected1,poly,8,8) [] = { 0x55, 0x55, 0x55, 0x55,
 					 0x55, 0x55, 0x55, 0x55 };
 VECT_VAR_DECL(expected1,poly,16,4) [] = { 0x66, 0x66, 0x66, 0x66 };
@@ -79,8 +71,6 @@ VECT_VAR_DECL(expected1,int,8,16) [] = { 0x11, 0x11, 0x11, 0x11,
 VECT_VAR_DECL(expected1,int,16,8) [] = { 0x22, 0x22, 0x22, 0x22,
 					 0x22, 0x22, 0x22, 0x22 };
 VECT_VAR_DECL(expected1,int,32,4) [] = { 0x33, 0x33, 0x33, 0x33 };
-VECT_VAR_DECL(expected1,int,64,2) [] = { 0x3333333333333333,
-					 0x3333333333333333 };
 VECT_VAR_DECL(expected1,uint,8,16) [] = { 0x55, 0x55, 0x55, 0x55,
 					  0x55, 0x55, 0x55, 0x55,
 					  0x55, 0x55, 0x55, 0x55,
@@ -88,8 +78,6 @@ VECT_VAR_DECL(expected1,uint,8,16) [] = { 0x55, 0x55, 0x55, 0x55,
 VECT_VAR_DECL(expected1,uint,16,8) [] = { 0x66, 0x66, 0x66, 0x66,
 					  0x66, 0x66, 0x66, 0x66 };
 VECT_VAR_DECL(expected1,uint,32,4) [] = { 0x77, 0x77, 0x77, 0x77 };
-VECT_VAR_DECL(expected1,uint,64,2) [] = { 0x3333333333333333,
-					  0x3333333333333333 };
 VECT_VAR_DECL(expected1,poly,8,16) [] = { 0x55, 0x55, 0x55, 0x55,
 					  0x55, 0x55, 0x55, 0x55,
 					  0x55, 0x55, 0x55, 0x55,
--- a/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vzip.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vzip.c
@@ -9,13 +9,11 @@ VECT_VAR_DECL(expected0,int,8,8) [] = { 0xf0, 0xf4, 0x11, 0x11,
 VECT_VAR_DECL(expected0,int,16,4) [] = { 0xfff0, 0xfff2,
 					 0x22, 0x22 };
 VECT_VAR_DECL(expected0,int,32,2) [] = { 0xfffffff0, 0xfffffff1 };
-VECT_VAR_DECL(expected0,int,64,1) [] = { 0x3333333333333333 };
 VECT_VAR_DECL(expected0,uint,8,8) [] = { 0xf0, 0xf4, 0x55, 0x55,
 					 0xf1, 0xf5, 0x55, 0x55 };
 VECT_VAR_DECL(expected0,uint,16,4) [] = { 0xfff0, 0xfff2,
 					  0x66, 0x66 };
 VECT_VAR_DECL(expected0,uint,32,2) [] = { 0xfffffff0, 0xfffffff1 };
-VECT_VAR_DECL(expected0,uint,64,1) [] = { 0x3333333333333333 };
 VECT_VAR_DECL(expected0,poly,8,8) [] = { 0xf0, 0xf4, 0x55, 0x55,
 					 0xf1, 0xf5, 0x55, 0x55 };
 VECT_VAR_DECL(expected0,poly,16,4) [] = { 0xfff0, 0xfff2,
@@ -29,8 +27,6 @@ VECT_VAR_DECL(expected0,int,16,8) [] = { 0xfff0, 0xfff4, 0x22, 0x22,
 					 0xfff1, 0xfff5, 0x22, 0x22 };
 VECT_VAR_DECL(expected0,int,32,4) [] = { 0xfffffff0, 0xfffffff2,
 					 0x33, 0x33 };
-VECT_VAR_DECL(expected0,int,64,2) [] = { 0x3333333333333333,
-					 0x3333333333333333 };
 VECT_VAR_DECL(expected0,uint,8,16) [] = { 0xf0, 0xf8, 0x55, 0x55,
 					  0xf1, 0xf9, 0x55, 0x55,
 					  0xf2, 0xfa, 0x55, 0x55,
@@ -39,8 +35,6 @@ VECT_VAR_DECL(expected0,uint,16,8) [] = { 0xfff0, 0xfff4, 0x66, 0x66,
 					  0xfff1, 0xfff5, 0x66, 0x66 };
 VECT_VAR_DECL(expected0,uint,32,4) [] = { 0xfffffff0, 0xfffffff2,
 					  0x77, 0x77 };
-VECT_VAR_DECL(expected0,uint,64,2) [] = { 0x3333333333333333,
-					  0x3333333333333333 };
 VECT_VAR_DECL(expected0,poly,8,16) [] = { 0xf0, 0xf8, 0x55, 0x55,
 					  0xf1, 0xf9, 0x55, 0x55,
 					  0xf2, 0xfa, 0x55, 0x55,
@@ -56,13 +50,11 @@ VECT_VAR_DECL(expected1,int,8,8) [] = { 0xf2, 0xf6, 0x11, 0x11,
 VECT_VAR_DECL(expected1,int,16,4) [] = { 0xfff1, 0xfff3,
 					 0x22, 0x22 };
 VECT_VAR_DECL(expected1,int,32,2) [] = { 0x33, 0x33 };
-VECT_VAR_DECL(expected1,int,64,1) [] = { 0x3333333333333333 };
 VECT_VAR_DECL(expected1,uint,8,8) [] = { 0xf2, 0xf6, 0x55, 0x55,
 					 0xf3, 0xf7, 0x55, 0x55 };
 VECT_VAR_DECL(expected1,uint,16,4) [] = { 0xfff1, 0xfff3,
 					  0x66, 0x66 };
 VECT_VAR_DECL(expected1,uint,32,2) [] = { 0x77, 0x77 };
-VECT_VAR_DECL(expected1,uint,64,1) [] = { 0x3333333333333333 };
 VECT_VAR_DECL(expected1,poly,8,8) [] = { 0xf2, 0xf6, 0x55, 0x55,
 					 0xf3, 0xf7, 0x55, 0x55 };
 VECT_VAR_DECL(expected1,poly,16,4) [] = { 0xfff1, 0xfff3,
@@ -76,8 +68,6 @@ VECT_VAR_DECL(expected1,int,16,8) [] = { 0xfff2, 0xfff6, 0x22, 0x22,
 					 0xfff3, 0xfff7, 0x22, 0x22 };
 VECT_VAR_DECL(expected1,int,32,4) [] = { 0xfffffff1, 0xfffffff3,
 					 0x33, 0x33 };
-VECT_VAR_DECL(expected1,int,64,2) [] = { 0x3333333333333333,
-					 0x3333333333333333 };
 VECT_VAR_DECL(expected1,uint,8,16) [] = { 0xf4, 0xfc, 0x55, 0x55,
 					  0xf5, 0xfd, 0x55, 0x55,
 					  0xf6, 0xfe, 0x55, 0x55,
@@ -86,8 +76,6 @@ VECT_VAR_DECL(expected1,uint,16,8) [] = { 0xfff2, 0xfff6, 0x66, 0x66,
 					  0xfff3, 0xfff7, 0x66, 0x66 };
 VECT_VAR_DECL(expected1,uint,32,4) [] = { 0xfffffff1, 0xfffffff3,
 					  0x77, 0x77 };
-VECT_VAR_DECL(expected1,uint,64,2) [] = { 0x3333333333333333,
-					  0x3333333333333333 };
 VECT_VAR_DECL(expected1,poly,8,16) [] = { 0xf4, 0xfc, 0x55, 0x55,
 					  0xf5, 0xfd, 0x55, 0x55,
 					  0xf6, 0xfe, 0x55, 0x55,
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/arm_align_max_pwr.c
@@ -0,0 +1,15 @@
+/* { dg-do run } */
+
+#include <stdio.h>
+#include <assert.h>
+
+#define align (1ul << __ARM_ALIGN_MAX_PWR)
+static int x __attribute__ ((aligned (align)));
+
+int
+main ()
+{
+  assert ((((unsigned long)&x) & (align - 1)) == 0);
+
+  return 0;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/arm_align_max_stack_pwr.c
@@ -0,0 +1,15 @@
+/* { dg-do run } */
+
+#include <stdio.h>
+#include <assert.h>
+
+#define align (1ul << __ARM_ALIGN_MAX_STACK_PWR)
+
+int
+main ()
+{
+  int x __attribute__ ((aligned (align)));
+
+  assert ((((unsigned long)&x) & (align - 1)) == 0);
+  return 0;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/atomic-inst-cas.c
@@ -0,0 +1,61 @@
+/* { dg-do compile } */
+/* { dg-options "-O2 -march=armv8-a+lse -fno-ipa-icf" } */
+
+/* Test ARMv8.1-A CAS instruction.  */
+
+#include "atomic-inst-ops.inc"
+
+#define TEST TEST_TWO
+
+#define CAS_ATOMIC(FN, TY, MODEL1, MODEL2)				\
+  int FNNAME (FN, TY) (TY* val, TY* foo, TY* bar)			\
+  {									\
+    int model_s = MODEL1;						\
+    int model_f = MODEL2;						\
+    /* The success memory ordering must be at least as strong as	\
+       the failure memory ordering.  */					\
+    if (model_s < model_f)						\
+      return 0;								\
+    /* Ignore invalid memory orderings.  */				\
+    if (model_f == __ATOMIC_RELEASE || model_f == __ATOMIC_ACQ_REL)	\
+      return 0;								\
+    return __atomic_compare_exchange_n (val, foo, bar, 0, model_s, model_f); \
+  }
+
+#define CAS_ATOMIC_NORETURN(FN, TY, MODEL1, MODEL2)			\
+  void FNNAME (FN, TY) (TY* val, TY* foo, TY* bar)			\
+  {									\
+    int model_s = MODEL1;						\
+    int model_f = MODEL2;						\
+    /* The success memory ordering must be at least as strong as	\
+       the failure memory ordering.  */					\
+    if (model_s < model_f)						\
+      return;								\
+    /* Ignore invalid memory orderings.  */				\
+    if (model_f == __ATOMIC_RELEASE || model_f == __ATOMIC_ACQ_REL)	\
+      return;								\
+    __atomic_compare_exchange_n (val, foo, bar, 0, model_s, model_f);	\
+  }
+
+TEST (cas_atomic, CAS_ATOMIC)
+TEST (cas_atomic_noreturn, CAS_ATOMIC_NORETURN)
+
+
+/* { dg-final { scan-assembler-times "casb\t" 4} } */
+/* { dg-final { scan-assembler-times "casab\t" 20} } */
+/* { dg-final { scan-assembler-times "caslb\t" 4} } */
+/* { dg-final { scan-assembler-times "casalb\t" 36} } */
+
+/* { dg-final { scan-assembler-times "cash\t" 4} } */
+/* { dg-final { scan-assembler-times "casah\t" 20} } */
+/* { dg-final { scan-assembler-times "caslh\t" 4} } */
+/* { dg-final { scan-assembler-times "casalh\t" 36} } */
+
+/* { dg-final { scan-assembler-times "cas\t" 8} } */
+/* { dg-final { scan-assembler-times "casa\t" 40} } */
+/* { dg-final { scan-assembler-times "casl\t" 8} } */
+/* { dg-final { scan-assembler-times "casal\t" 72} } */
+
+/* { dg-final { scan-assembler-not "ldaxr\t" } } */
+/* { dg-final { scan-assembler-not "stlxr\t" } } */
+/* { dg-final { scan-assembler-not "dmb" } } */
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/atomic-inst-ldadd.c
@@ -0,0 +1,87 @@
+/* { dg-do compile } */
+/* { dg-options "-O2 -march=armv8-a+lse -fno-ipa-icf" } */
+
+/* Test ARMv8.1-A Load-ADD instruction.  */
+
+#include "atomic-inst-ops.inc"
+
+#define TEST TEST_ONE
+
+#define LOAD_ADD(FN, TY, MODEL)						\
+  TY FNNAME (FN, TY) (TY* val, TY* foo)					\
+  {									\
+    return __atomic_fetch_add (val, foo, MODEL);			\
+  }
+
+#define LOAD_ADD_NORETURN(FN, TY, MODEL)				\
+  void FNNAME (FN, TY) (TY* val, TY* foo)				\
+  {									\
+    __atomic_fetch_add (val, foo, MODEL);				\
+  }
+
+#define LOAD_SUB(FN, TY, MODEL)						\
+  TY FNNAME (FN, TY) (TY* val, TY* foo)					\
+  {									\
+    return __atomic_fetch_sub (val, foo, MODEL);			\
+  }
+
+#define LOAD_SUB_NORETURN(FN, TY, MODEL)				\
+  void FNNAME (FN, TY) (TY* val, TY* foo)				\
+  {									\
+    __atomic_fetch_sub (val, foo, MODEL);				\
+  }
+
+#define ADD_LOAD(FN, TY, MODEL)						\
+  TY FNNAME (FN, TY) (TY* val, TY* foo)					\
+  {									\
+    return __atomic_add_fetch (val, foo, MODEL);			\
+  }
+
+#define ADD_LOAD_NORETURN(FN, TY, MODEL)				\
+  void FNNAME (FN, TY) (TY* val, TY* foo)				\
+  {									\
+    __atomic_add_fetch (val, foo, MODEL);				\
+  }
+
+#define SUB_LOAD(FN, TY, MODEL)						\
+  TY FNNAME (FN, TY) (TY* val, TY* foo)					\
+  {									\
+    return __atomic_sub_fetch (val, foo, MODEL);			\
+  }
+
+#define SUB_LOAD_NORETURN(FN, TY, MODEL)				\
+  void FNNAME (FN, TY) (TY* val, TY* foo)				\
+  {									\
+    __atomic_sub_fetch (val, foo, MODEL);				\
+  }
+
+TEST (load_add, LOAD_ADD)
+TEST (load_add_notreturn, LOAD_ADD_NORETURN)
+
+TEST (load_sub, LOAD_SUB)
+TEST (load_sub_notreturn, LOAD_SUB_NORETURN)
+
+TEST (add_load, ADD_LOAD)
+TEST (add_load_notreturn, ADD_LOAD_NORETURN)
+
+TEST (sub_load, SUB_LOAD)
+TEST (sub_load_notreturn, SUB_LOAD_NORETURN)
+
+/* { dg-final { scan-assembler-times "ldaddb\t" 16} } */
+/* { dg-final { scan-assembler-times "ldaddab\t" 32} } */
+/* { dg-final { scan-assembler-times "ldaddlb\t" 16} } */
+/* { dg-final { scan-assembler-times "ldaddalb\t" 32} } */
+
+/* { dg-final { scan-assembler-times "ldaddh\t" 16} } */
+/* { dg-final { scan-assembler-times "ldaddah\t" 32} } */
+/* { dg-final { scan-assembler-times "ldaddlh\t" 16} } */
+/* { dg-final { scan-assembler-times "ldaddalh\t" 32} } */
+
+/* { dg-final { scan-assembler-times "ldadd\t" 32} } */
+/* { dg-final { scan-assembler-times "ldadda\t" 64} } */
+/* { dg-final { scan-assembler-times "ldaddl\t" 32} } */
+/* { dg-final { scan-assembler-times "ldaddal\t" 64} } */
+
+/* { dg-final { scan-assembler-not "ldaxr\t" } } */
+/* { dg-final { scan-assembler-not "stlxr\t" } } */
+/* { dg-final { scan-assembler-not "dmb" } } */
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/atomic-inst-ldlogic.c
@@ -0,0 +1,155 @@
+/* { dg-do compile } */
+/* { dg-options "-O2 -march=armv8-a+lse -fno-ipa-icf" } */
+
+/* Test ARMv8.1-A LD<logic-op> instruction.  */
+
+#include "atomic-inst-ops.inc"
+
+#define TEST TEST_ONE
+
+#define LOAD_OR(FN, TY, MODEL)						\
+  TY FNNAME (FN, TY) (TY* val, TY* foo)					\
+  {									\
+    return __atomic_fetch_or (val, foo, MODEL);				\
+  }
+
+#define LOAD_OR_NORETURN(FN, TY, MODEL)					\
+  void FNNAME (FN, TY) (TY* val, TY* foo)				\
+  {									\
+    __atomic_fetch_or (val, foo, MODEL);				\
+  }
+
+#define LOAD_AND(FN, TY, MODEL)						\
+  TY FNNAME (FN, TY) (TY* val, TY* foo)					\
+  {									\
+    return __atomic_fetch_and (val, foo, MODEL);			\
+  }
+
+#define LOAD_AND_NORETURN(FN, TY, MODEL)				\
+  void FNNAME (FN, TY) (TY* val, TY* foo)				\
+  {									\
+    __atomic_fetch_and (val, foo, MODEL);				\
+  }
+
+#define LOAD_XOR(FN, TY, MODEL)						\
+  TY FNNAME (FN, TY) (TY* val, TY* foo)					\
+  {									\
+    return __atomic_fetch_xor (val, foo, MODEL);			\
+  }
+
+#define LOAD_XOR_NORETURN(FN, TY, MODEL)				\
+  void FNNAME (FN, TY) (TY* val, TY* foo)				\
+  {									\
+    __atomic_fetch_xor (val, foo, MODEL);				\
+  }
+
+#define OR_LOAD(FN, TY, MODEL)						\
+  TY FNNAME (FN, TY) (TY* val, TY* foo)					\
+  {									\
+    return __atomic_or_fetch (val, foo, MODEL);				\
+  }
+
+#define OR_LOAD_NORETURN(FN, TY, MODEL)					\
+  void FNNAME (FN, TY) (TY* val, TY* foo)				\
+  {									\
+    __atomic_or_fetch (val, foo, MODEL);				\
+  }
+
+#define AND_LOAD(FN, TY, MODEL)						\
+  TY FNNAME (FN, TY) (TY* val, TY* foo)					\
+  {									\
+    return __atomic_and_fetch (val, foo, MODEL);			\
+  }
+
+#define AND_LOAD_NORETURN(FN, TY, MODEL)				\
+  void FNNAME (FN, TY) (TY* val, TY* foo)				\
+  {									\
+    __atomic_and_fetch (val, foo, MODEL);				\
+  }
+
+#define XOR_LOAD(FN, TY, MODEL)						\
+  TY FNNAME (FN, TY) (TY* val, TY* foo)					\
+  {									\
+    return __atomic_xor_fetch (val, foo, MODEL);			\
+  }
+
+#define XOR_LOAD_NORETURN(FN, TY, MODEL)				\
+  void FNNAME (FN, TY) (TY* val, TY* foo)				\
+  {									\
+    __atomic_xor_fetch (val, foo, MODEL);				\
+  }
+
+
+TEST (load_or, LOAD_OR)
+TEST (load_or_notreturn, LOAD_OR_NORETURN)
+
+TEST (load_and, LOAD_AND)
+TEST (load_and_notreturn, LOAD_AND_NORETURN)
+
+TEST (load_xor, LOAD_XOR)
+TEST (load_xor_notreturn, LOAD_XOR_NORETURN)
+
+TEST (or_load, OR_LOAD)
+TEST (or_load_notreturn, OR_LOAD_NORETURN)
+
+TEST (and_load, AND_LOAD)
+TEST (and_load_notreturn, AND_LOAD_NORETURN)
+
+TEST (xor_load, XOR_LOAD)
+TEST (xor_load_notreturn, XOR_LOAD_NORETURN)
+
+
+/* Load-OR.  */
+
+/* { dg-final { scan-assembler-times "ldsetb\t" 8} } */
+/* { dg-final { scan-assembler-times "ldsetab\t" 16} } */
+/* { dg-final { scan-assembler-times "ldsetlb\t" 8} } */
+/* { dg-final { scan-assembler-times "ldsetalb\t" 16} } */
+
+/* { dg-final { scan-assembler-times "ldseth\t" 8} } */
+/* { dg-final { scan-assembler-times "ldsetah\t" 16} } */
+/* { dg-final { scan-assembler-times "ldsetlh\t" 8} } */
+/* { dg-final { scan-assembler-times "ldsetalh\t" 16} } */
+
+/* { dg-final { scan-assembler-times "ldset\t" 16} } */
+/* { dg-final { scan-assembler-times "ldseta\t" 32} } */
+/* { dg-final { scan-assembler-times "ldsetl\t" 16} } */
+/* { dg-final { scan-assembler-times "ldsetal\t" 32} } */
+
+/* Load-AND.  */
+
+/* { dg-final { scan-assembler-times "ldclrb\t" 8} } */
+/* { dg-final { scan-assembler-times "ldclrab\t" 16} } */
+/* { dg-final { scan-assembler-times "ldclrlb\t" 8} } */
+/* { dg-final { scan-assembler-times "ldclralb\t" 16} } */
+
+/* { dg-final { scan-assembler-times "ldclrh\t" 8} } */
+/* { dg-final { scan-assembler-times "ldclrah\t" 16} } */
+/* { dg-final { scan-assembler-times "ldclrlh\t" 8} } */
+/* { dg-final { scan-assembler-times "ldclralh\t" 16} } */
+
+/* { dg-final { scan-assembler-times "ldclr\t" 16} */
+/* { dg-final { scan-assembler-times "ldclra\t" 32} } */
+/* { dg-final { scan-assembler-times "ldclrl\t" 16} } */
+/* { dg-final { scan-assembler-times "ldclral\t" 32} } */
+
+/* Load-XOR.  */
+
+/* { dg-final { scan-assembler-times "ldeorb\t" 8} } */
+/* { dg-final { scan-assembler-times "ldeorab\t" 16} } */
+/* { dg-final { scan-assembler-times "ldeorlb\t" 8} } */
+/* { dg-final { scan-assembler-times "ldeoralb\t" 16} } */
+
+/* { dg-final { scan-assembler-times "ldeorh\t" 8} } */
+/* { dg-final { scan-assembler-times "ldeorah\t" 16} } */
+/* { dg-final { scan-assembler-times "ldeorlh\t" 8} } */
+/* { dg-final { scan-assembler-times "ldeoralh\t" 16} } */
+
+/* { dg-final { scan-assembler-times "ldeor\t" 16} */
+/* { dg-final { scan-assembler-times "ldeora\t" 32} } */
+/* { dg-final { scan-assembler-times "ldeorl\t" 16} } */
+/* { dg-final { scan-assembler-times "ldeoral\t" 32} } */
+
+/* { dg-final { scan-assembler-not "ldaxr\t" } } */
+/* { dg-final { scan-assembler-not "stlxr\t" } } */
+/* { dg-final { scan-assembler-not "dmb" } } */
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/atomic-inst-ops.inc
@@ -0,0 +1,66 @@
+/* Support code for atomic instruction tests.  */
+
+/* Define types names without spaces.  */
+typedef unsigned char uchar;
+typedef unsigned short ushort;
+typedef unsigned int uint;
+typedef long long longlong;
+typedef unsigned long long ulonglong;
+typedef __int128_t int128;
+typedef __uint128_t uint128;
+
+#define FNNAME(NAME,TY) NAME
+
+/* Expand one-model functions.  */
+#define TEST_M1(NAME, FN, TY, MODEL, DUMMY)	\
+  FN (test_##NAME##_##TY, TY, MODEL)
+
+/* Expand two-model functions.  */
+#define TEST_M2(NAME, FN, TY, MODEL1, MODEL2)	\
+  FN (test_##NAME##_##TY, TY, MODEL1, MODEL2)
+
+/* Typest to test.  */
+#define TEST_TY(NAME, FN, N, MODEL1, MODEL2)		\
+  TEST_M##N (NAME, FN, char, MODEL1, MODEL2)		\
+  TEST_M##N (NAME, FN, uchar, MODEL1, MODEL2)		\
+  TEST_M##N (NAME, FN, short, MODEL1, MODEL2)		\
+  TEST_M##N (NAME, FN, ushort, MODEL1, MODEL2)		\
+  TEST_M##N (NAME, FN, int, MODEL1, MODEL2)		\
+  TEST_M##N (NAME, FN, uint, MODEL1, MODEL2)		\
+  TEST_M##N (NAME, FN, longlong, MODEL1, MODEL2)	\
+  TEST_M##N (NAME, FN, ulonglong, MODEL1, MODEL2)	\
+  TEST_M##N (NAME, FN, int128, MODEL1, MODEL2)		\
+  TEST_M##N (NAME, FN, uint128, MODEL1, MODEL2)
+
+/* Models to test.  */
+#define TEST_MODEL(NAME, FN, N)					\
+  TEST_TY (NAME##_relaxed, FN, N, __ATOMIC_RELAXED, DUMMY)	\
+  TEST_TY (NAME##_consume, FN, N, __ATOMIC_CONSUME, DUMMY)	\
+  TEST_TY (NAME##_acquire, FN, N, __ATOMIC_ACQUIRE, DUMMY)	\
+  TEST_TY (NAME##_release, FN, N, __ATOMIC_RELEASE, DUMMY)	\
+  TEST_TY (NAME##_acq_rel, FN, N, __ATOMIC_ACQ_REL, DUMMY)	\
+  TEST_TY (NAME##_seq_cst, FN, N, __ATOMIC_SEQ_CST, DUMMY)	\
+
+/* Cross-product of models to test.  */
+#define TEST_MODEL_M1(NAME, FN, N, M)			\
+  TEST_TY (NAME##_relaxed, FN, N, M, __ATOMIC_RELAXED)	\
+  TEST_TY (NAME##_consume, FN, N, M, __ATOMIC_CONSUME)	\
+  TEST_TY (NAME##_acquire, FN, N, M, __ATOMIC_ACQUIRE)	\
+  TEST_TY (NAME##_release, FN, N, M, __ATOMIC_RELEASE)	\
+  TEST_TY (NAME##_acq_rel, FN, N, M, __ATOMIC_ACQ_REL)	\
+  TEST_TY (NAME##_seq_cst, FN, N, M, __ATOMIC_SEQ_CST)	\
+
+#define TEST_MODEL_M2(NAME, FN)					\
+  TEST_MODEL_M1 (NAME##_relaxed, FN, 2, __ATOMIC_RELAXED)	\
+  TEST_MODEL_M1 (NAME##_consume, FN, 2, __ATOMIC_CONSUME)	\
+  TEST_MODEL_M1 (NAME##_acquire, FN, 2, __ATOMIC_ACQUIRE)	\
+  TEST_MODEL_M1 (NAME##_release, FN, 2, __ATOMIC_RELEASE)	\
+  TEST_MODEL_M1 (NAME##_acq_rel, FN, 2, __ATOMIC_ACQ_REL)	\
+  TEST_MODEL_M1 (NAME##_seq_cst, FN, 2, __ATOMIC_SEQ_CST)	\
+
+/* Expand functions for a cross-product of memory models and types.  */
+#define TEST_TWO(NAME, FN) TEST_MODEL_M2 (NAME, FN)
+
+/* Expand functions for a set of memory models and types.  */
+#define TEST_ONE(NAME, FN) TEST_MODEL (NAME, FN, 1)
+
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/atomic-inst-swp.c
@@ -0,0 +1,44 @@
+/* { dg-do compile } */
+/* { dg-options "-O2 -march=armv8-a+lse -fno-ipa-icf" } */
+
+/* Test ARMv8.1-A SWP instruction.  */
+
+#include "atomic-inst-ops.inc"
+
+#define TEST TEST_ONE
+
+#define SWAP_ATOMIC(FN, TY, MODEL)					\
+  TY FNNAME (FN, TY) (TY* val, TY foo)					\
+  {									\
+    return __atomic_exchange_n (val, foo, MODEL);			\
+  }
+
+#define SWAP_ATOMIC_NORETURN(FN, TY, MODEL)				\
+  void FNNAME (FN, TY) (TY* val, TY* foo, TY* bar)			\
+  {									\
+    __atomic_exchange (val, foo, bar, MODEL);				\
+  }
+
+
+TEST (swap_atomic, SWAP_ATOMIC)
+TEST (swap_atomic_noreturn, SWAP_ATOMIC_NORETURN)
+
+
+/* { dg-final { scan-assembler-times "swpb\t" 4} } */
+/* { dg-final { scan-assembler-times "swpab\t" 8} } */
+/* { dg-final { scan-assembler-times "swplb\t" 4} } */
+/* { dg-final { scan-assembler-times "swpalb\t" 8} } */
+
+/* { dg-final { scan-assembler-times "swph\t" 4} } */
+/* { dg-final { scan-assembler-times "swpah\t" 8} } */
+/* { dg-final { scan-assembler-times "swplh\t" 4} } */
+/* { dg-final { scan-assembler-times "swpalh\t" 8} } */
+
+/* { dg-final { scan-assembler-times "swp\t" 8} } */
+/* { dg-final { scan-assembler-times "swpa\t" 16} } */
+/* { dg-final { scan-assembler-times "swpl\t" 8} } */
+/* { dg-final { scan-assembler-times "swpal\t" 16} } */
+
+/* { dg-final { scan-assembler-not "ldaxr\t" } } */
+/* { dg-final { scan-assembler-not "stlxr\t" } } */
+/* { dg-final { scan-assembler-not "dmb" } } */
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/c-output-template-4.c
@@ -0,0 +1,10 @@
+/* { dg-do compile } */
+/* { dg-options "-O0" } */
+
+void
+test (void)
+{
+    __asm__ ("@ %c0" : : "S" (&test + 4));
+}
+
+/* { dg-final { scan-assembler "@ test\\+4" } } */
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/fmovd-zero-mem.c
@@ -0,0 +1,10 @@
+/* { dg-do compile } */
+/* { dg-options "-O2" } */
+
+void
+foo (double *output)
+{
+  *output = 0.0;
+}
+
+/* { dg-final { scan-assembler "str\\txzr, \\\[x0\\\]" } } */
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/fmovd-zero-reg.c
@@ -0,0 +1,11 @@
+/* { dg-do compile } */
+/* { dg-options "-O2" } */
+
+void bar (double);
+void
+foo (void)
+{
+  bar (0.0);
+}
+
+/* { dg-final { scan-assembler "fmov\\td0, xzr" } } */
--- a/src/gcc/testsuite/gcc.target/aarch64/fmovd-zero.c
+++ b/src//dev/null
@@ -1,10 +0,0 @@
-/* { dg-do compile } */
-/* { dg-options "-O2" } */
-
-void
-foo (double *output)
-{
-  *output = 0.0;
-}
-
-/* { dg-final { scan-assembler "fmov\\td\[0-9\]+, xzr" } } */
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/fmovf-zero-mem.c
@@ -0,0 +1,10 @@
+/* { dg-do compile } */
+/* { dg-options "-O2" } */
+
+void
+foo (float *output)
+{
+  *output = 0.0;
+}
+
+/* { dg-final { scan-assembler "str\\twzr, \\\[x0\\\]" } } */
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/fmovf-zero-reg.c
@@ -0,0 +1,11 @@
+/* { dg-do compile } */
+/* { dg-options "-O2" } */
+
+void bar (float);
+void
+foo (void)
+{
+  bar (0.0);
+}
+
+/* { dg-final { scan-assembler "fmov\\ts0, wzr" } } */
--- a/src/gcc/testsuite/gcc.target/aarch64/fmovf-zero.c
+++ b/src//dev/null
@@ -1,10 +0,0 @@
-/* { dg-do compile } */
-/* { dg-options "-O2" } */
-
-void
-foo (float *output)
-{
-  *output = 0.0;
-}
-
-/* { dg-final { scan-assembler "fmov\\ts\[0-9\]+, wzr" } } */
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/fmovld-zero-mem.c
@@ -0,0 +1,10 @@
+/* { dg-do compile } */
+/* { dg-options "-O2" } */
+
+void
+foo (long double *output)
+{
+  *output = 0.0;
+}
+
+/* { dg-final { scan-assembler "stp\\txzr, xzr, \\\[x0\\\]" } } */
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/fmovld-zero-reg.c
@@ -0,0 +1,11 @@
+/* { dg-do compile } */
+/* { dg-options "-O2" } */
+
+void bar (long double);
+void
+foo (void)
+{
+  bar (0.0);
+}
+
+/* { dg-final { scan-assembler "movi\\tv0\.2d, #0" } } */
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/mgeneral-regs_1.c
@@ -0,0 +1,10 @@
+/* { dg-options "-mgeneral-regs-only" } */
+
+typedef int int32x2_t __attribute__ ((__vector_size__ ((8))));
+
+/* { dg-error "'-mgeneral-regs-only' is incompatible with vector return type" "" {target "aarch64*-*-*"} 7 } */
+/* { dg-error "'-mgeneral-regs-only' is incompatible with vector argument" "" {target "aarch64*-*-*"} 7 } */
+int32x2_t test (int32x2_t a, int32x2_t b)
+{
+  return a + b;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/mgeneral-regs_2.c
@@ -0,0 +1,15 @@
+/* { dg-options "-mgeneral-regs-only" } */
+
+#include <stdarg.h>
+
+typedef int int32x2_t __attribute__ ((__vector_size__ ((8))));
+
+int
+test (int i, ...)
+{
+  va_list argp;
+  va_start (argp, i);
+  int32x2_t x = (int32x2_t) {0, 1};
+  x += va_arg (argp, int32x2_t); /* { dg-error "'-mgeneral-regs-only' is incompatible with vector varargs" } */
+  return x[0] + x[1];
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/mgeneral-regs_3.c
@@ -0,0 +1,11 @@
+/* { dg-options "-mgeneral-regs-only -O2" } */
+
+extern void abort (void);
+
+int
+test (int i, ...)
+{
+  float f = (float) i; /* { dg-error "'-mgeneral-regs-only' is incompatible with floating-point code" } */
+  if (f != f) abort ();
+  return 2;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/neg_abs_1.c
@@ -0,0 +1,17 @@
+/* { dg-do compile } */
+/* { dg-options "-save-temps -O2" } */
+
+int
+f1 (int x)
+{
+  return x < 0 ? x : -x;
+}
+
+long long
+f2 (long long x)
+{
+  return x < 0 ? x : -x;
+}
+
+/* { dg-final { scan-assembler-not "\tneg\tw\[0-9\]*.*" } } */
+/* { dg-final { scan-assembler-not "\tneg\tx\[0-9\]*.*" } } */
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/nofp_1.c
@@ -0,0 +1,19 @@
+/* { dg-skip-if "conflicting -march" { *-*-* } { "-march=*" } { "-march=*+nofp" } } */
+/* If there are multiple -march's, the latest wins; skip the test either way.
+   -march overrides -mcpu, so there is no possibility of conflict.  */
+
+/* { dg-options "-march=armv8-a+nofp" } */
+
+#include <stdarg.h>
+
+typedef int int32x2_t __attribute__ ((__vector_size__ ((8))));
+
+int test (int i, ...);
+
+int
+main (int argc, char **argv)
+{
+  int32x2_t a = (int32x2_t) {0, 1};
+  int32x2_t b = (int32x2_t) {2, 3};
+  return test (2, a, b); /* { dg-error "'\\+nofp' feature modifier is incompatible with vector argument" } */
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/pr65491_1.c
@@ -0,0 +1,11 @@
+/* { dg-do compile } */
+/* { dg-options "-O2" } */
+
+typedef long double a __attribute__((vector_size (16)));
+
+a
+sum (a first, a second)
+{
+  return first + second;
+}
+
--- a/src/gcc/testsuite/gcc.target/aarch64/singleton_intrinsics_1.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/singleton_intrinsics_1.c
@@ -235,8 +235,8 @@ test_vrshl_u64 (uint64x1_t a, int64x1_t b)
   return vrshl_u64 (a, b);
 }
 
-/* For int64x1_t, sshr...#63 is output instead of the equivalent cmlt...#0.  */
-/* { dg-final { scan-assembler-times "\\tsshr\\td\[0-9\]+" 2 } } */
+/* For int64x1_t, sshr...#63 is equivalent to cmlt...#0.  */
+/* { dg-final { scan-assembler-times "\\t(?:sshr|cmlt)\\td\[0-9\]+" 2 } } */
 
 int64x1_t
 test_vshr_n_s64 (int64x1_t a)
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/unsigned-float.c
@@ -0,0 +1,18 @@
+/* { dg-do compile } */
+/* { dg-options "-O1" } */
+
+#include <stdint.h>
+
+double
+f1 (uint16_t x)
+{
+  return (double)(float)x;
+}
+
+float
+f2 (uint16_t x)
+{
+  return (float)(double)x;
+}
+
+/* { dg-final { scan-assembler-not "fcvt" } } */
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/vec_init_1.c
@@ -0,0 +1,34 @@
+/* { dg-do run } */
+/* { dg-options "-O2 -fomit-frame-pointer --save-temps -fno-inline" } */
+
+extern void abort (void);
+
+typedef float float16x4_t __attribute__ ((vector_size ((16))));
+
+float a;
+float b;
+
+float16x4_t
+make_vector ()
+{
+  return (float16x4_t) { 0, 0, a, b };
+}
+
+int
+main (int argc, char **argv)
+{
+  a = 4.0;
+  b = 3.0;
+  float16x4_t vec = make_vector ();
+  if (vec[0] != 0 || vec[1] != 0 || vec[2] != a || vec[3] != b)
+    abort ();
+  return 0;
+}
+
+/* { dg-final { scan-assembler-times "ins\\t" 2 } } */
+/* What we want to check, is that make_vector does not stp the whole vector
+   to the stack.  Unfortunately here we scan the body of main() too, which may
+   be a bit fragile - the test is currently passing only because of the option
+   -fomit-frame-pointer which avoids use of stp in the prologue to main().  */
+/* { dg-final { scan-assembler-not "stp\\t" } } */
+/* { dg-final { cleanup-saved-temps } } */
--- a/src/gcc/testsuite/gcc.target/aarch64/vld1-vst1_1.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/vld1-vst1_1.c
@@ -40,6 +40,7 @@ THING (int16x8_t, 8, int16_t, q_s16)	\
 THING (uint16x8_t, 8, uint16_t, q_u16)	\
 THING (int32x4_t, 4, int32_t, q_s32)	\
 THING (uint32x4_t, 4, uint32_t, q_u32)	\
+THING (float32x4_t, 4, float32_t, q_f32)\
 THING (int64x2_t, 2, int64_t, q_s64)	\
 THING (uint64x2_t, 2, uint64_t, q_u64)	\
 THING (float64x2_t, 2, float64_t, q_f64)
--- a/src/gcc/testsuite/gcc.target/aarch64/vld1_lane.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/vld1_lane.c
@@ -56,7 +56,7 @@ VARIANTS (TESTMETH)
 
 #define CHECK(BASE, Q, ELTS, SUFFIX, LANE)			\
   if (test_vld1##Q##_lane##SUFFIX ((const BASE##_t *)orig_data,	\
-				   BASE##_data) != 0)	\
+				   & BASE##_data) != 0)	\
     abort ();
 
 int
@@ -65,20 +65,19 @@ main (int argc, char **argv)
   /* Original data for all vector formats.  */
   uint64_t orig_data[2] = {0x1234567890abcdefULL, 0x13579bdf02468aceULL};
 
-  /* Data with which vldN_lane will overwrite some of previous.  */
-  uint8_t uint8_data[4] = { 7, 11, 13, 17 };
-  uint16_t uint16_data[4] = { 257, 263, 269, 271 };
-  uint32_t uint32_data[4] = { 65537, 65539, 65543, 65551 };
-  uint64_t uint64_data[4] = { 0xdeadbeefcafebabeULL, 0x0123456789abcdefULL,
-			      0xfedcba9876543210LL, 0xdeadbabecafebeefLL };
-  int8_t int8_data[4] = { -1, 3, -5, 7 };
-  int16_t int16_data[4] = { 257, -259, 261, -263 };
-  int32_t int32_data[4] = { 123456789, -987654321, -135792468, 975318642 };
-  int64_t *int64_data = (int64_t *)uint64_data;
-  poly8_t poly8_data[4] = { 0, 7, 13, 18, };
-  poly16_t poly16_data[4] = { 11111, 2222, 333, 44 };
-  float32_t float32_data[4] = { 3.14159, 2.718, 1.414, 100.0 };
-  float64_t float64_data[4] = { 1.010010001, 12345.6789, -9876.54321, 1.618 };
+  /* Data with which vld1_lane will overwrite one element of previous.  */
+  uint8_t uint8_data = 7;
+  uint16_t uint16_data = 257;
+  uint32_t uint32_data = 65537;
+  uint64_t uint64_data = 0xdeadbeefcafebabeULL;
+  int8_t int8_data = -1;
+  int16_t int16_data = -259;
+  int32_t int32_data = -987654321;
+  int64_t int64_data = 0x1234567890abcdefLL;
+  poly8_t poly8_data = 13;
+  poly16_t poly16_data = 11111;
+  float32_t float32_data = 3.14159;
+  float64_t float64_data = 1.010010001;
 
   VARIANTS (CHECK);
   return 0;
--- a/src/gcc/testsuite/gcc.target/aarch64/vldN_lane_1.c
+++ b/src/gcc/testsuite/gcc.target/aarch64/vldN_lane_1.c
@@ -54,11 +54,11 @@ test_vld##STRUCT##Q##_lane##SUFFIX (const BASE##_t *data,		\
 }
 
 
-/* Tests of vld2_dup and vld2q_dup.  */
+/* Tests of vld2_lane and vld2q_lane.  */
 VARIANTS (TESTMETH, 2)
-/* Tests of vld3_dup and vld3q_dup.  */
+/* Tests of vld3_lane and vld3q_lane.  */
 VARIANTS (TESTMETH, 3)
-/* Tests of vld4_dup and vld4q_dup.  */
+/* Tests of vld4_lane and vld4q_lane.  */
 VARIANTS (TESTMETH, 4)
 
 #define CHECK(BASE, Q, ELTS, SUFFIX, LANE, STRUCT)			\
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/aarch64/vstN_lane_1.c
@@ -0,0 +1,75 @@
+/* { dg-do run } */
+/* { dg-options "-O3 -fno-inline" } */
+
+#include <arm_neon.h>
+
+extern void abort (void);
+
+#define VARIANTS(VARIANT, STRUCT)	\
+VARIANT (uint8, , 8, _u8, 6, STRUCT)	\
+VARIANT (uint16, , 4, _u16, 3, STRUCT)	\
+VARIANT (uint32, , 2, _u32, 1, STRUCT)	\
+VARIANT (uint64, , 1, _u64, 0, STRUCT)	\
+VARIANT (int8, , 8, _s8, 5, STRUCT)	\
+VARIANT (int16, , 4, _s16, 2, STRUCT)	\
+VARIANT (int32, , 2, _s32, 0, STRUCT)	\
+VARIANT (int64, , 1, _s64, 0, STRUCT)	\
+VARIANT (poly8, , 8, _p8, 7, STRUCT)	\
+VARIANT (poly16, , 4, _p16, 1, STRUCT)	\
+VARIANT (float32, , 2, _f32, 1, STRUCT)	\
+VARIANT (float64, , 1, _f64, 0, STRUCT)	\
+VARIANT (uint8, q, 16, _u8, 14, STRUCT)	\
+VARIANT (uint16, q, 8, _u16, 4, STRUCT)	\
+VARIANT (uint32, q, 4, _u32, 3, STRUCT)	\
+VARIANT (uint64, q, 2, _u64, 0, STRUCT)	\
+VARIANT (int8, q, 16, _s8, 13, STRUCT)	\
+VARIANT (int16, q, 8, _s16, 6, STRUCT)	\
+VARIANT (int32, q, 4, _s32, 2, STRUCT)	\
+VARIANT (int64, q, 2, _s64, 1, STRUCT)	\
+VARIANT (poly8, q, 16, _p8, 12, STRUCT)	\
+VARIANT (poly16, q, 8, _p16, 5, STRUCT)	\
+VARIANT (float32, q, 4, _f32, 1, STRUCT)\
+VARIANT (float64, q, 2, _f64, 0, STRUCT)
+
+#define TESTMETH(BASE, Q, ELTS, SUFFIX, LANE, STRUCT)			\
+int									\
+test_vst##STRUCT##Q##_lane##SUFFIX (const BASE##_t *data)		\
+{									\
+  BASE##x##ELTS##x##STRUCT##_t vectors;					\
+  for (int i = 0; i < STRUCT; i++, data += ELTS)			\
+    vectors.val[i] = vld1##Q##SUFFIX (data);				\
+  BASE##_t temp[STRUCT];						\
+  vst##STRUCT##Q##_lane##SUFFIX (temp, vectors, LANE);			\
+  for (int i = 0; i < STRUCT; i++)					\
+    {									\
+      if (temp[i] != vget##Q##_lane##SUFFIX (vectors.val[i], LANE))	\
+	return 1;							\
+    }									\
+  return 0;								\
+}
+
+/* Tests of vst2_lane and vst2q_lane.  */
+VARIANTS (TESTMETH, 2)
+/* Tests of vst3_lane and vst3q_lane.  */
+VARIANTS (TESTMETH, 3)
+/* Tests of vst4_lane and vst4q_lane.  */
+VARIANTS (TESTMETH, 4)
+
+#define CHECK(BASE, Q, ELTS, SUFFIX, LANE, STRUCT)			\
+  if (test_vst##STRUCT##Q##_lane##SUFFIX ((const BASE##_t *)orig_data))	\
+    abort ();
+
+int
+main (int argc, char **argv)
+{
+  /* Original data for all vector formats.  */
+  uint64_t orig_data[8] = {0x1234567890abcdefULL, 0x13579bdf02468aceULL,
+			   0x012389ab4567cdefULL, 0xfeeddadacafe0431ULL,
+			   0x1032547698badcfeULL, 0xbadbadbadbad0badULL,
+			   0x0102030405060708ULL, 0x0f0e0d0c0b0a0908ULL};
+
+  VARIANTS (CHECK, 2);
+  VARIANTS (CHECK, 3);
+  VARIANTS (CHECK, 4);
+  return 0;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/attr_arm-err.c
@@ -0,0 +1,13 @@
+/* Check that attribute target arm is rejected for M profile.  */
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_arm_ok } */
+/* { dg-skip-if "avoid conflicting multilib options" { *-*-* } { "-march=*" } { "-march=armv6-m" } } */
+/* { dg-add-options arm_arch_v6m } */
+
+int __attribute__((target("arm")))
+foo(int a)
+{  /* { dg-error "does not support" } */
+  return a ? 1 : 5;
+}
+
+
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/attr_arm.c
@@ -0,0 +1,11 @@
+/* Check that attribute target arm is recognized.  */
+/* { dg-do compile } */
+/* { dg-final { scan-assembler "\\.arm" } } */
+/* { dg-final { scan-assembler-not "\\.thumb_func" } } */
+
+int __attribute__((target("arm")))
+foo(int a)
+{
+  return a ? 1 : 5;
+}
+
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/attr_thumb-static.c
@@ -0,0 +1,25 @@
+/* Check that a change mode to a static function is correctly handled. */
+/* { dg-do run } */
+
+static void
+ __attribute__((__noinline__)) 
+foo (void)
+{
+  __asm__ ("");
+}
+
+static void
+__attribute__((__noinline__)) 
+__attribute__((target("thumb")))
+bar (void)
+{
+  __asm__ ("");
+}
+
+int
+main (void)
+{
+  foo();
+  bar();
+  return 0;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/attr_thumb-static2.c
@@ -0,0 +1,40 @@
+/* Check interwork between static functions for thumb2. */
+
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_arch_v7a_ok } */
+/* { dg-options "-O0 -march=armv7-a" } */
+
+struct _NSPoint
+{
+  float x;
+  float y;
+};
+
+typedef struct _NSPoint NSPoint;
+
+static NSPoint
+__attribute__ ((target("arm")))
+NSMakePoint (float x, float y)
+{
+  NSPoint point;
+  point.x = x;
+  point.y = y;
+  return point;
+}
+
+static NSPoint
+__attribute__ ((target("thumb")))
+RelativePoint (NSPoint point, NSPoint refPoint)
+{
+  return NSMakePoint (refPoint.x + point.x, refPoint.y + point.y);
+}
+
+NSPoint
+__attribute__ ((target("arm")))
+g(NSPoint refPoint)
+{
+  float pointA, pointB;
+  return RelativePoint (NSMakePoint (0, pointA), refPoint);
+}
+
+/* { dg-final { scan-assembler-times "blx" 2 } } */
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/attr_thumb.c
@@ -0,0 +1,15 @@
+/* Check that attribute target thumb is recognized. */
+/* { dg-do compile } */
+/* Make sure the current multilib supports thumb.  */
+/* { dg-skip-if "" { ! { arm_thumb1_ok || arm_thumb2_ok } } } */
+/* { dg-options "-O2 -mno-restrict-it" } */
+/* { dg-final { scan-assembler-not "\\.arm"  } } */
+/* { dg-final { scan-assembler "\\.thumb_func" } } */
+
+int __attribute__((target("thumb")))
+foo(int a)
+{
+  /* { dg-final { scan-assembler "ite" { target { arm_thumb2_ok } } } } */
+  return a ? 1 : 5;
+}
+
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/bics_1.c
@@ -0,0 +1,54 @@
+/* { dg-do run } */
+/* { dg-options "-O2 --save-temps -fno-inline" } */
+/* { dg-require-effective-target arm32 } */
+
+extern void abort (void);
+
+int
+bics_si_test1 (int a, int b, int c)
+{
+  int d = a & ~b;
+
+  /* { dg-final { scan-assembler-times "bics\tr\[0-9\]+, r\[0-9\]+, r\[0-9\]+" 2 } } */
+  if (d == 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+int
+bics_si_test2 (int a, int b, int c)
+{
+  int d = a & ~(b << 3);
+
+  /* { dg-final { scan-assembler-times "bics\tr\[0-9\]+, r\[0-9\]+, r\[0-9\]+, .sl \#3" 1 } } */
+  if (d == 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+int
+main ()
+{
+  int x;
+
+  x = bics_si_test1 (29, ~4, 5);
+  if (x != ((29 & 4) + ~4 + 5))
+    abort ();
+
+  x = bics_si_test1 (5, ~2, 20);
+  if (x != 25)
+    abort ();
+
+    x = bics_si_test2 (35, ~4, 5);
+  if (x != ((35 & ~(~4 << 3)) + ~4 + 5))
+    abort ();
+
+  x = bics_si_test2 (96, ~2, 20);
+  if (x != 116)
+  abort ();
+
+  return 0;
+}
+/* { dg-final { cleanup-saved-temps } } */
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/bics_2.c
@@ -0,0 +1,57 @@
+/* { dg-do run } */
+/* { dg-options "-O2 --save-temps -fno-inline" } */
+/* { dg-require-effective-target arm32 } */
+
+extern void abort (void);
+
+int
+bics_si_test1 (int a, int b, int c)
+{
+  int d = a & ~b;
+
+  /* { dg-final { scan-assembler-not "bics\tr\[0-9\]+, r\[0-9\]+, r\[0-9\]+" } } */
+  /* { dg-final { scan-assembler-times "bic\tr\[0-9\]+, r\[0-9\]+, r\[0-9\]+" 2 } } */
+  if (d <= 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+int
+bics_si_test2 (int a, int b, int c)
+{
+  int d = a & ~(b << 3);
+
+  /* { dg-final { scan-assembler-not "bics\tr\[0-9\]+, r\[0-9\]+, r\[0-9\]+, .sl \#3" } } */
+  /* { dg-final { scan-assembler "bic\tr\[0-9\]+, r\[0-9\]+, r\[0-9\]+, .sl \#3" } } */
+  if (d <= 0)
+    return a + c;
+  else
+    return b + d + c;
+}
+
+int
+main ()
+{
+  int x;
+
+  x = bics_si_test1 (29, ~4, 5);
+  if (x != ((29 & 4) + ~4 + 5))
+    abort ();
+
+  x = bics_si_test1 (5, ~2, 20);
+  if (x != 25)
+    abort ();
+
+  x = bics_si_test2 (35, ~4, 5);
+  if (x != ((35 & ~(~4 << 3)) + ~4 + 5))
+    abort ();
+
+  x = bics_si_test2 (96, ~2, 20);
+  if (x != 116)
+    abort ();
+
+  return 0;
+}
+
+/* { dg-final { cleanup-saved-temps } } */
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/bics_3.c
@@ -0,0 +1,41 @@
+/* { dg-do run } */
+/* { dg-options "-O2 --save-temps -fno-inline" } */
+/* { dg-require-effective-target arm32 } */
+
+extern void abort (void);
+
+int
+bics_si_test (int a, int b)
+{
+  if (a & ~b)
+    return 1;
+  else
+    return 0;
+}
+
+int
+bics_si_test2 (int a, int b)
+{
+  if (a & ~ (b << 2))
+    return 1;
+  else
+    return 0;
+}
+
+int
+main (void)
+{
+  int a = 5;
+  int b = 5;
+  int c = 20;
+  if (bics_si_test (a, b))
+    abort ();
+  if (bics_si_test2 (c, b))
+    abort ();
+  return 0;
+}
+
+/* { dg-final { scan-assembler-times "bics\tr\[0-9\]+, r\[0-9\]+, r\[0-9\]+" 2 } } */
+/* { dg-final { scan-assembler-times "bics\tr\[0-9\]+, r\[0-9\]+, r\[0-9\]+, .sl #2" 1 } } */
+
+/* { dg-final { cleanup-saved-temps } } */
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/bics_4.c
@@ -0,0 +1,49 @@
+/* { dg-do run } */
+/* { dg-options "-O2 --save-temps -fno-inline" } */
+/* { dg-require-effective-target arm32 } */
+
+extern void abort (void);
+
+int
+bics_si_test1 (int a, int b, int c)
+{
+  if ((a & b) == a)
+    return a;
+  else
+    return c;
+}
+
+int
+bics_si_test2 (int a, int b, int c)
+{
+  if ((a & b) == b)
+    return b;
+  else
+    return c;
+}
+
+int
+main ()
+{
+  int x;
+  x = bics_si_test1 (0xf00d, 0xf11f, 0);
+  if (x != 0xf00d)
+    abort ();
+
+  x = bics_si_test1 (0xf11f, 0xf00d, 0);
+  if (x != 0)
+    abort ();
+
+  x = bics_si_test2 (0xf00d, 0xf11f, 0);
+  if (x != 0)
+    abort ();
+
+  x = bics_si_test2 (0xf11f, 0xf00d, 0);
+  if (x != 0xf00d)
+    abort ();
+
+  return 0;
+}
+
+/* { dg-final { scan-assembler-times "bics\tr\[0-9\]+, r\[0-9\]+, r\[0-9\]+" 2 } } */
+/* { dg-final { cleanup-saved-temps } } */
--- a/src/gcc/testsuite/gcc.target/arm/fixed_float_conversion.c
+++ b/src/gcc/testsuite/gcc.target/arm/fixed_float_conversion.c
@@ -3,6 +3,7 @@
 /* { dg-require-effective-target arm_vfp3_ok } */
 /* { dg-options "-O1" } */
 /* { dg-add-options arm_vfp3 } */
+/* { dg-skip-if "need fp instructions" { *-*-* } { "-mfloat-abi=soft" } { "" } } */
 
 float
 fixed_to_float (int i)
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/flip-thumb.c
@@ -0,0 +1,26 @@
+/* Check -mflip-thumb. */
+/* { dg-do compile } */
+/* Make sure the current multilib supports thumb.  */
+/* { dg-skip-if "" { ! { arm_thumb1_ok || arm_thumb2_ok } } } */
+/* { dg-options "-O2 -mflip-thumb -mno-restrict-it" } */
+/* { dg-final { scan-assembler "\\.arm" } } */
+/* { dg-final { scan-assembler-times "\\.thumb_func" 1} } */
+
+int 
+foo(int a)
+{
+  return a ? 1 : 5;
+}
+
+int 
+bar(int a)
+{
+  return a ? 1 : 5;
+}
+
+/* { dg-final { scan-assembler-times "ite" 1 { target { arm_thumb2_ok } } } } */
+
+
+
+
+
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/maskdata.c
@@ -0,0 +1,15 @@
+/* { dg-do compile } */
+/* { dg-options " -O2" }  */
+/* { dg-require-effective-target arm_thumb2_ok } */
+
+#define MASK 0xff00ff
+void maskdata (int * data, int len)
+{
+   int i = len;
+   for (; i > 0; i -= 2)
+    {
+      data[i] &= MASK;
+      data[i + 1] &= MASK;
+    }
+}
+/* { dg-final { scan-assembler-not "65280" } } */
--- a/src/gcc/testsuite/gcc.target/arm/memset-inline-10.c
+++ b/src/gcc/testsuite/gcc.target/arm/memset-inline-10.c
@@ -1,5 +1,7 @@
 /* { dg-do compile } */
 /* { dg-options "-march=armv7-a -mfloat-abi=hard -mfpu=neon -O2" } */
+/* { dg-skip-if "need SIMD instructions" { *-*-* } { "-mfloat-abi=soft" } { "" } } */
+/* { dg-skip-if "need SIMD instructions" { *-*-* } { "-mfpu=vfp*" } { "" } } */
 
 #define BUF 100
 long a[BUF];
--- a/src/gcc/testsuite/gcc.target/arm/neon-reload-class.c
+++ b/src/gcc/testsuite/gcc.target/arm/neon-reload-class.c
@@ -15,4 +15,4 @@ _op_blend_p_caa_dp(unsigned *s, unsigned* e, unsigned *d, unsigned c) {
 
 /* These constants should be emitted as immediates rather than loaded from memory.  */
 
-/* { dg-final { scan-assembler-not "(\\.d?word|mov(w|t))" } } */
+/* { dg-final { scan-assembler-not "(\\.d?word)" } } */
--- a/src/gcc/testsuite/gcc.target/arm/neon/pr51534.c
+++ b/src/gcc/testsuite/gcc.target/arm/neon/pr51534.c
@@ -58,18 +58,18 @@ GEN_COND_TESTS(vceq)
 /* { dg-final { scan-assembler-times "vcge\.u16\[ 	\]+\[qQ\]\[0-9\]+, \[qQ\]\[0-9\]+, \[qQ\]\[0-9\]+" 2 } } */
 /* { dg-final { scan-assembler "vcge\.s32\[ 	\]+\[qQ\]\[0-9\]+, \[qQ\]\[0-9\]+, #0" } } */
 /* { dg-final { scan-assembler-times "vcge\.u32\[ 	\]+\[qQ\]\[0-9\]+, \[qQ\]\[0-9\]+, \[qQ\]\[0-9\]+" 2 } } */
-/* { dg-final { scan-assembler "vcgt\.s8\[ 	\]+\[dD\]\[0-9\]+, \[dD\]\[0-9\]+, \[dD\]\[0-9\]+" } } */
-/* { dg-final { scan-assembler "vcgt\.s16\[ 	\]+\[dD\]\[0-9\]+, \[dD\]\[0-9\]+, \[dD\]\[0-9\]+" } } */
-/* { dg-final { scan-assembler "vcgt\.s32\[ 	\]+\[dD\]\[0-9\]+, \[dD\]\[0-9\]+, \[dD\]\[0-9\]+" } } */
-/* { dg-final { scan-assembler "vcgt\.s8\[ 	\]+\[qQ\]\[0-9\]+, \[qQ\]\[0-9\]+, \[qQ\]\[0-9\]+" } } */
-/* { dg-final { scan-assembler "vcgt\.s16\[ 	\]+\[qQ\]\[0-9\]+, \[qQ\]\[0-9\]+, \[qQ\]\[0-9\]+" } } */
-/* { dg-final { scan-assembler "vcgt\.s32\[ 	\]+\[qQ\]\[0-9\]+, \[qQ\]\[0-9\]+, \[qQ\]\[0-9\]+" } } */
-/* { dg-final { scan-assembler "vcge\.s8\[ 	\]+\[dD\]\[0-9\]+, \[dD\]\[0-9\]+, \[dD\]\[0-9\]+" } } */
-/* { dg-final { scan-assembler "vcge\.s16\[ 	\]+\[dD\]\[0-9\]+, \[dD\]\[0-9\]+, \[dD\]\[0-9\]+" } } */
-/* { dg-final { scan-assembler "vcge\.s32\[ 	\]+\[dD\]\[0-9\]+, \[dD\]\[0-9\]+, \[dD\]\[0-9\]+" } } */
-/* { dg-final { scan-assembler "vcge\.s8\[ 	\]+\[qQ\]\[0-9\]+, \[qQ\]\[0-9\]+, \[qQ\]\[0-9\]+" } } */
-/* { dg-final { scan-assembler "vcge\.s16\[ 	\]+\[qQ\]\[0-9\]+, \[qQ\]\[0-9\]+, \[qQ\]\[0-9\]+" } } */
-/* { dg-final { scan-assembler "vcge\.s32\[ 	\]+\[qQ\]\[0-9\]+, \[qQ\]\[0-9\]+, \[qQ\]\[0-9\]+" } } */
+/* { dg-final { scan-assembler "vclt\.s8\[ 	\]+\[dD\]\[0-9\]+, \[dD\]\[0-9\]+, #0" } } */
+/* { dg-final { scan-assembler "vclt\.s16\[ 	\]+\[dD\]\[0-9\]+, \[dD\]\[0-9\]+, #0" } } */
+/* { dg-final { scan-assembler "vclt\.s32\[ 	\]+\[dD\]\[0-9\]+, \[dD\]\[0-9\]+, #0" } } */
+/* { dg-final { scan-assembler "vclt\.s8\[ 	\]+\[qQ\]\[0-9\]+, \[qQ\]\[0-9\]+, #0" } } */
+/* { dg-final { scan-assembler "vclt\.s16\[ 	\]+\[qQ\]\[0-9\]+, \[qQ\]\[0-9\]+, #0" } } */
+/* { dg-final { scan-assembler "vclt\.s32\[ 	\]+\[qQ\]\[0-9\]+, \[qQ\]\[0-9\]+, #0" } } */
+/* { dg-final { scan-assembler "vcle\.s8\[ 	\]+\[dD\]\[0-9\]+, \[dD\]\[0-9\]+, #0" } } */
+/* { dg-final { scan-assembler "vcle\.s16\[ 	\]+\[dD\]\[0-9\]+, \[dD\]\[0-9\]+, #0" } } */
+/* { dg-final { scan-assembler "vcle\.s32\[ 	\]+\[dD\]\[0-9\]+, \[dD\]\[0-9\]+, #0" } } */
+/* { dg-final { scan-assembler "vcle\.s8\[ 	\]+\[qQ\]\[0-9\]+, \[qQ\]\[0-9\]+, #0" } } */
+/* { dg-final { scan-assembler "vcle\.s16\[ 	\]+\[qQ\]\[0-9\]+, \[qQ\]\[0-9\]+, #0" } } */
+/* { dg-final { scan-assembler "vcle\.s32\[ 	\]+\[qQ\]\[0-9\]+, \[qQ\]\[0-9\]+, #0" } } */
 /* { dg-final { scan-assembler-times "vceq\.i8\[ 	\]+\[dD\]\[0-9\]+, \[dD\]\[0-9\]+, #0" 2 } } */
 /* { dg-final { scan-assembler-times "vceq\.i16\[ 	\]+\[dD\]\[0-9\]+, \[dD\]\[0-9\]+, #0" 2 } } */
 /* { dg-final { scan-assembler-times "vceq\.i32\[ 	\]+\[dD\]\[0-9\]+, \[dD\]\[0-9\]+, #0" 2 } } */
--- a/src/gcc/testsuite/gcc.target/arm/no-volatile-in-it.c
+++ b/src/gcc/testsuite/gcc.target/arm/no-volatile-in-it.c
@@ -1,5 +1,6 @@
 /* { dg-do compile } */
 /* { dg-require-effective-target arm_thumb2_ok } */
+/* { dg-skip-if "do not override -mcpu" { *-*-* } { "-march=*" "-mcpu=*" } { "-mcpu=cortex-m7" } } */
 /* { dg-options "-Os -mthumb -mcpu=cortex-m7" } */
 
 int
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/pr26702.c
@@ -0,0 +1,4 @@
+/* { dg-do compile { target arm*-*-eabi* } } */
+/* { dg-final { scan-assembler "\\.size\[\\t \]+static_foo, 4" } } */
+int foo;
+static int static_foo;
--- a/src/gcc/testsuite/gcc.target/arm/pr42172-1.c
+++ b/src/gcc/testsuite/gcc.target/arm/pr42172-1.c
@@ -16,4 +16,4 @@ void init_A (struct A *this)
   this->f4 = 0;
 }
 
-/* { dg-final { scan-assembler-times "ldr" 1 } } */
+/* { dg-final { scan-assembler-times "str" 1 } } */
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/pr43404.c
@@ -0,0 +1,10 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target naked_functions } */
+/* { dg-options "-O0" } */
+
+__attribute__ ((naked))
+void __data_abort(void)
+{
+  long foo; /* { dg-error "cannot allocate stack for variable" } */
+  long* bar = &foo;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/pr48470.c
@@ -0,0 +1,11 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target naked_functions } */
+/* { dg-options "-O0" } */
+
+extern void g(int *x);
+
+void __attribute__((naked)) f(void)
+{
+    int x = 0; /* { dg-error "cannot allocate stack for variable" } */
+    g(&x);
+}
--- a/src/gcc/testsuite/gcc.target/arm/pr58784.c
+++ b/src/gcc/testsuite/gcc.target/arm/pr58784.c
@@ -1,6 +1,8 @@
 /* { dg-do compile } */
 /* { dg-skip-if "incompatible options" { arm_thumb1 } { "*" } { "" } } */
 /* { dg-options "-march=armv7-a -mfloat-abi=hard -mfpu=neon -marm -O2" } */
+/* { dg-skip-if "need hardfp ABI" { *-*-* } { "-mfloat-abi=soft" } { "" } } */
+
 
 typedef struct __attribute__ ((__packed__))
 {
--- a/src/gcc/testsuite/gcc.target/arm/pr59985.C
+++ b/src/gcc/testsuite/gcc.target/arm/pr59985.C
@@ -1,6 +1,7 @@
 /* { dg-do compile } */
 /* { dg-skip-if "incompatible options" { arm_thumb1 } { "*" } { "" } } */
 /* { dg-options "-g -fcompare-debug -O2 -march=armv7-a -mtune=cortex-a9 -mfpu=vfpv3-d16 -mfloat-abi=hard" } */
+/* { dg-skip-if "need hardfp abi" { *-*-* } { "-mfloat-abi=soft" } { "" } } */
 
 extern void *f1 (unsigned long, unsigned long);
 extern const struct line_map *f2 (void *, int, unsigned int, const char *, unsigned int);
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/pr64208.c
@@ -0,0 +1,25 @@
+/* { dg-do compile } */
+/* { dg-skip-if "Test is specific to the iWMMXt" { arm*-*-* } { "-mcpu=*" } { "-mcpu=iwmmxt" } } */
+/* { dg-skip-if "Test is specific to the iWMMXt" { arm*-*-* } { "-mabi=*" } { "-mabi=iwmmxt" } } */
+/* { dg-skip-if "Test is specific to the iWMMXt" { arm*-*-* } { "-march=*" } { "-march=iwmmxt" } } */
+/* { dg-skip-if "Test is specific to ARM mode" { arm*-*-* } { "-mthumb" } { "" } } */
+/* { dg-require-effective-target arm32 } */
+/* { dg-require-effective-target arm_iwmmxt_ok } */
+/* { dg-options "-O1 -mcpu=iwmmxt" } */
+
+long long x6(void);
+void x7(long long, long long);
+void x8(long long);
+
+int x0;
+long long *x1;
+
+void x2(void) {
+  long long *x3 = x1;
+  while (x1) {
+    long long x4 = x0, x5 = x6();
+    x7(x4, x5);
+    x8(x5);
+    *x3 = 0;
+  }
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/pr64616.c
@@ -0,0 +1,15 @@
+/* { dg-do compile } */
+/* { dg-options "-O2 -fdump-rtl-cprop2" } */
+
+int f (int);
+unsigned int glob;
+
+void
+g ()
+{
+  while (f (glob));
+  glob = 5;
+}
+
+/* { dg-final { scan-rtl-dump "GLOBAL COPY-PROP" "cprop2" } } */
+/* { dg-final { cleanup-rtl-dump "cprop2" } } */
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/pr64744-1.c
@@ -0,0 +1,40 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target naked_functions } */
+/* { dg-options "-O0" } */
+
+__attribute__((naked))
+void foo1 ()
+{
+  int aa = 0;
+  int ab = {0};
+}
+
+__attribute__((naked))
+void foo2() {
+  char aa [ ] = {}; /* { dg-error "cannot allocate stack for variable" } */
+  char ab [1] = {};
+  char ac [2] = {}; /* { dg-error "cannot allocate stack for variable" } */
+  char ad [3] = {}; /* { dg-error "cannot allocate stack for variable" } */
+}
+
+__attribute__((naked))
+void foo3() {
+  char aa [1] = {0};
+  char ab [2] = {0}; /* { dg-error "cannot allocate stack for variable" } */
+  char ac [3] = {0}; /* { dg-error "cannot allocate stack for variable" } */
+  char ad [4] = {0}; /* { dg-error "cannot allocate stack for variable" } */
+}
+
+__attribute__((naked))
+void foo4() {
+  char aa [2] = {0,0}; /* { dg-error "cannot allocate stack for variable" } */
+}
+__attribute__((naked))
+void foo5() {
+  char aa [3] = {0,0,0}; /* { dg-error "cannot allocate stack for variable" } */
+}
+
+__attribute__((naked))
+void foo6() {
+  char aa [4] = {0,0,0,0}; /* { dg-error "cannot allocate stack for variable" } */
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/pr64744-2.c
@@ -0,0 +1,13 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target naked_functions } */
+/* { dg-options "-O0" } */
+
+struct s {
+  char a;
+    int b;
+};
+
+__attribute__((naked))
+void foo () {
+  struct s x = {}; /* { dg-error "cannot allocate stack for variable" } */
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/pr64818.c
@@ -0,0 +1,30 @@
+/* { dg-do compile } */
+/* { dg-options "-O1" } */
+
+char temp[16];
+extern int foo1 (void);
+
+void foo (void)
+{
+  int i;
+  int len;
+
+  while (1)
+  {
+    len = foo1 ();
+    register int a asm ("r0") = 5;
+    register char *b asm ("r1") = temp;
+    register int c asm ("r2") = len;
+    asm volatile ("mov %[r0], %[r0]\n  mov %[r1], %[r1]\n  mov %[r2], %[r2]\n"
+		   : "+m"(*b)
+		   : [r0]"r"(a), [r1]"r"(b), [r2]"r"(c));
+
+    for (i = 0; i < len; i++)
+    {
+      if (temp[i] == 10)
+      return;
+    }
+  }
+}
+
+/* { dg-final { scan-assembler "\[\\t \]+mov\ r1,\ r1" } } */
--- a/src/gcc/testsuite/gcc.target/arm/pr65067.c
+++ b/src/gcc/testsuite/gcc.target/arm/pr65067.c
@@ -1,4 +1,5 @@
 /* { dg-do compile } */
+/* { dg-require-effective-target arm_thumb2_ok } */
 /* { dg-options "-mthumb -mcpu=cortex-m3 -O2" } */
 
 struct tmp {
--- a/src/gcc/testsuite/gcc.target/arm/pr65647.c
+++ b/src/gcc/testsuite/gcc.target/arm/pr65647.c
@@ -1,4 +1,6 @@
 /* { dg-do compile } */
+/* { dg-require-effective-target arm_arch_v6m_ok } */
+/* { dg-skip-if "do not override -mfloat-abi" { *-*-* } { "-mfloat-abi=*" } {"-mfloat-abi=soft" } } */
 /* { dg-options "-march=armv6-m -mthumb -O3 -w -mfloat-abi=soft" } */
 
 a, b, c, e, g = &e, h, i = 7, l = 1, m, n, o, q = &m, r, s = &r, u, w = 9, x,
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/pr65710.c
@@ -0,0 +1,120 @@
+/* { dg-do compile } */
+/* { dg-skip-if "do not override -mfloat-abi" { *-*-* } { "-mfloat-abi=*" } {"-mfloat-abi=soft" } } */
+/* { dg-options "-mthumb -O2 -mfloat-abi=soft -w" } */
+/* { dg-skip-if "" { ! { arm_thumb1_ok || arm_thumb2_ok } } } */
+
+struct ST {
+  char *buffer;
+  int used;
+};
+
+struct ST *h;
+
+enum { no_op, duplicate, pop_failure_jump, dummy_failure_jump };
+
+typedef struct {
+  unsigned pointer;
+} byte_fail_stack_elt_t;
+
+typedef struct { unsigned avail; } byte_fail_stack_type;
+
+typedef union {
+  byte_fail_stack_elt_t word;
+  struct {
+    unsigned match_null_string_p : 2;
+    unsigned is_active : 1;
+    unsigned ever_matched_something : 1;
+  } bits;
+} byte_register_info_type;
+
+static int a;
+int b = 0;
+int c, e, f;
+int *d, *g;
+
+int
+byte_re_match_2_internal_size2(const int p2, int p3, const int p4) {
+  int i, p;
+  char *j;
+  char k, l, m, n = h;
+  byte_fail_stack_type o;
+  byte_fail_stack_elt_t *q;
+  unsigned int s = (unsigned int)h;
+  long t, u;
+  char **v, *w, **x, **y, **t1;
+  byte_register_info_type *z, *t2 = __builtin_alloca(s);
+  x = __builtin_alloca(s);
+  y = __builtin_alloca(s);
+  z = __builtin_alloca(sizeof(byte_register_info_type));
+  k = p4 + byte_re_match_2_internal_size2;
+  if (p3)
+    f = p4;
+  for (;;) {
+    if (h == h->used) {
+      g = f;
+      if (o.avail) {
+        b = 1;
+        for (; i < s; i++)
+          t1[i] = w;
+        goto fail;
+      }
+      e = 30 > s;
+      d = p4;
+      d[s] = 1;
+      return;
+    }
+    switch (*h->buffer++) {
+    case no_op:
+      while (m && n ?: *g)
+        ;
+      y[*h->buffer] = z[*h->buffer].bits.match_null_string_p ? w == &a ?: w : w;
+      w = g;
+      if (t) {
+        char r = h;
+        while (r && z[r].bits.is_active)
+          r--;
+        if (r == 0)
+          ;
+        else
+          u = r;
+      }
+      switch (*j++)
+      case dummy_failure_jump:
+      i = j;
+      if (i)
+        if (z[*h->buffer].bits.ever_matched_something) {
+          unsigned r;
+          z[*h->buffer].bits.ever_matched_something = r = *h->buffer;
+          for (; r + *(h->buffer + 1); r++) {
+            v = x[r];
+            w[r] = y[r];
+          }
+        }
+      break;
+    case duplicate: {
+      char *t3 = p2 + p3;
+      if (t3)
+        break;
+    }
+      if ((p3 ?: p4) == k)
+        goto fail;
+    case pop_failure_jump:
+      for (; c; c--)
+        t2[c].word = q[o.avail];
+      char t4;
+      q = t4 = __builtin_allocamemcpy(t4 ?: (p <<= 1));
+    }
+    continue;
+  fail : {
+    unsigned t5;
+    t = q;
+    t5 = u;
+    for (; t5 >= t; t5--)
+      v[t5] = q[--o.avail].pointer;
+    switch (*h->buffer)
+    case pop_failure_jump:
+    goto fail;
+  }
+    m = &l;
+  }
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/pr65729.c
@@ -0,0 +1,10 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_hard_vfp_ok } */
+/* { dg-options "-O2 -march=armv7-a -mfloat-abi=hard -mfpu=vfpv3-d16" } */
+
+int foo (void)
+{
+  double x = 0.0;
+  asm volatile ("" : "+gw" (x));
+  return x;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/pr65924.c
@@ -0,0 +1,9 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_thumb2_ok } */
+/* { dg-options "-O2 -mthumb" } */
+
+int a, b, c;
+int fn1() {
+  if (b + a < 0)
+    c = 0;
+}
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/pragma_attribute.c
@@ -0,0 +1,35 @@
+/* Test for #prama target macros.  */
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_thumb1_ok } */
+
+#pragma GCC target ("thumb")
+
+#ifndef __thumb__
+#error "__thumb__ is not defined"
+#endif
+
+#ifdef __thumb2__
+#ifndef __ARM_32BIT_STATE
+#error  "__ARM_32BIT_STATE is not defined"
+#endif
+#else /* thumb1 */
+#ifdef __ARM_32BIT_STATE
+#error  "__ARM_32BIT_STATE is defined"
+#endif
+#endif /* thumb1 */
+
+#pragma GCC target ("arm")
+
+#ifdef __thumb__
+#error "__thumb__ is defined"
+#endif
+
+#if defined (__thumb2__) || defined (__thumb1__)
+#error "thumb is defined"
+#endif 
+
+#ifndef __ARM_32BIT_STATE
+#error  "__ARM_32BIT_STATE is not defined"
+#endif
+
+#pragma GCC reset_options
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/reg_equal_test.c
@@ -0,0 +1,24 @@
+/* { dg-do compile } */
+/* { dg-options "-O1 -fdump-rtl-expand" } */
+
+extern void abort (void);
+unsigned int a = 1;
+
+int
+main (void)
+{
+  unsigned int b, c, d;
+
+  if (sizeof (int) != 4 || (int) 0xc7d24b5e > 0)
+    return 0;
+
+  c = 0xc7d24b5e;
+  d = a | -2;
+  b = (d == 0) ? c : (c % d);
+  if (b != c)
+    abort ();
+
+  return 0;
+}
+
+/* { dg-final { scan-rtl-dump "expr_list:REG_EQUAL \\(const_int -942519458" "expand" } } */
--- a/src/gcc/testsuite/gcc.target/arm/scd42-1.c
+++ b/src/gcc/testsuite/gcc.target/arm/scd42-1.c
@@ -1,7 +1,8 @@
 /* Verify that mov is preferred on XScale for loading a 1 byte constant. */
 /* { dg-do compile } */
 /* { dg-skip-if "incompatible options" { arm*-*-* } { "-march=*" } { "" } } */
-/* { dg-options "-mcpu=xscale -O" } */
+/* { dg-skip-if "do not override -mfloat-abi" { *-*-* } { "-mfloat-abi=*" } { "-mfloat-abi=softfp" } } */
+/* { dg-options "-mcpu=xscale -O -mfloat-abi=softfp" } */
 
 unsigned load1(void) __attribute__ ((naked));
 unsigned load1(void)
--- a/src/gcc/testsuite/gcc.target/arm/scd42-3.c
+++ b/src/gcc/testsuite/gcc.target/arm/scd42-3.c
@@ -2,7 +2,8 @@
 /* { dg-do compile } */
 /* { dg-skip-if "Test is specific to Xscale" { arm*-*-* } { "-march=*" } { "-march=xscale" } } */
 /* { dg-skip-if "Test is specific to Xscale" { arm*-*-* } { "-mcpu=*" } { "-mcpu=xscale" } } */
-/* { dg-options "-mcpu=xscale -O" } */
+/* { dg-skip-if "do not override -mfloat-abi" { *-*-* } { "-mfloat-abi=*" } { "-mfloat-abi=softfp" } } */
+/* { dg-options "-mcpu=xscale -O -mfloat-abi=softfp" } */
 
 unsigned load4(void) __attribute__ ((naked));
 unsigned load4(void)
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/short-it-ifcvt-1.c
@@ -0,0 +1,23 @@
+/* Test that ifcvt is not being too aggressive when -mrestrict-it.  */
+/* { dg-do compile } */
+/* { dg-options "-O2 -mrestrict-it" } */
+/* { dg-require-effective-target arm_thumb2_ok } */
+
+int
+f1(int x, int y, int z)
+{
+  if (x > 100)
+    {
+      x++;
+      z = -z;
+    }
+  else
+    {
+      x = -x;
+      y = -y;
+      z = 1;
+    }
+  return x + y + z;
+}
+
+/* { dg-final { scan-assembler "b(gt|le)" } } */
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/short-it-ifcvt-2.c
@@ -0,0 +1,21 @@
+/* Test that ifcvt is not being too aggressive when -mrestrict-it.  */
+/* { dg-do compile } */
+/* { dg-options "-O2 -mrestrict-it" } */
+/* { dg-require-effective-target arm_thumb2_ok } */
+
+int
+f1(int x, int y, int z)
+{
+  if (x > 100)
+    {
+      x++;
+      z = -z;
+    }
+  else
+    {
+      x = -x;
+      y = -y;
+    }
+  return x + y + z;
+}
+/* { dg-final { scan-assembler "b(gt|le)" } } */
--- a/src/gcc/testsuite/gcc.target/arm/simd/simd.exp
+++ b/src/gcc/testsuite/gcc.target/arm/simd/simd.exp
@@ -27,9 +27,22 @@ load_lib gcc-dg.exp
 # Initialize `dg'.
 dg-init
 
+# If the target hardware supports NEON, the default action is "run", otherwise
+# just "compile".
+global dg-do-what-default
+set save-dg-do-what-default ${dg-do-what-default}
+if {![check_effective_target_arm_neon_ok]} then {
+  return
+} elseif {[is-effective-target arm_neon_hw]} then {
+  set dg-do-what-default run
+} else {
+  set dg-do-what-default compile
+}
+
 # Main loop.
 dg-runtest [lsort [glob -nocomplain $srcdir/$subdir/*.\[cCS\]]] \
 	"" ""
 
 # All done.
+set dg-do-what-default ${save-dg-do-what-default}
 dg-finish
--- a/src/gcc/testsuite/gcc.target/arm/simd/vextQf32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vextQf32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vextQf32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O3 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vextQp16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vextQp16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vextQp16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O3 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vextQp64_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vextQp64_1.c
@@ -1,6 +1,5 @@
 /* Test the `vextQp64' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
 /* { dg-require-effective-target arm_crypto_ok } */
 /* { dg-options "-save-temps -O3 -fno-inline" } */
 /* { dg-add-options arm_crypto } */
--- a/src/gcc/testsuite/gcc.target/arm/simd/vextQp8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vextQp8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vextQp8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O3 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vextQs16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vextQs16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vextQs16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O3 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vextQs32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vextQs32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vextQs32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O3 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vextQs64_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vextQs64_1.c
@@ -1,7 +1,5 @@
 /* Test the `vextQs64' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O3 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vextQs8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vextQs8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vextQs8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O3 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vextQu16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vextQu16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vextQu16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O3 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vextQu32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vextQu32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vextQu32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O3 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vextQu64_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vextQu64_1.c
@@ -1,7 +1,5 @@
 /* Test the `vextQu64' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O3 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vextQu8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vextQu8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vextQu8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O3 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vextf32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vextf32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vextf32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O3 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vextp16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vextp16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vextp16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O3 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vextp64_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vextp64_1.c
@@ -1,6 +1,5 @@
 /* Test the `vextp64' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
 /* { dg-require-effective-target arm_crypto_ok } */
 /* { dg-options "-save-temps -O3 -fno-inline" } */
 /* { dg-add-options arm_crypto } */
--- a/src/gcc/testsuite/gcc.target/arm/simd/vextp8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vextp8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vextp8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O3 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vexts16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vexts16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vexts16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O3 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vexts32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vexts32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vexts32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O3 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vexts64_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vexts64_1.c
@@ -1,7 +1,5 @@
 /* Test the `vexts64' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O3 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vexts8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vexts8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vexts8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O3 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vextu16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vextu16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vextu16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O3 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vextu32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vextu32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vextu32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O3 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vextu64_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vextu64_1.c
@@ -1,7 +1,5 @@
 /* Test the `vextu64' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O3 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vextu8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vextu8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vextu8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O3 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev16p8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev16p8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev16p8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev16qp8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev16qp8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev16q_p8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev16qs8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev16qs8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev16q_s8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev16qu8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev16qu8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev16q_u8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev16s8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev16s8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev16s8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev16u8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev16u8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev16u8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev32p16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev32p16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev32p16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev32p8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev32p8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev32p8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev32qp16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev32qp16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev32q_p16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev32qp8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev32qp8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev32q_p8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev32qs16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev32qs16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev32q_s16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev32qs8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev32qs8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev32q_s8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev32qu16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev32qu16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev32q_u16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev32qu8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev32qu8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev32q_u8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev32s16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev32s16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev32s16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev32s8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev32s8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev32s8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev32u16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev32u16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev32u16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev32u8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev32u8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev32u8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev64f32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev64f32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev64f32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev64p16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev64p16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev64p16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev64p8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev64p8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev64p8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev64qf32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev64qf32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev64q_f32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev64qp16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev64qp16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev64q_p16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev64qp8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev64qp8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev64q_p8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev64qs16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev64qs16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev64q_s16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev64qs32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev64qs32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev64q_s32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev64qs8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev64qs8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev64q_s8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev64qu16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev64qu16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev64q_u16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev64qu32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev64qu32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev64q_u32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev64qu8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev64qu8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev64q_u8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev64s16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev64s16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev64s16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev64s32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev64s32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev64s32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev64s8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev64s8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev64s8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev64u16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev64u16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev64u16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev64u32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev64u32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev64u32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vrev64u8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vrev64u8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vrev64u8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vtrnf32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vtrnf32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vtrnf32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vtrnp16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vtrnp16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vtrnp16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vtrnp8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vtrnp8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vtrnp8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vtrnqf32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vtrnqf32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vtrnQf32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vtrnqp16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vtrnqp16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vtrnQp16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vtrnqp8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vtrnqp8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vtrnQp8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vtrnqs16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vtrnqs16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vtrnQs16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vtrnqs32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vtrnqs32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vtrnQs32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vtrnqs8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vtrnqs8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vtrnQs8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vtrnqu16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vtrnqu16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vtrnQu16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vtrnqu32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vtrnqu32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vtrnQu32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vtrnqu8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vtrnqu8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vtrnQu8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vtrns16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vtrns16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vtrns16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vtrns32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vtrns32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vtrns32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vtrns8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vtrns8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vtrns8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vtrnu16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vtrnu16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vtrnu16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vtrnu32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vtrnu32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vtrnu32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vtrnu8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vtrnu8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vtrnu8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vuzpf32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vuzpf32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vuzpf32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vuzpp16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vuzpp16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vuzpp16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vuzpp8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vuzpp8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vuzpp8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vuzpqf32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vuzpqf32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vuzpQf32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vuzpqp16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vuzpqp16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vuzpQp16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vuzpqp8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vuzpqp8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vuzpQp8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vuzpqs16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vuzpqs16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vuzpQs16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vuzpqs32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vuzpqs32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vuzpQs32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vuzpqs8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vuzpqs8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vuzpQs8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vuzpqu16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vuzpqu16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vuzpQu16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vuzpqu32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vuzpqu32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vuzpQu32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vuzpqu8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vuzpqu8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vuzpQu8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vuzps16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vuzps16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vuzps16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vuzps32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vuzps32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vuzps32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vuzps8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vuzps8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vuzps8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vuzpu16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vuzpu16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vuzpu16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vuzpu32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vuzpu32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vuzpu32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vuzpu8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vuzpu8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vuzpu8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vzipf32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vzipf32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vzipf32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vzipp16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vzipp16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vzipp16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vzipp8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vzipp8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vzipp8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vzipqf32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vzipqf32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vzipQf32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vzipqp16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vzipqp16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vzipQp16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vzipqp8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vzipqp8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vzipQp8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vzipqs16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vzipqs16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vzipQs16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vzipqs32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vzipqs32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vzipQs32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vzipqs8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vzipqs8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vzipQs8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vzipqu16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vzipqu16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vzipQu16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vzipqu32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vzipqu32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vzipQu32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vzipqu8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vzipqu8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vzipQu8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vzips16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vzips16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vzips16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vzips32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vzips32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vzips32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vzips8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vzips8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vzips8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vzipu16_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vzipu16_1.c
@@ -1,7 +1,5 @@
 /* Test the `vzipu16' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vzipu32_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vzipu32_1.c
@@ -1,7 +1,5 @@
 /* Test the `vzipu32' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/simd/vzipu8_1.c
+++ b/src/gcc/testsuite/gcc.target/arm/simd/vzipu8_1.c
@@ -1,7 +1,5 @@
 /* Test the `vzipu8' ARM Neon intrinsic.  */
 
-/* { dg-do run } */
-/* { dg-require-effective-target arm_neon_ok } */
 /* { dg-options "-save-temps -O1 -fno-inline" } */
 /* { dg-add-options arm_neon } */
 
--- a/src/gcc/testsuite/gcc.target/arm/thumb-ifcvt.c
+++ b/src/gcc/testsuite/gcc.target/arm/thumb-ifcvt.c
@@ -1,7 +1,7 @@
 /* Check that Thumb 16-bit shifts can be if-converted.  */
 /* { dg-do compile } */
 /* { dg-require-effective-target arm_thumb2_ok } */
-/* { dg-options "-O2 -mthumb" } */
+/* { dg-options "-O2 -mthumb -mno-restrict-it" } */
 
 int
 foo (int a, int b)
--- a/src//dev/null
+++ b/src/gcc/testsuite/gcc.target/arm/unsigned-float.c
@@ -0,0 +1,20 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_vfp_ok } */
+/* { dg-options "-march=armv7-a -O1 -mfloat-abi=softfp" } */
+/* { dg-skip-if "need fp instructions" { *-*-* } { "-mfloat-abi=soft" } { "" } } */
+
+#include <stdint.h>
+
+double
+f1 (uint16_t x)
+{
+  return (double)(float)x;
+}
+
+float
+f2 (uint16_t x)
+{
+  return (float)(double)x;
+}
+
+/* { dg-final { scan-assembler-not "vcvt.(f32.f64|f64.f32)" } } */
--- a/src/gcc/testsuite/gcc.target/arm/vfp-1.c
+++ b/src/gcc/testsuite/gcc.target/arm/vfp-1.c
@@ -1,6 +1,7 @@
 /* { dg-do compile } */
-/* { dg-options "-O2 -mfpu=vfp -mfloat-abi=softfp" } */
+/* { dg-options "-O2 -mfpu=vfp -mfloat-abi=softfp -ffp-contract=off" } */
 /* { dg-require-effective-target arm_vfp_ok } */
+/* { dg-skip-if "need fp instructions" { *-*-* } { "-mfloat-abi=soft" } { "" } } */
 
 extern float fabsf (float);
 extern float sqrtf (float);
--- a/src/gcc/testsuite/lib/target-supports.exp
+++ b/src/gcc/testsuite/lib/target-supports.exp
@@ -4675,6 +4675,27 @@ proc check_effective_target_vect_call_copysignf { } {
     return $et_vect_call_copysignf_saved
 }
 
+# Return 1 if the target supports hardware square root instructions.
+
+proc check_effective_target_sqrt_insn { } {
+    global et_sqrt_insn_saved
+
+    if [info exists et_sqrt_insn_saved] {
+	verbose "check_effective_target_hw_sqrt: using cached result" 2
+    } else {
+	set et_sqrt_insn_saved 0
+	if { [istarget x86_64-*-*]
+	     || [istarget powerpc*-*-*]
+	     || [istarget aarch64*-*-*]
+	     || ([istarget arm*-*-*] && [check_effective_target_arm_vfp_ok]) } {
+	   set et_sqrt_insn_saved 1
+	}
+    }
+
+    verbose "check_effective_target_hw_sqrt: returning et_sqrt_insn_saved" 2
+    return $et_sqrt_insn_saved
+}
+
 # Return 1 if the target supports vector sqrtf calls.
 
 proc check_effective_target_vect_call_sqrtf { } {
--- a/src/gcc/tree-ssa-loop-ivopts.c
+++ b/src/gcc/tree-ssa-loop-ivopts.c
@@ -226,6 +226,7 @@ struct cost_pair
 struct iv_use
 {
   unsigned id;		/* The id of the use.  */
+  unsigned sub_id;	/* The id of the sub use.  */
   enum use_type type;	/* Type of the use.  */
   struct iv *iv;	/* The induction variable it is based on.  */
   gimple stmt;		/* Statement in that it occurs.  */
@@ -239,6 +240,11 @@ struct iv_use
 
   struct iv_cand *selected;
 			/* The selected candidate.  */
+
+  struct iv_use *next;	/* The next sub use.  */
+  tree addr_base;	/* Base address with const offset stripped.  */
+  unsigned HOST_WIDE_INT addr_offset;
+			/* Const offset stripped from base address.  */
 };
 
 /* The position where the iv is computed.  */
@@ -555,7 +561,11 @@ dump_iv (FILE *file, struct iv *iv)
 void
 dump_use (FILE *file, struct iv_use *use)
 {
-  fprintf (file, "use %d\n", use->id);
+  fprintf (file, "use %d", use->id);
+  if (use->sub_id)
+    fprintf (file, ".%d", use->sub_id);
+
+  fprintf (file, "\n");
 
   switch (use->type)
     {
@@ -604,8 +614,12 @@ dump_uses (FILE *file, struct ivopts_data *data)
   for (i = 0; i < n_iv_uses (data); i++)
     {
       use = iv_use (data, i);
-
-      dump_use (file, use);
+      do
+	{
+	  dump_use (file, use);
+	  use = use->next;
+	}
+      while (use);
       fprintf (file, "\n");
     }
 }
@@ -1326,33 +1340,84 @@ find_induction_variables (struct ivopts_data *data)
   return true;
 }
 
-/* Records a use of type USE_TYPE at *USE_P in STMT whose value is IV.  */
+/* Records a use of type USE_TYPE at *USE_P in STMT whose value is IV.
+   For address type use, ADDR_BASE is the stripped IV base, ADDR_OFFSET
+   is the const offset stripped from IV base.  For uses of other types,
+   ADDR_BASE and ADDR_OFFSET are zero by default.  */
 
 static struct iv_use *
 record_use (struct ivopts_data *data, tree *use_p, struct iv *iv,
-	    gimple stmt, enum use_type use_type)
+	    gimple stmt, enum use_type use_type, tree addr_base = NULL,
+	    unsigned HOST_WIDE_INT addr_offset = 0)
 {
   struct iv_use *use = XCNEW (struct iv_use);
 
   use->id = n_iv_uses (data);
+  use->sub_id = 0;
   use->type = use_type;
   use->iv = iv;
   use->stmt = stmt;
   use->op_p = use_p;
   use->related_cands = BITMAP_ALLOC (NULL);
+  use->next = NULL;
+  use->addr_base = addr_base;
+  use->addr_offset = addr_offset;
 
   /* To avoid showing ssa name in the dumps, if it was not reset by the
      caller.  */
   iv->ssa_name = NULL_TREE;
 
-  if (dump_file && (dump_flags & TDF_DETAILS))
-    dump_use (dump_file, use);
-
   data->iv_uses.safe_push (use);
 
   return use;
 }
 
+/* Records a sub use of type USE_TYPE at *USE_P in STMT whose value is IV.
+   The sub use is recorded under the one whose use id is ID_GROUP.  */
+
+static struct iv_use *
+record_sub_use (struct ivopts_data *data, tree *use_p,
+		    struct iv *iv, gimple stmt, enum use_type use_type,
+		    tree addr_base, unsigned HOST_WIDE_INT addr_offset,
+		    unsigned int id_group)
+{
+  struct iv_use *use = XCNEW (struct iv_use);
+  struct iv_use *group = iv_use (data, id_group);
+
+  use->id = group->id;
+  use->sub_id = 0;
+  use->type = use_type;
+  use->iv = iv;
+  use->stmt = stmt;
+  use->op_p = use_p;
+  use->related_cands = NULL;
+  use->addr_base = addr_base;
+  use->addr_offset = addr_offset;
+
+  /* Sub use list is maintained in offset ascending order.  */
+  if (addr_offset <= group->addr_offset)
+    {
+      use->related_cands = group->related_cands;
+      group->related_cands = NULL;
+      use->next = group;
+      data->iv_uses[id_group] = use;
+    }
+  else
+    {
+      struct iv_use *pre;
+      do
+	{
+	  pre = group;
+	  group = group->next;
+	}
+      while (group && addr_offset > group->addr_offset);
+      use->next = pre->next;
+      pre->next = use;
+    }
+
+  return use;
+}
+
 /* Checks whether OP is a loop-level invariant and if so, records it.
    NONLINEAR_USE is true if the invariant is used in a way we do not
    handle specially.  */
@@ -1837,6 +1902,50 @@ may_be_nonaddressable_p (tree expr)
   return false;
 }
 
+static tree
+strip_offset (tree expr, unsigned HOST_WIDE_INT *offset);
+
+/* Record a use of type USE_TYPE at *USE_P in STMT whose value is IV.
+   If there is an existing use which has same stripped iv base and step,
+   this function records this one as a sub use to that; otherwise records
+   it as a normal one.  */
+
+static struct iv_use *
+record_group_use (struct ivopts_data *data, tree *use_p,
+		  struct iv *iv, gimple stmt, enum use_type use_type)
+{
+  unsigned int i;
+  struct iv_use *use;
+  tree addr_base;
+  unsigned HOST_WIDE_INT addr_offset;
+
+  /* Only support sub use for address type uses, that is, with base
+     object.  */
+  if (!iv->base_object)
+    return record_use (data, use_p, iv, stmt, use_type);
+
+  addr_base = strip_offset (iv->base, &addr_offset);
+  for (i = 0; i < n_iv_uses (data); i++)
+    {
+      use = iv_use (data, i);
+      if (use->type != USE_ADDRESS || !use->iv->base_object)
+	continue;
+
+      /* Check if it has the same stripped base and step.  */
+      if (operand_equal_p (iv->base_object, use->iv->base_object, 0)
+	  && operand_equal_p (iv->step, use->iv->step, 0)
+	  && operand_equal_p (addr_base, use->addr_base, 0))
+	break;
+    }
+
+  if (i == n_iv_uses (data))
+    return record_use (data, use_p, iv, stmt,
+		       use_type, addr_base, addr_offset);
+  else
+    return record_sub_use (data, use_p, iv, stmt,
+			   use_type, addr_base, addr_offset, i);
+}
+
 /* Finds addresses in *OP_P inside STMT.  */
 
 static void
@@ -1947,7 +2056,7 @@ find_interesting_uses_address (struct ivopts_data *data, gimple stmt, tree *op_p
     }
 
   civ = alloc_iv (base, step);
-  record_use (data, op_p, civ, stmt, USE_ADDRESS);
+  record_group_use (data, op_p, civ, stmt, USE_ADDRESS);
   return;
 
 fail:
@@ -2133,6 +2242,172 @@ find_interesting_uses (struct ivopts_data *data)
   free (body);
 }
 
+/* Compute maximum offset of [base + offset] addressing mode
+   for memory reference represented by USE.  */
+
+static HOST_WIDE_INT
+compute_max_addr_offset (struct iv_use *use)
+{
+  int width;
+  rtx reg, addr;
+  HOST_WIDE_INT i, off;
+  unsigned list_index, num;
+  addr_space_t as;
+  machine_mode mem_mode, addr_mode;
+  static vec<HOST_WIDE_INT> max_offset_list;
+
+  as = TYPE_ADDR_SPACE (TREE_TYPE (use->iv->base));
+  mem_mode = TYPE_MODE (TREE_TYPE (*use->op_p));
+
+  num = max_offset_list.length ();
+  list_index = (unsigned) as * MAX_MACHINE_MODE + (unsigned) mem_mode;
+  if (list_index >= num)
+    {
+      max_offset_list.safe_grow (list_index + MAX_MACHINE_MODE);
+      for (; num < max_offset_list.length (); num++)
+	max_offset_list[num] = -1;
+    }
+
+  off = max_offset_list[list_index];
+  if (off != -1)
+    return off;
+
+  addr_mode = targetm.addr_space.address_mode (as);
+  reg = gen_raw_REG (addr_mode, LAST_VIRTUAL_REGISTER + 1);
+  addr = gen_rtx_fmt_ee (PLUS, addr_mode, reg, NULL_RTX);
+
+  width = GET_MODE_BITSIZE (addr_mode) - 1;
+  if (width > (HOST_BITS_PER_WIDE_INT - 1))
+    width = HOST_BITS_PER_WIDE_INT - 1;
+
+  for (i = width; i > 0; i--)
+    {
+      off = ((unsigned HOST_WIDE_INT) 1 << i) - 1;
+      XEXP (addr, 1) = gen_int_mode (off, addr_mode);
+      if (memory_address_addr_space_p (mem_mode, addr, as))
+	break;
+
+      /* For some strict-alignment targets, the offset must be naturally
+	 aligned.  Try an aligned offset if mem_mode is not QImode.  */
+      off = ((unsigned HOST_WIDE_INT) 1 << i);
+      if (off > GET_MODE_SIZE (mem_mode) && mem_mode != QImode)
+	{
+	  off -= GET_MODE_SIZE (mem_mode);
+	  XEXP (addr, 1) = gen_int_mode (off, addr_mode);
+	  if (memory_address_addr_space_p (mem_mode, addr, as))
+	    break;
+	}
+    }
+  if (i == 0)
+    off = 0;
+
+  max_offset_list[list_index] = off;
+  return off;
+}
+
+/* Check if all small groups should be split.  Return true if and
+   only if:
+
+     1) At least one groups contain two uses with different offsets.
+     2) No group contains more than two uses with different offsets.
+
+   Return false otherwise.  We want to split such groups because:
+
+     1) Small groups don't have much benefit and may interfer with
+	general candidate selection.
+     2) Size for problem with only small groups is usually small and
+	general algorithm can handle it well.
+
+   TODO -- Above claim may not hold when auto increment is supported.  */
+
+static bool
+split_all_small_groups (struct ivopts_data *data)
+{
+  bool split_p = false;
+  unsigned int i, n, distinct;
+  struct iv_use *pre, *use;
+
+  n = n_iv_uses (data);
+  for (i = 0; i < n; i++)
+    {
+      use = iv_use (data, i);
+      if (!use->next)
+	continue;
+
+      distinct = 1;
+      gcc_assert (use->type == USE_ADDRESS);
+      for (pre = use, use = use->next; use; pre = use, use = use->next)
+	{
+	  if (pre->addr_offset != use->addr_offset)
+	    distinct++;
+
+	  if (distinct > 2)
+	    return false;
+	}
+      if (distinct == 2)
+	split_p = true;
+    }
+
+  return split_p;
+}
+
+/* For each group of address type uses, this function further groups
+   these uses according to the maximum offset supported by target's
+   [base + offset] addressing mode.  */
+
+static void
+group_address_uses (struct ivopts_data *data)
+{
+  HOST_WIDE_INT max_offset = -1;
+  unsigned int i, n, sub_id;
+  struct iv_use *pre, *use;
+  unsigned HOST_WIDE_INT addr_offset_first;
+
+  /* Reset max offset to split all small groups.  */
+  if (split_all_small_groups (data))
+    max_offset = 0;
+
+  n = n_iv_uses (data);
+  for (i = 0; i < n; i++)
+    {
+      use = iv_use (data, i);
+      if (!use->next)
+	continue;
+
+      gcc_assert (use->type == USE_ADDRESS);
+      if (max_offset != 0)
+	max_offset = compute_max_addr_offset (use);
+
+      while (use)
+	{
+	  sub_id = 0;
+	  addr_offset_first = use->addr_offset;
+	  /* Only uses with offset that can fit in offset part against
+	     the first use can be grouped together.  */
+	  for (pre = use, use = use->next;
+	       use && (use->addr_offset - addr_offset_first
+		       <= (unsigned HOST_WIDE_INT) max_offset);
+	       pre = use, use = use->next)
+	    {
+	      use->id = pre->id;
+	      use->sub_id = ++sub_id;
+	    }
+
+	  /* Break the list and create new group.  */
+	  if (use)
+	    {
+	      pre->next = NULL;
+	      use->id = n_iv_uses (data);
+	      use->related_cands = BITMAP_ALLOC (NULL);
+	      data->iv_uses.safe_push (use);
+	    }
+	}
+    }
+
+  if (dump_file && (dump_flags & TDF_DETAILS))
+    dump_uses (dump_file, data);
+}
+
 /* Strips constant offsets from EXPR and stores them to OFFSET.  If INSIDE_ADDR
    is true, assume we are inside an address.  If TOP_COMPREF is true, assume
    we are at the top-level of the processed address.  */
@@ -2556,6 +2831,8 @@ static void
 add_candidate (struct ivopts_data *data,
 	       tree base, tree step, bool important, struct iv_use *use)
 {
+  gcc_assert (use == NULL || use->sub_id == 0);
+
   if (ip_normal_pos (data->current_loop))
     add_candidate_1 (data, base, step, important, IP_NORMAL, use, NULL);
   if (ip_end_pos (data->current_loop)
@@ -2785,11 +3062,22 @@ new_cost (unsigned runtime, unsigned complexity)
   return cost;
 }
 
+/* Returns true if COST is infinite.  */
+
+static bool
+infinite_cost_p (comp_cost cost)
+{
+  return cost.cost == INFTY;
+}
+
 /* Adds costs COST1 and COST2.  */
 
 static comp_cost
 add_costs (comp_cost cost1, comp_cost cost2)
 {
+  if (infinite_cost_p (cost1) || infinite_cost_p (cost2))
+    return infinite_cost;
+
   cost1.cost += cost2.cost;
   cost1.complexity += cost2.complexity;
 
@@ -2818,14 +3106,6 @@ compare_costs (comp_cost cost1, comp_cost cost2)
   return cost1.cost - cost2.cost;
 }
 
-/* Returns true if COST is infinite.  */
-
-static bool
-infinite_cost_p (comp_cost cost)
-{
-  return cost.cost == INFTY;
-}
-
 /* Sets cost of (USE, CANDIDATE) pair to COST and record that it depends
    on invariants DEPENDS_ON and that the value used in expressing it
    is VALUE, and in case of iv elimination the comparison operator is COMP.  */
@@ -4300,7 +4580,15 @@ get_computation_cost_at (struct ivopts_data *data,
       cost.cost += add_cost (data->speed, TYPE_MODE (ctype));
     }
 
-  if (inv_expr_id)
+  /* Set of invariants depended on by sub use has already been computed
+     for the first use in the group.  */
+  if (use->sub_id)
+    {
+      cost.cost = 0;
+      if (depends_on && *depends_on)
+	bitmap_clear (*depends_on);
+    }
+  else if (inv_expr_id)
     {
       *inv_expr_id =
           get_loop_invariant_expr_id (data, ubase, cbase, ratio, address_p);
@@ -4429,6 +4717,8 @@ determine_use_iv_cost_address (struct ivopts_data *data,
   bitmap depends_on;
   bool can_autoinc;
   int inv_expr_id = -1;
+  struct iv_use *sub_use;
+  comp_cost sub_cost;
   comp_cost cost = get_computation_cost (data, use, cand, true, &depends_on,
 					 &can_autoinc, &inv_expr_id);
 
@@ -4442,6 +4732,15 @@ determine_use_iv_cost_address (struct ivopts_data *data,
       else if (cand->pos == IP_AFTER_USE || cand->pos == IP_BEFORE_USE)
 	cost = infinite_cost;
     }
+  for (sub_use = use->next;
+       sub_use && !infinite_cost_p (cost);
+       sub_use = sub_use->next)
+    {
+       sub_cost = get_computation_cost (data, sub_use, cand, true, &depends_on,
+					&can_autoinc, &inv_expr_id);
+       cost = add_costs (cost, sub_cost);
+    }
+
   set_use_iv_cost (data, use, cand, cost, depends_on, NULL_TREE, ERROR_MARK,
                    inv_expr_id);
 
@@ -6588,8 +6887,8 @@ adjust_iv_update_pos (struct iv_cand *cand, struct iv_use *use)
 /* Rewrites USE (address that is an iv) using candidate CAND.  */
 
 static void
-rewrite_use_address (struct ivopts_data *data,
-		     struct iv_use *use, struct iv_cand *cand)
+rewrite_use_address_1 (struct ivopts_data *data,
+		       struct iv_use *use, struct iv_cand *cand)
 {
   aff_tree aff;
   gimple_stmt_iterator bsi = gsi_for_stmt (use->stmt);
@@ -6624,6 +6923,28 @@ rewrite_use_address (struct ivopts_data *data,
   *use->op_p = ref;
 }
 
+/* Rewrites USE (address that is an iv) using candidate CAND.  If it's the
+   first use of a group, rewrites sub uses in the group too.  */
+
+static void
+rewrite_use_address (struct ivopts_data *data,
+		      struct iv_use *use, struct iv_cand *cand)
+{
+  struct iv_use *next;
+
+  gcc_assert (use->sub_id == 0);
+  rewrite_use_address_1 (data, use, cand);
+  update_stmt (use->stmt);
+
+  for (next = use->next; next != NULL; next = next->next)
+    {
+      rewrite_use_address_1 (data, next, cand);
+      update_stmt (next->stmt);
+    }
+
+  return;
+}
+
 /* Rewrites USE (the condition such that one of the arguments is an iv) using
    candidate CAND.  */
 
@@ -6899,6 +7220,18 @@ free_loop_data (struct ivopts_data *data)
   for (i = 0; i < n_iv_uses (data); i++)
     {
       struct iv_use *use = iv_use (data, i);
+      struct iv_use *pre = use, *sub = use->next;
+
+      while (sub)
+	{
+	  gcc_assert (sub->related_cands == NULL);
+	  gcc_assert (sub->n_map_members == 0 && sub->cost_map == NULL);
+
+	  free (sub->iv);
+	  pre = sub;
+	  sub = sub->next;
+	  free (pre);
+	}
 
       free (use->iv);
       BITMAP_FREE (use->related_cands);
@@ -7025,6 +7358,7 @@ tree_ssa_iv_optimize_loop (struct ivopts_data *data, struct loop *loop)
 
   /* Finds interesting uses (item 1).  */
   find_interesting_uses (data);
+  group_address_uses (data);
   if (n_iv_uses (data) > MAX_CONSIDERED_USES)
     goto finish;
 
--- a/src/gcc/tree-ssa-math-opts.c
+++ b/src/gcc/tree-ssa-math-opts.c
@@ -143,6 +143,7 @@ along with GCC; see the file COPYING3.  If not see
 #include "target.h"
 #include "gimple-pretty-print.h"
 #include "builtins.h"
+#include "params.h"
 
 /* FIXME: RTL headers have to be included here for optabs.  */
 #include "rtl.h"		/* Because optabs.h wants enum rtx_code.  */
@@ -1148,6 +1149,357 @@ build_and_insert_cast (gimple_stmt_iterator *gsi, location_t loc,
   return result;
 }
 
+struct pow_synth_sqrt_info
+{
+  bool *factors;
+  unsigned int deepest;
+  unsigned int num_mults;
+};
+
+/* Return true iff the real value C can be represented as a
+   sum of powers of 0.5 up to N.  That is:
+   C == SUM<i from 1..N> (a[i]*(0.5**i)) where a[i] is either 0 or 1.
+   Record in INFO the various parameters of the synthesis algorithm such
+   as the factors a[i], the maximum 0.5 power and the number of
+   multiplications that will be required.  */
+
+bool
+representable_as_half_series_p (REAL_VALUE_TYPE c, unsigned n,
+				 struct pow_synth_sqrt_info *info)
+{
+  REAL_VALUE_TYPE factor = dconsthalf;
+  REAL_VALUE_TYPE remainder = c;
+
+  info->deepest = 0;
+  info->num_mults = 0;
+  memset (info->factors, 0, n * sizeof (bool));
+
+  for (unsigned i = 0; i < n; i++)
+    {
+      REAL_VALUE_TYPE res;
+
+      /* If something inexact happened bail out now.  */
+      if (REAL_ARITHMETIC (res, MINUS_EXPR, remainder, factor))
+	return false;
+
+      /* We have hit zero.  The number is representable as a sum
+         of powers of 0.5.  */
+      if (REAL_VALUES_EQUAL (res, dconst0))
+	{
+	  info->factors[i] = true;
+	  info->deepest = i + 1;
+	  return true;
+	}
+      else if (!REAL_VALUE_NEGATIVE (res))
+	{
+	  remainder = res;
+	  info->factors[i] = true;
+	  info->num_mults++;
+	}
+      else
+	info->factors[i] = false;
+
+      REAL_ARITHMETIC (factor, MULT_EXPR, factor, dconsthalf);
+    }
+  return false;
+}
+
+/* Return the tree corresponding to FN being applied
+   to ARG N times at GSI and LOC.
+   Look up previous results from CACHE if need be.
+   cache[0] should contain just plain ARG i.e. FN applied to ARG 0 times.  */
+
+static tree
+get_fn_chain (tree arg, unsigned int n, gimple_stmt_iterator *gsi,
+	      tree fn, location_t loc, tree *cache)
+{
+  tree res = cache[n];
+  if (!res)
+    {
+      tree prev = get_fn_chain (arg, n - 1, gsi, fn, loc, cache);
+      res = build_and_insert_call (gsi, loc, fn, prev);
+      cache[n] = res;
+    }
+
+  return res;
+}
+
+/* Print to STREAM the repeated application of function FNAME to ARG
+   N times.  So, for FNAME = "foo", ARG = "x", N = 2 it would print:
+   "foo (foo (x))".  */
+
+static void
+print_nested_fn (FILE* stream, const char *fname, const char* arg,
+		 unsigned int n)
+{
+  if (n == 0)
+    fprintf (stream, "%s", arg);
+  else
+    {
+      fprintf (stream, "%s (", fname);
+      print_nested_fn (stream, fname, arg, n - 1);
+      fprintf (stream, ")");
+    }
+}
+
+/* Print to STREAM the fractional sequence of sqrt chains
+   applied to ARG, described by INFO.  Used for the dump file.  */
+
+static void
+dump_fractional_sqrt_sequence (FILE *stream, const char *arg,
+			        struct pow_synth_sqrt_info *info)
+{
+  for (unsigned int i = 0; i < info->deepest; i++)
+    {
+      bool is_set = info->factors[i];
+      if (is_set)
+	{
+	  print_nested_fn (stream, "sqrt", arg, i + 1);
+	  if (i != info->deepest - 1)
+	    fprintf (stream, " * ");
+	}
+    }
+}
+
+/* Print to STREAM a representation of raising ARG to an integer
+   power N.  Used for the dump file.  */
+
+static void
+dump_integer_part (FILE *stream, const char* arg, HOST_WIDE_INT n)
+{
+  if (n > 1)
+    fprintf (stream, "powi (%s, " HOST_WIDE_INT_PRINT_DEC ")", arg, n);
+  else if (n == 1)
+    fprintf (stream, "%s", arg);
+}
+
+/* Attempt to synthesize a POW[F] (ARG0, ARG1) call using chains of
+   square roots.  Place at GSI and LOC.  Limit the maximum depth
+   of the sqrt chains to MAX_DEPTH.  Return the tree holding the
+   result of the expanded sequence or NULL_TREE if the expansion failed.
+
+   This routine assumes that ARG1 is a real number with a fractional part
+   (the integer exponent case will have been handled earlier in
+   gimple_expand_builtin_pow).
+
+   For ARG1 > 0.0:
+   * For ARG1 composed of a whole part WHOLE_PART and a fractional part
+     FRAC_PART i.e. WHOLE_PART == floor (ARG1) and
+                    FRAC_PART == ARG1 - WHOLE_PART:
+     Produce POWI (ARG0, WHOLE_PART) * POW (ARG0, FRAC_PART) where
+     POW (ARG0, FRAC_PART) is expanded as a product of square root chains
+     if it can be expressed as such, that is if FRAC_PART satisfies:
+     FRAC_PART == <SUM from i = 1 until MAX_DEPTH> (a[i] * (0.5**i))
+     where integer a[i] is either 0 or 1.
+
+     Example:
+     POW (x, 3.625) == POWI (x, 3) * POW (x, 0.625)
+       --> POWI (x, 3) * SQRT (x) * SQRT (SQRT (SQRT (x)))
+
+   For ARG1 < 0.0 there are two approaches:
+   * (A) Expand to 1.0 / POW (ARG0, -ARG1) where POW (ARG0, -ARG1)
+         is calculated as above.
+
+     Example:
+     POW (x, -5.625) == 1.0 / POW (x, 5.625)
+       -->  1.0 / (POWI (x, 5) * SQRT (x) * SQRT (SQRT (SQRT (x))))
+
+   * (B) : WHOLE_PART := - ceil (abs (ARG1))
+           FRAC_PART  := ARG1 - WHOLE_PART
+     and expand to POW (x, FRAC_PART) / POWI (x, WHOLE_PART).
+     Example:
+     POW (x, -5.875) == POW (x, 0.125) / POWI (X, 6)
+       --> SQRT (SQRT (SQRT (x))) / (POWI (x, 6))
+
+   For ARG1 < 0.0 we choose between (A) and (B) depending on
+   how many multiplications we'd have to do.
+   So, for the example in (B): POW (x, -5.875), if we were to
+   follow algorithm (A) we would produce:
+   1.0 / POWI (X, 5) * SQRT (X) * SQRT (SQRT (X)) * SQRT (SQRT (SQRT (X)))
+   which contains more multiplications than approach (B).
+
+   Hopefully, this approach will eliminate potentially expensive POW library
+   calls when unsafe floating point math is enabled and allow the compiler to
+   further optimise the multiplies, square roots and divides produced by this
+   function.  */
+
+static tree
+expand_pow_as_sqrts (gimple_stmt_iterator *gsi, location_t loc,
+		     tree arg0, tree arg1, HOST_WIDE_INT max_depth)
+{
+  tree type = TREE_TYPE (arg0);
+  machine_mode mode = TYPE_MODE (type);
+  tree sqrtfn = mathfn_built_in (type, BUILT_IN_SQRT);
+  bool one_over = true;
+
+  if (!sqrtfn)
+    return NULL_TREE;
+
+  if (TREE_CODE (arg1) != REAL_CST)
+    return NULL_TREE;
+
+  REAL_VALUE_TYPE exp_init = TREE_REAL_CST (arg1);
+
+  gcc_assert (max_depth > 0);
+  tree *cache = XALLOCAVEC (tree, max_depth + 1);
+
+  struct pow_synth_sqrt_info synth_info;
+  synth_info.factors = XALLOCAVEC (bool, max_depth + 1);
+  synth_info.deepest = 0;
+  synth_info.num_mults = 0;
+
+  bool neg_exp = REAL_VALUE_NEGATIVE (exp_init);
+  REAL_VALUE_TYPE exp = real_value_abs (&exp_init);
+
+  /* The whole and fractional parts of exp.  */
+  REAL_VALUE_TYPE whole_part;
+  REAL_VALUE_TYPE frac_part;
+
+  real_floor (&whole_part, mode, &exp);
+  REAL_ARITHMETIC (frac_part, MINUS_EXPR, exp, whole_part);
+
+
+  REAL_VALUE_TYPE ceil_whole = dconst0;
+  REAL_VALUE_TYPE ceil_fract = dconst0;
+
+  if (neg_exp)
+    {
+      real_ceil (&ceil_whole, mode, &exp);
+      REAL_ARITHMETIC (ceil_fract, MINUS_EXPR, ceil_whole, exp);
+    }
+
+  if (!representable_as_half_series_p (frac_part, max_depth, &synth_info))
+    return NULL_TREE;
+
+  /* Check whether it's more profitable to not use 1.0 / ...  */
+  if (neg_exp)
+    {
+      struct pow_synth_sqrt_info alt_synth_info;
+      alt_synth_info.factors = XALLOCAVEC (bool, max_depth + 1);
+      alt_synth_info.deepest = 0;
+      alt_synth_info.num_mults = 0;
+
+      if (representable_as_half_series_p (ceil_fract, max_depth,
+					   &alt_synth_info)
+	  && alt_synth_info.deepest <= synth_info.deepest
+	  && alt_synth_info.num_mults < synth_info.num_mults)
+	{
+	  whole_part = ceil_whole;
+	  frac_part = ceil_fract;
+	  synth_info.deepest = alt_synth_info.deepest;
+	  synth_info.num_mults = alt_synth_info.num_mults;
+	  memcpy (synth_info.factors, alt_synth_info.factors,
+		  (max_depth + 1) * sizeof (bool));
+	  one_over = false;
+	}
+    }
+
+  HOST_WIDE_INT n = real_to_integer (&whole_part);
+  REAL_VALUE_TYPE cint;
+  real_from_integer (&cint, VOIDmode, n, SIGNED);
+
+  if (!real_identical (&whole_part, &cint))
+    return NULL_TREE;
+
+  if (powi_cost (n) + synth_info.num_mults > POWI_MAX_MULTS)
+    return NULL_TREE;
+
+  memset (cache, 0, (max_depth + 1) * sizeof (tree));
+
+  tree integer_res = n == 0 ? build_real (type, dconst1) : arg0;
+
+  /* Calculate the integer part of the exponent.  */
+  if (n > 1)
+    {
+      integer_res = gimple_expand_builtin_powi (gsi, loc, arg0, n);
+      if (!integer_res)
+	return NULL_TREE;
+    }
+
+  if (dump_file)
+    {
+      char string[64];
+
+      real_to_decimal (string, &exp_init, sizeof (string), 0, 1);
+      fprintf (dump_file, "synthesizing pow (x, %s) as:\n", string);
+
+      if (neg_exp)
+	{
+	  if (one_over)
+	    {
+	      fprintf (dump_file, "1.0 / (");
+	      dump_integer_part (dump_file, "x", n);
+	      if (n > 0)
+	        fprintf (dump_file, " * ");
+	      dump_fractional_sqrt_sequence (dump_file, "x", &synth_info);
+	      fprintf (dump_file, ")");
+	    }
+	  else
+	    {
+	      dump_fractional_sqrt_sequence (dump_file, "x", &synth_info);
+	      fprintf (dump_file, " / (");
+	      dump_integer_part (dump_file, "x", n);
+	      fprintf (dump_file, ")");
+	    }
+	}
+      else
+	{
+	  dump_fractional_sqrt_sequence (dump_file, "x", &synth_info);
+	  if (n > 0)
+	    fprintf (dump_file, " * ");
+	  dump_integer_part (dump_file, "x", n);
+	}
+
+      fprintf (dump_file, "\ndeepest sqrt chain: %d\n", synth_info.deepest);
+    }
+
+
+  tree fract_res = NULL_TREE;
+  cache[0] = arg0;
+
+  /* Calculate the fractional part of the exponent.  */
+  for (unsigned i = 0; i < synth_info.deepest; i++)
+    {
+      if (synth_info.factors[i])
+	{
+	  tree sqrt_chain = get_fn_chain (arg0, i + 1, gsi, sqrtfn, loc, cache);
+
+	  if (!fract_res)
+	      fract_res = sqrt_chain;
+
+	  else
+	    fract_res = build_and_insert_binop (gsi, loc, "powroot", MULT_EXPR,
+					   fract_res, sqrt_chain);
+	}
+    }
+
+  tree res = NULL_TREE;
+
+  if (neg_exp)
+    {
+      if (one_over)
+	{
+	  if (n > 0)
+	    res = build_and_insert_binop (gsi, loc, "powroot", MULT_EXPR,
+					   fract_res, integer_res);
+	  else
+	    res = fract_res;
+
+	  res = build_and_insert_binop (gsi, loc, "powrootrecip", RDIV_EXPR,
+					  build_real (type, dconst1), res);
+	}
+      else
+	{
+	  res = build_and_insert_binop (gsi, loc, "powroot", RDIV_EXPR,
+					 fract_res, integer_res);
+	}
+    }
+  else
+    res = build_and_insert_binop (gsi, loc, "powroot", MULT_EXPR,
+				   fract_res, integer_res);
+  return res;
+}
+
 /* ARG0 and ARG1 are the two arguments to a pow builtin call in GSI
    with location info LOC.  If possible, create an equivalent and
    less expensive sequence of statements prior to GSI, and return an
@@ -1157,13 +1509,17 @@ static tree
 gimple_expand_builtin_pow (gimple_stmt_iterator *gsi, location_t loc, 
 			   tree arg0, tree arg1)
 {
-  REAL_VALUE_TYPE c, cint, dconst1_4, dconst3_4, dconst1_3, dconst1_6;
+  REAL_VALUE_TYPE c, cint, dconst1_3, dconst1_4, dconst1_6;
   REAL_VALUE_TYPE c2, dconst3;
   HOST_WIDE_INT n;
-  tree type, sqrtfn, cbrtfn, sqrt_arg0, sqrt_sqrt, result, cbrt_x, powi_cbrt_x;
+  tree type, sqrtfn, cbrtfn, sqrt_arg0, result, cbrt_x, powi_cbrt_x;
   machine_mode mode;
+  bool speed_p = optimize_bb_for_speed_p (gsi_bb (*gsi));
   bool hw_sqrt_exists, c_is_int, c2_is_int;
 
+  dconst1_4 = dconst1;
+  SET_REAL_EXP (&dconst1_4, REAL_EXP (&dconst1_4) - 2);
+
   /* If the exponent isn't a constant, there's nothing of interest
      to be done.  */
   if (TREE_CODE (arg1) != REAL_CST)
@@ -1179,7 +1535,7 @@ gimple_expand_builtin_pow (gimple_stmt_iterator *gsi, location_t loc,
   if (c_is_int
       && ((n >= -1 && n <= 2)
 	  || (flag_unsafe_math_optimizations
-	      && optimize_bb_for_speed_p (gsi_bb (*gsi))
+	      && speed_p
 	      && powi_cost (n) <= POWI_MAX_MULTS)))
     return gimple_expand_builtin_powi (gsi, loc, arg0, n);
 
@@ -1196,49 +1552,8 @@ gimple_expand_builtin_pow (gimple_stmt_iterator *gsi, location_t loc,
       && !HONOR_SIGNED_ZEROS (mode))
     return build_and_insert_call (gsi, loc, sqrtfn, arg0);
 
-  /* Optimize pow(x,0.25) = sqrt(sqrt(x)).  Assume on most machines that
-     a builtin sqrt instruction is smaller than a call to pow with 0.25,
-     so do this optimization even if -Os.  Don't do this optimization
-     if we don't have a hardware sqrt insn.  */
-  dconst1_4 = dconst1;
-  SET_REAL_EXP (&dconst1_4, REAL_EXP (&dconst1_4) - 2);
   hw_sqrt_exists = optab_handler (sqrt_optab, mode) != CODE_FOR_nothing;
 
-  if (flag_unsafe_math_optimizations
-      && sqrtfn
-      && REAL_VALUES_EQUAL (c, dconst1_4)
-      && hw_sqrt_exists)
-    {
-      /* sqrt(x)  */
-      sqrt_arg0 = build_and_insert_call (gsi, loc, sqrtfn, arg0);
-
-      /* sqrt(sqrt(x))  */
-      return build_and_insert_call (gsi, loc, sqrtfn, sqrt_arg0);
-    }
-      
-  /* Optimize pow(x,0.75) = sqrt(x) * sqrt(sqrt(x)) unless we are
-     optimizing for space.  Don't do this optimization if we don't have
-     a hardware sqrt insn.  */
-  real_from_integer (&dconst3_4, VOIDmode, 3, SIGNED);
-  SET_REAL_EXP (&dconst3_4, REAL_EXP (&dconst3_4) - 2);
-
-  if (flag_unsafe_math_optimizations
-      && sqrtfn
-      && optimize_function_for_speed_p (cfun)
-      && REAL_VALUES_EQUAL (c, dconst3_4)
-      && hw_sqrt_exists)
-    {
-      /* sqrt(x)  */
-      sqrt_arg0 = build_and_insert_call (gsi, loc, sqrtfn, arg0);
-
-      /* sqrt(sqrt(x))  */
-      sqrt_sqrt = build_and_insert_call (gsi, loc, sqrtfn, sqrt_arg0);
-
-      /* sqrt(x) * sqrt(sqrt(x))  */
-      return build_and_insert_binop (gsi, loc, "powroot", MULT_EXPR,
-				     sqrt_arg0, sqrt_sqrt);
-    }
-
   /* Optimize pow(x,1./3.) = cbrt(x).  This requires unsafe math
      optimizations since 1./3. is not exactly representable.  If x
      is negative and finite, the correct value of pow(x,1./3.) is
@@ -1263,7 +1578,7 @@ gimple_expand_builtin_pow (gimple_stmt_iterator *gsi, location_t loc,
       && sqrtfn
       && cbrtfn
       && (gimple_val_nonnegative_real_p (arg0) || !HONOR_NANS (mode))
-      && optimize_function_for_speed_p (cfun)
+      && speed_p
       && hw_sqrt_exists
       && REAL_VALUES_EQUAL (c, dconst1_6))
     {
@@ -1274,54 +1589,31 @@ gimple_expand_builtin_pow (gimple_stmt_iterator *gsi, location_t loc,
       return build_and_insert_call (gsi, loc, cbrtfn, sqrt_arg0);
     }
 
-  /* Optimize pow(x,c), where n = 2c for some nonzero integer n
-     and c not an integer, into
-
-       sqrt(x) * powi(x, n/2),                n > 0;
-       1.0 / (sqrt(x) * powi(x, abs(n/2))),   n < 0.
-
-     Do not calculate the powi factor when n/2 = 0.  */
-  real_arithmetic (&c2, MULT_EXPR, &c, &dconst2);
-  n = real_to_integer (&c2);
-  real_from_integer (&cint, VOIDmode, n, SIGNED);
-  c2_is_int = real_identical (&c2, &cint);
 
+  /* Attempt to expand the POW as a product of square root chains.
+     Expand the 0.25 case even when otpimising for size.  */
   if (flag_unsafe_math_optimizations
       && sqrtfn
-      && c2_is_int
-      && !c_is_int
-      && optimize_function_for_speed_p (cfun))
+      && hw_sqrt_exists
+      && (speed_p || REAL_VALUES_EQUAL (c, dconst1_4))
+      && !HONOR_SIGNED_ZEROS (mode))
     {
-      tree powi_x_ndiv2 = NULL_TREE;
-
-      /* Attempt to fold powi(arg0, abs(n/2)) into multiplies.  If not
-         possible or profitable, give up.  Skip the degenerate case when
-         n is 1 or -1, where the result is always 1.  */
-      if (absu_hwi (n) != 1)
-	{
-	  powi_x_ndiv2 = gimple_expand_builtin_powi (gsi, loc, arg0,
-						     abs_hwi (n / 2));
-	  if (!powi_x_ndiv2)
-	    return NULL_TREE;
-	}
+      unsigned int max_depth = speed_p
+				? PARAM_VALUE (PARAM_MAX_POW_SQRT_DEPTH)
+				: 2;
 
-      /* Calculate sqrt(x).  When n is not 1 or -1, multiply it by the
-	 result of the optimal multiply sequence just calculated.  */
-      sqrt_arg0 = build_and_insert_call (gsi, loc, sqrtfn, arg0);
+      tree expand_with_sqrts
+	= expand_pow_as_sqrts (gsi, loc, arg0, arg1, max_depth);
 
-      if (absu_hwi (n) == 1)
-	result = sqrt_arg0;
-      else
-	result = build_and_insert_binop (gsi, loc, "powroot", MULT_EXPR,
-					 sqrt_arg0, powi_x_ndiv2);
-
-      /* If n is negative, reciprocate the result.  */
-      if (n < 0)
-	result = build_and_insert_binop (gsi, loc, "powroot", RDIV_EXPR,
-					 build_real (type, dconst1), result);
-      return result;
+      if (expand_with_sqrts)
+	return expand_with_sqrts;
     }
 
+  real_arithmetic (&c2, MULT_EXPR, &c, &dconst2);
+  n = real_to_integer (&c2);
+  real_from_integer (&cint, VOIDmode, n, SIGNED);
+  c2_is_int = real_identical (&c2, &cint);
+
   /* Optimize pow(x,c), where 3c = n for some nonzero integer n, into
 
      powi(x, n/3) * powi(cbrt(x), n%3),                    n > 0;
--- a/src/gcc/tree-ssa-phiopt.c
+++ b/src/gcc/tree-ssa-phiopt.c
@@ -90,6 +90,7 @@ along with GCC; see the file COPYING3.  If not see
 static unsigned int tree_ssa_phiopt_worker (bool, bool);
 static bool conditional_replacement (basic_block, basic_block,
 				     edge, edge, gphi *, tree, tree);
+static bool factor_out_conditional_conversion (edge, edge, gphi *, tree, tree);
 static int value_replacement (basic_block, basic_block,
 			      edge, edge, gimple, tree, tree);
 static bool minmax_replacement (basic_block, basic_block,
@@ -356,6 +357,19 @@ tree_ssa_phiopt_worker (bool do_store_elim, bool do_hoist_loads)
 	     node.  */
 	  gcc_assert (arg0 != NULL && arg1 != NULL);
 
+	  if (factor_out_conditional_conversion (e1, e2, phi, arg0, arg1))
+	    {
+	      /* factor_out_conditional_conversion may create a new PHI in
+		 BB2 and eliminate an existing PHI in BB2.  Recompute values
+		 that may be affected by that change.  */
+	      phis = phi_nodes (bb2);
+	      phi = single_non_singleton_phi_for_edges (phis, e1, e2);
+	      gcc_assert (phi);
+	      arg0 = gimple_phi_arg_def (phi, e1->dest_idx);
+	      arg1 = gimple_phi_arg_def (phi, e2->dest_idx);
+	      gcc_assert (arg0 != NULL && arg1 != NULL);
+	    }
+
 	  /* Do the replacement of conditional if it can be done.  */
 	  if (conditional_replacement (bb, bb1, e1, e2, phi, arg0, arg1))
 	    cfgchanged = true;
@@ -431,6 +445,134 @@ replace_phi_edge_with_variable (basic_block cond_block,
 	      bb->index);
 }
 
+/* PR66726: Factor conversion out of COND_EXPR.  If the arguments of the PHI
+   stmt are CONVERT_STMT, factor out the conversion and perform the conversion
+   to the result of PHI stmt.  */
+
+static bool
+factor_out_conditional_conversion (edge e0, edge e1, gphi *phi,
+				   tree arg0, tree arg1)
+{
+  gimple arg0_def_stmt = NULL, arg1_def_stmt = NULL, new_stmt;
+  tree new_arg0 = NULL_TREE, new_arg1 = NULL_TREE;
+  tree temp, result;
+  gphi *newphi;
+  gimple_stmt_iterator gsi, gsi_for_def;
+  source_location locus = gimple_location (phi);
+  enum tree_code convert_code;
+
+  /* Handle only PHI statements with two arguments.  TODO: If all
+     other arguments to PHI are INTEGER_CST or if their defining
+     statement have the same unary operation, we can handle more
+     than two arguments too.  */
+  if (gimple_phi_num_args (phi) != 2)
+    return false;
+
+  /* First canonicalize to simplify tests.  */
+  if (TREE_CODE (arg0) != SSA_NAME)
+    {
+      std::swap (arg0, arg1);
+      std::swap (e0, e1);
+    }
+
+  if (TREE_CODE (arg0) != SSA_NAME
+      || (TREE_CODE (arg1) != SSA_NAME
+	  && TREE_CODE (arg1) != INTEGER_CST))
+    return false;
+
+  /* Check if arg0 is an SSA_NAME and the stmt which defines arg0 is
+     a conversion.  */
+  arg0_def_stmt = SSA_NAME_DEF_STMT (arg0);
+  if (!is_gimple_assign (arg0_def_stmt)
+      || !gimple_assign_cast_p (arg0_def_stmt))
+    return false;
+
+  /* Use the RHS as new_arg0.  */
+  convert_code = gimple_assign_rhs_code (arg0_def_stmt);
+  new_arg0 = gimple_assign_rhs1 (arg0_def_stmt);
+  if (convert_code == VIEW_CONVERT_EXPR)
+    new_arg0 = TREE_OPERAND (new_arg0, 0);
+
+  if (TREE_CODE (arg1) == SSA_NAME)
+    {
+      /* Check if arg1 is an SSA_NAME and the stmt which defines arg1
+	 is a conversion.  */
+      arg1_def_stmt = SSA_NAME_DEF_STMT (arg1);
+      if (!is_gimple_assign (arg1_def_stmt)
+	  || gimple_assign_rhs_code (arg1_def_stmt) != convert_code)
+	return false;
+
+      /* Use the RHS as new_arg1.  */
+      new_arg1 = gimple_assign_rhs1 (arg1_def_stmt);
+      if (convert_code == VIEW_CONVERT_EXPR)
+	new_arg1 = TREE_OPERAND (new_arg1, 0);
+    }
+  else
+    {
+      /* If arg1 is an INTEGER_CST, fold it to new type.  */
+      if (INTEGRAL_TYPE_P (TREE_TYPE (new_arg0))
+	  && int_fits_type_p (arg1, TREE_TYPE (new_arg0)))
+	{
+	  if (gimple_assign_cast_p (arg0_def_stmt))
+	    new_arg1 = fold_convert (TREE_TYPE (new_arg0), arg1);
+	  else
+	    return false;
+	}
+      else
+	return false;
+    }
+
+  /*  If arg0/arg1 have > 1 use, then this transformation actually increases
+      the number of expressions evaluated at runtime.  */
+  if (!has_single_use (arg0)
+      || (arg1_def_stmt && !has_single_use (arg1)))
+    return false;
+
+  /* If types of new_arg0 and new_arg1 are different bailout.  */
+  if (!types_compatible_p (TREE_TYPE (new_arg0), TREE_TYPE (new_arg1)))
+    return false;
+
+  /* Create a new PHI stmt.  */
+  result = PHI_RESULT (phi);
+  temp = make_ssa_name (TREE_TYPE (new_arg0), NULL);
+  newphi = create_phi_node (temp, gimple_bb (phi));
+
+  if (dump_file && (dump_flags & TDF_DETAILS))
+    {
+      fprintf (dump_file, "PHI ");
+      print_generic_expr (dump_file, gimple_phi_result (phi), 0);
+      fprintf (dump_file,
+	       " changed to factor conversion out from COND_EXPR.\n");
+      fprintf (dump_file, "New stmt with CAST that defines ");
+      print_generic_expr (dump_file, result, 0);
+      fprintf (dump_file, ".\n");
+    }
+
+  /* Remove the old cast(s) that has single use.  */
+  gsi_for_def = gsi_for_stmt (arg0_def_stmt);
+  gsi_remove (&gsi_for_def, true);
+  if (arg1_def_stmt)
+    {
+      gsi_for_def = gsi_for_stmt (arg1_def_stmt);
+      gsi_remove (&gsi_for_def, true);
+    }
+
+  add_phi_arg (newphi, new_arg0, e0, locus);
+  add_phi_arg (newphi, new_arg1, e1, locus);
+
+  /* Create the conversion stmt and insert it.  */
+  if (convert_code == VIEW_CONVERT_EXPR)
+    temp = fold_build1 (VIEW_CONVERT_EXPR, TREE_TYPE (result), temp);
+  new_stmt = gimple_build_assign (result,  convert_code, temp);
+  gsi = gsi_after_labels (gimple_bb (phi));
+  gsi_insert_before (&gsi, new_stmt, GSI_SAME_STMT);
+
+  /* Remove he original PHI stmt.  */
+  gsi = gsi_for_stmt (phi);
+  gsi_remove (&gsi, true);
+  return true;
+}
+
 /*  The function conditional_replacement does the main work of doing the
     conditional replacement.  Return true if the replacement is done.
     Otherwise return false.
@@ -2173,6 +2315,26 @@ gate_hoist_loads (void)
    This pass also performs a fifth transformation of a slightly different
    flavor.
 
+   Factor conversion in COND_EXPR
+   ------------------------------
+
+   This transformation factors the conversion out of COND_EXPR with
+   factor_out_conditional_conversion.
+
+   For example:
+   if (a <= CST) goto <bb 3>; else goto <bb 4>;
+   <bb 3>:
+   tmp = (int) a;
+   <bb 4>:
+   tmp = PHI <tmp, CST>
+
+   Into:
+   if (a <= CST) goto <bb 3>; else goto <bb 4>;
+   <bb 3>:
+   <bb 4>:
+   a = PHI <a, CST>
+   tmp = (int) a;
+
    Adjacent Load Hoisting
    ----------------------
 
--- a/src/gcc/tree-vrp.c
+++ b/src/gcc/tree-vrp.c
@@ -3145,14 +3145,33 @@ extract_range_from_binary_expr_1 (value_range_t *vr,
 		 and all numbers from min to 0 for negative min.  */
 	      cmp = compare_values (vr0.max, zero);
 	      if (cmp == -1)
-		max = zero;
+		{
+		  /* When vr0.max < 0, vr1.min != 0 and value
+		     ranges for dividend and divisor are available.  */
+		  if (vr1.type == VR_RANGE
+		      && !symbolic_range_p (&vr0)
+		      && !symbolic_range_p (&vr1)
+		      && !compare_values (vr1.min, zero))
+		    max = int_const_binop (code, vr0.max, vr1.min);
+		  else
+		    max = zero;
+		}
 	      else if (cmp == 0 || cmp == 1)
 		max = vr0.max;
 	      else
 		type = VR_VARYING;
 	      cmp = compare_values (vr0.min, zero);
 	      if (cmp == 1)
-		min = zero;
+		{
+		  /* For unsigned division when value ranges for dividend
+		     and divisor are available.  */
+		  if (vr1.type == VR_RANGE
+		      && !symbolic_range_p (&vr0)
+		      && !symbolic_range_p (&vr1))
+		    min = int_const_binop (code, vr0.min, vr1.max);
+		  else
+		    min = zero;
+		}
 	      else if (cmp == 0 || cmp == -1)
 		min = vr0.min;
 	      else
--- a/src/libgcc/config.host
+++ b/src/libgcc/config.host
@@ -382,14 +382,15 @@ arm*-*-netbsdelf*)
 	tmake_file="$tmake_file arm/t-arm arm/t-netbsd t-slibgcc-gld-nover"
 	;;
 arm*-*-linux*)			# ARM GNU/Linux with ELF
-	tmake_file="${tmake_file} arm/t-arm t-fixedpoint-gnu-prefix"
+	tmake_file="${tmake_file} arm/t-arm t-fixedpoint-gnu-prefix t-crtfm"
 	tmake_file="${tmake_file} arm/t-elf arm/t-bpabi arm/t-linux-eabi t-slibgcc-libgcc"
 	tm_file="$tm_file arm/bpabi-lib.h"
 	unwind_header=config/arm/unwind-arm.h
 	tmake_file="$tmake_file t-softfp-sfdf t-softfp-excl arm/t-softfp t-softfp"
+	extra_parts="$extra_parts crtfastmath.o"
 	;;
 arm*-*-uclinux*)		# ARM ucLinux
-	tmake_file="${tmake_file} t-fixedpoint-gnu-prefix"
+	tmake_file="${tmake_file} t-fixedpoint-gnu-prefix t-crtfm"
 	tmake_file="$tmake_file arm/t-arm arm/t-elf t-softfp-sfdf t-softfp-excl arm/t-softfp t-softfp"
 	tmake_file="${tmake_file} arm/t-bpabi"
 	tm_file="$tm_file arm/bpabi-lib.h"
@@ -401,7 +402,7 @@ arm*-*-eabi* | arm*-*-symbianelf* | arm*-*-rtems*)
 	tm_file="$tm_file arm/bpabi-lib.h"
 	case ${host} in
 	arm*-*-eabi* | arm*-*-rtems*)
-	  tmake_file="${tmake_file} arm/t-bpabi"
+	  tmake_file="${tmake_file} arm/t-bpabi t-crtfm"
 	  extra_parts="crtbegin.o crtend.o crti.o crtn.o"
 	  ;;
 	arm*-*-symbianelf*)
--- a/src//dev/null
+++ b/src/libgcc/config/arm/crtfastmath.c
@@ -0,0 +1,40 @@
+/*
+ * Copyright (C) 2014 Free Software Foundation, Inc.
+ *
+ * This file is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License as published by the
+ * Free Software Foundation; either version 3, or (at your option) any
+ * later version.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * Under Section 7 of GPL version 3, you are granted additional
+ * permissions described in the GCC Runtime Library Exception, version
+ * 3.1, as published by the Free Software Foundation.
+ *
+ * You should have received a copy of the GNU General Public License and
+ * a copy of the GCC Runtime Library Exception along with this program;
+ * see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
+ * <http://www.gnu.org/licenses/>.
+ */
+
+/* Enable flush-to-zero support for -ffast-math on VFP targets.  */
+#ifndef __SOFTFP__
+
+#define FPSCR_FZ		(1 << 24)
+
+static void __attribute__((constructor))
+__arm_set_fast_math (void)
+{
+  unsigned int fpscr_save;
+
+  /* Set the FZ (flush-to-zero) bit in FPSCR.  */
+  __asm__("vmrs %0, fpscr" : "=r" (fpscr_save));
+  fpscr_save |= FPSCR_FZ;
+  __asm__("vmsr fpscr, %0" : : "r" (fpscr_save));
+}
+
+#endif /* __SOFTFP__  */
--- a/src/libgcc/config/arm/ieee754-df.S
+++ b/src/libgcc/config/arm/ieee754-df.S
@@ -33,8 +33,12 @@
  * Only the default rounding mode is intended for best performances.
  * Exceptions aren't supported yet, but that can be added quite easily
  * if necessary without impacting performances.
+ *
+ * In the CFI related comments, 'previousOffset' refers to the previous offset
+ * from sp used to compute the CFA.
  */
 
+	.cfi_sections .debug_frame
 
 #ifndef __ARMEB__
 #define xl r0
@@ -53,11 +57,13 @@
 
 ARM_FUNC_START negdf2
 ARM_FUNC_ALIAS aeabi_dneg negdf2
+	CFI_START_FUNCTION
 
 	@ flip sign bit
 	eor	xh, xh, #0x80000000
 	RET
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_dneg
 	FUNC_END negdf2
 
@@ -66,6 +72,7 @@ ARM_FUNC_ALIAS aeabi_dneg negdf2
 #ifdef L_arm_addsubdf3
 
 ARM_FUNC_START aeabi_drsub
+	CFI_START_FUNCTION
 
 	eor	xh, xh, #0x80000000	@ flip sign bit of first arg
 	b	1f	
@@ -81,7 +88,11 @@ ARM_FUNC_ALIAS aeabi_dsub subdf3
 ARM_FUNC_START adddf3
 ARM_FUNC_ALIAS aeabi_dadd adddf3
 
-1:	do_push	{r4, r5, lr}
+1:  do_push {r4, r5, lr}        @ sp -= 12
+	.cfi_adjust_cfa_offset 12   @ CFA is now sp + previousOffset + 12
+	.cfi_rel_offset r4, 0       @ Registers are saved from sp to sp + 8
+	.cfi_rel_offset r5, 4
+	.cfi_rel_offset lr, 8
 
 	@ Look for zeroes, equal values, INF, or NAN.
 	shift1	lsl, r4, xh, #1
@@ -148,6 +159,11 @@ ARM_FUNC_ALIAS aeabi_dadd adddf3
 	@ Since this is not common case, rescale them off line.
 	teq	r4, r5
 	beq	LSYM(Lad_d)
+
+@ CFI note: we're lucky that the branches to Lad_* that appear after this function
+@ have a CFI state that's exactly the same as the one we're in at this
+@ point. Otherwise the CFI would change to a different state after the branch,
+@ which would be disastrous for backtracing.
 LSYM(Lad_x):
 
 	@ Compensate for the exponent overlapping the mantissa MSB added later
@@ -413,6 +429,7 @@ LSYM(Lad_i):
 	orrne	xh, xh, #0x00080000	@ quiet NAN
 	RETLDM	"r4, r5"
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_dsub
 	FUNC_END subdf3
 	FUNC_END aeabi_dadd
@@ -420,12 +437,19 @@ LSYM(Lad_i):
 
 ARM_FUNC_START floatunsidf
 ARM_FUNC_ALIAS aeabi_ui2d floatunsidf
+	CFI_START_FUNCTION
 
 	teq	r0, #0
 	do_it	eq, t
 	moveq	r1, #0
 	RETc(eq)
-	do_push	{r4, r5, lr}
+
+	do_push {r4, r5, lr}        @ sp -= 12
+	.cfi_adjust_cfa_offset 12   @ CFA is now sp + previousOffset + 12
+	.cfi_rel_offset r4, 0       @ Registers are saved from sp + 0 to sp + 8.
+	.cfi_rel_offset r5, 4
+	.cfi_rel_offset lr, 8
+
 	mov	r4, #0x400		@ initial exponent
 	add	r4, r4, #(52-1 - 1)
 	mov	r5, #0			@ sign bit is 0
@@ -435,17 +459,25 @@ ARM_FUNC_ALIAS aeabi_ui2d floatunsidf
 	mov	xh, #0
 	b	LSYM(Lad_l)
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_ui2d
 	FUNC_END floatunsidf
 
 ARM_FUNC_START floatsidf
 ARM_FUNC_ALIAS aeabi_i2d floatsidf
+	CFI_START_FUNCTION
 
 	teq	r0, #0
 	do_it	eq, t
 	moveq	r1, #0
 	RETc(eq)
-	do_push	{r4, r5, lr}
+
+	do_push {r4, r5, lr}        @ sp -= 12
+	.cfi_adjust_cfa_offset 12   @ CFA is now sp + previousOffset + 12
+	.cfi_rel_offset r4, 0       @ Registers are saved from sp + 0 to sp + 8.
+	.cfi_rel_offset r5, 4
+	.cfi_rel_offset lr, 8
+
 	mov	r4, #0x400		@ initial exponent
 	add	r4, r4, #(52-1 - 1)
 	ands	r5, r0, #0x80000000	@ sign bit in r5
@@ -457,11 +489,13 @@ ARM_FUNC_ALIAS aeabi_i2d floatsidf
 	mov	xh, #0
 	b	LSYM(Lad_l)
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_i2d
 	FUNC_END floatsidf
 
 ARM_FUNC_START extendsfdf2
 ARM_FUNC_ALIAS aeabi_f2d extendsfdf2
+	CFI_START_FUNCTION
 
 	movs	r2, r0, lsl #1		@ toss sign bit
 	mov	xh, r2, asr #3		@ stretch exponent
@@ -480,34 +514,54 @@ ARM_FUNC_ALIAS aeabi_f2d extendsfdf2
 
 	@ value was denormalized.  We can normalize it now.
 	do_push	{r4, r5, lr}
+	.cfi_adjust_cfa_offset 12   @ CFA is now sp + previousOffset + 12
+	.cfi_rel_offset r4, 0       @ Registers are saved from sp + 0 to sp + 8.
+	.cfi_rel_offset r5, 4
+	.cfi_rel_offset lr, 8
+
 	mov	r4, #0x380		@ setup corresponding exponent
 	and	r5, xh, #0x80000000	@ move sign bit in r5
 	bic	xh, xh, #0x80000000
 	b	LSYM(Lad_l)
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_f2d
 	FUNC_END extendsfdf2
 
 ARM_FUNC_START floatundidf
 ARM_FUNC_ALIAS aeabi_ul2d floatundidf
+	CFI_START_FUNCTION
+	.cfi_remember_state        @ Save the current CFA state.
 
 	orrs	r2, r0, r1
 	do_it	eq
 	RETc(eq)
 
-	do_push	{r4, r5, lr}
+	do_push {r4, r5, lr}       @ sp -= 12
+	.cfi_adjust_cfa_offset 12  @ CFA is now sp + previousOffset + 12
+	.cfi_rel_offset r4, 0      @ Registers are saved from sp + 0 to sp + 8
+	.cfi_rel_offset r5, 4
+	.cfi_rel_offset lr, 8
 
 	mov	r5, #0
 	b	2f
 
 ARM_FUNC_START floatdidf
 ARM_FUNC_ALIAS aeabi_l2d floatdidf
+	.cfi_restore_state
+	@ Restore the CFI state we saved above. If we didn't do this then the
+	@ following instructions would have the CFI state that was set by the
+	@ offset adjustments made in floatundidf.
 
 	orrs	r2, r0, r1
 	do_it	eq
 	RETc(eq)
 
-	do_push	{r4, r5, lr}
+	do_push {r4, r5, lr}       @ sp -= 12
+	.cfi_adjust_cfa_offset 12  @ CFA is now sp + previousOffset + 12
+	.cfi_rel_offset r4, 0      @ Registers are saved from sp to sp + 8
+	.cfi_rel_offset r5, 4
+	.cfi_rel_offset lr, 8
 
 	ands	r5, ah, #0x80000000	@ sign bit in r5
 	bpl	2f
@@ -550,6 +604,7 @@ ARM_FUNC_ALIAS aeabi_l2d floatdidf
 	add	r4, r4, r2
 	b	LSYM(Lad_p)
 
+	CFI_END_FUNCTION
 	FUNC_END floatdidf
 	FUNC_END aeabi_l2d
 	FUNC_END floatundidf
@@ -561,7 +616,14 @@ ARM_FUNC_ALIAS aeabi_l2d floatdidf
 
 ARM_FUNC_START muldf3
 ARM_FUNC_ALIAS aeabi_dmul muldf3
-	do_push	{r4, r5, r6, lr}
+	CFI_START_FUNCTION
+
+	do_push {r4, r5, r6, lr}    @ sp -= 16
+	.cfi_adjust_cfa_offset 16   @ CFA is now sp + previousOffset + 16
+	.cfi_rel_offset r4, 0       @ Registers are saved from sp to sp + 12.
+	.cfi_rel_offset r5, 4
+	.cfi_rel_offset r6, 8
+	.cfi_rel_offset lr, 12
 
 	@ Mask out exponents, trap any zero/denormal/INF/NAN.
 	mov	ip, #0xff
@@ -596,7 +658,16 @@ ARM_FUNC_ALIAS aeabi_dmul muldf3
 	and   r6, r6, #0x80000000
 
 	@ Well, no way to make it shorter without the umull instruction.
-	stmfd	sp!, {r6, r7, r8, r9, sl, fp}
+	stmfd   sp!, {r6, r7, r8, r9, sl, fp}   @ sp -= 24
+	.cfi_remember_state         @ Save the current CFI state.
+	.cfi_adjust_cfa_offset 24   @ CFA is now sp + previousOffset + 24.
+	.cfi_rel_offset r6, 0       @ Registers are saved from sp to sp + 20.
+	.cfi_rel_offset r7, 4
+	.cfi_rel_offset r8, 8
+	.cfi_rel_offset r9, 12
+	.cfi_rel_offset sl, 16
+	.cfi_rel_offset fp, 20
+
 	mov	r7, xl, lsr #16
 	mov	r8, yl, lsr #16
 	mov	r9, xh, lsr #16
@@ -648,8 +719,8 @@ ARM_FUNC_ALIAS aeabi_dmul muldf3
 	mul	fp, xh, yh
 	adcs	r5, r5, fp
 	adc	r6, r6, #0
-	ldmfd	sp!, {yl, r7, r8, r9, sl, fp}
-
+	ldmfd   sp!, {yl, r7, r8, r9, sl, fp}   @ sp += 24
+	.cfi_restore_state   @ Restore the previous CFI state.
 #else
 
 	@ Here is the actual multiplication.
@@ -715,7 +786,6 @@ LSYM(Lml_1):
 	orr	xh, xh, #0x00100000
 	mov	lr, #0
 	subs	r4, r4, #1
-
 LSYM(Lml_u):
 	@ Overflow?
 	bgt	LSYM(Lml_o)
@@ -863,13 +933,20 @@ LSYM(Lml_n):
 	orr	xh, xh, #0x00f80000
 	RETLDM	"r4, r5, r6"
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_dmul
 	FUNC_END muldf3
 
 ARM_FUNC_START divdf3
 ARM_FUNC_ALIAS aeabi_ddiv divdf3
+	CFI_START_FUNCTION
 	
 	do_push	{r4, r5, r6, lr}
+	.cfi_adjust_cfa_offset 16
+	.cfi_rel_offset r4, 0
+	.cfi_rel_offset r5, 4
+	.cfi_rel_offset r6, 8
+	.cfi_rel_offset lr, 12
 
 	@ Mask out exponents, trap any zero/denormal/INF/NAN.
 	mov	ip, #0xff
@@ -1052,6 +1129,7 @@ LSYM(Ldv_s):
 	bne	LSYM(Lml_z)		@ 0 / <non_zero> -> 0
 	b	LSYM(Lml_n)		@ 0 / 0 -> NAN
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_ddiv
 	FUNC_END divdf3
 
@@ -1063,6 +1141,7 @@ LSYM(Ldv_s):
 
 ARM_FUNC_START gtdf2
 ARM_FUNC_ALIAS gedf2 gtdf2
+	CFI_START_FUNCTION
 	mov	ip, #-1
 	b	1f
 
@@ -1077,6 +1156,10 @@ ARM_FUNC_ALIAS eqdf2 cmpdf2
 	mov	ip, #1			@ how should we specify unordered here?
 
 1:	str	ip, [sp, #-4]!
+	.cfi_adjust_cfa_offset 4        @ CFA is now sp + previousOffset + 4.
+	@ We're not adding CFI for ip as it's pushed into the stack
+	@ only because @ it may be popped off later as a return value
+	@ (i.e. we're not preserving @ it anyways).
 
 	@ Trap any INF/NAN first.
 	mov	ip, xh, lsl #1
@@ -1085,10 +1168,18 @@ ARM_FUNC_ALIAS eqdf2 cmpdf2
 	do_it	ne
 	COND(mvn,s,ne)	ip, ip, asr #21
 	beq	3f
-
-	@ Test for equality.
-	@ Note that 0.0 is equal to -0.0.
+	.cfi_remember_state
+	@ Save the current CFI state. This is done because the branch
+	@ is conditional, @ and if we don't take it we'll issue a
+	@ .cfi_adjust_cfa_offset and return.  @ If we do take it,
+	@ however, the .cfi_adjust_cfa_offset from the non-branch @ code
+	@ will affect the branch code as well. To avoid this we'll
+	@ restore @ the current state before executing the branch code.
+
+	@ Test for equality.  @ Note that 0.0 is equal to -0.0.
 2:	add	sp, sp, #4
+	.cfi_adjust_cfa_offset -4       @ CFA is now sp + previousOffset.
+
 	orrs	ip, xl, xh, lsl #1	@ if x == 0.0 or -0.0
 	do_it	eq, e
 	COND(orr,s,eq)	ip, yl, yh, lsl #1	@ and y == 0.0 or -0.0
@@ -1117,8 +1208,13 @@ ARM_FUNC_ALIAS eqdf2 cmpdf2
 	orr	r0, r0, #1
 	RET
 
-	@ Look for a NAN.
-3:	mov	ip, xh, lsl #1
+3:  @ Look for a NAN.
+
+	@ Restore the previous CFI state (i.e. keep the CFI state as it was
+	@ before the branch).
+	.cfi_restore_state
+
+	mov ip, xh, lsl #1
 	mvns	ip, ip, asr #21
 	bne	4f
 	orrs	ip, xl, xh, lsl #12
@@ -1128,9 +1224,13 @@ ARM_FUNC_ALIAS eqdf2 cmpdf2
 	bne	2b
 	orrs	ip, yl, yh, lsl #12
 	beq	2b			@ y is not NAN
+
 5:	ldr	r0, [sp], #4		@ unordered return code
+	.cfi_adjust_cfa_offset -4       @ CFA is now sp + previousOffset.
+
 	RET
 
+	CFI_END_FUNCTION
 	FUNC_END gedf2
 	FUNC_END gtdf2
 	FUNC_END ledf2
@@ -1140,6 +1240,7 @@ ARM_FUNC_ALIAS eqdf2 cmpdf2
 	FUNC_END cmpdf2
 
 ARM_FUNC_START aeabi_cdrcmple
+	CFI_START_FUNCTION
 
 	mov	ip, r0
 	mov	r0, r2
@@ -1148,13 +1249,17 @@ ARM_FUNC_START aeabi_cdrcmple
 	mov	r1, r3
 	mov	r3, ip
 	b	6f
-	
+
 ARM_FUNC_START aeabi_cdcmpeq
 ARM_FUNC_ALIAS aeabi_cdcmple aeabi_cdcmpeq
 
 	@ The status-returning routines are required to preserve all
 	@ registers except ip, lr, and cpsr.
 6:	do_push	{r0, lr}
+	.cfi_adjust_cfa_offset 8  @ CFA is now sp + previousOffset + 8.
+	.cfi_rel_offset r0, 0     @ Previous r0 is saved at sp.
+	.cfi_rel_offset lr, 4     @ Previous lr is saved at sp + 4.
+
 	ARM_CALL cmpdf2
 	@ Set the Z flag correctly, and the C flag unconditionally.
 	cmp	r0, #0
@@ -1162,59 +1267,86 @@ ARM_FUNC_ALIAS aeabi_cdcmple aeabi_cdcmpeq
 	@ that the first operand was smaller than the second.
 	do_it	mi
 	cmnmi	r0, #0
+
 	RETLDM	"r0"
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_cdcmple
 	FUNC_END aeabi_cdcmpeq
 	FUNC_END aeabi_cdrcmple
 	
 ARM_FUNC_START	aeabi_dcmpeq
+	CFI_START_FUNCTION
+
+	str lr, [sp, #-8]!        @ sp -= 8
+	.cfi_adjust_cfa_offset 8  @ CFA is now sp + previousOffset + 8
+	.cfi_rel_offset lr, 0     @ lr is at sp
 
-	str	lr, [sp, #-8]!
 	ARM_CALL aeabi_cdcmple
 	do_it	eq, e
 	moveq	r0, #1	@ Equal to.
 	movne	r0, #0	@ Less than, greater than, or unordered.
+
 	RETLDM
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_dcmpeq
 
 ARM_FUNC_START	aeabi_dcmplt
+	CFI_START_FUNCTION
+
+	str lr, [sp, #-8]!        @ sp -= 8
+	.cfi_adjust_cfa_offset 8  @ CFA is now sp + previousOffset + 8
+	.cfi_rel_offset lr, 0     @ lr is at sp
 
-	str	lr, [sp, #-8]!
 	ARM_CALL aeabi_cdcmple
 	do_it	cc, e
 	movcc	r0, #1	@ Less than.
 	movcs	r0, #0	@ Equal to, greater than, or unordered.
 	RETLDM
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_dcmplt
 
 ARM_FUNC_START	aeabi_dcmple
+	CFI_START_FUNCTION
+
+	str lr, [sp, #-8]!        @ sp -= 8
+	.cfi_adjust_cfa_offset 8  @ CFA is now sp + previousOffset + 8
+	.cfi_rel_offset lr, 0     @ lr is at sp
 
-	str	lr, [sp, #-8]!
 	ARM_CALL aeabi_cdcmple
 	do_it	ls, e
 	movls	r0, #1  @ Less than or equal to.
 	movhi	r0, #0	@ Greater than or unordered.
 	RETLDM
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_dcmple
 
 ARM_FUNC_START	aeabi_dcmpge
+	CFI_START_FUNCTION
+
+	str lr, [sp, #-8]!        @ sp -= 8
+	.cfi_adjust_cfa_offset 8  @ CFA is now sp + previousOffset + 8
+	.cfi_rel_offset lr, 0     @ lr is at sp
 
-	str	lr, [sp, #-8]!
 	ARM_CALL aeabi_cdrcmple
 	do_it	ls, e
 	movls	r0, #1	@ Operand 2 is less than or equal to operand 1.
 	movhi	r0, #0	@ Operand 2 greater than operand 1, or unordered.
 	RETLDM
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_dcmpge
 
 ARM_FUNC_START	aeabi_dcmpgt
+	CFI_START_FUNCTION
+
+	str lr, [sp, #-8]!        @ sp -= 8
+	.cfi_adjust_cfa_offset 8  @ CFA is now sp + previousOffset + 8
+	.cfi_rel_offset lr, 0     @ lr is at sp
 
-	str	lr, [sp, #-8]!
 	ARM_CALL aeabi_cdrcmple
 	do_it	cc, e
 	movcc	r0, #1	@ Operand 2 is less than operand 1.
@@ -1222,6 +1354,7 @@ ARM_FUNC_START	aeabi_dcmpgt
 			@ or they are unordered.
 	RETLDM
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_dcmpgt
 
 #endif /* L_cmpdf2 */
@@ -1230,6 +1363,7 @@ ARM_FUNC_START	aeabi_dcmpgt
 
 ARM_FUNC_START unorddf2
 ARM_FUNC_ALIAS aeabi_dcmpun unorddf2
+	.cfi_startproc
 
 	mov	ip, xh, lsl #1
 	mvns	ip, ip, asr #21
@@ -1247,6 +1381,7 @@ ARM_FUNC_ALIAS aeabi_dcmpun unorddf2
 3:	mov	r0, #1			@ arguments are unordered.
 	RET
 
+	.cfi_endproc
 	FUNC_END aeabi_dcmpun
 	FUNC_END unorddf2
 
@@ -1256,6 +1391,7 @@ ARM_FUNC_ALIAS aeabi_dcmpun unorddf2
 
 ARM_FUNC_START fixdfsi
 ARM_FUNC_ALIAS aeabi_d2iz fixdfsi
+	CFI_START_FUNCTION
 
 	@ check exponent range.
 	mov	r2, xh, lsl #1
@@ -1289,6 +1425,7 @@ ARM_FUNC_ALIAS aeabi_d2iz fixdfsi
 4:	mov	r0, #0			@ How should we convert NAN?
 	RET
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_d2iz
 	FUNC_END fixdfsi
 
@@ -1298,6 +1435,7 @@ ARM_FUNC_ALIAS aeabi_d2iz fixdfsi
 
 ARM_FUNC_START fixunsdfsi
 ARM_FUNC_ALIAS aeabi_d2uiz fixunsdfsi
+	CFI_START_FUNCTION
 
 	@ check exponent range.
 	movs	r2, xh, lsl #1
@@ -1327,6 +1465,7 @@ ARM_FUNC_ALIAS aeabi_d2uiz fixunsdfsi
 4:	mov	r0, #0			@ How should we convert NAN?
 	RET
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_d2uiz
 	FUNC_END fixunsdfsi
 
@@ -1336,6 +1475,7 @@ ARM_FUNC_ALIAS aeabi_d2uiz fixunsdfsi
 
 ARM_FUNC_START truncdfsf2
 ARM_FUNC_ALIAS aeabi_d2f truncdfsf2
+	CFI_START_FUNCTION
 
 	@ check exponent range.
 	mov	r2, xh, lsl #1
@@ -1400,6 +1540,7 @@ ARM_FUNC_ALIAS aeabi_d2f truncdfsf2
 	orr	r0, r0, #0x00800000
 	RET
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_d2f
 	FUNC_END truncdfsf2
 
--- a/src/libgcc/config/arm/ieee754-sf.S
+++ b/src/libgcc/config/arm/ieee754-sf.S
@@ -31,16 +31,21 @@
  * Only the default rounding mode is intended for best performances.
  * Exceptions aren't supported yet, but that can be added quite easily
  * if necessary without impacting performances.
+ *
+ * In the CFI related comments, 'previousOffset' refers to the previous offset
+ * from sp used to compute the CFA.
  */
 
 #ifdef L_arm_negsf2
 	
 ARM_FUNC_START negsf2
 ARM_FUNC_ALIAS aeabi_fneg negsf2
+	CFI_START_FUNCTION
 
 	eor	r0, r0, #0x80000000	@ flip sign bit
 	RET
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_fneg
 	FUNC_END negsf2
 
@@ -49,6 +54,7 @@ ARM_FUNC_ALIAS aeabi_fneg negsf2
 #ifdef L_arm_addsubsf3
 
 ARM_FUNC_START aeabi_frsub
+	CFI_START_FUNCTION
 
 	eor	r0, r0, #0x80000000	@ flip sign bit of first arg
 	b	1f
@@ -284,6 +290,7 @@ LSYM(Lad_i):
 	orrne	r0, r0, #0x00400000	@ quiet NAN
 	RET
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_frsub
 	FUNC_END aeabi_fadd
 	FUNC_END addsf3
@@ -292,6 +299,7 @@ LSYM(Lad_i):
 
 ARM_FUNC_START floatunsisf
 ARM_FUNC_ALIAS aeabi_ui2f floatunsisf
+	CFI_START_FUNCTION
 		
 	mov	r3, #0
 	b	1f
@@ -316,6 +324,7 @@ ARM_FUNC_ALIAS aeabi_i2f floatsisf
 	mov	al, #0
 	b	2f
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_i2f
 	FUNC_END floatsisf
 	FUNC_END aeabi_ui2f
@@ -323,6 +332,7 @@ ARM_FUNC_ALIAS aeabi_i2f floatsisf
 
 ARM_FUNC_START floatundisf
 ARM_FUNC_ALIAS aeabi_ul2f floatundisf
+	CFI_START_FUNCTION
 
 	orrs	r2, r0, r1
 	do_it	eq
@@ -409,6 +419,7 @@ ARM_FUNC_ALIAS aeabi_l2f floatdisf
 	biceq	r0, r0, ip, lsr #31
 	RET
 
+	CFI_END_FUNCTION
 	FUNC_END floatdisf
 	FUNC_END aeabi_l2f
 	FUNC_END floatundisf
@@ -420,6 +431,7 @@ ARM_FUNC_ALIAS aeabi_l2f floatdisf
 
 ARM_FUNC_START mulsf3
 ARM_FUNC_ALIAS aeabi_fmul mulsf3
+	CFI_START_FUNCTION
 
 	@ Mask out exponents, trap any zero/denormal/INF/NAN.
 	mov	ip, #0xff
@@ -454,7 +466,13 @@ LSYM(Lml_x):
 	and	r3, ip, #0x80000000
 
 	@ Well, no way to make it shorter without the umull instruction.
-	do_push	{r3, r4, r5}
+	do_push	{r3, r4, r5}       @ sp -= 12
+	.cfi_remember_state        @ Save the current CFI state
+	.cfi_adjust_cfa_offset 12  @ CFA is now sp + previousOffset + 12
+	.cfi_rel_offset r3, 0      @ Registers are saved from sp to sp + 8
+	.cfi_rel_offset r4, 4
+	.cfi_rel_offset r5, 8
+
 	mov	r4, r0, lsr #16
 	mov	r5, r1, lsr #16
 	bic	r0, r0, r4, lsl #16
@@ -465,7 +483,8 @@ LSYM(Lml_x):
 	mla	r0, r4, r1, r0
 	adds	r3, r3, r0, lsl #16
 	adc	r1, ip, r0, lsr #16
-	do_pop	{r0, r4, r5}
+	do_pop	{r0, r4, r5}       @ sp += 12
+	.cfi_restore_state         @ Restore the previous CFI state
 
 #else
 
@@ -618,11 +637,13 @@ LSYM(Lml_n):
 	orr	r0, r0, #0x00c00000
 	RET
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_fmul
 	FUNC_END mulsf3
 
 ARM_FUNC_START divsf3
 ARM_FUNC_ALIAS aeabi_fdiv divsf3
+	CFI_START_FUNCTION
 
 	@ Mask out exponents, trap any zero/denormal/INF/NAN.
 	mov	ip, #0xff
@@ -758,6 +779,7 @@ LSYM(Ldv_s):
 	bne	LSYM(Lml_z)		@ 0 / <non_zero> -> 0
 	b	LSYM(Lml_n)		@ 0 / 0 -> NAN
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_fdiv
 	FUNC_END divsf3
 
@@ -782,6 +804,7 @@ LSYM(Ldv_s):
 
 ARM_FUNC_START gtsf2
 ARM_FUNC_ALIAS gesf2 gtsf2
+	CFI_START_FUNCTION
 	mov	ip, #-1
 	b	1f
 
@@ -796,6 +819,10 @@ ARM_FUNC_ALIAS eqsf2 cmpsf2
 	mov	ip, #1			@ how should we specify unordered here?
 
 1:	str	ip, [sp, #-4]!
+	.cfi_adjust_cfa_offset 4  @ CFA is now sp + previousOffset + 4.
+	@ We're not adding CFI for ip as it's pushed into the stack only because
+	@ it may be popped off later as a return value (i.e. we're not preserving
+	@ it anyways).
 
 	@ Trap any INF/NAN first.
 	mov	r2, r0, lsl #1
@@ -804,10 +831,18 @@ ARM_FUNC_ALIAS eqsf2 cmpsf2
 	do_it	ne
 	COND(mvn,s,ne)	ip, r3, asr #24
 	beq	3f
+	.cfi_remember_state
+	@ Save the current CFI state. This is done because the branch is conditional,
+	@ and if we don't take it we'll issue a .cfi_adjust_cfa_offset and return.
+	@ If we do take it, however, the .cfi_adjust_cfa_offset from the non-branch
+	@ code will affect the branch code as well. To avoid this we'll restore
+	@ the current state before executing the branch code.
 
 	@ Compare values.
 	@ Note that 0.0 is equal to -0.0.
 2:	add	sp, sp, #4
+	.cfi_adjust_cfa_offset -4       @ CFA is now sp + previousOffset.
+
 	orrs	ip, r2, r3, lsr #1	@ test if both are 0, clear C flag
 	do_it	ne
 	teqne	r0, r1			@ if not 0 compare sign
@@ -823,8 +858,13 @@ ARM_FUNC_ALIAS eqsf2 cmpsf2
 	orrne	r0, r0, #1
 	RET
 
-	@ Look for a NAN. 
-3:	mvns	ip, r2, asr #24
+3:	@ Look for a NAN.
+
+	@ Restore the previous CFI state (i.e. keep the CFI state as it was
+	@ before the branch).
+	.cfi_restore_state
+
+	mvns	ip, r2, asr #24
 	bne	4f
 	movs	ip, r0, lsl #9
 	bne	5f			@ r0 is NAN
@@ -832,9 +872,12 @@ ARM_FUNC_ALIAS eqsf2 cmpsf2
 	bne	2b
 	movs	ip, r1, lsl #9
 	beq	2b			@ r1 is not NAN
+
 5:	ldr	r0, [sp], #4		@ return unordered code.
+	.cfi_adjust_cfa_offset -4       @ CFA is now sp + previousOffset.
 	RET
 
+	CFI_END_FUNCTION
 	FUNC_END gesf2
 	FUNC_END gtsf2
 	FUNC_END lesf2
@@ -844,6 +887,7 @@ ARM_FUNC_ALIAS eqsf2 cmpsf2
 	FUNC_END cmpsf2
 
 ARM_FUNC_START aeabi_cfrcmple
+	CFI_START_FUNCTION
 
 	mov	ip, r0
 	mov	r0, r1
@@ -856,6 +900,13 @@ ARM_FUNC_ALIAS aeabi_cfcmple aeabi_cfcmpeq
 	@ The status-returning routines are required to preserve all
 	@ registers except ip, lr, and cpsr.
 6:	do_push	{r0, r1, r2, r3, lr}
+	.cfi_adjust_cfa_offset 20  @ CFA is at sp + previousOffset + 20
+	.cfi_rel_offset r0, 0      @ Registers are saved from sp to sp + 16
+	.cfi_rel_offset r1, 4
+	.cfi_rel_offset r2, 8
+	.cfi_rel_offset r3, 12
+	.cfi_rel_offset lr, 16
+
 	ARM_CALL cmpsf2
 	@ Set the Z flag correctly, and the C flag unconditionally.
 	cmp	r0, #0
@@ -865,57 +916,82 @@ ARM_FUNC_ALIAS aeabi_cfcmple aeabi_cfcmpeq
 	cmnmi	r0, #0
 	RETLDM	"r0, r1, r2, r3"
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_cfcmple
 	FUNC_END aeabi_cfcmpeq
 	FUNC_END aeabi_cfrcmple
 
 ARM_FUNC_START	aeabi_fcmpeq
+	CFI_START_FUNCTION
+
+	str	lr, [sp, #-8]!    @ sp -= 8
+	.cfi_adjust_cfa_offset 8  @ CFA is now sp + previousOffset + 8
+	.cfi_rel_offset lr, 0     @ lr is at sp
 
-	str	lr, [sp, #-8]!
 	ARM_CALL aeabi_cfcmple
 	do_it	eq, e
 	moveq	r0, #1	@ Equal to.
 	movne	r0, #0	@ Less than, greater than, or unordered.
 	RETLDM
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_fcmpeq
 
 ARM_FUNC_START	aeabi_fcmplt
+	CFI_START_FUNCTION
+
+	str	lr, [sp, #-8]!    @ sp -= 8
+	.cfi_adjust_cfa_offset 8  @ CFA is now sp + previousOffset + 8
+	.cfi_rel_offset lr, 0     @ lr is at sp
 
-	str	lr, [sp, #-8]!
 	ARM_CALL aeabi_cfcmple
 	do_it	cc, e
 	movcc	r0, #1	@ Less than.
 	movcs	r0, #0	@ Equal to, greater than, or unordered.
 	RETLDM
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_fcmplt
 
 ARM_FUNC_START	aeabi_fcmple
+	CFI_START_FUNCTION
+
+	str	lr, [sp, #-8]!    @ sp -= 8
+	.cfi_adjust_cfa_offset 8  @ CFA is now sp + previousOffset + 8
+	.cfi_rel_offset lr, 0     @ lr is at sp
 
-	str	lr, [sp, #-8]!
 	ARM_CALL aeabi_cfcmple
 	do_it	ls, e
 	movls	r0, #1  @ Less than or equal to.
 	movhi	r0, #0	@ Greater than or unordered.
 	RETLDM
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_fcmple
 
 ARM_FUNC_START	aeabi_fcmpge
+	CFI_START_FUNCTION
+
+	str	lr, [sp, #-8]!    @ sp -= 8
+	.cfi_adjust_cfa_offset 8  @ CFA is now sp + previousOffset + 8
+	.cfi_rel_offset lr, 0     @ lr is at sp
 
-	str	lr, [sp, #-8]!
 	ARM_CALL aeabi_cfrcmple
 	do_it	ls, e
 	movls	r0, #1	@ Operand 2 is less than or equal to operand 1.
 	movhi	r0, #0	@ Operand 2 greater than operand 1, or unordered.
 	RETLDM
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_fcmpge
 
 ARM_FUNC_START	aeabi_fcmpgt
+	CFI_START_FUNCTION
+
+	str	lr, [sp, #-8]!    @ sp -= 8
+	.cfi_adjust_cfa_offset 8  @ CFA is now sp + previousOffset + 8
+	.cfi_rel_offset lr, 0     @ lr is at sp
 
-	str	lr, [sp, #-8]!
 	ARM_CALL aeabi_cfrcmple
 	do_it	cc, e
 	movcc	r0, #1	@ Operand 2 is less than operand 1.
@@ -923,6 +999,7 @@ ARM_FUNC_START	aeabi_fcmpgt
 			@ or they are unordered.
 	RETLDM
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_fcmpgt
 
 #endif /* L_cmpsf2 */
@@ -931,6 +1008,7 @@ ARM_FUNC_START	aeabi_fcmpgt
 
 ARM_FUNC_START unordsf2
 ARM_FUNC_ALIAS aeabi_fcmpun unordsf2
+	CFI_START_FUNCTION
 
 	mov	r2, r0, lsl #1
 	mov	r3, r1, lsl #1
@@ -947,6 +1025,7 @@ ARM_FUNC_ALIAS aeabi_fcmpun unordsf2
 3:	mov	r0, #1			@ arguments are unordered.
 	RET
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_fcmpun
 	FUNC_END unordsf2
 
@@ -956,6 +1035,7 @@ ARM_FUNC_ALIAS aeabi_fcmpun unordsf2
 
 ARM_FUNC_START fixsfsi
 ARM_FUNC_ALIAS aeabi_f2iz fixsfsi
+	CFI_START_FUNCTION
 
 	@ check exponent range.
 	mov	r2, r0, lsl #1
@@ -989,6 +1069,7 @@ ARM_FUNC_ALIAS aeabi_f2iz fixsfsi
 4:	mov	r0, #0			@ What should we convert NAN to?
 	RET
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_f2iz
 	FUNC_END fixsfsi
 
@@ -998,6 +1079,7 @@ ARM_FUNC_ALIAS aeabi_f2iz fixsfsi
 
 ARM_FUNC_START fixunssfsi
 ARM_FUNC_ALIAS aeabi_f2uiz fixunssfsi
+	CFI_START_FUNCTION
 
 	@ check exponent range.
 	movs	r2, r0, lsl #1
@@ -1027,6 +1109,7 @@ ARM_FUNC_ALIAS aeabi_f2uiz fixunssfsi
 4:	mov	r0, #0			@ What should we convert NAN to?
 	RET
 
+	CFI_END_FUNCTION
 	FUNC_END aeabi_f2uiz
 	FUNC_END fixunssfsi
 
--- a/src/libgcc/config/arm/lib1funcs.S
+++ b/src/libgcc/config/arm/lib1funcs.S
@@ -1965,6 +1965,16 @@ LSYM(Lchange_\register):
 
 #endif /* Arch supports thumb.  */
 
+.macro CFI_START_FUNCTION
+	.cfi_startproc
+	.cfi_remember_state
+.endm
+
+.macro CFI_END_FUNCTION
+	.cfi_restore_state
+	.cfi_endproc
+.endm
+
 #ifndef __symbian__
 #ifndef __ARM_ARCH_6M__
 #include "ieee754-df.S"
--- a/src/libgcc/unwind-dw2-fde-dip.c
+++ b/src/libgcc/unwind-dw2-fde-dip.c
@@ -59,6 +59,12 @@
 
 #if !defined(inhibit_libc) && defined(HAVE_LD_EH_FRAME_HDR) \
     && defined(TARGET_DL_ITERATE_PHDR) \
+    && defined(__linux__)
+# define USE_PT_GNU_EH_FRAME
+#endif
+
+#if !defined(inhibit_libc) && defined(HAVE_LD_EH_FRAME_HDR) \
+    && defined(TARGET_DL_ITERATE_PHDR) \
     && (defined(__DragonFly__) || defined(__FreeBSD__))
 # define ElfW __ElfN
 # define USE_PT_GNU_EH_FRAME
--- a/src/libgfortran/acinclude.m4
+++ b/src/libgfortran/acinclude.m4
@@ -100,7 +100,7 @@ void foo (void);
 	      [Define to 1 if the target supports #pragma weak])
   fi
   case "$host" in
-    *-*-darwin* | *-*-hpux* | *-*-cygwin* | *-*-mingw* )
+    *-*-darwin* | *-*-hpux* | *-*-cygwin* | *-*-mingw* | *-*-musl* )
       AC_DEFINE(GTHREAD_USE_WEAK, 0,
 		[Define to 0 if the target shouldn't use #pragma weak])
       ;;
--- a/src/libgfortran/configure
+++ b/src/libgfortran/configure
@@ -26456,7 +26456,7 @@ $as_echo "#define SUPPORTS_WEAK 1" >>confdefs.h
 
   fi
   case "$host" in
-    *-*-darwin* | *-*-hpux* | *-*-cygwin* | *-*-mingw* )
+    *-*-darwin* | *-*-hpux* | *-*-cygwin* | *-*-mingw* | *-*-musl* )
 
 $as_echo "#define GTHREAD_USE_WEAK 0" >>confdefs.h
 
--- a/src/libitm/config/arm/hwcap.cc
+++ b/src/libitm/config/arm/hwcap.cc
@@ -40,7 +40,7 @@ int GTM_hwcap HIDDEN = 0
 
 #ifdef __linux__
 #include <unistd.h>
-#include <sys/fcntl.h>
+#include <fcntl.h>
 #include <elf.h>
 
 static void __attribute__((constructor))
--- a/src/libitm/config/linux/x86/tls.h
+++ b/src/libitm/config/linux/x86/tls.h
@@ -25,16 +25,19 @@
 #ifndef LIBITM_X86_TLS_H
 #define LIBITM_X86_TLS_H 1
 
-#if defined(__GLIBC_PREREQ) && __GLIBC_PREREQ(2, 10)
+#if defined(__GLIBC_PREREQ)
+#if __GLIBC_PREREQ(2, 10)
 /* Use slots in the TCB head rather than __thread lookups.
    GLIBC has reserved words 10 through 13 for TM.  */
 #define HAVE_ARCH_GTM_THREAD 1
 #define HAVE_ARCH_GTM_THREAD_DISP 1
 #endif
+#endif
 
 #include "config/generic/tls.h"
 
-#if defined(__GLIBC_PREREQ) && __GLIBC_PREREQ(2, 10)
+#if defined(__GLIBC_PREREQ)
+#if __GLIBC_PREREQ(2, 10)
 namespace GTM HIDDEN {
 
 #ifdef __x86_64__
@@ -101,5 +104,6 @@ static inline void set_abi_disp(struct abi_dispatch *x)
 
 } // namespace GTM
 #endif /* >= GLIBC 2.10 */
+#endif
 
 #endif // LIBITM_X86_TLS_H
--- a/src//dev/null
+++ b/src/libstdc++-v3/config/cpu/arm/cpu_defines.h
@@ -0,0 +1,40 @@
+// Specific definitions for generic platforms  -*- C++ -*-
+
+// Copyright (C) 2015 Free Software Foundation, Inc.
+//
+// This file is part of the GNU ISO C++ Library.  This library is free
+// software; you can redistribute it and/or modify it under the
+// terms of the GNU General Public License as published by the
+// Free Software Foundation; either version 3, or (at your option)
+// any later version.
+
+// This library is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+// GNU General Public License for more details.
+
+// Under Section 7 of GPL version 3, you are granted additional
+// permissions described in the GCC Runtime Library Exception, version
+// 3.1, as published by the Free Software Foundation.
+
+// You should have received a copy of the GNU General Public License and
+// a copy of the GCC Runtime Library Exception along with this program;
+// see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
+// <http://www.gnu.org/licenses/>.
+
+/** @file bits/cpu_defines.h
+ *  This is an internal header file, included by other library headers.
+ *  Do not attempt to use it directly. @headername{iosfwd}
+ */
+
+#ifndef _GLIBCXX_CPU_DEFINES
+#define _GLIBCXX_CPU_DEFINES 1
+
+// Integer divide instructions don't trap on ARM.
+#ifdef __ARM_ARCH_EXT_IDIV__
+#define __glibcxx_integral_traps false
+#else
+#define __glibcxx_integral_traps true
+#endif
+
+#endif
--- a/src/libstdc++-v3/config/os/generic/os_defines.h
+++ b/src/libstdc++-v3/config/os/generic/os_defines.h
@@ -33,4 +33,9 @@
 // System-specific #define, typedefs, corrections, etc, go here.  This
 // file will come before all others.
 
+// Disable the weak reference logic in gthr.h for os/generic because it
+// is broken on every platform unless there is implementation specific
+// workaround in gthr-posix.h and at link-time for static linking.
+#define _GLIBCXX_GTHREAD_USE_WEAK 0
+
 #endif
--- a/src/libstdc++-v3/configure.host
+++ b/src/libstdc++-v3/configure.host
@@ -143,6 +143,9 @@ cpu_include_dir=cpu/${try_cpu}
 # Set specific CPU overrides for cpu_defines_dir. Most can just use generic.
 # THIS TABLE IS SORTED.  KEEP IT THAT WAY.
 case "${host_cpu}" in
+  arm*)
+    cpu_defines_dir=cpu/arm
+    ;;
   powerpc* | rs6000)
     cpu_defines_dir=cpu/powerpc
     ;;
@@ -273,6 +276,9 @@ case "${host_os}" in
   freebsd*)
     os_include_dir="os/bsd/freebsd"
     ;;
+  linux-musl*)
+    os_include_dir="os/generic"
+    ;;
   gnu* | linux* | kfreebsd*-gnu | knetbsd*-gnu)
     if [ "$uclibc" = "yes" ]; then
       os_include_dir="os/uclibc"
